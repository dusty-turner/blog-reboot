[["index.html", "Modeling in R and Python 1 Class Introduction 1.1 Topics &amp; Class Structure 1.2 Software Prerequisites 1.3 Human Prerequisites 1.4 Tutorial Challenges 1.5 End State 1.6 Instructors Introduction 1.7 Lets Get Started", " Modeling in R and Python MAJ Dusty Turner and Mr.Â Robert Ward 8 DEC 2021 1 Class Introduction Disclaimer: The appearance of U.S. Department of Defense (DoD) visual information does not imply or constitute DoD endorsement. The views expressed in this presentation are those only of the author and do not represent the official position of the U.S. Army, DoD, or the federal government. 1.1 Topics &amp; Class Structure Overview of modeling Tidymodels (R) scikit-learn (Python) 1.2 Software Prerequisites R 3.6.x or newer RStudio 1.2.x or newer Python 3.6 or newer scikit-learn 1.0.0 or newer 1.3 Human Prerequisites We assume you have: A working knowledge of R and RStudio and/or Python; Some experience with contemporary tidy coding concepts; An understanding of modeling principals. Lets take an informal poll to see everyones experience / comfort level with these topics. Do your best to follow along. We are happy to answer questions. This presentation is available at https://rwward.github.io/etf2021-r-py-modeling/. 1.4 Tutorial Challenges We recognize everyone has different statistical and coding backgrounds. Dont be afraid to ask questions. If you miss something we said, it is likely others have too - youll be helping them by speaking up. Its difficult to know how we should pace the class, so please communicate! 1.5 End State Students generally understand the modeling process in R and Python; Students have access to resources to learn more. 1.6 Instructors Introduction 1.6.1 MAJ Dusty Turner Army Combat Engineer Platoon Leader / Executive Officer / Company Commander Geospatial / Sapper / Route Clearance Hawaii / White Sands Missile Range / Iraq / Afghanistan Education West Point 07 Operations Research, BS Missouri University of Science and Technology 12 Engineering Management, MS THE Ohio State 16 Integrated Systems Engineering, MS Applied Statistics, Graduate Minor Data Science R User Since 14 Catch me on Twitter @dtdusty http://dustysturner.com/ 1.6.2 Robert Ward Education University of Chicago, 13 Political Science &amp; English, BA Columbia University School of International and Public Affairs, 18 Master of International Affairs, Specialization in Advanced Policy and Economic Analysis Data Science R user since 2011; also know some python and forgot some Stata Worked for Government Accountability Office Applied Research &amp; Methods Operations Research Systems Analyst at the Center for Army Analysis (CAA) and Army Leader Dashboard/Vantage PM team 1.7 Lets Get Started 1.7.1 Prerequisite Packages install.packages(c(&quot;tidyverse&quot;, &quot;tidymodels&quot;), dependencies = TRUE) pip install scikit-learn pandas matplotlib 1.7.2 Follow Along! Book: https://rwward.github.io/etf2021-r-py-modeling/ GitHub repo for data and code: https://github.com/rwward/etf2021-r-py-modeling "],["2-modeling-in-r-with-tidymodels.html", "2 Modeling in R with Tidymodels 2.1 Tidymodels Packages 2.2 Explore Data 2.3 Split Data 2.4 Prepare Data 2.5 Specify Model 2.6 Create Workflow 2.7 Specify Grid of Training Parameters 2.8 Train Model 2.9 Build Model on all Training Data, Test on Validation Set 2.10 Change Model to Random Forest 2.11 Build Model on all Training and Validation Data Using the Best Model 2.12 Make Predictions with New Data", " 2 Modeling in R with Tidymodels The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org/ Many modeling techniques in R require different syntaxes and different data structures. Tidymodels provides a modeling workflow that standardizes syntaxes and data structures regardless of the model type. lm() glm() glmnet() randomForest() xgboost() c(&quot;linear_reg&quot;, &quot;logistic_reg&quot;, &quot;surv_reg&quot;, &quot;multinom_reg&quot;, &quot;rand_forest&quot;, &quot;boost_tree&quot;, &quot;svm_poly&quot;, &quot;decision_tree&quot;) %&gt;% map_dfr(.f = ~show_engines(x = .x) %&gt;% mutate(type = .x)) %&gt;% DT::datatable() 2.1 Tidymodels Packages Like the tidyverse, tidymodels is a meta package consisting of the following packages: {rsample}: Creates different types of resamples and corresponding classes for analysis {recipes}: Uses dplyr-like pipeable sequences of feature engineering steps to get data ready for modeling {workflows}: Creates an object that can bundle together your pre-processing, modeling, and post-processing steps {parsnip}: Provides a tidy, unified interface to models than can by used to try a range of models without getting bogged down in the syntactical minutiae of the underlying packages {tune}: Facilitates hyperparameter tuning for the tidymodels packages {yardstick}: Estimates how well models are working using tidy data principles {infer}: Performs statistical inference using an expressive statistical grammar that coheres with the tidyverse design framework 2.1.1 Tidymodels Road Map What we plan to do: Explore data {dplyr} Manipulate data {ggplot2} Visualize data Create model {rsample} Split data into test/train {recipes} Preprocess data {parsnip} Specify model {workflows} Create workflow {tune} / {dials} Train and tune parameters {parsnip} Finalize model {yardstick} Validate model Predict on new data 2.1.2 Modeling Goal We would like to create a model to predict which future Major League Baseball players will make the Hall of Fame. We will use historical data to build a model and then use that model to predict who may make the Hall of Fame from the players in the eligible data. 2.2 Explore Data library(tidyverse) historical &lt;- read_csv(&quot;01_data/historical_baseball.csv&quot;) %&gt;% mutate(inducted = fct_rev(as.factor(inducted))) %&gt;% filter(ab &gt; 250) eligible &lt;- read_csv(&quot;01_data/eligible_baseball.csv&quot;) historical ## # A tibble: 2,664 x 15 ## player_id inducted g ab r h x2b x3b hr rbi sb cs bb so last_year ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 1 3298 12364 2174 3771 624 98 755 2297 240 73 1402 1383 1976 ## 2 aaronto01 0 437 944 102 216 42 6 13 94 9 8 86 145 1971 ## 3 abbated01 0 855 3044 355 772 99 43 11 324 142 0 289 16 1910 ## 4 adairje01 0 1165 4019 378 1022 163 19 57 366 29 29 208 499 1970 ## 5 adamsba01 0 482 1019 79 216 31 15 3 75 1 1 53 177 1926 ## 6 adamsbe01 0 267 678 37 137 17 4 2 45 9 0 23 79 1919 ## 7 adamsbo03 0 1281 4019 591 1082 188 49 37 303 67 30 414 447 1959 ## 8 adamssp01 0 1424 5557 844 1588 249 48 9 394 154 50 453 223 1934 ## 9 adcocjo01 0 1959 6606 823 1832 295 35 336 1122 20 25 594 1059 1966 ## 10 ageeto01 0 1129 3912 558 999 170 27 130 433 167 81 342 918 1973 ## # ... with 2,654 more rows The historical data contains career statistics for every baseball batter from 1880-2011 who no longer meets Hall of Fame eligibility requirements or has already made the Hall of Fame. Hall of Fame Qualifications: Played at least 10 years Retired for at least 5 years Players have only 10 years of eligibility The eligible data contains everyone who is still eligible for the Hall of Fame. You can see from the data below, the players who make the Hall of Fame tend to perform better in a few standard baseball statistics. historical %&gt;% select(-last_year) %&gt;% group_by(inducted) %&gt;% summarise(across(.cols = where(is.numeric), .fns = ~round(mean(.),0))) %&gt;% gt::gt() ## renders the table html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #zbmjqdayig .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #zbmjqdayig .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #zbmjqdayig .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #zbmjqdayig .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #zbmjqdayig .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #zbmjqdayig .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #zbmjqdayig .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #zbmjqdayig .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #zbmjqdayig .gt_column_spanner_outer:first-child { padding-left: 0; } #zbmjqdayig .gt_column_spanner_outer:last-child { padding-right: 0; } #zbmjqdayig .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #zbmjqdayig .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #zbmjqdayig .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #zbmjqdayig .gt_from_md > :first-child { margin-top: 0; } #zbmjqdayig .gt_from_md > :last-child { margin-bottom: 0; } #zbmjqdayig .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #zbmjqdayig .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #zbmjqdayig .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #zbmjqdayig .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #zbmjqdayig .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #zbmjqdayig .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #zbmjqdayig .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #zbmjqdayig .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #zbmjqdayig .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #zbmjqdayig .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #zbmjqdayig .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #zbmjqdayig .gt_sourcenote { font-size: 90%; padding: 4px; } #zbmjqdayig .gt_left { text-align: left; } #zbmjqdayig .gt_center { text-align: center; } #zbmjqdayig .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #zbmjqdayig .gt_font_normal { font-weight: normal; } #zbmjqdayig .gt_font_bold { font-weight: bold; } #zbmjqdayig .gt_font_italic { font-style: italic; } #zbmjqdayig .gt_super { font-size: 65%; } #zbmjqdayig .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } inducted g ab r h x2b x3b hr rbi sb cs bb so 1 1675 5941 958 1747 295 77 149 874 165 37 643 564 0 907 2849 374 751 122 28 50 336 60 20 264 325 The plot of the data supports this as well. historical %&gt;% pivot_longer(g:so) %&gt;% ggplot(aes(x = inducted, y = value)) + geom_boxplot() + facet_wrap(~name, scales = &quot;free&quot;) + labs(y = &quot;&quot;,x = &quot;Hall of Fame Indicator&quot;) Of note, we are dealing with imbalance classes, which will take unique considerations. To have a quality model, we hope to achieve greater than ~93% accuracy since this is what we could do by simply saying that no one should be in the Hall of Fame. historical %&gt;% count(inducted) %&gt;% mutate(Percent = str_c(round(n / sum(n),4)*100,&quot;%&quot;)) %&gt;% gt::gt() ## renders the table html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #xnnwubadwm .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #xnnwubadwm .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xnnwubadwm .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #xnnwubadwm .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #xnnwubadwm .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xnnwubadwm .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xnnwubadwm .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #xnnwubadwm .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #xnnwubadwm .gt_column_spanner_outer:first-child { padding-left: 0; } #xnnwubadwm .gt_column_spanner_outer:last-child { padding-right: 0; } #xnnwubadwm .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #xnnwubadwm .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #xnnwubadwm .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #xnnwubadwm .gt_from_md > :first-child { margin-top: 0; } #xnnwubadwm .gt_from_md > :last-child { margin-bottom: 0; } #xnnwubadwm .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #xnnwubadwm .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #xnnwubadwm .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xnnwubadwm .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #xnnwubadwm .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xnnwubadwm .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #xnnwubadwm .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #xnnwubadwm .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xnnwubadwm .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xnnwubadwm .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #xnnwubadwm .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xnnwubadwm .gt_sourcenote { font-size: 90%; padding: 4px; } #xnnwubadwm .gt_left { text-align: left; } #xnnwubadwm .gt_center { text-align: center; } #xnnwubadwm .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #xnnwubadwm .gt_font_normal { font-weight: normal; } #xnnwubadwm .gt_font_bold { font-weight: bold; } #xnnwubadwm .gt_font_italic { font-style: italic; } #xnnwubadwm .gt_super { font-size: 65%; } #xnnwubadwm .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } inducted n Percent 1 232 8.71% 0 2432 91.29% 2.3 Split Data To begin the analysis, we will load the {tidymodels} library. library(tidymodels) We will split the data into a training (two-thirds of the data) and testing set (one-third) of the data. We set the seed so the analysis is reproducible. The output of this function is an rsplit object. An rsplit object is one that can be used with the training and testing functions to extract the data in each split. set.seed(42) data_split &lt;- initial_split(historical, prop = 2/3, strata = inducted) data_split ## &lt;Analysis/Assess/Total&gt; ## &lt;1776/888/2664&gt; We can extract the data from the rsplit object. train_data &lt;- training(data_split) test_data &lt;- testing(data_split) train_data ## # A tibble: 1,776 x 15 ## player_id inducted g ab r h x2b x3b hr rbi sb cs bb so last_year ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 1 3298 12364 2174 3771 624 98 755 2297 240 73 1402 1383 1976 ## 2 aaronto01 0 437 944 102 216 42 6 13 94 9 8 86 145 1971 ## 3 abbated01 0 855 3044 355 772 99 43 11 324 142 0 289 16 1910 ## 4 adairje01 0 1165 4019 378 1022 163 19 57 366 29 29 208 499 1970 ## 5 adamsbe01 0 267 678 37 137 17 4 2 45 9 0 23 79 1919 ## 6 adamssp01 0 1424 5557 844 1588 249 48 9 394 154 50 453 223 1934 ## 7 ageeto01 0 1129 3912 558 999 170 27 130 433 167 81 342 918 1973 ## 8 aguaylu01 0 568 1104 142 260 43 10 37 109 7 5 94 220 1989 ## 9 aguirha01 0 447 388 14 33 7 1 0 21 1 0 14 236 1970 ## 10 ainsmed01 0 1078 3048 299 707 108 54 22 317 86 16 263 315 1924 ## # ... with 1,766 more rows From the training data, we further split the data into a training set (two-thirds of the training data) and a validation set (one-third of the training data) for parameter tuning and model assessment. set.seed(42) validation_set &lt;- validation_split(data = train_data, prop = 2/3, strata = inducted) validation_set ## # Validation Set Split (0.67/0.33) using stratification ## # A tibble: 1 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [1184/592]&gt; validation 2.4 Prepare Data What preprocessing steps do you want to do to your data every time you model? We need to specify the following things: Specify the modeling formula Specify the roles of each of the factors Do all preprocessing steps In the {tidymodels} construct, we do this by creating a recipe. baseball_recipe &lt;- recipe(inducted ~ ., data = train_data) %&gt;% update_role(player_id, new_role = &quot;ID&quot;) %&gt;% step_center(all_numeric()) %&gt;% step_scale(all_numeric()) %&gt;% step_nzv(all_numeric()) %&gt;% step_rm(&quot;last_year&quot;) baseball_recipe ## Recipe ## ## Inputs: ## ## role #variables ## ID 1 ## outcome 1 ## predictor 13 ## ## Operations: ## ## Centering for all_numeric() ## Scaling for all_numeric() ## Sparse, unbalanced variable filter on all_numeric() ## Delete terms &quot;last_year&quot; 2.5 Specify Model Now that weve prepared our data, we need to specify the model we wish to execute. Here we identify the model type, specify parameters that need tuning, and then set our desired engine (essentially, the modeling algorithm). lr_mod &lt;- logistic_reg(mode = &quot;classification&quot;, penalty = tune(), mixture = 1) %&gt;% set_engine(engine = &quot;glmnet&quot;) lr_mod ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet 2.6 Create Workflow Now that weve prepared the data and specified the model, we put it all together in a workflow. In a workflow, we add the specified model and the preprocessing recipe. baseball_workflow &lt;- workflow() %&gt;% add_model(lr_mod) %&gt;% add_recipe(baseball_recipe) baseball_workflow ## == Workflow ========================================================================================================================================= ## Preprocessor: Recipe ## Model: logistic_reg() ## ## -- Preprocessor ------------------------------------------------------------------------------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_center() ## * step_scale() ## * step_nzv() ## * step_rm() ## ## -- Model -------------------------------------------------------------------------------------------------------------------------------------------- ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet 2.7 Specify Grid of Training Parameters This step not only executes the model building procedure, but also tunes the penalty hyperparameter by running the model with every penalty option in a specified search grid. First, we specify the parameters and search grids that well use for tuning. lr_reg_grid &lt;- tibble(penalty = 10^seq(-4, -1, length.out = 30)) lr_reg_grid ## # A tibble: 30 x 1 ## penalty ## &lt;dbl&gt; ## 1 0.0001 ## 2 0.000127 ## 3 0.000161 ## 4 0.000204 ## 5 0.000259 ## 6 0.000329 ## 7 0.000418 ## 8 0.000530 ## 9 0.000672 ## 10 0.000853 ## # ... with 20 more rows 2.8 Train Model Next, we use tune_grid() to execute the model one time for each parameter set. In this instance, this is 30 times. This function has several arguments: grid: The tibble we created that contains the parameters we have specified. control: Controls various aspects of the grid search process. metrics: Specifies the model quality metrics we wish to save for each model in cross validation. We also specify that we wish to save the performance metrics for each of the 30 iterations. set.seed(42) lr_validation &lt;- baseball_workflow %&gt;% tune_grid(validation_set, grid = lr_reg_grid, control = control_grid(save_pred = TRUE, verbose = TRUE, allow_par = FALSE), metrics = metric_set(roc_auc, accuracy)) lr_validation ## # Tuning results ## # Validation Set Split (0.67/0.33) using stratification ## # A tibble: 1 x 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [1184/592]&gt; validation &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [17,760 x 7]&gt; Here, we extract out the best 25 models based on accuracy and plot them versus the penalty from the tuning parameter grid. lr_validation %&gt;% show_best(&quot;accuracy&quot;, n = 25) %&gt;% arrange(penalty) %&gt;% as.data.frame() %&gt;% ggplot(aes(x = penalty, y = mean)) + geom_point() + geom_line() + scale_x_log10() We select the smallest penalty that results in the highest accuracy. lr_best &lt;- lr_validation %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% filter(mean == max(mean)) %&gt;% slice(1) We show the Receiver Operator Characteristic (ROC) curve for the selected model. lr_validation %&gt;% collect_predictions(parameters = lr_best) %&gt;% roc_curve(inducted, .pred_1) %&gt;% autoplot() 2.9 Build Model on all Training Data, Test on Validation Set Now that weve found the best parameter set, we need to apply this model to the entire training set. Well make one tweak to our previous model specification: we specify our chosen penalty from the tuning process, instead of allowing the penalty to be tuned automatically. last_lr_mod &lt;- logistic_reg(mode = &quot;classification&quot;, penalty = lr_best$penalty, mixture = 1) %&gt;% set_engine(engine = &quot;glmnet&quot;) last_lr_mod ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = lr_best$penalty ## mixture = 1 ## ## Computational engine: glmnet We update our workflow to have the best parameter set with the function finalize_workflow(). last_lr_workflow &lt;- baseball_workflow %&gt;% finalize_workflow(lr_best) last_lr_workflow ## == Workflow ========================================================================================================================================= ## Preprocessor: Recipe ## Model: logistic_reg() ## ## -- Preprocessor ------------------------------------------------------------------------------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_center() ## * step_scale() ## * step_nzv() ## * step_rm() ## ## -- Model -------------------------------------------------------------------------------------------------------------------------------------------- ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 1e-04 ## mixture = 1 ## ## Computational engine: glmnet We fit the model on the entire training set. last_lr_fit &lt;- last_lr_workflow %&gt;% last_fit(data_split) We can see the performance of the model below in the next two outputs. last_lr_fit %&gt;% collect_metrics() ## # A tibble: 2 x 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.923 Preprocessor1_Model1 ## 2 roc_auc binary 0.767 Preprocessor1_Model1 last_lr_workflow %&gt;% fit(data = historical) %&gt;% predict(historical, type = &quot;prob&quot;) %&gt;% bind_cols(historical) %&gt;% mutate(pred_class = fct_rev(as.factor(round(.pred_1)))) %&gt;% conf_mat(inducted, pred_class) ## Truth ## Prediction 1 0 ## 1 76 10 ## 0 156 2422 And we can take a view look at the ROC curve of our final model. last_lr_fit %&gt;% collect_predictions() %&gt;% roc_curve(inducted, .pred_1) %&gt;% autoplot() 2.10 Change Model to Random Forest 2.10.1 Update Model Type rf_mod &lt;- rand_forest(mode = &quot;classification&quot;, mtry = tune(), min_n = tune(), trees = tune()) %&gt;% set_engine(engine = &quot;randomForest&quot;) rf_mod ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = tune() ## min_n = tune() ## ## Computational engine: randomForest 2.10.2 Update Workflow baseball_workflow_rf &lt;- workflow() %&gt;% add_model(rf_mod) %&gt;% add_recipe(baseball_recipe) baseball_workflow_rf ## == Workflow ========================================================================================================================================= ## Preprocessor: Recipe ## Model: rand_forest() ## ## -- Preprocessor ------------------------------------------------------------------------------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_center() ## * step_scale() ## * step_nzv() ## * step_rm() ## ## -- Model -------------------------------------------------------------------------------------------------------------------------------------------- ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = tune() ## min_n = tune() ## ## Computational engine: randomForest 2.10.3 Update Tuning Parameters rf_reg_grid &lt;- dials::grid_latin_hypercube(mtry(c(1,10)), min_n(), trees(), size = 30) rf_reg_grid ## # A tibble: 30 x 3 ## mtry min_n trees ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 7 36 1267 ## 2 7 4 111 ## 3 9 5 1244 ## 4 2 27 795 ## 5 8 13 1934 ## 6 4 24 832 ## 7 2 22 1795 ## 8 10 16 199 ## 9 6 10 537 ## 10 6 7 40 ## # ... with 20 more rows 2.10.4 Re Execute Cross Validation set.seed(42) rf_validation &lt;- baseball_workflow_rf %&gt;% tune_grid(validation_set, grid = rf_reg_grid, control = control_grid(save_pred = TRUE, verbose = TRUE, allow_par = FALSE), metrics = metric_set(roc_auc, accuracy)) rf_validation ## # Tuning results ## # Validation Set Split (0.67/0.33) using stratification ## # A tibble: 1 x 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [1184/592]&gt; validation &lt;tibble [60 x 7]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [17,760 x 9]&gt; 2.10.5 Explore Tuning Parameters rf_validation %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% pivot_longer(cols = mtry:min_n) %&gt;% mutate(best_mod = mean == max(mean)) %&gt;% ggplot(aes(x = value, y = mean)) + # geom_line(alpha = 0.5, size = 1.5) + geom_point(aes(color = best_mod)) + facet_wrap(~name, scales = &quot;free_x&quot;) + scale_x_continuous(breaks = scales::pretty_breaks()) + labs(y = &quot;Accuracy&quot;, x = &quot;&quot;, color = &quot;Best Model&quot;, title = &quot;Random Forest Cross Validation Tuning Parameters&quot;) 2.10.6 Select Best Tuning Parameters for Random Forest rf_best &lt;- rf_validation %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% arrange(desc(mean)) %&gt;% slice(1) 2.10.7 Show ROC Curve for Best Random Forest Model rf_validation %&gt;% collect_predictions(parameters = rf_best) %&gt;% roc_curve(inducted, .pred_1) %&gt;% autoplot() last_rf_mod &lt;- rand_forest(mode = &quot;classification&quot;, mtry = rf_best$mtry, trees = rf_best$trees, min_n = rf_best$min_n) %&gt;% set_engine(engine = &quot;randomForest&quot;) last_rf_mod ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = rf_best$mtry ## trees = rf_best$trees ## min_n = rf_best$min_n ## ## Computational engine: randomForest We update our workflow to have the best parameter set with the function finalize_workflow(). last_rf_workflow &lt;- baseball_workflow_rf %&gt;% finalize_workflow(rf_best) last_rf_workflow ## == Workflow ========================================================================================================================================= ## Preprocessor: Recipe ## Model: rand_forest() ## ## -- Preprocessor ------------------------------------------------------------------------------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_center() ## * step_scale() ## * step_nzv() ## * step_rm() ## ## -- Model -------------------------------------------------------------------------------------------------------------------------------------------- ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = 4 ## trees = 1034 ## min_n = 6 ## ## Computational engine: randomForest We fit the model on the entire training set. last_rf_fit &lt;- last_rf_workflow %&gt;% last_fit(data_split) We can see the performance of the model below in the next two outputs. last_rf_fit %&gt;% collect_metrics() ## # A tibble: 2 x 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.934 Preprocessor1_Model1 ## 2 roc_auc binary 0.909 Preprocessor1_Model1 last_rf_workflow %&gt;% fit(data = historical) %&gt;% predict(historical, type = &quot;prob&quot;) %&gt;% bind_cols(historical) %&gt;% mutate(pred_class = fct_rev(as.factor(round(.pred_1)))) %&gt;% conf_mat(inducted, pred_class) ## Truth ## Prediction 1 0 ## 1 189 2 ## 0 43 2430 And we can take a view look at the ROC curve of our final model. last_lr_fit %&gt;% collect_predictions() %&gt;% roc_curve(inducted, .pred_1) %&gt;% autoplot() 2.11 Build Model on all Training and Validation Data Using the Best Model Now, we can use the fit() function to build the model on the entire historical data. last_rf_workflow %&gt;% fit(data = historical) %&gt;% extract_fit_parsnip() ## parsnip model object ## ## Fit time: 2.4s ## ## Call: ## randomForest(x = maybe_data_frame(x), y = y, ntree = ~1034L, mtry = min_cols(~4L, x), nodesize = min_rows(~6L, x)) ## Type of random forest: classification ## Number of trees: 1034 ## No. of variables tried at each split: 4 ## ## OOB estimate of error rate: 5.78% ## Confusion matrix: ## 1 0 class.error ## 1 103 129 0.55603448 ## 0 25 2407 0.01027961 2.12 Make Predictions with New Data Now that we have the model, we can make predictions on the eligible data. How did we do? last_rf_workflow %&gt;% fit(data = historical) %&gt;% predict(eligible, type = &quot;prob&quot;) %&gt;% bind_cols(eligible) %&gt;% arrange(-.pred_1) %&gt;% filter(.pred_1 &gt;.4) %&gt;% mutate(across(contains(&quot;pred&quot;), ~round(.,3))) %&gt;% # print(n = Inf) %&gt;% DT::datatable() "],["3-modeling-in-python-with-scikit-learn.html", "3 Modeling in Python with scikit-learn 3.1 scikit-learn Overview 3.2 Explore Data 3.3 Split Data 3.4 Define a Pipeline 3.5 Fit the Model (Using the Pipeline) 3.6 Score and Evaluate the Model 3.7 Try Another Method: Random Forest 3.8 Predict on Eligible Data 3.9 Conclusion", " 3 Modeling in Python with scikit-learn 3.1 scikit-learn Overview Like {tidymodels}, scikit-learn offers a suite of tools for predictive modeling and machine learning: it will help you split data, preprocess model inputs, fit models, and compare and assess them. Unlike {tidymodels}, scikit-learn is a single monolithic package with functions for the entire modeling pipeline. Users will likely still want to use the pandas library to ingest and prepare data, and may want to use other libraries to supplement scikit-learns data visualization capabilities, but scikit-learn will do most of the work by itself - and often with less and simpler code than tidymodels, at least for standard machine learning workflows. 3.1.1 scikit-learn Road Map What we plan to do: Read in and explore data (pandas and R) Create model (scikit-learn) split data define pipeline with preprocessors and model with cross-validation for parameter tuning fit model Predict on new data and assess model (scikit-learn) 3.1.2 Modeling Goal We plan to create a model using the historical data and use that model to predict who is most likely to make it into the Hall of Fame in the eligible data. 3.2 Explore Data Well load the pandas library to import and set up the data. import pandas as pd Here, we use pandas read_csv() to import the data, and then we print the first few rows of the historical dataframe to the console. historical = pd.read_csv(&#39;01_data/historical_baseball.csv&#39;).query(&quot;ab &gt; 250&quot;) eligible = pd.read_csv(&#39;01_data/eligible_baseball.csv&#39;).query(&quot;ab &gt; 250&quot;) historical ## player_id inducted g ab r ... sb cs bb so last_year ## 0 aaronha01 1 3298 12364 2174 ... 240 73 1402 1383 1976 ## 1 aaronto01 0 437 944 102 ... 9 8 86 145 1971 ## 3 abbated01 0 855 3044 355 ... 142 0 289 16 1910 ## 8 adairje01 0 1165 4019 378 ... 29 29 208 499 1970 ## 9 adamsba01 0 482 1019 79 ... 1 1 53 177 1926 ## ... ... ... ... ... ... ... ... .. ... ... ... ## 3227 zimmedo01 0 1095 3283 353 ... 45 25 246 678 1965 ## 3228 zimmehe01 0 1456 5304 695 ... 175 33 242 404 1919 ## 3230 ziskri01 0 1453 5144 681 ... 8 15 533 910 1983 ## 3231 zitzmbi01 0 406 1004 197 ... 42 11 83 85 1929 ## 3234 zuvelpa01 0 209 491 41 ... 2 0 34 50 1991 ## ## [2664 rows x 15 columns] As a reminder, the historical data contains career statistics for every baseball batter from 1880-2011 who no longer meets Hall of Fame eligibility requirements or has already made the Hall of Fame, while the eligible data contains all players who are currently eligible for Hall of Fame induction. You can see from the data below, the players who make the Hall of Fame tend to perform better in a few standard baseball statistics. This pandas code does the same thing as the R code in Chapter 2 - it groups the historical data by whether or not the player was inducted into the Hall of Fame, and then takes the mean of each column in each group. hist_means_inducted_groups = historical.drop(&#39;last_year&#39;, axis = 1).groupby(&#39;inducted&#39;).mean().round() We can bring the data back into R, using RStudios very simple Python-R interface, and use the same R code as in Chapter 1 to print a table and produce boxplots of the means. We dont need to do anything special to convert a pandas DataFrame to an R data frame: {reticulate} handles it for us when we call py$hist_means-inducted_groups. py$hist_means_inducted_groups %&gt;% rownames_to_column(var = &quot;inducted&quot;) %&gt;% gt::gt() ## renders the table html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #sivccuwaox .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #sivccuwaox .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #sivccuwaox .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #sivccuwaox .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #sivccuwaox .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #sivccuwaox .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #sivccuwaox .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #sivccuwaox .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #sivccuwaox .gt_column_spanner_outer:first-child { padding-left: 0; } #sivccuwaox .gt_column_spanner_outer:last-child { padding-right: 0; } #sivccuwaox .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #sivccuwaox .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #sivccuwaox .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #sivccuwaox .gt_from_md > :first-child { margin-top: 0; } #sivccuwaox .gt_from_md > :last-child { margin-bottom: 0; } #sivccuwaox .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #sivccuwaox .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #sivccuwaox .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #sivccuwaox .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #sivccuwaox .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #sivccuwaox .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #sivccuwaox .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #sivccuwaox .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #sivccuwaox .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #sivccuwaox .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #sivccuwaox .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #sivccuwaox .gt_sourcenote { font-size: 90%; padding: 4px; } #sivccuwaox .gt_left { text-align: left; } #sivccuwaox .gt_center { text-align: center; } #sivccuwaox .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #sivccuwaox .gt_font_normal { font-weight: normal; } #sivccuwaox .gt_font_bold { font-weight: bold; } #sivccuwaox .gt_font_italic { font-style: italic; } #sivccuwaox .gt_super { font-size: 65%; } #sivccuwaox .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } inducted g ab r h x2b x3b hr rbi sb cs bb so 0 907 2849 374 751 122 28 50 336 60 20 264 325 1 1675 5941 958 1747 295 77 149 874 165 37 643 564 py$historical %&gt;% mutate(inducted = as_factor(inducted)) %&gt;% # did this on data read in R but not python, so we do it here instead pivot_longer(g:so) %&gt;% ggplot(aes(x = inducted, y = value)) + geom_boxplot() + facet_wrap(~name, scales = &quot;free&quot;) + labs(y = &quot;&quot;,x = &quot;Hall of Fame Indicator&quot;) 3.3 Split Data As we did in R, we will split the data into a training set (two-thirds of the data) and testing set (one-third) of the data. We set the seed so the analysis is reproducible - here, we do this using the random_state parameter in train_test_split(). Instead of an rsplit object that contains resampling metadata, train_test_split() returns four objects: x (predictor) pandas DataFrame objects for the training and test sets, and y (target) pandas Series objects for the training and test sets. Note that before splitting the data, we set the index of the dataframe to be player_id. This carries through to the outputs of train_test_split(), which all have player_id as a common index (and not as a predictor or target variable.) In a way, this serves a similar purpose to the update_role(player_id, new_role = \"ID\") line that we added to the recipe in R. from sklearn.model_selection import train_test_split historical_pidindex = historical.set_index(&#39;player_id&#39;) X = historical_pidindex.drop([&#39;inducted&#39;, &#39;last_year&#39;], axis = 1) y = historical_pidindex.inducted X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size = 1/3) 3.4 Define a Pipeline Scikit-learns pipelines serve the combined purpose of workflows and recipes in {tidymodels}. They allow you to define a set of preprocessing and modeling steps that you can then apply to any dataset. They are defined by the function make_pipeline(), with the steps, in order, as arguments. The first two steps in our pipeline will take care of preprocessing. In Chapter 2, we centered and scaled our data; here, well use StandardScaler(), which accomplishes both of those steps. Well also apply VarianceThreshold(); in its default form, this only removes zero-variance predictors, but the user can set a custom variance threshold. None of our predictors have low variance, so this feature selection mechanism does nothing to our data anyway. The third step in our pipeline is our model. Here, weve chosen LogisticRegressionCV(). The first three parameters should produce a model very similar to the one in Chapter 2: Cs = 10: the modeling function will automatically select a grid of 10 C values (inverse penalties) to search over. This is the default value. The user can also specify a specific list of C values to search over. penalty = \"elasticnet\" lets us use a hybrid L1 and L2 penalty, or a mix between Lasso and Ridge regression, much like engine = glmnet in R; solver = \"saga\" chooses a solver that is compatible with our other options; l1_ratios = [1.0] is the equivalent of mixture = 1 in R - it gives us a pure Lasso regression; max_iter = 2000 allows the solver to attempt up to 2,000 iterations as it searches for a solution, because the default of 100 was insufficient for this model specification; We also have two parameters related to the cross-validation (CV) part of the model specification: cv = 10. This means that the data will be split into 10 folds, and the model will be fit 10 times for each set of hyperparameters in an automatically generated search grid, with one fold being held out as a validation set for computing accuracy in each run. This process will allow the model to tune the size of penalty, which we have not specified explicitly. refit = True: the function will find the best C (inverse penalty) value by averaging the cross-validation scores of each one, and then refit the model using the best C value on all of the data. Finally, we set n_jobs = 4 to allow for multithreading. In my own highly unscientific testing, moving from one to four threads reduces model fit time from 15 seconds to 6 seconds. from sklearn import preprocessing from sklearn.linear_model import LogisticRegression from sklearn.linear_model import LogisticRegressionCV from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.feature_selection import VarianceThreshold pipe_scale_lr_lasso = make_pipeline(StandardScaler(), VarianceThreshold(), LogisticRegressionCV(Cs = 10, penalty = &quot;elasticnet&quot;, solver = &quot;saga&quot;, l1_ratios = [1.0], cv = 10, max_iter = 2000, n_jobs = 4)) It is also possible to use a parameter tuning method more like the one in Chapter 2, using gridsearchCV and a predefined grid of search values The scikit-learn user guide has a very detailed section on this method, available at: https://scikit-learn.org/stable/modules/grid_search.html 3.5 Fit the Model (Using the Pipeline) With our pipeline defined, fitting the model on the training data is very easy: we simply call the fit() method on the pipeline, with our X_train and y_train data as the inputs. pipe_scale_lr_lasso.fit(X_train, y_train) # apply scaling on training data ## Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()), ## (&#39;variancethreshold&#39;, VarianceThreshold()), ## (&#39;logisticregressioncv&#39;, ## LogisticRegressionCV(cv=10, l1_ratios=[1.0], max_iter=2000, ## n_jobs=4, penalty=&#39;elasticnet&#39;, ## solver=&#39;saga&#39;))]) Because we used LogisticRegressionCV(), several of the steps we went through more carefully in Chapter 2 have been done for us: hyperparameter tuning was done, using an automatically-generated grid of 10 penalty values; the highest-accuracy C value was selected, using the mean scores across all cross-validation runs for each value; the model was refit using all of the data and the highest C value. In some cases, it may be a better practice not to allow an algorithm to make all of these decisions automatically. It is, of course, possible to more precisely replicate the process shown in Chapter 1, by manually selecting a search grid for the penalty value, plotting, and evaluating each penalty value, and manually refitting on the training set. For instance, see here for an example of how to balance model complexity and cross-validated score, which, in this case, means finding a model with the least number of components from principal components analysis while maintaining a good-enough accuracy score. 3.6 Score and Evaluate the Model 3.6.1 Accuracy and Predictions The most basic way to assess the performance of a fitted scikit-learn model is the score() function, with the test set as inputs. This uses the fitted model to predict on the test set and returns the proportion of correct predictions. Our model has nearly 93% accuracy, which sounds good, although well dive a little deeper into the results below. pipe_scale_lr_lasso.score(X_test, y_test) ## 0.9290540540540541 We can also get predictions using the predict() method, with our X_test DataFrame as the sole input. y_pred = pipe_scale_lr_lasso.predict(X_test) 3.6.2 Confusion Matrices and Unbalanced Classes pred_series = pd.Series(y_pred) counts = pred_series.value_counts() It looks like our model predicted that just 20 out of 888 players in the test set would be inducted into the Hall of Fame. While we already know that our model was 94% accurate in the test set, its also useful to compare the predictions to the actual y_test values with a confusion matrix, especially in a classification problem like ours with just two classes - and even more so when the classes are highly unbalanced. In this situation, a predictive model can often score very well by simply predicting the more popular class (here, the negative result of zero or not inducted into the Hall of Fame) in nearly every case. sklearn.metrics.confusion_matrix() will produce a confusion matrix as a numpy array; this is useful for further processing, but not especially easy to read. import sklearn.metrics cm_array = sklearn.metrics.confusion_matrix(y_test, y_pred) print(cm_array) ## [[808 3] ## [ 60 17]] tn = cm_array[0,0] fn = cm_array[1, 0] fp = cm_array[0, 1] tp = cm_array[1, 1] Fortunately, scikit-learn will also generate a much prettier and easier-to-read confusion matrix, with the help of matplotlib.pyplot. As expected, our model seriously underpredicted Hall of Fame induction: we had just 3 false positives and 60 false negatives! With 17 true positives, this means that we correctly predicted less than 25% of the actual Hall of Fame inductees in the test set. This makes our model look quite a bit less useful than the accuracy figure alone might have led us to believe. import matplotlib.pyplot as plt sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred) ## &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x00000261D791DFA0&gt; plt.show() 3.6.3 The Decision Boundary, Precision-Recall Curves, and Receiver Operating Characteristic (ROC) Curves One possible way to better understand and/or ameliorate this issue is to look more closely at our decision boundary. By default, the decision boundary is 0.5: we predict whichever class our model says has a higher probability. However, we might want to lower this threshold, so that we predict the positive class (Hall of Fame induction) when the model says that a player has a reasonably high but less than 50% chance. This should return more true positives, but at the cost of having more false positives, as well. One way to assess this tradeoff is the precision-recall curve. Precision is the proportion of our positive predictions that were correct; ours was quite high, at 17/20 or 0.85. Recall is the proportion of actual positives that we predicted correctly; ours was quite poor, at 17/77 or 0.22. The precision-recall curve plots precision versus recall at different decision boundaries. Here, well mark our current precision and recall, at the 0.5 decision boundary, with red lines. y_score = pipe_scale_lr_lasso.decision_function(X_test) sklearn.metrics.PrecisionRecallDisplay.from_predictions(y_test, y_score, name=&quot;LogisticRegressionCV&quot;) ## &lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay object at 0x00000261D83DE640&gt; plt.axhline(y=tp/(tp + fp), color=&#39;r&#39;, linestyle=&#39;-&#39;, zorder = -10) recall_score = sklearn.metrics.recall_score(y_test, y_pred) plt.axvline(x = recall_score, color = &#39;r&#39;, linestyle = &#39;-&#39;, zorder = -10) plt.show() This plot suggests that we have the option of shifting our decision boundary downward to trade precision for recall. Unfortunately, the tradeoff looks nearly linear - ideally, we would have found that we could gain a lot of recall while only losing a small amount of precision. Another way to assess our choice of decision boundary, and the models performance at different boundaries, is the ROC curve, which plots the true positive rate (recall) and the false positive rate (the proportion of actual negatives that we predicted incorrectly.) We can plot the ROC curve using RocCurveDisplay from scikit-learn along with scores from decision_function() and the test set labels. Again, we add red lines to show the current decision boundary. y_df = pipe_scale_lr_lasso.decision_function(X_test) sklearn.metrics.RocCurveDisplay.from_predictions(y_test, y_df) ## &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay object at 0x00000261D83D6BB0&gt; plt.axhline(y = recall_score, color=&#39;r&#39;, linestyle=&#39;-&#39;, zorder = -10) # true positive rate plt.axvline(x = fp/(fp+tn), color = &#39;r&#39;, linestyle = &#39;-&#39;, zorder = -20) # false positive rate plt.show() As with the precision-recall curve, it seems that we could shift the decision boundary downward to get a higher true positive rate - and, in this case, it looks like our false positive rate would barely budge, thanks to the very large number of true negatives in the dataset that we would still be predicting correctly. This plot also includes the area under the curve (AUC), often referred to as the AUROC for this curve. The AUROC is, of course, independent of our specific choice of decision boundary, and it is frequently used as a metric for assessing and comparing classification models. Lets shift the decision boundary down to 0.33 and see how it changes our results. Caveat: tuning your decision boundary on test set predictions is generally a bad idea, because it can easily lead to overfitting to the test set. Were doing it here for simplicity, but youre better off dealing with unbalanced classes while training your model, and then assessing the trained models in the test set. As expected, we added both true and false positives, with a much higher proportional increase in the number of false positives than true positives. This doesnt sound great, but it might be worthwhile if we care much more about detecting actual Hall of Fame inductees than we do about making a few more wrong predictions. probs = pd.DataFrame(pipe_scale_lr_lasso.predict_proba(X_test), columns = [&#39;prob_zero&#39;, &#39;prob_one&#39;]) preds_onethird = (probs[&#39;prob_one&#39;] &gt; 0.33).astype(int) sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, preds_onethird) ## &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x00000261DC5C6580&gt; plt.show() Lets also see where this puts us on our precision-recall and ROC curves. Our new decision boundary will be marked in blue. recall_onethird = sklearn.metrics.recall_score(y_test, preds_onethird) cm_array_onethird = sklearn.metrics.confusion_matrix(y_test, preds_onethird) tn_onethird = cm_array_onethird[0,0] fn_onethird = cm_array_onethird[1, 0] fp_onethird = cm_array_onethird[0, 1] tp_onethird = cm_array_onethird[1, 1] precision_onethird = tp_onethird/(tp_onethird + fp_onethird) fpr_onethird = 1 - tn_onethird/(tn_onethird + fp_onethird) As expected, we moved to the right on the precision-recall curve, trading precision for recall. ## y_score = pipe_scale_lr_lasso.decision_function(X_test) sklearn.metrics.PrecisionRecallDisplay.from_predictions(y_test, y_score, name=&quot;LogisticRegressionCV&quot;) ## &lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay object at 0x00000261DC91C160&gt; plt.axhline(y=tp/(tp + fp), color=&#39;r&#39;, linestyle=&#39;-&#39;, zorder = -10) plt.axhline(y=precision_onethird, color=&#39;blue&#39;, linestyle=&#39;-&#39;, zorder = -10) recall_score = sklearn.metrics.recall_score(y_test, y_pred) plt.axvline(x = recall_score, color = &#39;r&#39;, linestyle = &#39;-&#39;, zorder = -10) plt.axvline(x = recall_onethird, color = &#39;blue&#39;, linestyle = &#39;-&#39;, zorder = -10) plt.show() We also moved to the right on the ROC curve, but just barely! Our true positive rate increased quite a bit more than our false positive rate. Which of these plots and scores we care most about depends on the problem were trying to solve and our sensitivity to false positives and negatives. y_df = pipe_scale_lr_lasso.decision_function(X_test) sklearn.metrics.RocCurveDisplay.from_predictions(y_test, y_df) ## &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay object at 0x00000261D83DE640&gt; plt.axhline(y = recall_score, color=&#39;r&#39;, linestyle=&#39;-&#39;, zorder = -10) # true positive rate plt.axhline(y = recall_onethird, color=&#39;blue&#39;, linestyle=&#39;-&#39;, zorder = -10) # true positive rate plt.axvline(x = fp/(fp+tn), color = &#39;r&#39;, linestyle = &#39;-&#39;, zorder = -20) # false positive rate plt.axvline(x = fpr_onethird, color = &#39;blue&#39;, linestyle = &#39;-&#39;, zorder = -20) plt.show() 3.6.4 Class Weights If we want to avoid tuning the decision boundary directly, another option is to use the class_weights parameter found in many classifiers in scikit-learn. This allows us to increase the penalty for misclassifying the higher-weighted class (here, the less-frequent inducted into Hall of Fame class) while fitting the model. There is a balanced option for class weights that attempts to fully balance classes by setting class weights inversely proportional to class proportions; unfortunately, our classes are so unbalanced that this method doesnt work on this dataset. Instead, we weight the positive class at four times the weight of the negative class. pipe_scale_lr_lasso_weighted = make_pipeline(StandardScaler(), VarianceThreshold(), LogisticRegressionCV(Cs = 10, penalty = &quot;elasticnet&quot;, solver = &quot;saga&quot;, l1_ratios = [1.0], cv = 10, max_iter = 3000, n_jobs = 4, class_weight = {0: 0.2, 1: 0.8})) pipe_scale_lr_lasso_weighted.fit(X_train, y_train) ## Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()), ## (&#39;variancethreshold&#39;, VarianceThreshold()), ## (&#39;logisticregressioncv&#39;, ## LogisticRegressionCV(class_weight={0: 0.2, 1: 0.8}, cv=10, ## l1_ratios=[1.0], max_iter=3000, n_jobs=4, ## penalty=&#39;elasticnet&#39;, solver=&#39;saga&#39;))]) This produces a very small improvement in accuracy. weighted_accuracy = pipe_scale_lr_lasso_weighted.score(X_test, y_test) print(&quot;Weighted accuracy: &quot;, weighted_accuracy.round(4)) ## Weighted accuracy: 0.9347 print(&quot;Improvement: &quot;, (weighted_accuracy - pipe_scale_lr_lasso.score(X_test, y_test)).round(4)) ## Improvement: 0.0056 The difference in the confusion matrix, however, is much more noticeable: our recall is up to 0.45, although our precision has decreased as the number of false positives grows. y_pred_weighted = pipe_scale_lr_lasso_weighted.predict(X_test) sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred_weighted) ## &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x00000261DC58B130&gt; plt.show() Our ROC curve looks fairly similar, and our AUC remained the same. y_df_weighted = pipe_scale_lr_lasso_weighted.decision_function(X_test) sklearn.metrics.RocCurveDisplay.from_predictions(y_test, y_df_weighted) ## &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay object at 0x00000261DCF68BE0&gt; plt.show() Similarly, our precision-recall curve is fairly similar, with no change in average precision. sklearn.metrics.PrecisionRecallDisplay.from_predictions(y_test, y_df_weighted, name=&quot;LogisticRegressionCV Weighted&quot;) ## &lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay object at 0x00000261DC95D6D0&gt; plt.show() If our model accuracy metrics didnt even change, should we have even bothered with class weights? The answer is that it depends on the problem were trying to solve. While the baseball example we used here is clearly not relevant our work, classification problems absolutely do show up in military applications of predictive modeling and machine learning, and unbalanced classes are fairly common: for example, computer vision algorithms used to detect features such as military vehicles in satellite imagery. In many such cases, analyts do not blindly trust computer models to detect the features of interest or to make decisions about what to do in response to a feature being detected. Instead, they are used to focus human experts on a smaller subset of cases where the algorithm has signalled that there is a high probability that there is something worth looking at. In such a case, false negatives are typically a much larger issue than false positives: we want the algorithm to flag as many of the true positives as possible, even if it also produces a moderate amount of false positives that human experts have to sift through. As long as it eliminates a large number of true negatives, it still saves humans time and effort, allowing them to work more efficiently. 3.7 Try Another Method: Random Forest Like tidymodels, scikit-learn makes it easy to substitute another modeling method in place of logistic regression. Here, we insert a random forest classifier, using all default values. from sklearn.ensemble import RandomForestClassifier pipe_scale_randomforest = make_pipeline(StandardScaler(), VarianceThreshold(), RandomForestClassifier()) pipe_scale_randomforest.fit(X_train, y_train) ## Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()), ## (&#39;variancethreshold&#39;, VarianceThreshold()), ## (&#39;randomforestclassifier&#39;, RandomForestClassifier())]) pipe_scale_randomforest.score(X_test, y_test) ## 0.9414414414414415 It looks like the random forest, with zero customization, produces a higher accuracy than either of our lasso logistic regression models. The confusion matrix below shows that the random forest is clearly better than the unweighted logistic regression model, with substantially more true positives and just one more false positive. However, it still had lower recall than our class-weighted logistic regression, suggesting that it might need further tuning if we care primarily about missing as few actual positive values as possible. y_pred_rf = pipe_scale_randomforest.predict(X_test) sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf) ## &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x00000261DD370AC0&gt; plt.show() 3.8 Predict on Eligible Data As we did in R, lets make predictions on the eligible dataset, using our class-weighted model. eligible = eligible.set_index(&quot;player_id&quot;) eligible_preds = pipe_scale_lr_lasso_weighted.predict_proba(eligible.drop(&#39;last_year&#39;, axis = 1)).round(3) df_eligible_preds = pd.DataFrame(eligible_preds, columns = [&#39;pred_0&#39;, &#39;pred_1&#39;]).set_index(eligible.index) elig_joined = eligible.join(df_eligible_preds).sort_values(&quot;pred_1&quot;, ascending = False).reset_index().query(&quot;pred_1 &gt; .4&quot;) py$elig_joined %&gt;% select(player_id, pred_1, pred_0, everything()) %&gt;% DT::datatable() 3.9 Conclusion For most predictive modeling tasks - especially if you are not trying to use any cutting-edge methods or dealing with very large datasets - there is no clear winner between Rs tidymodels or Pythons scikit-learn. They both support a very wide range of machine learning methods and make it relatively easy to optimize hyperparameters, evaluate model performance, and compare model specifications. There are differences in their user interfaces and in the ways that analysts construct modeling pipelines, but which one makes more sense to each user will likely come down to personal preference and familiarity with the language. While building this tutorial, however, one noticeable advantage for scikit-learn did become clear: it has a very extensive and detailed user guide, full of easy-to-follow examples, arguably making it easier to learn. The tidymodels framework may close this gap over time, but the wealth of existing scikit-learn documentation and the fact that the R predictive modeling landscape is relatively fragmented and has not fully coalesced around tidymodels could slow that down. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
