<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Modeling in Python with scikit-learn | Modeling in R and Python</title>
  <meta name="description" content="3 Modeling in Python with scikit-learn | Modeling in R and Python" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Modeling in Python with scikit-learn | Modeling in R and Python" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Modeling in Python with scikit-learn | Modeling in R and Python" />
  
  
  

<meta name="author" content="MAJ Dusty Turner and Mr. Robert Ward" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-modeling-in-r-with-tidymodels.html"/>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.19/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<center><li><a href = "https://dustysturner.com/MORS_ETF_2021/">Tutorial Home</a></li>
<img src="img/caa_seal.png" width="80"></center>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Class Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#topics-class-structure"><i class="fa fa-check"></i><b>1.1</b> Topics &amp; Class Structure</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#software-prerequisites"><i class="fa fa-check"></i><b>1.2</b> Software Prerequisites</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#human-prerequisites"><i class="fa fa-check"></i><b>1.3</b> Human Prerequisites</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#tutorial-challenges"><i class="fa fa-check"></i><b>1.4</b> Tutorial Challenges</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#end-state"><i class="fa fa-check"></i><b>1.5</b> End State</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#instructors-introduction"><i class="fa fa-check"></i><b>1.6</b> Instructors Introduction</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#lets-get-started"><i class="fa fa-check"></i><b>1.7</b> Let’s Get Started…</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html"><i class="fa fa-check"></i><b>2</b> Modeling in R with Tidymodels</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html#tidymodels-packages"><i class="fa fa-check"></i><b>2.1</b> Tidymodels Packages</a></li>
<li class="chapter" data-level="2.2" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html#explore-data"><i class="fa fa-check"></i><b>2.2</b> Explore Data</a></li>
<li class="chapter" data-level="2.3" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html#split-data"><i class="fa fa-check"></i><b>2.3</b> Split Data</a></li>
<li class="chapter" data-level="2.4" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html#prepare-data"><i class="fa fa-check"></i><b>2.4</b> Prepare Data</a></li>
<li class="chapter" data-level="2.5" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html#specify-model"><i class="fa fa-check"></i><b>2.5</b> Specify Model</a></li>
<li class="chapter" data-level="2.6" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html#create-workflow"><i class="fa fa-check"></i><b>2.6</b> Create Workflow</a></li>
<li class="chapter" data-level="2.7" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html#specify-grid-of-training-parameters"><i class="fa fa-check"></i><b>2.7</b> Specify Grid of Training Parameters</a></li>
<li class="chapter" data-level="2.8" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html#train-model"><i class="fa fa-check"></i><b>2.8</b> Train Model</a></li>
<li class="chapter" data-level="2.9" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html#build-model-on-all-training-data-test-on-validation-set"><i class="fa fa-check"></i><b>2.9</b> Build Model on all Training Data, Test on Validation Set</a></li>
<li class="chapter" data-level="2.10" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html#change-model-to-random-forest"><i class="fa fa-check"></i><b>2.10</b> Change Model to Random Forest</a></li>
<li class="chapter" data-level="2.11" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html#build-model-on-all-training-and-validation-data-using-the-best-model"><i class="fa fa-check"></i><b>2.11</b> Build Model on all Training and Validation Data Using the Best Model</a></li>
<li class="chapter" data-level="2.12" data-path="2-modeling-in-r-with-tidymodels.html"><a href="2-modeling-in-r-with-tidymodels.html#make-predictions-with-new-data"><i class="fa fa-check"></i><b>2.12</b> Make Predictions with New Data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-modeling-in-python-with-scikit-learn.html"><a href="3-modeling-in-python-with-scikit-learn.html"><i class="fa fa-check"></i><b>3</b> Modeling in Python with scikit-learn</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-modeling-in-python-with-scikit-learn.html"><a href="3-modeling-in-python-with-scikit-learn.html#scikit-learn-overview"><i class="fa fa-check"></i><b>3.1</b> scikit-learn Overview</a></li>
<li class="chapter" data-level="3.2" data-path="3-modeling-in-python-with-scikit-learn.html"><a href="3-modeling-in-python-with-scikit-learn.html#explore-data-1"><i class="fa fa-check"></i><b>3.2</b> Explore Data</a></li>
<li class="chapter" data-level="3.3" data-path="3-modeling-in-python-with-scikit-learn.html"><a href="3-modeling-in-python-with-scikit-learn.html#split-data-1"><i class="fa fa-check"></i><b>3.3</b> Split Data</a></li>
<li class="chapter" data-level="3.4" data-path="3-modeling-in-python-with-scikit-learn.html"><a href="3-modeling-in-python-with-scikit-learn.html#define-a-pipeline"><i class="fa fa-check"></i><b>3.4</b> Define a Pipeline</a></li>
<li class="chapter" data-level="3.5" data-path="3-modeling-in-python-with-scikit-learn.html"><a href="3-modeling-in-python-with-scikit-learn.html#fit-the-model-using-the-pipeline"><i class="fa fa-check"></i><b>3.5</b> Fit the Model (Using the Pipeline)</a></li>
<li class="chapter" data-level="3.6" data-path="3-modeling-in-python-with-scikit-learn.html"><a href="3-modeling-in-python-with-scikit-learn.html#score-and-evaluate-the-model"><i class="fa fa-check"></i><b>3.6</b> Score and Evaluate the Model</a></li>
<li class="chapter" data-level="3.7" data-path="3-modeling-in-python-with-scikit-learn.html"><a href="3-modeling-in-python-with-scikit-learn.html#try-another-method-random-forest"><i class="fa fa-check"></i><b>3.7</b> Try Another Method: Random Forest</a></li>
<li class="chapter" data-level="3.8" data-path="3-modeling-in-python-with-scikit-learn.html"><a href="3-modeling-in-python-with-scikit-learn.html#predict-on-eligible-data"><i class="fa fa-check"></i><b>3.8</b> Predict on Eligible Data</a></li>
<li class="chapter" data-level="3.9" data-path="3-modeling-in-python-with-scikit-learn.html"><a href="3-modeling-in-python-with-scikit-learn.html#conclusion"><i class="fa fa-check"></i><b>3.9</b> Conclusion</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modeling in R and Python</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modeling-in-python-with-scikit-learn" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Modeling in Python with scikit-learn</h1>
<div id="scikit-learn-overview" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> scikit-learn Overview</h2>
<p>Like {tidymodels}, scikit-learn offers a suite of tools for predictive modeling and machine learning: it will help you split data, preprocess model inputs, fit models, and compare and assess them.</p>
<p>Unlike {tidymodels}, scikit-learn is a single monolithic package with functions for the entire modeling pipeline. Users will likely still want to use the pandas library to ingest and prepare data, and may want to use other libraries to supplement scikit-learn’s data visualization capabilities, but scikit-learn will do most of the work by itself - and often with less and simpler code than tidymodels, at least for standard machine learning workflows.</p>
<div id="scikit-learn-road-map" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> scikit-learn Road Map</h3>
<p>What we plan to do:</p>
<ol style="list-style-type: decimal">
<li>Read in and explore data (pandas and R)</li>
<li>Create model (scikit-learn)
<ul>
<li>split data</li>
<li>define pipeline with preprocessors and model with cross-validation for parameter tuning</li>
<li>fit model</li>
</ul></li>
<li>Predict on new data and assess model (scikit-learn)</li>
</ol>
</div>
<div id="modeling-goal-1" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Modeling Goal</h3>
<p>We plan to create a model using the <code>historical</code> data and use that model to predict who is most likely to make it into the Hall of Fame in the <code>eligible</code> data.</p>
</div>
</div>
<div id="explore-data-1" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Explore Data</h2>
<p>We’ll load the pandas library to import and set up the data.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span></code></pre></div>
<p>Here, we use panda’s <code>read_csv()</code> to import the data, and then we print the first few rows of the historical dataframe to the console.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb65-1" aria-hidden="true" tabindex="-1"></a>historical <span class="op">=</span> pd.read_csv(<span class="st">&#39;01_data/historical_baseball.csv&#39;</span>).query(<span class="st">&quot;ab &gt; 250&quot;</span>)</span>
<span id="cb65-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb65-2" aria-hidden="true" tabindex="-1"></a>eligible <span class="op">=</span> pd.read_csv(<span class="st">&#39;01_data/eligible_baseball.csv&#39;</span>).query(<span class="st">&quot;ab &gt; 250&quot;</span>)</span>
<span id="cb65-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb65-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb65-4" aria-hidden="true" tabindex="-1"></a>historical</span></code></pre></div>
<pre><code>##       player_id  inducted     g     ab     r  ...   sb  cs    bb    so  last_year
## 0     aaronha01         1  3298  12364  2174  ...  240  73  1402  1383       1976
## 1     aaronto01         0   437    944   102  ...    9   8    86   145       1971
## 3     abbated01         0   855   3044   355  ...  142   0   289    16       1910
## 8     adairje01         0  1165   4019   378  ...   29  29   208   499       1970
## 9     adamsba01         0   482   1019    79  ...    1   1    53   177       1926
## ...         ...       ...   ...    ...   ...  ...  ...  ..   ...   ...        ...
## 3227  zimmedo01         0  1095   3283   353  ...   45  25   246   678       1965
## 3228  zimmehe01         0  1456   5304   695  ...  175  33   242   404       1919
## 3230   ziskri01         0  1453   5144   681  ...    8  15   533   910       1983
## 3231  zitzmbi01         0   406   1004   197  ...   42  11    83    85       1929
## 3234  zuvelpa01         0   209    491    41  ...    2   0    34    50       1991
## 
## [2664 rows x 15 columns]</code></pre>
<p>As a reminder, the <code>historical</code> data contains career statistics for every baseball batter from 1880-2011 who no longer meets Hall of Fame eligibility requirements or has already made the Hall of Fame, while the <code>eligible</code> data contains all players who are currently eligible for Hall of Fame induction.</p>
<p>You can see from the data below, the players who make the Hall of Fame tend to perform better in a few standard baseball statistics. This pandas code does the same thing as the R code in Chapter 2 - it groups the historical data by whether or not the player was inducted into the Hall of Fame, and then takes the mean of each column in each group.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb67-1" aria-hidden="true" tabindex="-1"></a>hist_means_inducted_groups <span class="op">=</span> historical.drop(<span class="st">&#39;last_year&#39;</span>, axis <span class="op">=</span> <span class="dv">1</span>).groupby(<span class="st">&#39;inducted&#39;</span>).mean().<span class="bu">round</span>()</span></code></pre></div>
<p>We can bring the data back into R, using RStudio’s very simple Python-R interface, and use the same R code as in Chapter 1 to print a table and produce boxplots of the means. We don’t need to do anything special to convert a pandas DataFrame to an R data frame: {reticulate} handles it for us when we call <code>py$hist_means-inducted_groups</code>.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb68-1" aria-hidden="true" tabindex="-1"></a>py<span class="sc">$</span>hist_means_inducted_groups <span class="sc">%&gt;%</span></span>
<span id="cb68-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb68-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames_to_column</span>(<span class="at">var =</span> <span class="st">&quot;inducted&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb68-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb68-3" aria-hidden="true" tabindex="-1"></a>  gt<span class="sc">::</span><span class="fu">gt</span>() <span class="do">## renders the table </span></span></code></pre></div>
<div id="sivccuwaox" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#sivccuwaox .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#sivccuwaox .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#sivccuwaox .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#sivccuwaox .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#sivccuwaox .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#sivccuwaox .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#sivccuwaox .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#sivccuwaox .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#sivccuwaox .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#sivccuwaox .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#sivccuwaox .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#sivccuwaox .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#sivccuwaox .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#sivccuwaox .gt_from_md > :first-child {
  margin-top: 0;
}

#sivccuwaox .gt_from_md > :last-child {
  margin-bottom: 0;
}

#sivccuwaox .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#sivccuwaox .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#sivccuwaox .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#sivccuwaox .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#sivccuwaox .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#sivccuwaox .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#sivccuwaox .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#sivccuwaox .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#sivccuwaox .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#sivccuwaox .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#sivccuwaox .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#sivccuwaox .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#sivccuwaox .gt_left {
  text-align: left;
}

#sivccuwaox .gt_center {
  text-align: center;
}

#sivccuwaox .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#sivccuwaox .gt_font_normal {
  font-weight: normal;
}

#sivccuwaox .gt_font_bold {
  font-weight: bold;
}

#sivccuwaox .gt_font_italic {
  font-style: italic;
}

#sivccuwaox .gt_super {
  font-size: 65%;
}

#sivccuwaox .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 65%;
}
</style>
<table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1">inducted</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">g</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">ab</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">r</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">h</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">x2b</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">x3b</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">hr</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">rbi</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">sb</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">cs</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">bb</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">so</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td class="gt_row gt_left">0</td>
<td class="gt_row gt_right">907</td>
<td class="gt_row gt_right">2849</td>
<td class="gt_row gt_right">374</td>
<td class="gt_row gt_right">751</td>
<td class="gt_row gt_right">122</td>
<td class="gt_row gt_right">28</td>
<td class="gt_row gt_right">50</td>
<td class="gt_row gt_right">336</td>
<td class="gt_row gt_right">60</td>
<td class="gt_row gt_right">20</td>
<td class="gt_row gt_right">264</td>
<td class="gt_row gt_right">325</td></tr>
    <tr><td class="gt_row gt_left">1</td>
<td class="gt_row gt_right">1675</td>
<td class="gt_row gt_right">5941</td>
<td class="gt_row gt_right">958</td>
<td class="gt_row gt_right">1747</td>
<td class="gt_row gt_right">295</td>
<td class="gt_row gt_right">77</td>
<td class="gt_row gt_right">149</td>
<td class="gt_row gt_right">874</td>
<td class="gt_row gt_right">165</td>
<td class="gt_row gt_right">37</td>
<td class="gt_row gt_right">643</td>
<td class="gt_row gt_right">564</td></tr>
  </tbody>
  
  
</table>
</div>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb69-1" aria-hidden="true" tabindex="-1"></a>py<span class="sc">$</span>historical <span class="sc">%&gt;%</span></span>
<span id="cb69-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb69-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">inducted =</span> <span class="fu">as_factor</span>(inducted)) <span class="sc">%&gt;%</span> <span class="co"># did this on data read in R but not python, so we do it here instead</span></span>
<span id="cb69-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb69-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(g<span class="sc">:</span>so) <span class="sc">%&gt;%</span> </span>
<span id="cb69-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb69-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> inducted, <span class="at">y =</span> value)) <span class="sc">+</span></span>
<span id="cb69-5"><a href="3-modeling-in-python-with-scikit-learn.html#cb69-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb69-6"><a href="3-modeling-in-python-with-scikit-learn.html#cb69-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>name, <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>)  <span class="sc">+</span></span>
<span id="cb69-7"><a href="3-modeling-in-python-with-scikit-learn.html#cb69-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;&quot;</span>,<span class="at">x =</span> <span class="st">&quot;Hall of Fame Indicator&quot;</span>)</span></code></pre></div>
<p><img src="MORS-ETF-2021_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
</div>
<div id="split-data-1" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Split Data</h2>
<p>As we did in R, we will split the data into a training set (two-thirds of the data) and testing set (one-third) of the data.</p>
<p>We set the seed so the analysis is reproducible - here, we do this using the <code>random_state</code> parameter in <code>train_test_split()</code>.</p>
<!-- Robert, did you mean to capitalize the 'X' below -- like its a vector?  Otherwise I made the x so it is small like the y -->
<p>Instead of an <code>rsplit</code> object that contains resampling metadata, <code>train_test_split()</code> returns four objects: x (predictor) pandas DataFrame objects for the training and test sets, and y (target) pandas Series objects for the training and test sets.</p>
<p>Note that before splitting the data, we set the index of the dataframe to be <code>player_id</code>. This carries through to the outputs of <code>train_test_split()</code>, which all have <code>player_id</code> as a common index (and not as a predictor or target variable.) In a way, this serves a similar purpose to the <code>update_role(player_id, new_role = "ID")</code> line that we added to the recipe in R.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb70-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb70-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb70-3" aria-hidden="true" tabindex="-1"></a>historical_pidindex <span class="op">=</span> historical.set_index(<span class="st">&#39;player_id&#39;</span>)</span>
<span id="cb70-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="3-modeling-in-python-with-scikit-learn.html#cb70-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> historical_pidindex.drop([<span class="st">&#39;inducted&#39;</span>, <span class="st">&#39;last_year&#39;</span>], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb70-6"><a href="3-modeling-in-python-with-scikit-learn.html#cb70-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> historical_pidindex.inducted</span>
<span id="cb70-7"><a href="3-modeling-in-python-with-scikit-learn.html#cb70-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-8"><a href="3-modeling-in-python-with-scikit-learn.html#cb70-8" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, random_state<span class="op">=</span><span class="dv">0</span>, test_size <span class="op">=</span> <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)</span></code></pre></div>
</div>
<div id="define-a-pipeline" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Define a Pipeline</h2>
<p>Scikit-learn’s “pipelines” serve the combined purpose of “workflows” and “recipes” in {tidymodels}. They allow you to define a set of preprocessing and modeling steps that you can then apply to any dataset. They are defined by the function <code>make_pipeline()</code>, with the steps, in order, as arguments.</p>
<p>The first two steps in our pipeline will take care of preprocessing. In Chapter 2, we centered and scaled our data; here, we’ll use <code>StandardScaler()</code>, which accomplishes both of those steps. We’ll also apply <code>VarianceThreshold()</code>; in its default form, this only removes zero-variance predictors, but the user can set a custom variance threshold. None of our predictors have low variance, so this feature selection mechanism does nothing to our data anyway.</p>
<p>The third step in our pipeline is our model. Here, we’ve chosen <code>LogisticRegressionCV()</code>. The first three parameters should produce a model very similar to the one in Chapter 2:</p>
<ul>
<li><code>Cs = 10</code>: the modeling function will automatically select a grid of 10 C values (inverse penalties) to search over. This is the default value. The user can also specify a specific list of C values to search over.<br />
</li>
<li><code>penalty = "elasticnet"</code> lets us use a hybrid L1 and L2 penalty, or a mix between Lasso and Ridge regression, much like <code>engine = glmnet</code> in R;<br />
</li>
<li><code>solver = "saga"</code> chooses a solver that is compatible with our other options;<br />
</li>
<li><code>l1_ratios = [1.0]</code> is the equivalent of <code>mixture = 1</code> in R - it gives us a pure Lasso regression;<br />
</li>
<li><code>max_iter = 2000</code> allows the solver to attempt up to 2,000 iterations as it searches for a solution, because the default of 100 was insufficient for this model specification;</li>
</ul>
<p>We also have two parameters related to the cross-validation (CV) part of the model specification:</p>
<ul>
<li><code>cv = 10</code>. This means that the data will be split into 10 folds, and the model will be fit 10 times for each set of hyperparameters in an automatically generated search grid, with one fold being held out as a validation set for computing accuracy in each run. This process will allow the model to tune the size of penalty, which we have not specified explicitly.<br />
</li>
<li><code>refit = True</code>: the function will find the best C (inverse penalty) value by averaging the cross-validation scores of each one, and then refit the model using the best C value on all of the data.</li>
</ul>
<p>Finally, we set <code>n_jobs = 4</code> to allow for multithreading. In my own highly unscientific testing, moving from one to four threads reduces model fit time from 15 seconds to 6 seconds.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> preprocessing</span>
<span id="cb71-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb71-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegressionCV</span>
<span id="cb71-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb71-5"><a href="3-modeling-in-python-with-scikit-learn.html#cb71-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb71-6"><a href="3-modeling-in-python-with-scikit-learn.html#cb71-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb71-7"><a href="3-modeling-in-python-with-scikit-learn.html#cb71-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb71-8"><a href="3-modeling-in-python-with-scikit-learn.html#cb71-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-9"><a href="3-modeling-in-python-with-scikit-learn.html#cb71-9" aria-hidden="true" tabindex="-1"></a>pipe_scale_lr_lasso <span class="op">=</span> make_pipeline(StandardScaler(), VarianceThreshold(), LogisticRegressionCV(Cs <span class="op">=</span> <span class="dv">10</span>, penalty <span class="op">=</span> <span class="st">&quot;elasticnet&quot;</span>, solver <span class="op">=</span> <span class="st">&quot;saga&quot;</span>, l1_ratios <span class="op">=</span> [<span class="fl">1.0</span>], cv <span class="op">=</span> <span class="dv">10</span>, max_iter <span class="op">=</span> <span class="dv">2000</span>, n_jobs <span class="op">=</span> <span class="dv">4</span>))</span></code></pre></div>
<p>It is also possible to use a parameter tuning method more like the one in Chapter 2, using <code>gridsearchCV</code> and a predefined grid of search values The scikit-learn user guide has a very detailed section on this method, available at: <a href="https://scikit-learn.org/stable/modules/grid_search.html" class="uri">https://scikit-learn.org/stable/modules/grid_search.html</a></p>
</div>
<div id="fit-the-model-using-the-pipeline" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Fit the Model (Using the Pipeline)</h2>
<p>With our pipeline defined, fitting the model on the training data is very easy: we simply call the <code>fit()</code> method on the pipeline, with our <code>X_train</code> and <code>y_train</code> data as the inputs.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb72-1" aria-hidden="true" tabindex="-1"></a>pipe_scale_lr_lasso.fit(X_train, y_train)  <span class="co"># apply scaling on training data</span></span></code></pre></div>
<pre><code>## Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),
##                 (&#39;variancethreshold&#39;, VarianceThreshold()),
##                 (&#39;logisticregressioncv&#39;,
##                  LogisticRegressionCV(cv=10, l1_ratios=[1.0], max_iter=2000,
##                                       n_jobs=4, penalty=&#39;elasticnet&#39;,
##                                       solver=&#39;saga&#39;))])</code></pre>
<p>Because we used <code>LogisticRegressionCV()</code>, several of the steps we went through more carefully in Chapter 2 have been done for us:</p>
<ul>
<li>hyperparameter tuning was done, using an automatically-generated grid of 10 penalty values;</li>
<li>the highest-accuracy C value was selected, using the mean scores across all cross-validation runs for each value;</li>
<li>the model was refit using all of the data and the highest C value.</li>
</ul>
<p>In some cases, it may be a better practice not to allow an algorithm to make all of these decisions automatically. It is, of course, possible to more precisely replicate the process shown in Chapter 1, by manually selecting a search grid for the penalty value, plotting, and evaluating each penalty value, and manually refitting on the training set. For instance, <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_refit_callable.html#sphx-glr-auto-examples-model-selection-plot-grid-search-refit-callable-py">see here</a> for an example of how to “balance model complexity and cross-validated score,” which, in this case, means finding a model with the least number of components from principal components analysis while maintaining a good-enough accuracy score.</p>
</div>
<div id="score-and-evaluate-the-model" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Score and Evaluate the Model</h2>
<div id="accuracy-and-predictions" class="section level3" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> Accuracy and Predictions</h3>
<p>The most basic way to assess the performance of a fitted scikit-learn model is the <code>score()</code> function, with the test set as inputs. This uses the fitted model to predict on the test set and returns the proportion of correct predictions. Our model has nearly 93% accuracy, which sounds good, although we’ll dive a little deeper into the results below.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb74-1" aria-hidden="true" tabindex="-1"></a>pipe_scale_lr_lasso.score(X_test, y_test)</span></code></pre></div>
<pre><code>## 0.9290540540540541</code></pre>
<p>We can also get predictions using the <code>predict()</code> method, with our <code>X_test</code> DataFrame as the sole input.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb76-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> pipe_scale_lr_lasso.predict(X_test)</span></code></pre></div>
</div>
<div id="confusion-matrices-and-unbalanced-classes" class="section level3" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Confusion Matrices and Unbalanced Classes</h3>
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb77-1" aria-hidden="true" tabindex="-1"></a>pred_series <span class="op">=</span> pd.Series(y_pred)</span>
<span id="cb77-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb77-2" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> pred_series.value_counts()</span></code></pre></div>
<p>It looks like our model predicted that just 20 out of 888 players in the test set would be inducted into the Hall of Fame.</p>
<p>While we already know that our model was 94% accurate in the test set, it’s also useful to compare the predictions to the actual y_test values with a confusion matrix, especially in a classification problem like ours with just two classes - and even more so when the classes are highly unbalanced. In this situation, a predictive model can often score very well by simply predicting the more popular class (here, the negative result of zero or “not inducted into the Hall of Fame”) in nearly every case.</p>
<p><code>sklearn.metrics.confusion_matrix()</code> will produce a confusion matrix as a numpy array; this is useful for further processing, but not especially easy to read.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.metrics</span>
<span id="cb78-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb78-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb78-3" aria-hidden="true" tabindex="-1"></a>cm_array <span class="op">=</span> sklearn.metrics.confusion_matrix(y_test, y_pred)</span>
<span id="cb78-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cm_array)</span></code></pre></div>
<pre><code>## [[808   3]
##  [ 60  17]]</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb80-1" aria-hidden="true" tabindex="-1"></a>tn <span class="op">=</span> cm_array[<span class="dv">0</span>,<span class="dv">0</span>]</span>
<span id="cb80-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb80-2" aria-hidden="true" tabindex="-1"></a>fn <span class="op">=</span> cm_array[<span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb80-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb80-3" aria-hidden="true" tabindex="-1"></a>fp <span class="op">=</span> cm_array[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb80-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb80-4" aria-hidden="true" tabindex="-1"></a>tp <span class="op">=</span> cm_array[<span class="dv">1</span>, <span class="dv">1</span>]</span></code></pre></div>
<p>Fortunately, scikit-learn will also generate a much prettier and easier-to-read confusion matrix, with the help of <code>matplotlib.pyplot</code>. As expected, our model seriously underpredicted Hall of Fame induction: we had just 3 false positives and 60 false negatives! With 17 true positives, this means that we correctly predicted less than 25% of the actual Hall of Fame inductees in the test set. This makes our model look quite a bit less useful than the accuracy figure alone might have led us to believe.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb81-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb81-3" aria-hidden="true" tabindex="-1"></a>sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred)</span></code></pre></div>
<pre><code>## &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x00000261D791DFA0&gt;</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb83-1" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="MORS-ETF-2021_files/figure-html/unnamed-chunk-56-1.png" width="672" /></p>
</div>
<div id="the-decision-boundary-precision-recall-curves-and-receiver-operating-characteristic-roc-curves" class="section level3" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> The Decision Boundary, Precision-Recall Curves, and Receiver Operating Characteristic (ROC) Curves</h3>
<p>One possible way to better understand and/or ameliorate this issue is to look more closely at our decision boundary. By default, the decision boundary is 0.5: we predict whichever class our model says has a higher probability. However, we might want to lower this threshold, so that we predict the positive class (Hall of Fame induction) when the model says that a player has a reasonably high but less than 50% chance. This should return more true positives, but at the cost of having more false positives, as well. One way to assess this tradeoff is the precision-recall curve. Precision is the proportion of our positive predictions that were correct; ours was quite high, at 17/20 or 0.85. Recall is the proportion of actual positives that we predicted correctly; ours was quite poor, at 17/77 or 0.22. The precision-recall curve plots precision versus recall at different decision boundaries. Here, we’ll mark our current precision and recall, at the 0.5 decision boundary, with red lines.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb84-1" aria-hidden="true" tabindex="-1"></a>y_score <span class="op">=</span> pipe_scale_lr_lasso.decision_function(X_test)</span>
<span id="cb84-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb84-2" aria-hidden="true" tabindex="-1"></a>sklearn.metrics.PrecisionRecallDisplay.from_predictions(y_test, y_score, name<span class="op">=</span><span class="st">&quot;LogisticRegressionCV&quot;</span>)</span></code></pre></div>
<pre><code>## &lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay object at 0x00000261D83DE640&gt;</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb86-1" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span>tp<span class="op">/</span>(tp <span class="op">+</span> fp), color<span class="op">=</span><span class="st">&#39;r&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">10</span>)</span>
<span id="cb86-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb86-2" aria-hidden="true" tabindex="-1"></a>recall_score <span class="op">=</span> sklearn.metrics.recall_score(y_test, y_pred)</span>
<span id="cb86-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb86-3" aria-hidden="true" tabindex="-1"></a>plt.axvline(x <span class="op">=</span> recall_score, color <span class="op">=</span> <span class="st">&#39;r&#39;</span>, linestyle <span class="op">=</span> <span class="st">&#39;-&#39;</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">10</span>)</span>
<span id="cb86-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb86-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-5"><a href="3-modeling-in-python-with-scikit-learn.html#cb86-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="MORS-ETF-2021_files/figure-html/unnamed-chunk-57-3.png" width="672" /></p>
<p>This plot suggests that we have the option of shifting our decision boundary downward to trade precision for recall. Unfortunately, the tradeoff looks nearly linear - ideally, we would have found that we could gain a lot of recall while only losing a small amount of precision.</p>
<p>Another way to assess our choice of decision boundary, and the model’s performance at different boundaries, is the ROC curve, which plots the true positive rate (recall) and the false positive rate (the proportion of actual negatives that we predicted incorrectly.) We can plot the ROC curve using <code>RocCurveDisplay</code> from scikit-learn along with scores from <code>decision_function()</code> and the test set labels. Again, we add red lines to show the current decision boundary.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb87-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb87-1" aria-hidden="true" tabindex="-1"></a>y_df <span class="op">=</span> pipe_scale_lr_lasso.decision_function(X_test)</span>
<span id="cb87-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb87-2" aria-hidden="true" tabindex="-1"></a>sklearn.metrics.RocCurveDisplay.from_predictions(y_test, y_df)</span></code></pre></div>
<pre><code>## &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay object at 0x00000261D83D6BB0&gt;</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb89-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb89-1" aria-hidden="true" tabindex="-1"></a>plt.axhline(y <span class="op">=</span> recall_score, color<span class="op">=</span><span class="st">&#39;r&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">10</span>) <span class="co"># true positive rate</span></span>
<span id="cb89-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb89-2" aria-hidden="true" tabindex="-1"></a>plt.axvline(x <span class="op">=</span> fp<span class="op">/</span>(fp<span class="op">+</span>tn), color <span class="op">=</span> <span class="st">&#39;r&#39;</span>, linestyle <span class="op">=</span> <span class="st">&#39;-&#39;</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">20</span>) <span class="co"># false positive rate</span></span>
<span id="cb89-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb89-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb89-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="MORS-ETF-2021_files/figure-html/unnamed-chunk-58-5.png" width="672" /></p>
<p>As with the precision-recall curve, it seems that we could shift the decision boundary downward to get a higher true positive rate - and, in this case, it looks like our false positive rate would barely budge, thanks to the very large number of true negatives in the dataset that we would still be predicting correctly.</p>
<p>This plot also includes the area under the curve (AUC), often referred to as the AUROC for this curve. The AUROC is, of course, independent of our specific choice of decision boundary, and it is frequently used as a metric for assessing and comparing classification models.</p>
<p>Let’s shift the decision boundary down to 0.33 and see how it changes our results.</p>
<p><strong>Caveat: tuning your decision boundary on test set predictions is generally a bad idea, because it can easily lead to overfitting to the test set. We’re doing it here for simplicity, but you’re better off dealing with unbalanced classes while training your model, and then assessing the trained models in the test set.</strong></p>
<p>As expected, we added both true and false positives, with a much higher proportional increase in the number of false positives than true positives. This doesn’t sound great, but it might be worthwhile if we care much more about detecting actual Hall of Fame inductees than we do about making a few more wrong predictions.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb90-1" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> pd.DataFrame(pipe_scale_lr_lasso.predict_proba(X_test), columns <span class="op">=</span> [<span class="st">&#39;prob_zero&#39;</span>, <span class="st">&#39;prob_one&#39;</span>])</span>
<span id="cb90-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb90-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb90-3" aria-hidden="true" tabindex="-1"></a>preds_onethird <span class="op">=</span> (probs[<span class="st">&#39;prob_one&#39;</span>] <span class="op">&gt;</span> <span class="fl">0.33</span>).astype(<span class="bu">int</span>)</span>
<span id="cb90-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb90-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-5"><a href="3-modeling-in-python-with-scikit-learn.html#cb90-5" aria-hidden="true" tabindex="-1"></a>sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, preds_onethird)</span></code></pre></div>
<pre><code>## &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x00000261DC5C6580&gt;</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb92-1" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="MORS-ETF-2021_files/figure-html/unnamed-chunk-59-7.png" width="672" /></p>
<p>Let’s also see where this puts us on our precision-recall and ROC curves. Our new decision boundary will be marked in blue.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb93-1" aria-hidden="true" tabindex="-1"></a>recall_onethird <span class="op">=</span> sklearn.metrics.recall_score(y_test, preds_onethird)</span>
<span id="cb93-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb93-3" aria-hidden="true" tabindex="-1"></a>cm_array_onethird <span class="op">=</span> sklearn.metrics.confusion_matrix(y_test, preds_onethird)</span>
<span id="cb93-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb93-4" aria-hidden="true" tabindex="-1"></a>tn_onethird <span class="op">=</span> cm_array_onethird[<span class="dv">0</span>,<span class="dv">0</span>]</span>
<span id="cb93-5"><a href="3-modeling-in-python-with-scikit-learn.html#cb93-5" aria-hidden="true" tabindex="-1"></a>fn_onethird <span class="op">=</span> cm_array_onethird[<span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb93-6"><a href="3-modeling-in-python-with-scikit-learn.html#cb93-6" aria-hidden="true" tabindex="-1"></a>fp_onethird <span class="op">=</span> cm_array_onethird[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb93-7"><a href="3-modeling-in-python-with-scikit-learn.html#cb93-7" aria-hidden="true" tabindex="-1"></a>tp_onethird <span class="op">=</span> cm_array_onethird[<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb93-8"><a href="3-modeling-in-python-with-scikit-learn.html#cb93-8" aria-hidden="true" tabindex="-1"></a>precision_onethird <span class="op">=</span> tp_onethird<span class="op">/</span>(tp_onethird <span class="op">+</span> fp_onethird)</span>
<span id="cb93-9"><a href="3-modeling-in-python-with-scikit-learn.html#cb93-9" aria-hidden="true" tabindex="-1"></a>fpr_onethird <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> tn_onethird<span class="op">/</span>(tn_onethird <span class="op">+</span> fp_onethird)</span></code></pre></div>
<p>As expected, we moved to the right on the precision-recall curve, trading precision for recall.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="co">## y_score = pipe_scale_lr_lasso.decision_function(X_test)</span></span>
<span id="cb94-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb94-2" aria-hidden="true" tabindex="-1"></a>sklearn.metrics.PrecisionRecallDisplay.from_predictions(y_test, y_score, name<span class="op">=</span><span class="st">&quot;LogisticRegressionCV&quot;</span>)</span></code></pre></div>
<pre><code>## &lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay object at 0x00000261DC91C160&gt;</code></pre>
<div class="sourceCode" id="cb96"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb96-1" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span>tp<span class="op">/</span>(tp <span class="op">+</span> fp), color<span class="op">=</span><span class="st">&#39;r&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">10</span>)</span>
<span id="cb96-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb96-2" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span>precision_onethird, color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">10</span>)</span>
<span id="cb96-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb96-3" aria-hidden="true" tabindex="-1"></a>recall_score <span class="op">=</span> sklearn.metrics.recall_score(y_test, y_pred)</span>
<span id="cb96-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb96-4" aria-hidden="true" tabindex="-1"></a>plt.axvline(x <span class="op">=</span> recall_score, color <span class="op">=</span> <span class="st">&#39;r&#39;</span>, linestyle <span class="op">=</span> <span class="st">&#39;-&#39;</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">10</span>)</span>
<span id="cb96-5"><a href="3-modeling-in-python-with-scikit-learn.html#cb96-5" aria-hidden="true" tabindex="-1"></a>plt.axvline(x <span class="op">=</span> recall_onethird, color <span class="op">=</span> <span class="st">&#39;blue&#39;</span>, linestyle <span class="op">=</span> <span class="st">&#39;-&#39;</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">10</span>)</span>
<span id="cb96-6"><a href="3-modeling-in-python-with-scikit-learn.html#cb96-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="MORS-ETF-2021_files/figure-html/unnamed-chunk-61-9.png" width="672" /></p>
<p>We also moved to the right on the ROC curve, but just barely! Our true positive rate increased quite a bit more than our false positive rate. Which of these plots and scores we care most about depends on the problem we’re trying to solve and our sensitivity to false positives and negatives.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb97-1" aria-hidden="true" tabindex="-1"></a>y_df <span class="op">=</span> pipe_scale_lr_lasso.decision_function(X_test)</span>
<span id="cb97-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb97-2" aria-hidden="true" tabindex="-1"></a>sklearn.metrics.RocCurveDisplay.from_predictions(y_test, y_df)</span></code></pre></div>
<pre><code>## &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay object at 0x00000261D83DE640&gt;</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb99-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb99-1" aria-hidden="true" tabindex="-1"></a>plt.axhline(y <span class="op">=</span> recall_score, color<span class="op">=</span><span class="st">&#39;r&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">10</span>) <span class="co"># true positive rate</span></span>
<span id="cb99-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb99-2" aria-hidden="true" tabindex="-1"></a>plt.axhline(y <span class="op">=</span> recall_onethird, color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">10</span>) <span class="co"># true positive rate</span></span>
<span id="cb99-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb99-3" aria-hidden="true" tabindex="-1"></a>plt.axvline(x <span class="op">=</span> fp<span class="op">/</span>(fp<span class="op">+</span>tn), color <span class="op">=</span> <span class="st">&#39;r&#39;</span>, linestyle <span class="op">=</span> <span class="st">&#39;-&#39;</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">20</span>) <span class="co"># false positive rate</span></span>
<span id="cb99-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb99-4" aria-hidden="true" tabindex="-1"></a>plt.axvline(x <span class="op">=</span> fpr_onethird, color <span class="op">=</span> <span class="st">&#39;blue&#39;</span>, linestyle <span class="op">=</span> <span class="st">&#39;-&#39;</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">20</span>)</span>
<span id="cb99-5"><a href="3-modeling-in-python-with-scikit-learn.html#cb99-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="MORS-ETF-2021_files/figure-html/unnamed-chunk-62-11.png" width="672" /></p>
</div>
<div id="class-weights" class="section level3" number="3.6.4">
<h3><span class="header-section-number">3.6.4</span> Class Weights</h3>
<p>If we want to avoid tuning the decision boundary directly, another option is to use the <code>class_weights</code> parameter found in many classifiers in scikit-learn. This allows us to increase the penalty for misclassifying the higher-weighted class (here, the less-frequent “inducted into Hall of Fame” class) while fitting the model. There is a “balanced” option for class weights that attempts to fully balance classes by setting class weights inversely proportional to class proportions; unfortunately, our classes are so unbalanced that this method doesn’t work on this dataset. Instead, we weight the “positive” class at four times the weight of the “negative” class.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb100-1" aria-hidden="true" tabindex="-1"></a>pipe_scale_lr_lasso_weighted <span class="op">=</span> make_pipeline(StandardScaler(), VarianceThreshold(), LogisticRegressionCV(Cs <span class="op">=</span> <span class="dv">10</span>, penalty <span class="op">=</span> <span class="st">&quot;elasticnet&quot;</span>, solver <span class="op">=</span> <span class="st">&quot;saga&quot;</span>, l1_ratios <span class="op">=</span> [<span class="fl">1.0</span>], cv <span class="op">=</span> <span class="dv">10</span>, max_iter <span class="op">=</span> <span class="dv">3000</span>, n_jobs <span class="op">=</span> <span class="dv">4</span>, class_weight <span class="op">=</span> {<span class="dv">0</span>: <span class="fl">0.2</span>, <span class="dv">1</span>: <span class="fl">0.8</span>}))</span>
<span id="cb100-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb100-2" aria-hidden="true" tabindex="-1"></a>pipe_scale_lr_lasso_weighted.fit(X_train, y_train)</span></code></pre></div>
<pre><code>## Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),
##                 (&#39;variancethreshold&#39;, VarianceThreshold()),
##                 (&#39;logisticregressioncv&#39;,
##                  LogisticRegressionCV(class_weight={0: 0.2, 1: 0.8}, cv=10,
##                                       l1_ratios=[1.0], max_iter=3000, n_jobs=4,
##                                       penalty=&#39;elasticnet&#39;, solver=&#39;saga&#39;))])</code></pre>
<p>This produces a very small improvement in accuracy.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb102-1" aria-hidden="true" tabindex="-1"></a>weighted_accuracy <span class="op">=</span> pipe_scale_lr_lasso_weighted.score(X_test, y_test)</span>
<span id="cb102-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb102-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Weighted accuracy: &quot;</span>, weighted_accuracy.<span class="bu">round</span>(<span class="dv">4</span>))</span></code></pre></div>
<pre><code>## Weighted accuracy:  0.9347</code></pre>
<div class="sourceCode" id="cb104"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Improvement: &quot;</span>, (weighted_accuracy <span class="op">-</span> pipe_scale_lr_lasso.score(X_test, y_test)).<span class="bu">round</span>(<span class="dv">4</span>))</span></code></pre></div>
<pre><code>## Improvement:  0.0056</code></pre>
<p>The difference in the confusion matrix, however, is much more noticeable: our recall is up to 0.45, although our precision has decreased as the number of false positives grows.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb106-1" aria-hidden="true" tabindex="-1"></a>y_pred_weighted <span class="op">=</span> pipe_scale_lr_lasso_weighted.predict(X_test)</span>
<span id="cb106-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb106-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb106-3" aria-hidden="true" tabindex="-1"></a>sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred_weighted)</span></code></pre></div>
<pre><code>## &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x00000261DC58B130&gt;</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb108-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb108-1" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="MORS-ETF-2021_files/figure-html/unnamed-chunk-66-13.png" width="672" /></p>
<p>Our ROC curve looks fairly similar, and our AUC remained the same.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb109-1" aria-hidden="true" tabindex="-1"></a>y_df_weighted <span class="op">=</span> pipe_scale_lr_lasso_weighted.decision_function(X_test)</span>
<span id="cb109-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb109-2" aria-hidden="true" tabindex="-1"></a>sklearn.metrics.RocCurveDisplay.from_predictions(y_test, y_df_weighted)</span></code></pre></div>
<pre><code>## &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay object at 0x00000261DCF68BE0&gt;</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb111-1" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="MORS-ETF-2021_files/figure-html/unnamed-chunk-67-15.png" width="672" /></p>
<p>Similarly, our precision-recall curve is fairly similar, with no change in average precision.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb112-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb112-1" aria-hidden="true" tabindex="-1"></a>sklearn.metrics.PrecisionRecallDisplay.from_predictions(y_test, y_df_weighted, name<span class="op">=</span><span class="st">&quot;LogisticRegressionCV Weighted&quot;</span>)</span></code></pre></div>
<pre><code>## &lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay object at 0x00000261DC95D6D0&gt;</code></pre>
<div class="sourceCode" id="cb114"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb114-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb114-1" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="MORS-ETF-2021_files/figure-html/unnamed-chunk-68-17.png" width="672" /></p>
<p>If our model accuracy metrics didn’t even change, should we have even bothered with class weights? The answer is that it depends on the problem we’re trying to solve.</p>
<p>While the baseball example we used here is clearly not relevant our work, classification problems absolutely do show up in military applications of predictive modeling and machine learning, and unbalanced classes are fairly common: for example, computer vision algorithms used to detect features such as military vehicles in satellite imagery. In many such cases, analyts do not blindly trust computer models to detect the features of interest or to make decisions about what to do in response to a feature being detected. Instead, they are used to focus human experts on a smaller subset of cases where the algorithm has signalled that there is a high probability that there is something worth looking at. In such a case, false negatives are typically a much larger issue than false positives: we want the algorithm to flag as many of the true positives as possible, even if it also produces a moderate amount of false positives that human experts have to sift through. As long as it eliminates a large number of true negatives, it still saves humans time and effort, allowing them to work more efficiently.</p>
</div>
</div>
<div id="try-another-method-random-forest" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Try Another Method: Random Forest</h2>
<p>Like tidymodels, scikit-learn makes it easy to substitute another modeling method in place of logistic regression. Here, we insert a random forest classifier, using all default values.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb115-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb115-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb115-2" aria-hidden="true" tabindex="-1"></a>pipe_scale_randomforest <span class="op">=</span> make_pipeline(StandardScaler(), VarianceThreshold(), RandomForestClassifier())</span>
<span id="cb115-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb115-3" aria-hidden="true" tabindex="-1"></a>pipe_scale_randomforest.fit(X_train, y_train)</span></code></pre></div>
<pre><code>## Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),
##                 (&#39;variancethreshold&#39;, VarianceThreshold()),
##                 (&#39;randomforestclassifier&#39;, RandomForestClassifier())])</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb117-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb117-1" aria-hidden="true" tabindex="-1"></a>pipe_scale_randomforest.score(X_test, y_test)</span></code></pre></div>
<pre><code>## 0.9414414414414415</code></pre>
<p>It looks like the random forest, with zero customization, produces a higher accuracy than either of our lasso logistic regression models.</p>
<p>The confusion matrix below shows that the random forest is clearly better than the unweighted logistic regression model, with substantially more true positives and just one more false positive. However, it still had lower recall than our class-weighted logistic regression, suggesting that it might need further tuning if we care primarily about missing as few actual positive values as possible.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb119-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb119-1" aria-hidden="true" tabindex="-1"></a>y_pred_rf <span class="op">=</span> pipe_scale_randomforest.predict(X_test)</span>
<span id="cb119-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb119-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb119-3" aria-hidden="true" tabindex="-1"></a>sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf)</span></code></pre></div>
<pre><code>## &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x00000261DD370AC0&gt;</code></pre>
<div class="sourceCode" id="cb121"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb121-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb121-1" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="MORS-ETF-2021_files/figure-html/unnamed-chunk-70-19.png" width="672" /></p>
</div>
<div id="predict-on-eligible-data" class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> Predict on Eligible Data</h2>
<p>As we did in R, let’s make predictions on the <code>eligible</code> dataset, using our class-weighted model.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb122-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb122-1" aria-hidden="true" tabindex="-1"></a>eligible <span class="op">=</span> eligible.set_index(<span class="st">&quot;player_id&quot;</span>)</span>
<span id="cb122-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb122-2" aria-hidden="true" tabindex="-1"></a>eligible_preds <span class="op">=</span> pipe_scale_lr_lasso_weighted.predict_proba(eligible.drop(<span class="st">&#39;last_year&#39;</span>, axis <span class="op">=</span> <span class="dv">1</span>)).<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb122-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb122-3" aria-hidden="true" tabindex="-1"></a>df_eligible_preds <span class="op">=</span> pd.DataFrame(eligible_preds, columns <span class="op">=</span> [<span class="st">&#39;pred_0&#39;</span>, <span class="st">&#39;pred_1&#39;</span>]).set_index(eligible.index)</span>
<span id="cb122-4"><a href="3-modeling-in-python-with-scikit-learn.html#cb122-4" aria-hidden="true" tabindex="-1"></a>elig_joined <span class="op">=</span> eligible.join(df_eligible_preds).sort_values(<span class="st">&quot;pred_1&quot;</span>, ascending <span class="op">=</span> <span class="va">False</span>).reset_index().query(<span class="st">&quot;pred_1 &gt; .4&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="3-modeling-in-python-with-scikit-learn.html#cb123-1" aria-hidden="true" tabindex="-1"></a>py<span class="sc">$</span>elig_joined <span class="sc">%&gt;%</span></span>
<span id="cb123-2"><a href="3-modeling-in-python-with-scikit-learn.html#cb123-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(player_id, pred_1, pred_0, <span class="fu">everything</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb123-3"><a href="3-modeling-in-python-with-scikit-learn.html#cb123-3" aria-hidden="true" tabindex="-1"></a>  DT<span class="sc">::</span><span class="fu">datatable</span>()</span></code></pre></div>
<div id="htmlwidget-c19b20433e565e4c88e9" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-c19b20433e565e4c88e9">{"x":{"filter":"none","vertical":false,"data":[["0","1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98"],["bondsba01","rodrial01","palmera01","pujolal01","ramirma02","jeterde01","sheffga01","thomeji01","jonesch06","sosasa01","ortizda01","beltrca01","beltrad01","damonjo01","gonzalu01","mcgrifr01","abreubo01","kentje01","cabremi01","guerrvl01","heltoto01","finlest01","delgaca01","walkela01","hunteto01","willibe02","giambja01","galaran01","konerpa01","tejadmi01","francju01","rolensc01","burksel01","jonesan01","ramirar01","rolliji01","vizquom01","leeca01","martied01","loftoke01","edmonji01","gonzaju03","anderga01","sierrru01","aloumo01","berkmla01","teixema01","gracema01","olerujo01","soriaal01","ordonma01","ibanera01","suzukic01","martiti02","hollima01","gilesbr02","dunnad01","surhobj01","willima04","youngmi02","durhara01","greensh01","grissma02","leede02","venturo01","canoro01","renteed01","gantro01","zeileto01","gonzaad01","camermi01","vaughgr01","sandere02","utleych01","dyeje01","bellja01","howarry01","castivi02","salmoti01","boonebr01","coninje01","posadjo01","grandcu01","wrighda03","crawfca02","wellsve01","garcino01","burnije01","vaughmo01","lankfra01","martivi01","kinslia01","reyesjo01","kleskry01","fieldpr01","cabreor01","braunry02","phillbr01","glaustr01"],[0.887,0.866,0.781,0.774,0.754,0.747,0.746,0.74,0.736,0.717,0.717,0.71,0.685,0.672,0.664,0.66,0.654,0.653,0.652,0.65,0.645,0.629,0.624,0.621,0.617,0.609,0.602,0.598,0.579,0.575,0.574,0.574,0.572,0.57,0.569,0.569,0.566,0.562,0.561,0.561,0.558,0.557,0.557,0.555,0.546,0.541,0.54,0.533,0.532,0.525,0.52,0.518,0.515,0.511,0.511,0.506,0.506,0.504,0.5,0.5,0.499,0.499,0.497,0.487,0.487,0.486,0.481,0.477,0.468,0.468,0.467,0.467,0.463,0.461,0.459,0.459,0.452,0.447,0.446,0.434,0.434,0.431,0.431,0.427,0.423,0.422,0.422,0.422,0.421,0.419,0.418,0.417,0.415,0.414,0.413,0.411,0.407,0.404,0.402],[0.113,0.134,0.219,0.226,0.246,0.253,0.254,0.26,0.264,0.283,0.283,0.29,0.315,0.328,0.336,0.34,0.346,0.347,0.348,0.35,0.355,0.371,0.376,0.379,0.383,0.391,0.398,0.402,0.421,0.425,0.426,0.426,0.428,0.43,0.431,0.431,0.434,0.438,0.439,0.439,0.442,0.443,0.443,0.445,0.454,0.459,0.46,0.467,0.468,0.475,0.48,0.482,0.485,0.489,0.489,0.494,0.494,0.496,0.5,0.5,0.501,0.501,0.503,0.513,0.513,0.514,0.519,0.523,0.532,0.532,0.533,0.533,0.537,0.539,0.541,0.541,0.548,0.553,0.554,0.566,0.566,0.569,0.569,0.573,0.577,0.578,0.578,0.578,0.579,0.581,0.582,0.583,0.585,0.586,0.587,0.589,0.593,0.596,0.598],[2986,2784,2831,2426,2302,2747,2576,2543,2499,2354,2408,2457,2720,2490,2591,2460,2425,2298,2096,2147,2247,2583,2035,1988,2372,2076,2260,2257,2349,2171,2527,2038,2000,2196,2194,2275,2968,2099,2055,2103,2011,1689,2228,2186,1942,1879,1862,2245,2234,1975,1848,2161,2500,2023,1773,1847,2001,2313,1866,1970,1975,1951,2165,1942,2079,1848,2152,1832,2158,1804,1955,1731,1777,1723,1763,2063,1572,1854,1672,1780,2024,1829,1649,1583,1716,1731,1434,1694,1512,1701,1733,1534,1622,1736,1611,1985,1354,1749,1537],[9847,10566,10472,9138,8244,11195,9217,8422,8984,8813,8640,9301,10295,9736,9157,8757,8480,8498,7853,8155,7962,9397,7283,6907,8857,7869,7267,8096,8393,8434,8677,7398,7232,7599,8136,9294,10586,7983,7213,8120,6858,6556,8640,8044,7037,6491,6936,8065,7592,7750,6978,7471,9689,7111,6583,6527,6883,8258,7000,7918,7408,7082,8275,6962,7064,7210,8142,6449,7573,6739,6839,6103,6241,6384,6487,7398,5707,6822,5934,6683,6957,6092,6127,5996,6655,6642,5586,5710,5532,5747,6438,6127,6823,5611,5821,7562,5249,6783,5410],[2227,2021,1663,1670,1544,1923,1636,1583,1619,1475,1419,1522,1428,1668,1412,1349,1453,1320,1321,1328,1401,1443,1241,1355,1296,1366,1227,1195,1162,1230,1285,1211,1253,1204,1098,1421,1445,1125,1219,1528,1251,1061,1084,1084,1109,1146,1099,1179,1139,1152,1076,1055,1396,1008,1104,1121,1097,1062,997,1137,1249,1129,1187,1081,1006,1065,1200,1080,986,968,1064,1017,1037,1042,984,1123,848,902,986,927,870,900,1039,949,998,930,927,917,861,968,844,1059,1075,874,862,985,879,920,889],[2935,3115,3020,2825,2574,3465,2689,2328,2726,2408,2472,2617,2942,2769,2591,2490,2470,2461,2519,2590,2519,2548,2038,2160,2452,2336,2010,2333,2340,2407,2586,2077,2107,1933,2303,2455,2877,2273,2247,2428,1949,1936,2529,2152,2134,1905,1862,2445,2239,2095,2156,2034,3030,1925,1995,1897,1631,2326,1878,2375,2054,2003,2251,1959,1885,2210,2327,1651,2004,1954,1700,1475,1666,1777,1779,1963,1475,1884,1674,1775,1982,1664,1564,1777,1931,1794,1747,1447,1620,1561,1936,1696,1972,1564,1645,2055,1597,1863,1375],[601,548,585,602,547,544,467,451,549,379,632,536,591,522,596,441,574,560,523,477,592,449,483,471,498,449,405,444,410,468,407,517,402,383,495,511,456,469,514,383,437,388,522,428,421,422,408,511,500,481,426,424,356,365,448,411,334,440,338,441,440,445,386,432,338,479,436,302,397,415,383,284,341,381,363,394,277,349,339,366,385,379,283,390,309,379,370,298,270,356,386,353,350,343,321,459,317,334,293],[77,31,38,16,20,66,27,26,38,45,19,78,36,109,68,24,59,47,17,46,37,124,18,62,39,55,9,32,8,23,54,43,63,36,24,115,77,19,15,116,25,25,36,59,39,30,18,45,13,31,21,51,96,21,32,55,10,42,35,60,79,35,56,30,14,33,29,50,23,12,59,23,60,53,25,67,21,28,24,28,36,10,89,26,123,34,52,29,10,54,3,38,121,33,10,32,43,34,10],[762,696,569,591,555,260,509,612,468,609,541,421,445,235,354,493,288,377,446,449,369,304,473,383,353,287,440,399,439,307,173,316,352,434,386,231,80,358,309,130,393,434,287,306,332,366,409,173,255,412,294,305,114,339,295,287,462,188,378,185,192,328,227,331,294,278,140,321,253,308,278,355,305,250,325,195,382,320,299,252,214,275,293,242,136,270,229,315,328,238,227,212,126,278,319,123,285,197,320],[1996,2086,1835,1817,1831,1311,1676,1699,1623,1667,1768,1536,1571,1139,1439,1550,1363,1518,1553,1496,1406,1167,1512,1311,1391,1257,1441,1425,1412,1302,1194,1287,1206,1289,1417,936,951,1363,1261,781,1199,1404,1365,1322,1287,1234,1298,1146,1230,1159,1236,1207,760,1271,1153,1078,1168,1153,1218,1030,875,1070,967,1078,1182,1086,923,1008,1110,1146,968,1072,983,977,1072,860,1194,1105,1016,1021,1071,1065,801,970,766,958,936,981,1064,874,1077,787,645,987,1028,854,937,889,950],[514,329,97,107,38,358,253,19,150,234,17,312,119,408,128,72,400,94,38,181,37,320,14,230,195,147,20,128,9,85,281,118,181,152,29,470,404,125,49,622,67,26,80,142,106,86,26,70,11,289,94,50,508,27,107,109,63,141,53,90,273,162,429,104,24,50,294,243,53,6,297,121,304,145,46,91,12,33,48,94,54,20,145,196,480,109,95,74,30,258,7,211,488,91,18,216,181,198,56],[141,76,40,41,33,97,104,20,46,107,9,49,42,103,87,38,128,60,20,94,29,118,8,76,99,87,12,81,4,38,107,49,84,59,18,105,167,47,30,160,50,19,47,52,37,48,7,48,14,84,50,29,116,20,37,45,25,84,35,30,97,52,116,48,38,38,108,102,51,6,83,59,115,20,26,60,5,43,42,53,29,21,44,65,109,37,31,58,18,117,7,58,119,41,11,57,50,76,29],[2558,1338,1353,1214,1329,1082,1475,1747,1512,929,1319,1051,775,1003,1155,1305,1476,801,1011,737,1335,844,1109,913,661,1069,1366,583,921,553,917,899,793,891,633,813,1028,655,1283,945,998,457,429,610,737,1201,918,1075,1275,496,651,713,626,780,744,1183,1317,640,469,575,820,744,553,874,1075,501,718,770,945,751,867,865,674,675,597,853,709,423,970,552,671,936,758,761,377,472,403,739,725,828,662,579,517,817,847,514,473,395,854],[1539,2287,1348,1053,1813,1840,1171,2548,1409,2306,1750,1693,1584,1257,1218,1882,1840,1522,1516,985,1175,1299,1745,1231,1741,1212,1572,2003,1391,1079,1341,1410,1340,1748,1238,1264,1087,984,1202,1016,1729,1273,1224,1239,894,1300,1441,642,1016,1803,852,1374,1037,1069,1230,835,2379,839,1363,1235,1201,1315,1240,1622,1179,964,1182,1411,1279,1324,1901,1513,1614,1102,1308,1443,1843,1069,1360,1295,1168,1453,1589,1292,1067,956,554,1376,1429,1550,779,842,796,1077,1155,745,1070,1012,1377],[2007,2016,2005,2016,2011,2014,2009,2012,2012,2007,2016,2016,2016,2012,2008,2004,2014,2008,2016,2011,2013,2007,2009,2005,2015,2006,2014,2004,2014,2013,2007,2012,2004,2012,2015,2016,2012,2012,2004,2007,2010,2005,2010,2006,2008,2013,2016,2003,2005,2014,2011,2014,2016,2005,2016,2009,2014,2005,2003,2013,2008,2007,2005,2011,2004,2016,2011,2003,2004,2016,2011,2003,2007,2016,2009,2003,2016,2006,2006,2005,2007,2011,2016,2016,2016,2013,2009,2006,2003,2004,2016,2016,2016,2007,2016,2011,2016,2016,2010]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>player_id<\/th>\n      <th>pred_1<\/th>\n      <th>pred_0<\/th>\n      <th>g<\/th>\n      <th>ab<\/th>\n      <th>r<\/th>\n      <th>h<\/th>\n      <th>x2b<\/th>\n      <th>x3b<\/th>\n      <th>hr<\/th>\n      <th>rbi<\/th>\n      <th>sb<\/th>\n      <th>cs<\/th>\n      <th>bb<\/th>\n      <th>so<\/th>\n      <th>last_year<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="conclusion" class="section level2" number="3.9">
<h2><span class="header-section-number">3.9</span> Conclusion</h2>
<p>For most predictive modeling tasks - especially if you are not trying to use any cutting-edge methods or dealing with very large datasets - there is no clear winner between R’s tidymodels or Python’s scikit-learn. They both support a very wide range of machine learning methods and make it relatively easy to optimize hyperparameters, evaluate model performance, and compare model specifications. There are differences in their user interfaces and in the ways that analysts construct modeling pipelines, but which one makes more sense to each user will likely come down to personal preference and familiarity with the language.</p>
<p>While building this tutorial, however, one noticeable advantage for scikit-learn did become clear: it has a very extensive and detailed user guide, full of easy-to-follow examples, arguably making it easier to learn. The tidymodels framework may close this gap over time, but the wealth of existing scikit-learn documentation and the fact that the R predictive modeling landscape is relatively fragmented and has not fully coalesced around tidymodels could slow that down.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-modeling-in-r-with-tidymodels.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

</body>

</html>
