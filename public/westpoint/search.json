[
  {
    "objectID": "MA206-AY26-1/lesson-9.html",
    "href": "MA206-AY26-1/lesson-9.html",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\\[\nF_X(x)=P(X\\le x)=\n\\begin{cases}\n0, & x&lt;1, \\\\[6pt]\n\\dfrac{1}{6}, & 1\\le x&lt;2, \\\\[6pt]\n\\dfrac{2}{6}, & 2\\le x&lt;3, \\\\[6pt]\n\\dfrac{3}{6}, & 3\\le x&lt;4, \\\\[6pt]\n\\dfrac{4}{6}, & 4\\le x&lt;5, \\\\[6pt]\n\\dfrac{5}{6}, & 5\\le x&lt;6, \\\\[6pt]\n1, & x\\ge 6~.\n\\end{cases}\n\\]\n\n\n\n\n\\(P(X=2)\\)\n\\[\nP(X=2) = \\dfrac{1}{6}\n\\]\n\\(P(X \\le 4)\\)\n\\[\nP(X \\le 4) = P(X=1)+P(X=2)+P(X=3)+P(X=4)\n= \\dfrac{1}{6}+\\dfrac{1}{6}+\\dfrac{1}{6}+\\dfrac{1}{6}\n= \\dfrac{4}{6} = \\dfrac{2}{3}\n\\] This is an unnamed distribution.\nA named distribution is a probability distribution with a standard formula and notation that’s been given a name because it occurs so often in applications.\n\n\n\nSuppose a basketball player makes a free throw with probability \\(p=0.7\\) (and misses with probability \\(0.3\\)).\nIf they shoot 4 times, what is the probability of exactly 2 made shots?\n\nOne sequence: Make, Make, Miss, Miss.\nProbability \\(= 0.7 \\times 0.7 \\times 0.3 \\times 0.3\\).\nBut there are many possible sequences with 2 makes and 2 misses.\nThe number of such sequences is\n\\[\n\\binom{4}{2} = 6.\n\\]\n\nSo\n\\[\nP(\\text{2 makes in 4 shots}) = \\binom{4}{2}(0.7^2)(0.3^2).\n\\]\n\n\n\nIf \\(X\\) is the number of successes in \\(n\\) independent trials, each with success probability \\(p\\), then\n\\[\nP(X = x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x=0,1,\\dots,n.\n\\]\nWe write this as\n\\[\nX \\sim \\text{Binomial}(n,p).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Binomial(4, 0.7): distribution of made shots in 4 attempts\n\n\n\n\n\n\n\n\nFor \\(X \\sim \\mathrm{Binomial}(n,p)\\):\n- \\(\\mathbb{E}[X] = np\\)\n- \\(\\mathrm{Var}(X) = np(1-p)\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{np(1-p)}\\)\nFor \\(n=4\\), \\(p=0.7\\):\n- \\(\\mathbb{E}[X] = 4(0.7) = 2.8\\)\n- \\(\\mathrm{Var}(X) = 4(0.7)(0.3) = 0.84\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{0.84} \\approx 0.917\\)\n\\[\nP(X = x) = \\binom{n}{x} p^x (1-p)^{\\,n-x}, \\quad x=0,1,\\dots,n.\n\\]\n\n\n\n\n\\(P(X=2)\\)\n\\[\n\\binom{4}{2}(0.7)^2(0.3)^2\n\\]\n\n\ndbinom(2, size = 4, prob = .7)\n\n[1] 0.2646\n\n\n\n\\(P(X \\le 2)\\)\n\\[\n\\sum_{x=0}^{2} \\binom{4}{x}(0.7)^x(0.3)^{4-x}\n\\]\n\n\npbinom(2, size = 4, prob = .7)\n\n[1] 0.3483\n\n\n\n\\(P(X &gt; 2)\\) \\[\n1 - P(X \\le 2)\n\\]\n\n\n1 - pbinom(2, size = 4, prob = .7)\n\n[1] 0.6517\n\n\n\n\n\n\nSuppose a basketball player makes a free throw with probability \\(p=0.7\\) (and misses with probability \\(0.3\\)).\nLet \\(X\\) be the number of failures before the first success. Then:\n- \\(X=0\\): success on the first shot.\n- \\(X=2\\): two misses, then a make on the 3rd shot.\nFor example, the probability of \\(X=2\\) is \\[\nP(X=2) = (1-p)^2\\,p = (0.3)^2(0.7).\n\\]\n\n\n\nIf \\(X\\) is the number of failures before the first success in independent trials, then \\[\nP(X = x) = (1-p)^x\\,p, \\quad x=0,1,2,\\dots\n\\]\nWe write this as \\[\nX \\sim \\mathrm{Geometric}(p).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Geometric(\\(p=0.7\\)): probability of \\(x\\) failures before first success\n\n\n\n\n\n\n\n\n\nFor \\(X \\sim \\mathrm{Geometric}(p)\\):\n- \\(\\mathbb{E}[X] = \\dfrac{1-p}{p}\\)\n- \\(\\mathrm{Var}(X) = \\dfrac{1-p}{p^2}\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{\\dfrac{1-p}{p^2}}\\)\nFor \\(p=0.7\\):\n- \\(\\mathbb{E}[X] = \\dfrac{0.3}{0.7} \\approx 0.429\\)\n- \\(\\mathrm{Var}(X) = \\dfrac{0.3}{0.49} \\approx 0.612\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{0.612} \\approx 0.782\\)\n\n\n\n\n\n\\(P(X=2)\\) \\[\n(1-p)^2\\,p\n\\]\n\n\ndgeom(2, prob = 0.7)\n\n[1] 0.063\n\n\n\n\\(P(X \\le 2)\\) \\[\n\\sum_{x=0}^{2} (1-p)^x\\,p\n\\]\n\n\npgeom(2, prob = 0.7)\n\n[1] 0.973\n\n\n\n\\(P(X &gt; 2)\\) \\[\n1 - P(X \\le 2) = (1-p)^3\n\\]\n\n\n1 - pgeom(2, prob = 0.7)\n\n[1] 0.027\n\n\n\n\n\n\n\n\n\n\n\\[\nf(x) =\n\\begin{cases}\n2(1-x), & 0 \\le x \\le 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\\[\nF_X(x) =\n\\begin{cases}\n0, & x &lt; 0, \\\\[6pt]\n2x - x^2, & 0 \\le x \\le 1, \\\\[6pt]\n1, & x &gt; 1~.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\\(P(X \\le 0.5)\\)\n\\[\nF_X(0.5) = 2(0.5) - (0.5)^2 = 1 - 0.25 = 0.75\n\\]\n\\(P(0.25 \\le X \\le 0.75)\\)\n\\[\nF_X(0.75) - F_X(0.25) = (2(0.75) - 0.75^2) - (2(0.25) - 0.25^2) = 0.9375 - 0.4375 = 0.5\n\\]\n\n\n\n\nSuppose the heights of adult men in a population are approximately Normal with mean \\(\\mu = 68\\) inches and standard deviation \\(\\sigma = 3\\) inches.\nWhat is the probability that a randomly chosen man is taller than 72 inches?\nWe compute this directly from the Normal distribution:\n\\[\nP(X &gt; 72) = \\int_{72}^{\\infty} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx.\n\\]\n\n\n\nIf \\(X\\) is a continuous random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right), \\quad -\\infty &lt; x &lt; \\infty.\n\\]\nWe write this as\n\\[\nX \\sim \\text{Normal}(\\mu,\\sigma^2).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Normal(68, 3): density with shading for P(X &gt; 72)\n\n\n\n\n\n\n\n\n\nFor \\(X \\sim \\mathrm{Normal}(\\mu,\\sigma^2)\\):\n- \\(\\mathbb{E}[X] = \\mu\\)\n- \\(\\mathrm{Var}(X) = \\sigma^2\\)\n- \\(\\mathrm{SD}(X) = \\sigma\\)\nFor \\(\\mu=68\\), \\(\\sigma=3\\):\n- \\(\\mathbb{E}[X] = 68\\)\n- \\(\\mathrm{Var}(X) = 9\\)\n- \\(\\mathrm{SD}(X) = 3\\)\n\n\n\n\n\n\\(P(X=70)\\)\n\\[\nP(X=70) = 0 \\quad \\text{(point probability in a continuous distribution is zero)}\n\\]\n\n\n\n\\(P(X \\le 70)\\)\n\\[\nP(X \\le 70) = \\int_{-\\infty}^{70} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx\n\\]\n\n\npnorm(70, mean = 68, sd = 3)\n\n[1] 0.7475075\n\n\n\n\n\\(P(X &gt; 72)\\)\n\\[\nP(X &gt; 72) = \\int_{72}^{\\infty} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx\n\\]\n\n\n1 - pnorm(72, mean = 68, sd = 3)\n\n[1] 0.09121122\n\n\n\n\n\\(P(65 \\le X \\le 70)\\)\n\\[\nP(65 \\le X \\le 70) = \\int_{65}^{70} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx\n\\]\n\n\npnorm(70, mean = 68, sd = 3) - pnorm(65, mean = 68, sd = 3)\n\n[1] 0.5888522\n\n\n\n\n\n\n\n\nA soldier hits a target with probability \\(p=0.6\\) on each shot, independently of other shots. Suppose they fire \\(n=5\\) times. Let \\(X\\) be the number of hits.\n\nWrite the distribution of \\(X\\).\n\nFind \\(\\mathbb{E}[X]\\), \\(\\mathrm{Var}(X)\\), and \\(\\mathrm{SD}(X)\\).\n\nCompute:\n\n\\(P(X=3)\\)\n\n\\(P(X \\le 2)\\)\n\n\\(P(X &lt; 2)\\)\n\n\\(P(X &gt; 4)\\)\n\n\\(P(X \\ge 4)\\)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(X \\sim \\mathrm{Binomial}(n=5, p=0.6)\\)\n\\(\\mathbb{E}[X] = np = 5(0.6) = 3\\)\n\\(\\mathrm{Var}(X) = np(1-p) = 5(0.6)(0.4) = 1.2\\)\n\\(\\mathrm{SD}(X) = \\sqrt{1.2} \\approx 1.095\\)\nProbabilities:\n\n\ndbinom(3, size=5, prob=0.6)        # P(X=3)\n\n[1] 0.3456\n\npbinom(2, size=5, prob=0.6)        # P(X ≤ 2)\n\n[1] 0.31744\n\npbinom(1, size=5, prob=0.6)        # P(X ≤ 2)\n\n[1] 0.08704\n\n1 - pbinom(4, size=5, prob=0.6)    # P(X &gt; 4)\n\n[1] 0.07776\n\n1 - pbinom(3, size=5, prob=0.6)    # P(X &gt; 4)\n\n[1] 0.33696\n\n\n\n\n\n\n\n\n\nA cadet passes a marksmanship test with probability \\(p=0.4\\) on each attempt, independently of other attempts. Let \\(Y\\) be the number of failures before the first pass.\n\nWrite the distribution of \\(Y\\).\n\nFind \\(\\mathbb{E}[Y]\\), \\(\\mathrm{Var}(Y)\\), and \\(\\mathrm{SD}(Y)\\).\n\nCompute:\n\n\\(P(Y=2)\\)\n\n\\(P(Y \\le 3)\\)\n\n\\(P(Y &gt; 4)\\)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(Y \\sim \\mathrm{Geometric}(p=0.4)\\), where \\(Y =\\) failures before first success\n\\(\\mathbb{E}[Y] = \\dfrac{1-p}{p} = \\dfrac{0.6}{0.4} = 1.5\\)\n\\(\\mathrm{Var}(Y) = \\dfrac{1-p}{p^2} = \\dfrac{0.6}{0.16} = 3.75\\)\n\\(\\mathrm{SD}(Y) = \\sqrt{3.75} \\approx 1.936\\)\nProbabilities:\n\n\ndgeom(2, prob=0.4)        # P(Y=2)\n\n[1] 0.144\n\npgeom(3, prob=0.4)        # P(Y ≤ 3)\n\n[1] 0.8704\n\n1 - pgeom(4, prob=0.4)    # P(Y &gt; 4)\n\n[1] 0.07776\n\n\n\n\n\n\n\n\n\nSuppose the times (in minutes) to complete a training run are approximately Normal with mean \\(\\mu=50\\) and standard deviation \\(\\sigma=8.\\) Let \\(Z\\) be the time for a randomly chosen runner.\n\nWrite the distribution of \\(Z\\).\n\nFind \\(\\mathbb{E}[Z]\\), \\(\\mathrm{Var}(Z)\\), and \\(\\mathrm{SD}(Z)\\).\n\nCompute:\n\n\\(P(Z \\le 45)\\)\n\n\\(P(Z &gt; 60)\\)\n\n\\(P(40 \\le Z \\le 55)\\)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(Z \\sim \\mathrm{Normal}(\\mu=50, \\sigma^2=64)\\)\n\\(\\mathbb{E}[Z] = \\mu = 50\\)\n\\(\\mathrm{Var}(Z) = \\sigma^2 = 64\\)\n\\(\\mathrm{SD}(Z) = \\sigma = 8\\)\nProbabilities:\n\n\npnorm(45, mean=50, sd=8)                         # P(Z ≤ 45)\n\n[1] 0.2659855\n\n1 - pnorm(60, mean=50, sd=8)                     # P(Z &gt; 60)\n\n[1] 0.1056498\n\npnorm(55, mean=50, sd=8) - pnorm(40, mean=50, sd=8)   # P(40 ≤ Z ≤ 55)\n\n[1] 0.6283647\n\n\n\n\n\n\n\n\nLet the random variable \\(X\\) denote the time (in minutes) a patient waits at a doctor’s office before being called.\n\\[\nf_X(x)=\n\\begin{cases}\n\\dfrac{1}{2}, & 2 \\le x &lt; 3, \\\\[6pt]\n\\dfrac{x}{16}, & 3 \\le x \\le 5, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nIs \\(X\\) a discrete or continuous random variable? Sketch the PDF and label the axes.\n\nFind the cumulative distribution function (CDF), \\(F_X(x)\\), in curly-brace notation.\n\nCompute \\(P(X \\le 2.6)\\).\n\nCompute \\(P(2.5 \\le X \\le 4)\\).\n\nVerify that this is a valid probability distribution.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nVerify Valid PDF \\[\n\\int_{2}^{3}\\frac{1}{2}\\,dx + \\int_{3}^{5}\\frac{x}{16}\\,dx\n= \\frac{1}{2} + \\frac{1}{16}\\cdot \\frac{25-9}{2}\n= \\frac{1}{2} + \\frac{16}{32}\n= \\frac{1}{2} + \\frac{1}{2}\n= 1~.\n\\]\n\n\n\n\\(X\\) is continuous. The PDF is flat at height \\(1/2\\) on \\([2,3)\\), then jumps down to \\(f(3)=3/16\\) and increases linearly to \\(f(5)=5/16\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFrom PDF to CDF\n\nWe want the cumulative distribution function (CDF):\n\\[\nF_X(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f_X(t)\\,dt\n\\]\nwhere the PDF is\n\\[\nf_X(t) =\n\\begin{cases}\n\\dfrac{1}{2}, & 2 \\le t &lt; 3, \\\\[6pt]\n\\dfrac{t}{16}, & 3 \\le t \\le 5, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\nNo probability has accumulated yet:\n\\[\nF_X(x) = 0.\n\\]\n\n\n\nWe integrate from 2 up to \\(x\\):\n\\[\nF_X(x) = \\int_{2}^{x} \\frac{1}{2}\\,dt\n= \\left[\\frac{t}{2}\\right]_{t=2}^{t=x}\n= \\frac{x}{2} - \\frac{2}{2}\n= \\frac{x}{2} - 1.\n\\]\n\n\n\nWe add all mass up to 3 plus the integral from 3 to \\(x\\):\n\\[\nF_X(x) = \\int_{2}^{3} \\frac{1}{2}\\,dt \\;+\\; \\int_{3}^{x} \\frac{t}{16}\\,dt.\n\\]\nFirst part:\n\\[\n\\int_{2}^{3} \\frac{1}{2}\\,dt\n= \\left[\\frac{t}{2}\\right]_{2}^{3}\n= \\frac{3}{2} - \\frac{2}{2}\n= 0.5.\n\\]\nSecond part (leaving the constant inside):\n\\[\n\\int_{3}^{x} \\frac{t}{16}\\,dt\n= \\left[\\frac{t^2}{32}\\right]_{t=3}^{t=x}\n= \\frac{x^2}{32} - \\frac{9}{32}\n= \\frac{x^2 - 9}{32}.\n\\]\nSo together:\n\\[\nF_X(x) = 0.5 + \\frac{x^2 - 9}{32}\n= \\frac{16}{32} + \\frac{x^2 - 9}{32}\n= \\frac{x^2 + 7}{32}.\n\\]\n\n\n\nAll probability mass is included:\n\\[\nF_X(x) = 1.\n\\]\n\n\n\n\\[\nF_X(x) =\n\\begin{cases}\n0, & x &lt; 2, \\\\[6pt]\n\\dfrac{x}{2} - 1, & 2 \\le x &lt; 3, \\\\[6pt]\n\\dfrac{x^2 + 7}{32}, & 3 \\le x \\le 5, \\\\[6pt]\n1, & x &gt; 5~.\n\\end{cases}\n\\]\n\n\nSince \\(2.6 \\in [2,3)\\), \\[\nP(X \\le 2.6)=F_X(2.6)=\\frac{2.6-2}{2}=0.3.\n\\]\n\n\n\nSplit at \\(3\\): \\[\n\\begin{aligned}\nP(2.5 \\le X \\le 4)\n&= \\int_{2.5}^{3} \\frac{1}{2}\\,dx + \\int_{3}^{4} \\frac{x}{16}\\,dx \\\\[6pt]\n&= \\frac{1}{2}(0.5) + \\frac{1}{16}\\cdot \\frac{4^2-3^2}{2} \\\\[6pt]\n&= 0.25 + \\frac{1}{16}\\cdot \\frac{16-9}{2}\n= 0.25 + \\frac{1}{16}\\cdot \\frac{7}{2}\n= 0.25 + \\frac{7}{32}\n= \\frac{8}{32} + \\frac{7}{32}\n= \\frac{15}{32}\n\\approx 0.46875~.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\nResources:\n\nComputer with blank script in RStudio\nCourse Guide\nTidyverse Tutorial\nOne Page (Front and Back) of personally handwritten notes\nIssued Calculator\n\n\n\n\n\n\nVenn Diagrams\n\nDefinitions of Union/Intersection\n\nAddition Rule\n\nComplement Rule\n\nConditional Probability\n\nMultiplication Rule\n\nLaw of Total Probability\n\nBayes’ Theorem\n\nTree Diagrams\n\nRules of Independence and Mutually Exclusive Events\n\nTake Scenarios and Answer Questions Related to Probability\n\n\n\n\n\nIdentify appropriate distribution (named or unnamed)\n\nGo from PDF to CDF for unnamed distributions\n\nFind probabilities\n\nFind expected values\n\nFind variances\n\nLinear Transformation Rules (Adding/Subtracting Expected Values and Variances)\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#discrete-distribution-reminders",
    "href": "MA206-AY26-1/lesson-9.html#discrete-distribution-reminders",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\\[\nF_X(x)=P(X\\le x)=\n\\begin{cases}\n0, & x&lt;1, \\\\[6pt]\n\\dfrac{1}{6}, & 1\\le x&lt;2, \\\\[6pt]\n\\dfrac{2}{6}, & 2\\le x&lt;3, \\\\[6pt]\n\\dfrac{3}{6}, & 3\\le x&lt;4, \\\\[6pt]\n\\dfrac{4}{6}, & 4\\le x&lt;5, \\\\[6pt]\n\\dfrac{5}{6}, & 5\\le x&lt;6, \\\\[6pt]\n1, & x\\ge 6~.\n\\end{cases}\n\\]\n\n\n\n\n\\(P(X=2)\\)\n\\[\nP(X=2) = \\dfrac{1}{6}\n\\]\n\\(P(X \\le 4)\\)\n\\[\nP(X \\le 4) = P(X=1)+P(X=2)+P(X=3)+P(X=4)\n= \\dfrac{1}{6}+\\dfrac{1}{6}+\\dfrac{1}{6}+\\dfrac{1}{6}\n= \\dfrac{4}{6} = \\dfrac{2}{3}\n\\] This is an unnamed distribution.\nA named distribution is a probability distribution with a standard formula and notation that’s been given a name because it occurs so often in applications.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#the-binomial-distribution",
    "href": "MA206-AY26-1/lesson-9.html#the-binomial-distribution",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "Suppose a basketball player makes a free throw with probability \\(p=0.7\\) (and misses with probability \\(0.3\\)).\nIf they shoot 4 times, what is the probability of exactly 2 made shots?\n\nOne sequence: Make, Make, Miss, Miss.\nProbability \\(= 0.7 \\times 0.7 \\times 0.3 \\times 0.3\\).\nBut there are many possible sequences with 2 makes and 2 misses.\nThe number of such sequences is\n\\[\n\\binom{4}{2} = 6.\n\\]\n\nSo\n\\[\nP(\\text{2 makes in 4 shots}) = \\binom{4}{2}(0.7^2)(0.3^2).\n\\]\n\n\n\nIf \\(X\\) is the number of successes in \\(n\\) independent trials, each with success probability \\(p\\), then\n\\[\nP(X = x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x=0,1,\\dots,n.\n\\]\nWe write this as\n\\[\nX \\sim \\text{Binomial}(n,p).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Binomial(4, 0.7): distribution of made shots in 4 attempts\n\n\n\n\n\n\n\n\nFor \\(X \\sim \\mathrm{Binomial}(n,p)\\):\n- \\(\\mathbb{E}[X] = np\\)\n- \\(\\mathrm{Var}(X) = np(1-p)\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{np(1-p)}\\)\nFor \\(n=4\\), \\(p=0.7\\):\n- \\(\\mathbb{E}[X] = 4(0.7) = 2.8\\)\n- \\(\\mathrm{Var}(X) = 4(0.7)(0.3) = 0.84\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{0.84} \\approx 0.917\\)\n\\[\nP(X = x) = \\binom{n}{x} p^x (1-p)^{\\,n-x}, \\quad x=0,1,\\dots,n.\n\\]\n\n\n\n\n\\(P(X=2)\\)\n\\[\n\\binom{4}{2}(0.7)^2(0.3)^2\n\\]\n\n\ndbinom(2, size = 4, prob = .7)\n\n[1] 0.2646\n\n\n\n\\(P(X \\le 2)\\)\n\\[\n\\sum_{x=0}^{2} \\binom{4}{x}(0.7)^x(0.3)^{4-x}\n\\]\n\n\npbinom(2, size = 4, prob = .7)\n\n[1] 0.3483\n\n\n\n\\(P(X &gt; 2)\\) \\[\n1 - P(X \\le 2)\n\\]\n\n\n1 - pbinom(2, size = 4, prob = .7)\n\n[1] 0.6517",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#the-geometric-distribution",
    "href": "MA206-AY26-1/lesson-9.html#the-geometric-distribution",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "Suppose a basketball player makes a free throw with probability \\(p=0.7\\) (and misses with probability \\(0.3\\)).\nLet \\(X\\) be the number of failures before the first success. Then:\n- \\(X=0\\): success on the first shot.\n- \\(X=2\\): two misses, then a make on the 3rd shot.\nFor example, the probability of \\(X=2\\) is \\[\nP(X=2) = (1-p)^2\\,p = (0.3)^2(0.7).\n\\]\n\n\n\nIf \\(X\\) is the number of failures before the first success in independent trials, then \\[\nP(X = x) = (1-p)^x\\,p, \\quad x=0,1,2,\\dots\n\\]\nWe write this as \\[\nX \\sim \\mathrm{Geometric}(p).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Geometric(\\(p=0.7\\)): probability of \\(x\\) failures before first success\n\n\n\n\n\n\n\n\n\nFor \\(X \\sim \\mathrm{Geometric}(p)\\):\n- \\(\\mathbb{E}[X] = \\dfrac{1-p}{p}\\)\n- \\(\\mathrm{Var}(X) = \\dfrac{1-p}{p^2}\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{\\dfrac{1-p}{p^2}}\\)\nFor \\(p=0.7\\):\n- \\(\\mathbb{E}[X] = \\dfrac{0.3}{0.7} \\approx 0.429\\)\n- \\(\\mathrm{Var}(X) = \\dfrac{0.3}{0.49} \\approx 0.612\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{0.612} \\approx 0.782\\)\n\n\n\n\n\n\\(P(X=2)\\) \\[\n(1-p)^2\\,p\n\\]\n\n\ndgeom(2, prob = 0.7)\n\n[1] 0.063\n\n\n\n\\(P(X \\le 2)\\) \\[\n\\sum_{x=0}^{2} (1-p)^x\\,p\n\\]\n\n\npgeom(2, prob = 0.7)\n\n[1] 0.973\n\n\n\n\\(P(X &gt; 2)\\) \\[\n1 - P(X \\le 2) = (1-p)^3\n\\]\n\n\n1 - pgeom(2, prob = 0.7)\n\n[1] 0.027",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#continuous-distribution-reminder",
    "href": "MA206-AY26-1/lesson-9.html#continuous-distribution-reminder",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "\\[\nf(x) =\n\\begin{cases}\n2(1-x), & 0 \\le x \\le 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\\[\nF_X(x) =\n\\begin{cases}\n0, & x &lt; 0, \\\\[6pt]\n2x - x^2, & 0 \\le x \\le 1, \\\\[6pt]\n1, & x &gt; 1~.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\\(P(X \\le 0.5)\\)\n\\[\nF_X(0.5) = 2(0.5) - (0.5)^2 = 1 - 0.25 = 0.75\n\\]\n\\(P(0.25 \\le X \\le 0.75)\\)\n\\[\nF_X(0.75) - F_X(0.25) = (2(0.75) - 0.75^2) - (2(0.25) - 0.25^2) = 0.9375 - 0.4375 = 0.5\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#the-normal-distribution",
    "href": "MA206-AY26-1/lesson-9.html#the-normal-distribution",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "Suppose the heights of adult men in a population are approximately Normal with mean \\(\\mu = 68\\) inches and standard deviation \\(\\sigma = 3\\) inches.\nWhat is the probability that a randomly chosen man is taller than 72 inches?\nWe compute this directly from the Normal distribution:\n\\[\nP(X &gt; 72) = \\int_{72}^{\\infty} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx.\n\\]\n\n\n\nIf \\(X\\) is a continuous random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right), \\quad -\\infty &lt; x &lt; \\infty.\n\\]\nWe write this as\n\\[\nX \\sim \\text{Normal}(\\mu,\\sigma^2).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Normal(68, 3): density with shading for P(X &gt; 72)\n\n\n\n\n\n\n\n\n\nFor \\(X \\sim \\mathrm{Normal}(\\mu,\\sigma^2)\\):\n- \\(\\mathbb{E}[X] = \\mu\\)\n- \\(\\mathrm{Var}(X) = \\sigma^2\\)\n- \\(\\mathrm{SD}(X) = \\sigma\\)\nFor \\(\\mu=68\\), \\(\\sigma=3\\):\n- \\(\\mathbb{E}[X] = 68\\)\n- \\(\\mathrm{Var}(X) = 9\\)\n- \\(\\mathrm{SD}(X) = 3\\)\n\n\n\n\n\n\\(P(X=70)\\)\n\\[\nP(X=70) = 0 \\quad \\text{(point probability in a continuous distribution is zero)}\n\\]\n\n\n\n\\(P(X \\le 70)\\)\n\\[\nP(X \\le 70) = \\int_{-\\infty}^{70} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx\n\\]\n\n\npnorm(70, mean = 68, sd = 3)\n\n[1] 0.7475075\n\n\n\n\n\\(P(X &gt; 72)\\)\n\\[\nP(X &gt; 72) = \\int_{72}^{\\infty} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx\n\\]\n\n\n1 - pnorm(72, mean = 68, sd = 3)\n\n[1] 0.09121122\n\n\n\n\n\\(P(65 \\le X \\le 70)\\)\n\\[\nP(65 \\le X \\le 70) = \\int_{65}^{70} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx\n\\]\n\n\npnorm(70, mean = 68, sd = 3) - pnorm(65, mean = 68, sd = 3)\n\n[1] 0.5888522",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#practice-problems",
    "href": "MA206-AY26-1/lesson-9.html#practice-problems",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "A soldier hits a target with probability \\(p=0.6\\) on each shot, independently of other shots. Suppose they fire \\(n=5\\) times. Let \\(X\\) be the number of hits.\n\nWrite the distribution of \\(X\\).\n\nFind \\(\\mathbb{E}[X]\\), \\(\\mathrm{Var}(X)\\), and \\(\\mathrm{SD}(X)\\).\n\nCompute:\n\n\\(P(X=3)\\)\n\n\\(P(X \\le 2)\\)\n\n\\(P(X &lt; 2)\\)\n\n\\(P(X &gt; 4)\\)\n\n\\(P(X \\ge 4)\\)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(X \\sim \\mathrm{Binomial}(n=5, p=0.6)\\)\n\\(\\mathbb{E}[X] = np = 5(0.6) = 3\\)\n\\(\\mathrm{Var}(X) = np(1-p) = 5(0.6)(0.4) = 1.2\\)\n\\(\\mathrm{SD}(X) = \\sqrt{1.2} \\approx 1.095\\)\nProbabilities:\n\n\ndbinom(3, size=5, prob=0.6)        # P(X=3)\n\n[1] 0.3456\n\npbinom(2, size=5, prob=0.6)        # P(X ≤ 2)\n\n[1] 0.31744\n\npbinom(1, size=5, prob=0.6)        # P(X ≤ 2)\n\n[1] 0.08704\n\n1 - pbinom(4, size=5, prob=0.6)    # P(X &gt; 4)\n\n[1] 0.07776\n\n1 - pbinom(3, size=5, prob=0.6)    # P(X &gt; 4)\n\n[1] 0.33696\n\n\n\n\n\n\n\n\n\nA cadet passes a marksmanship test with probability \\(p=0.4\\) on each attempt, independently of other attempts. Let \\(Y\\) be the number of failures before the first pass.\n\nWrite the distribution of \\(Y\\).\n\nFind \\(\\mathbb{E}[Y]\\), \\(\\mathrm{Var}(Y)\\), and \\(\\mathrm{SD}(Y)\\).\n\nCompute:\n\n\\(P(Y=2)\\)\n\n\\(P(Y \\le 3)\\)\n\n\\(P(Y &gt; 4)\\)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(Y \\sim \\mathrm{Geometric}(p=0.4)\\), where \\(Y =\\) failures before first success\n\\(\\mathbb{E}[Y] = \\dfrac{1-p}{p} = \\dfrac{0.6}{0.4} = 1.5\\)\n\\(\\mathrm{Var}(Y) = \\dfrac{1-p}{p^2} = \\dfrac{0.6}{0.16} = 3.75\\)\n\\(\\mathrm{SD}(Y) = \\sqrt{3.75} \\approx 1.936\\)\nProbabilities:\n\n\ndgeom(2, prob=0.4)        # P(Y=2)\n\n[1] 0.144\n\npgeom(3, prob=0.4)        # P(Y ≤ 3)\n\n[1] 0.8704\n\n1 - pgeom(4, prob=0.4)    # P(Y &gt; 4)\n\n[1] 0.07776\n\n\n\n\n\n\n\n\n\nSuppose the times (in minutes) to complete a training run are approximately Normal with mean \\(\\mu=50\\) and standard deviation \\(\\sigma=8.\\) Let \\(Z\\) be the time for a randomly chosen runner.\n\nWrite the distribution of \\(Z\\).\n\nFind \\(\\mathbb{E}[Z]\\), \\(\\mathrm{Var}(Z)\\), and \\(\\mathrm{SD}(Z)\\).\n\nCompute:\n\n\\(P(Z \\le 45)\\)\n\n\\(P(Z &gt; 60)\\)\n\n\\(P(40 \\le Z \\le 55)\\)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(Z \\sim \\mathrm{Normal}(\\mu=50, \\sigma^2=64)\\)\n\\(\\mathbb{E}[Z] = \\mu = 50\\)\n\\(\\mathrm{Var}(Z) = \\sigma^2 = 64\\)\n\\(\\mathrm{SD}(Z) = \\sigma = 8\\)\nProbabilities:\n\n\npnorm(45, mean=50, sd=8)                         # P(Z ≤ 45)\n\n[1] 0.2659855\n\n1 - pnorm(60, mean=50, sd=8)                     # P(Z &gt; 60)\n\n[1] 0.1056498\n\npnorm(55, mean=50, sd=8) - pnorm(40, mean=50, sd=8)   # P(40 ≤ Z ≤ 55)\n\n[1] 0.6283647\n\n\n\n\n\n\n\n\nLet the random variable \\(X\\) denote the time (in minutes) a patient waits at a doctor’s office before being called.\n\\[\nf_X(x)=\n\\begin{cases}\n\\dfrac{1}{2}, & 2 \\le x &lt; 3, \\\\[6pt]\n\\dfrac{x}{16}, & 3 \\le x \\le 5, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nIs \\(X\\) a discrete or continuous random variable? Sketch the PDF and label the axes.\n\nFind the cumulative distribution function (CDF), \\(F_X(x)\\), in curly-brace notation.\n\nCompute \\(P(X \\le 2.6)\\).\n\nCompute \\(P(2.5 \\le X \\le 4)\\).\n\nVerify that this is a valid probability distribution.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nVerify Valid PDF \\[\n\\int_{2}^{3}\\frac{1}{2}\\,dx + \\int_{3}^{5}\\frac{x}{16}\\,dx\n= \\frac{1}{2} + \\frac{1}{16}\\cdot \\frac{25-9}{2}\n= \\frac{1}{2} + \\frac{16}{32}\n= \\frac{1}{2} + \\frac{1}{2}\n= 1~.\n\\]\n\n\n\n\\(X\\) is continuous. The PDF is flat at height \\(1/2\\) on \\([2,3)\\), then jumps down to \\(f(3)=3/16\\) and increases linearly to \\(f(5)=5/16\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFrom PDF to CDF\n\nWe want the cumulative distribution function (CDF):\n\\[\nF_X(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f_X(t)\\,dt\n\\]\nwhere the PDF is\n\\[\nf_X(t) =\n\\begin{cases}\n\\dfrac{1}{2}, & 2 \\le t &lt; 3, \\\\[6pt]\n\\dfrac{t}{16}, & 3 \\le t \\le 5, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\nNo probability has accumulated yet:\n\\[\nF_X(x) = 0.\n\\]\n\n\n\nWe integrate from 2 up to \\(x\\):\n\\[\nF_X(x) = \\int_{2}^{x} \\frac{1}{2}\\,dt\n= \\left[\\frac{t}{2}\\right]_{t=2}^{t=x}\n= \\frac{x}{2} - \\frac{2}{2}\n= \\frac{x}{2} - 1.\n\\]\n\n\n\nWe add all mass up to 3 plus the integral from 3 to \\(x\\):\n\\[\nF_X(x) = \\int_{2}^{3} \\frac{1}{2}\\,dt \\;+\\; \\int_{3}^{x} \\frac{t}{16}\\,dt.\n\\]\nFirst part:\n\\[\n\\int_{2}^{3} \\frac{1}{2}\\,dt\n= \\left[\\frac{t}{2}\\right]_{2}^{3}\n= \\frac{3}{2} - \\frac{2}{2}\n= 0.5.\n\\]\nSecond part (leaving the constant inside):\n\\[\n\\int_{3}^{x} \\frac{t}{16}\\,dt\n= \\left[\\frac{t^2}{32}\\right]_{t=3}^{t=x}\n= \\frac{x^2}{32} - \\frac{9}{32}\n= \\frac{x^2 - 9}{32}.\n\\]\nSo together:\n\\[\nF_X(x) = 0.5 + \\frac{x^2 - 9}{32}\n= \\frac{16}{32} + \\frac{x^2 - 9}{32}\n= \\frac{x^2 + 7}{32}.\n\\]\n\n\n\nAll probability mass is included:\n\\[\nF_X(x) = 1.\n\\]\n\n\n\n\\[\nF_X(x) =\n\\begin{cases}\n0, & x &lt; 2, \\\\[6pt]\n\\dfrac{x}{2} - 1, & 2 \\le x &lt; 3, \\\\[6pt]\n\\dfrac{x^2 + 7}{32}, & 3 \\le x \\le 5, \\\\[6pt]\n1, & x &gt; 5~.\n\\end{cases}\n\\]\n\n\nSince \\(2.6 \\in [2,3)\\), \\[\nP(X \\le 2.6)=F_X(2.6)=\\frac{2.6-2}{2}=0.3.\n\\]\n\n\n\nSplit at \\(3\\): \\[\n\\begin{aligned}\nP(2.5 \\le X \\le 4)\n&= \\int_{2.5}^{3} \\frac{1}{2}\\,dx + \\int_{3}^{4} \\frac{x}{16}\\,dx \\\\[6pt]\n&= \\frac{1}{2}(0.5) + \\frac{1}{16}\\cdot \\frac{4^2-3^2}{2} \\\\[6pt]\n&= 0.25 + \\frac{1}{16}\\cdot \\frac{16-9}{2}\n= 0.25 + \\frac{1}{16}\\cdot \\frac{7}{2}\n= 0.25 + \\frac{7}{32}\n= \\frac{8}{32} + \\frac{7}{32}\n= \\frac{15}{32}\n\\approx 0.46875~.\n\\end{aligned}\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#lesson-10",
    "href": "MA206-AY26-1/lesson-9.html#lesson-10",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "Resources:\n\nComputer with blank script in RStudio\nCourse Guide\nTidyverse Tutorial\nOne Page (Front and Back) of personally handwritten notes\nIssued Calculator\n\n\n\n\n\n\nVenn Diagrams\n\nDefinitions of Union/Intersection\n\nAddition Rule\n\nComplement Rule\n\nConditional Probability\n\nMultiplication Rule\n\nLaw of Total Probability\n\nBayes’ Theorem\n\nTree Diagrams\n\nRules of Independence and Mutually Exclusive Events\n\nTake Scenarios and Answer Questions Related to Probability\n\n\n\n\n\nIdentify appropriate distribution (named or unnamed)\n\nGo from PDF to CDF for unnamed distributions\n\nFind probabilities\n\nFind expected values\n\nFind variances\n\nLinear Transformation Rules (Adding/Subtracting Expected Values and Variances)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-9.html#before-you-leave",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html",
    "href": "MA206-AY26-1/lesson-7.html",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "Notation is hard\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Distribution Function (PDF)\nLet \\(X\\in\\{1,2,3,4,5,6\\}\\) be the points shown on one fair die roll.\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P(X=x)\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\n\n\n\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nCalculate the expected value of a discrete random variable\n\\[\n\\mu_X = E[X] = \\sum_x x \\cdot P(X=x).\n\\]\n\\[\nE[X] = 1\\cdot\\tfrac16 + 2\\cdot\\tfrac16 + 3\\cdot\\tfrac16 + 4\\cdot\\tfrac16 + 5\\cdot\\tfrac16 + 6\\cdot\\tfrac16\n= \\tfrac{21}{6} = 3.5.\n\\]\nCalculate the variance and standard deviation of a discrete random variable\n\\[\n\\mathrm{Var}(X)=\\sum_x (x-\\mu_X)^2\\,P(X=x)\n\\]\n\\[\n\\mathrm{Var}(X)=\\left(1-3.5\\right)^2\\cdot\\tfrac16\n+\\left(2-3.5\\right)^2\\cdot\\tfrac16\n+\\left(3-3.5\\right)^2\\cdot\\tfrac16\n+\\left(4-3.5\\right)^2\\cdot\\tfrac16\n+\\left(5-3.5\\right)^2\\cdot\\tfrac16\n+\\left(6-3.5\\right)^2\\cdot\\tfrac16\n=\\tfrac{35}{12}\n\\]\n\\[\n\\mathrm{SD}(X)=\\sqrt{\\mathrm{Var}(X)}.\n\\]\n\\[\n\\mathrm{SD}(X)=\\sqrt{\\tfrac{35}{12}}\\approx1.7078.\n\\]\n\n\n\n\n\nFor a discrete random variable \\(X\\), the cumulative distribution function (CDF) is defined as\n\\[\nF(x) = P(X \\leq x) = \\sum_{t \\leq x} P(X=t).\n\\]\nFor the fair die,\nCumulative Distribution Function (CDF)\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(F(x) = P(X \\leq x)\\)\n\\(1/6\\)\n\\(2/6\\)\n\\(3/6\\)\n\\(4/6\\)\n\\(5/6\\)\n\\(6/6\\)\n\n\n\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 1, \\\\[6pt]\n\\dfrac{1}{6}, & 1 \\leq x &lt; 2, \\\\[6pt]\n\\dfrac{2}{6}, & 2 \\leq x &lt; 3, \\\\[6pt]\n\\dfrac{3}{6}, & 3 \\leq x &lt; 4, \\\\[6pt]\n\\dfrac{4}{6}, & 4 \\leq x &lt; 5, \\\\[6pt]\n\\dfrac{5}{6}, & 5 \\leq x &lt; 6, \\\\[6pt]\n1, & x \\geq 6.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is \\(P(X \\leq 2)\\, ?\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe sum probabilities for \\(X=1\\) and \\(X=2\\):\n\\[\nP(X \\leq 2) = P(1) + P(2) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]\n\n\n\n\nWhat is \\(P\\!\\big(X &lt; \\mu_X + 1\\,\\mathrm{SD}\\big), \\quad \\mu_X=3.5,\\ \\mathrm{SD}\\approx1.7078 \\, ?\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCompute the cutoff:\n\\[\n\\mu_X + 1\\,\\mathrm{SD} \\approx 3.5 + 1.7078 = 5.2078.\n\\]\nSo we want \\(P(X &lt; 5.2078)\\). Since \\(X\\) is discrete, this means \\(X \\leq 5\\).\n\\[\nP(X \\leq 5) = \\tfrac{5}{6}.\n\\]\n\n\n\n\nWhat is \\(P(X &gt; 4)\\, ?\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPossible outcomes are \\(X=5\\) or \\(X=6\\):\n\\[\nP(X &gt; 4) = P(5) + P(6) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]\n\n\n\n\n\n\n\n\n\nDraw one ball uniformly at random from a hat with three balls: - Red earns $3, Blue earns $6, Green earns $12.\nOutcomes and probabilities (uniform over three balls):\n\n\n\nOutcome \\(x\\)\n$3\n$6\n$12\n\n\n\n\n\\(P(X_A = x)\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1/3\\)\n\n\n\nExpected value (from the PDF): \\[\nE[X_A] \\;=\\; 3\\cdot\\tfrac13 + 6\\cdot\\tfrac13 + 12\\cdot\\tfrac13\n= \\tfrac{3+6+12}{3} \\;=\\; 7.\n\\]\nVariance (from the PDF): \\[\n\\operatorname{Var}(X_A)\n= \\sum_x (x - E[X_A])^2 P(X_A=x)\n= \\tfrac13(3-7)^2 + \\tfrac13(6-7)^2 + \\tfrac13(12-7)^2.\n\\]\nCompute: \\[\n(3-7)^2=16,\\quad (6-7)^2=1,\\quad (12-7)^2=25 \\;\\;\\Rightarrow\\;\\;\n\\operatorname{Var}(X_A) = \\tfrac{16+1+25}{3} = \\tfrac{42}{3} = 14.\n\\]\nThus \\(\\mathrm{SD}(X_A)=\\sqrt{14}\\approx 3.7417\\).\n\n\n\nFlip a fair coin:\n- Tails earns $0, Heads earns $5. - Heads/Tails with probability \\(1/2\\) each.\n\n\n\nOutcome \\(x\\)\n$0\n$5\n\n\n\n\n\\(P(X_B = x)\\)\n\\(1/2\\)\n\\(1/2\\)\n\n\n\nExpected value (from the PDF): \\[\nE[X_B] \\;=\\; 0\\cdot\\tfrac12 + 5\\cdot\\tfrac12 \\;=\\; 2.5.\n\\]\nVariance (from the PDF): \\[\n\\operatorname{Var}(X_B)\n= \\sum_x (x - E[X_B])^2 P(X_B=x)\n= \\tfrac12(0-2.5)^2 + \\tfrac12(5-2.5)^2\n= \\tfrac12(6.25) + \\tfrac12(6.25) = 6.25.\n\\]\nThus \\(\\mathrm{SD}(X_B)=\\sqrt{6.25}=2.5\\).\n\n\n\nNow imagine a scenario where one draws a ball and flips a coin and earns the value of the draw and the flip.\nNew PDF. Each pair \\((X_A, X_B)\\) has probability \\((1/3)(1/2)=1/6\\). The possible sums:\n\n\\(3+0=3\\)\n\n\\(3+5=8\\)\n\n\\(6+0=6\\)\n\n\\(6+5=11\\)\n\n\\(12+0=12\\)\n\n\\(12+5=17\\)\n\nSo:\n\n\n\n\\(y\\)\n3\n6\n8\n11\n12\n17\n\n\n\n\n\\(P(Y=y)\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\n\n\nExpected Value (from the PDF) $$ \\[\\begin{align*}\nE[Y]\n&= \\sum_y y \\, P(Y=y) \\\\\n&= \\tfrac{1}{6}(3+6+8+11+12+17) \\\\\n&= \\tfrac{57}{6} \\\\\n&= 9.5.\n\\end{align*}\\]\n$$\nVariance From the PDF \\[\n\\begin{align*}\n\\operatorname{Var}(Y)\n&= \\tfrac{1}{6}\\bigl((3-9.5)^2 + (6-9.5)^2 + (8-9.5)^2 + (11-9.5)^2 + (12-9.5)^2 + (17-9.5)^2\\bigr) \\\\\n&= \\tfrac{1}{6}(42.25 + 12.25 + 2.25 + 2.25 + 6.25 + 56.25) \\\\\n&= \\tfrac{121.5}{6} \\\\\n&= 20.25.\n\\end{align*}\n\\]\n\\[\n\\mathrm{SD}(Y) = \\sqrt{20.25} = 4.5.\n\\]\n\n\n\n\n\nExpectation is linear (no conditions on indepenence): \\[\nE[aX + bY + c] \\;=\\; a\\,E[X] + b\\,E[Y] + c\n\\]\nVariance (independence required): \\[\n\\operatorname{Var}(aX + bY + c) \\;=\\; a^2 \\operatorname{Var}(X) + b^2 \\operatorname{Var}(Y)\n\\]\nStandard deviation:\n\\[\n\\mathrm{SD}(X) = \\sqrt{\\operatorname{Var}(X)}.\n\\]\n\n\n\nWe could grind through the PMF, but this is faster—everything collapses into two lines:\n\nLinearity of expectation (no conditions on independence): \\[\nE[aX + bY + c] = a\\,E[X] + b\\,E[Y] + c\n\\]\nVariance (Independence Required): \\[\n\\operatorname{Var}(aX + bY + c) = a^2 \\operatorname{Var}(X) + b^2 \\operatorname{Var}(Y)\n\\quad\\text{if } X \\perp Y.\n\\] Apply to \\(Y=X_A+X_B\\) with \\(X_A \\perp X_B\\):\n\nWe wanted to know \\(E[Y] = E[X_A + X_B]\\) and \\(\\operatorname{Var}(Y) = \\operatorname{Var}(X_A + X_B)\\)\n\\[\nE[Y]=E[X_A]+E[X_B]=7+2.5=9.5,\\qquad\n\\operatorname{Var}(Y)=\\operatorname{Var}(X_A)+\\operatorname{Var}(X_B)=14+6.25=20.25,\n\\] \\[\n\\mathrm{SD}(Y)=\\sqrt{20.25}=4.5.\n\\]\n\n\n\nNow suppose the player pays $10 up front before playing the ball game (\\(X_A\\)) and coin game (\\(X_B\\)).\nThe net winnings are \\[\nW = X_A + X_B - 10.\n\\]\nUsing the linear rules from the start:\n\nExpectation: \\[\nE[W] = E[X_A] + E[X_B] - 10 = 7 + 2.5 - 10 = -0.5.\n\\]\nVariance (independence of \\(X_A\\) and \\(X_B\\)): \\[\n\\operatorname{Var}(W) = \\operatorname{Var}(X_A + X_B - 10)\n= \\operatorname{Var}(X_A) + \\operatorname{Var}(X_B)\n= 14 + 6.25 = 20.25.\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(W) = \\sqrt{20.25} = 4.5.\n\\]\n\n\n\n\n\nSpinner A:\n- $2 with probability \\(0.5\\)\n- $5 with probability \\(0.3\\)\n- $10 with probability \\(0.2\\)\nSo \\(X_A\\) is a discrete random variable.\nSpinner B:\n- $1 with probability \\(0.4\\)\n- $4 with probability \\(0.6\\)\nSo \\(X_B\\) is a discrete random variable.\n\n\nWrite out the probability distribution for each spinner.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSpinner A: \\[\nf_{X_A}(x) =\n\\begin{cases}\n0.5, & x = 2 \\\\\n0.3, & x = 5 \\\\\n0.2, & x = 10 \\\\\n0,   & \\text{otherwise.}\n\\end{cases}\n\\]\nSpinner B: \\[\nf_{X_B}(x) =\n\\begin{cases}\n0.4, & x = 1 \\\\\n0.6, & x = 4 \\\\\n0,   & \\text{otherwise.}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSpinner A: \\[\nE[X_A] = 2(0.5) + 5(0.3) + 10(0.2) = 4.5\n\\]\n\\[\n\\operatorname{Var}(X_A)\n= 0.5(2-4.5)^2 + 0.3(5-4.5)^2 + 0.2(10-4.5)^2\n= 9.25\n\\]\nSpinner B: \\[\nE[X_B] = 1(0.4) + 4(0.6) = 2.8\n\\]\n\\[\n\\operatorname{Var}(X_B)\n= 0.4(1-2.8)^2 + 0.6(4-2.8)^2\n= 2.16\n\\]\n\n\n\n\n\n\nIn this game show, you play both games concurrently.\nLet \\(Y = X_A + X_B\\), assuming independence.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nE[Y] = E[X_A] + E[X_B] = 4.5 + 2.8 = 7.3\n\\]\n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(X_A) + \\operatorname{Var}(X_B) = 9.25 + 2.16 = 11.41\n\\]\n\\[\n\\mathrm{SD}(Y) = \\sqrt{11.41} \\approx 3.38\n\\]\n\n\n\n\n\n\nNow suppose in a bonus round the winnings are doubled for Spinner A and tripled for Spinner B:\n\\[\nW = 2X_A + 3X_B.\n\\]\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nExpectation: \\[\nE[W] = 2E[X_A] + 3E[X_B] = 17.4\n\\]\nVariance: \\[\n\\operatorname{Var}(W) = 2^2 \\operatorname{Var}(X_A) + 3^2 \\operatorname{Var}(X_B) = 56.44\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(W) = \\sqrt{56.44} \\approx 7.52\n\\]\n\n\n\n\n\n\nNow suppose it costs $15 to enter the bonus round.\nThe net winnings are \\[\nZ = 2X_A + 3X_B - 15.\n\\]\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nExpectation: \\[\nE[Z] = 2E[X_A] + 3E[X_B] - 15 = 2.4\n\\]\nVariance (Assuming Independence): \\[\n\\operatorname{Var}(Z) = 56.44\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(Z) = \\sqrt{56.44} \\approx 7.52\n\\]\n\n\n\n\n\n\n\nYou draw one ball from each bag, independently.\nBag A (5 balls total):\n- 2 red balls worth $3 each,\n- 2 blue balls worth $7 each,\n- 1 gold ball worth $15.\nLet \\(X_A\\) be the payout from Bag A.\nBag B (4 balls total):\n- 1 black ball worth $0,\n- 1 green ball worth $4,\n- 2 purple balls worth $8.\nLet \\(X_B\\) be the payout from Bag B.\n\nFind \\(E[X_A]\\) and \\(\\operatorname{Var}(X_A)\\) from the PDF definition.\n\nFind \\(E[X_B]\\) and \\(\\operatorname{Var}(X_B)\\) from the PDF definition.\n\nCompute the expected value of both games played together assuming independence.\n\nBonus round with scaling and a fee. In a special round, the payout is multiplied: you get three times the Bag A value but half the Bag B value, then pay a flat fee of $20.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n1) Bag A. Probabilities: \\(P(X_A=3)=\\tfrac{2}{5}\\), \\(P(X_A=7)=\\tfrac{2}{5}\\), \\(P(X_A=15)=\\tfrac{1}{5}\\).\n\\[\nE[X_A] = 3\\cdot\\tfrac{2}{5} + 7\\cdot\\tfrac{2}{5} + 15\\cdot\\tfrac{1}{5}\n= \\tfrac{6+14+15}{5} = 7.\n\\]\n\\[\n\\operatorname{Var}(X_A) = 0.4(3-7)^2 + 0.4(7-7)^2 + 0.2(15-7)^2\n= 0.4(16) + 0 + 0.2(64) = 6.4 + 12.8 = 19.2.\n\\]\n2) Bag B. Probabilities: \\(P(X_B=0)=\\tfrac{1}{4}\\), \\(P(X_B=4)=\\tfrac{1}{4}\\), \\(P(X_B=8)=\\tfrac{1}{2}\\).\n\\[\nE[X_B] = 0\\cdot\\tfrac{1}{4} + 4\\cdot\\tfrac{1}{4} + 8\\cdot\\tfrac{1}{2}\n= 1 + 4 = 5.\n\\]\n\\[\n\\operatorname{Var}(X_B) = 0.25(0-5)^2 + 0.25(4-5)^2 + 0.5(8-5)^2\n= 0.25(25) + 0.25(1) + 0.5(9)\n= 6.25 + 0.25 + 4.5 = 11.\n\\]\n3) Both bags together. Let \\(Y = X_A + X_B\\).\n\\[\nE[Y] = E[X_A] + E[X_B] = 7 + 5 = 12.\n\\]\n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(X_A) + \\operatorname{Var}(X_B)\n= 19.2 + 11 = 30.2.\n\\]\n\\[\n\\mathrm{SD}(Y) = \\sqrt{30.2} \\approx 5.50.\n\\]\n4) Bonus round with fee.\n\\[\nZ = 3X_A + \\tfrac{1}{2}X_B - 20.\n\\]\nExpectation: \\[\nE[Z] = 3E[X_A] + \\tfrac{1}{2}E[X_B] - 20\n= 3(7) + 0.5(5) - 20\n= 21 + 2.5 - 20 = 3.5.\n\\]\nVariance: \\[\n\\operatorname{Var}(Z) = 3^2 \\operatorname{Var}(X_A) + \\left(\\tfrac{1}{2}\\right)^2 \\operatorname{Var}(X_B)\n= 9(19.2) + 0.25(11) = 172.8 + 2.75 = 175.55.\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(Z) = \\sqrt{175.55} \\approx 13.25.\n\\]\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\n\n\n\nWPR 1: Lesson 10\nExploration Exercise 11.3B\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#review",
    "href": "MA206-AY26-1/lesson-7.html#review",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "Probability Distribution Function (PDF)\nLet \\(X\\in\\{1,2,3,4,5,6\\}\\) be the points shown on one fair die roll.\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P(X=x)\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\n\n\n\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nCalculate the expected value of a discrete random variable\n\\[\n\\mu_X = E[X] = \\sum_x x \\cdot P(X=x).\n\\]\n\\[\nE[X] = 1\\cdot\\tfrac16 + 2\\cdot\\tfrac16 + 3\\cdot\\tfrac16 + 4\\cdot\\tfrac16 + 5\\cdot\\tfrac16 + 6\\cdot\\tfrac16\n= \\tfrac{21}{6} = 3.5.\n\\]\nCalculate the variance and standard deviation of a discrete random variable\n\\[\n\\mathrm{Var}(X)=\\sum_x (x-\\mu_X)^2\\,P(X=x)\n\\]\n\\[\n\\mathrm{Var}(X)=\\left(1-3.5\\right)^2\\cdot\\tfrac16\n+\\left(2-3.5\\right)^2\\cdot\\tfrac16\n+\\left(3-3.5\\right)^2\\cdot\\tfrac16\n+\\left(4-3.5\\right)^2\\cdot\\tfrac16\n+\\left(5-3.5\\right)^2\\cdot\\tfrac16\n+\\left(6-3.5\\right)^2\\cdot\\tfrac16\n=\\tfrac{35}{12}\n\\]\n\\[\n\\mathrm{SD}(X)=\\sqrt{\\mathrm{Var}(X)}.\n\\]\n\\[\n\\mathrm{SD}(X)=\\sqrt{\\tfrac{35}{12}}\\approx1.7078.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#from-pdf-to-cdf-cumulative-distribution-function",
    "href": "MA206-AY26-1/lesson-7.html#from-pdf-to-cdf-cumulative-distribution-function",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "For a discrete random variable \\(X\\), the cumulative distribution function (CDF) is defined as\n\\[\nF(x) = P(X \\leq x) = \\sum_{t \\leq x} P(X=t).\n\\]\nFor the fair die,\nCumulative Distribution Function (CDF)\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(F(x) = P(X \\leq x)\\)\n\\(1/6\\)\n\\(2/6\\)\n\\(3/6\\)\n\\(4/6\\)\n\\(5/6\\)\n\\(6/6\\)\n\n\n\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 1, \\\\[6pt]\n\\dfrac{1}{6}, & 1 \\leq x &lt; 2, \\\\[6pt]\n\\dfrac{2}{6}, & 2 \\leq x &lt; 3, \\\\[6pt]\n\\dfrac{3}{6}, & 3 \\leq x &lt; 4, \\\\[6pt]\n\\dfrac{4}{6}, & 4 \\leq x &lt; 5, \\\\[6pt]\n\\dfrac{5}{6}, & 5 \\leq x &lt; 6, \\\\[6pt]\n1, & x \\geq 6.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is \\(P(X \\leq 2)\\, ?\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe sum probabilities for \\(X=1\\) and \\(X=2\\):\n\\[\nP(X \\leq 2) = P(1) + P(2) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]\n\n\n\n\nWhat is \\(P\\!\\big(X &lt; \\mu_X + 1\\,\\mathrm{SD}\\big), \\quad \\mu_X=3.5,\\ \\mathrm{SD}\\approx1.7078 \\, ?\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCompute the cutoff:\n\\[\n\\mu_X + 1\\,\\mathrm{SD} \\approx 3.5 + 1.7078 = 5.2078.\n\\]\nSo we want \\(P(X &lt; 5.2078)\\). Since \\(X\\) is discrete, this means \\(X \\leq 5\\).\n\\[\nP(X \\leq 5) = \\tfrac{5}{6}.\n\\]\n\n\n\n\nWhat is \\(P(X &gt; 4)\\, ?\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPossible outcomes are \\(X=5\\) or \\(X=6\\):\n\\[\nP(X &gt; 4) = P(5) + P(6) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#calculate-expected-values-of-linear-transformations-of-random-variables",
    "href": "MA206-AY26-1/lesson-7.html#calculate-expected-values-of-linear-transformations-of-random-variables",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "Draw one ball uniformly at random from a hat with three balls: - Red earns $3, Blue earns $6, Green earns $12.\nOutcomes and probabilities (uniform over three balls):\n\n\n\nOutcome \\(x\\)\n$3\n$6\n$12\n\n\n\n\n\\(P(X_A = x)\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1/3\\)\n\n\n\nExpected value (from the PDF): \\[\nE[X_A] \\;=\\; 3\\cdot\\tfrac13 + 6\\cdot\\tfrac13 + 12\\cdot\\tfrac13\n= \\tfrac{3+6+12}{3} \\;=\\; 7.\n\\]\nVariance (from the PDF): \\[\n\\operatorname{Var}(X_A)\n= \\sum_x (x - E[X_A])^2 P(X_A=x)\n= \\tfrac13(3-7)^2 + \\tfrac13(6-7)^2 + \\tfrac13(12-7)^2.\n\\]\nCompute: \\[\n(3-7)^2=16,\\quad (6-7)^2=1,\\quad (12-7)^2=25 \\;\\;\\Rightarrow\\;\\;\n\\operatorname{Var}(X_A) = \\tfrac{16+1+25}{3} = \\tfrac{42}{3} = 14.\n\\]\nThus \\(\\mathrm{SD}(X_A)=\\sqrt{14}\\approx 3.7417\\).\n\n\n\nFlip a fair coin:\n- Tails earns $0, Heads earns $5. - Heads/Tails with probability \\(1/2\\) each.\n\n\n\nOutcome \\(x\\)\n$0\n$5\n\n\n\n\n\\(P(X_B = x)\\)\n\\(1/2\\)\n\\(1/2\\)\n\n\n\nExpected value (from the PDF): \\[\nE[X_B] \\;=\\; 0\\cdot\\tfrac12 + 5\\cdot\\tfrac12 \\;=\\; 2.5.\n\\]\nVariance (from the PDF): \\[\n\\operatorname{Var}(X_B)\n= \\sum_x (x - E[X_B])^2 P(X_B=x)\n= \\tfrac12(0-2.5)^2 + \\tfrac12(5-2.5)^2\n= \\tfrac12(6.25) + \\tfrac12(6.25) = 6.25.\n\\]\nThus \\(\\mathrm{SD}(X_B)=\\sqrt{6.25}=2.5\\).\n\n\n\nNow imagine a scenario where one draws a ball and flips a coin and earns the value of the draw and the flip.\nNew PDF. Each pair \\((X_A, X_B)\\) has probability \\((1/3)(1/2)=1/6\\). The possible sums:\n\n\\(3+0=3\\)\n\n\\(3+5=8\\)\n\n\\(6+0=6\\)\n\n\\(6+5=11\\)\n\n\\(12+0=12\\)\n\n\\(12+5=17\\)\n\nSo:\n\n\n\n\\(y\\)\n3\n6\n8\n11\n12\n17\n\n\n\n\n\\(P(Y=y)\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\n\n\nExpected Value (from the PDF) $$ \\[\\begin{align*}\nE[Y]\n&= \\sum_y y \\, P(Y=y) \\\\\n&= \\tfrac{1}{6}(3+6+8+11+12+17) \\\\\n&= \\tfrac{57}{6} \\\\\n&= 9.5.\n\\end{align*}\\]\n$$\nVariance From the PDF \\[\n\\begin{align*}\n\\operatorname{Var}(Y)\n&= \\tfrac{1}{6}\\bigl((3-9.5)^2 + (6-9.5)^2 + (8-9.5)^2 + (11-9.5)^2 + (12-9.5)^2 + (17-9.5)^2\\bigr) \\\\\n&= \\tfrac{1}{6}(42.25 + 12.25 + 2.25 + 2.25 + 6.25 + 56.25) \\\\\n&= \\tfrac{121.5}{6} \\\\\n&= 20.25.\n\\end{align*}\n\\]\n\\[\n\\mathrm{SD}(Y) = \\sqrt{20.25} = 4.5.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#expected-value-and-variance-rules",
    "href": "MA206-AY26-1/lesson-7.html#expected-value-and-variance-rules",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "Expectation is linear (no conditions on indepenence): \\[\nE[aX + bY + c] \\;=\\; a\\,E[X] + b\\,E[Y] + c\n\\]\nVariance (independence required): \\[\n\\operatorname{Var}(aX + bY + c) \\;=\\; a^2 \\operatorname{Var}(X) + b^2 \\operatorname{Var}(Y)\n\\]\nStandard deviation:\n\\[\n\\mathrm{SD}(X) = \\sqrt{\\operatorname{Var}(X)}.\n\\]\n\n\n\nWe could grind through the PMF, but this is faster—everything collapses into two lines:\n\nLinearity of expectation (no conditions on independence): \\[\nE[aX + bY + c] = a\\,E[X] + b\\,E[Y] + c\n\\]\nVariance (Independence Required): \\[\n\\operatorname{Var}(aX + bY + c) = a^2 \\operatorname{Var}(X) + b^2 \\operatorname{Var}(Y)\n\\quad\\text{if } X \\perp Y.\n\\] Apply to \\(Y=X_A+X_B\\) with \\(X_A \\perp X_B\\):\n\nWe wanted to know \\(E[Y] = E[X_A + X_B]\\) and \\(\\operatorname{Var}(Y) = \\operatorname{Var}(X_A + X_B)\\)\n\\[\nE[Y]=E[X_A]+E[X_B]=7+2.5=9.5,\\qquad\n\\operatorname{Var}(Y)=\\operatorname{Var}(X_A)+\\operatorname{Var}(X_B)=14+6.25=20.25,\n\\] \\[\n\\mathrm{SD}(Y)=\\sqrt{20.25}=4.5.\n\\]\n\n\n\nNow suppose the player pays $10 up front before playing the ball game (\\(X_A\\)) and coin game (\\(X_B\\)).\nThe net winnings are \\[\nW = X_A + X_B - 10.\n\\]\nUsing the linear rules from the start:\n\nExpectation: \\[\nE[W] = E[X_A] + E[X_B] - 10 = 7 + 2.5 - 10 = -0.5.\n\\]\nVariance (independence of \\(X_A\\) and \\(X_B\\)): \\[\n\\operatorname{Var}(W) = \\operatorname{Var}(X_A + X_B - 10)\n= \\operatorname{Var}(X_A) + \\operatorname{Var}(X_B)\n= 14 + 6.25 = 20.25.\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(W) = \\sqrt{20.25} = 4.5.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#spinner-game",
    "href": "MA206-AY26-1/lesson-7.html#spinner-game",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "Spinner A:\n- $2 with probability \\(0.5\\)\n- $5 with probability \\(0.3\\)\n- $10 with probability \\(0.2\\)\nSo \\(X_A\\) is a discrete random variable.\nSpinner B:\n- $1 with probability \\(0.4\\)\n- $4 with probability \\(0.6\\)\nSo \\(X_B\\) is a discrete random variable.\n\n\nWrite out the probability distribution for each spinner.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSpinner A: \\[\nf_{X_A}(x) =\n\\begin{cases}\n0.5, & x = 2 \\\\\n0.3, & x = 5 \\\\\n0.2, & x = 10 \\\\\n0,   & \\text{otherwise.}\n\\end{cases}\n\\]\nSpinner B: \\[\nf_{X_B}(x) =\n\\begin{cases}\n0.4, & x = 1 \\\\\n0.6, & x = 4 \\\\\n0,   & \\text{otherwise.}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSpinner A: \\[\nE[X_A] = 2(0.5) + 5(0.3) + 10(0.2) = 4.5\n\\]\n\\[\n\\operatorname{Var}(X_A)\n= 0.5(2-4.5)^2 + 0.3(5-4.5)^2 + 0.2(10-4.5)^2\n= 9.25\n\\]\nSpinner B: \\[\nE[X_B] = 1(0.4) + 4(0.6) = 2.8\n\\]\n\\[\n\\operatorname{Var}(X_B)\n= 0.4(1-2.8)^2 + 0.6(4-2.8)^2\n= 2.16\n\\]\n\n\n\n\n\n\nIn this game show, you play both games concurrently.\nLet \\(Y = X_A + X_B\\), assuming independence.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nE[Y] = E[X_A] + E[X_B] = 4.5 + 2.8 = 7.3\n\\]\n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(X_A) + \\operatorname{Var}(X_B) = 9.25 + 2.16 = 11.41\n\\]\n\\[\n\\mathrm{SD}(Y) = \\sqrt{11.41} \\approx 3.38\n\\]\n\n\n\n\n\n\nNow suppose in a bonus round the winnings are doubled for Spinner A and tripled for Spinner B:\n\\[\nW = 2X_A + 3X_B.\n\\]\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nExpectation: \\[\nE[W] = 2E[X_A] + 3E[X_B] = 17.4\n\\]\nVariance: \\[\n\\operatorname{Var}(W) = 2^2 \\operatorname{Var}(X_A) + 3^2 \\operatorname{Var}(X_B) = 56.44\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(W) = \\sqrt{56.44} \\approx 7.52\n\\]\n\n\n\n\n\n\nNow suppose it costs $15 to enter the bonus round.\nThe net winnings are \\[\nZ = 2X_A + 3X_B - 15.\n\\]\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nExpectation: \\[\nE[Z] = 2E[X_A] + 3E[X_B] - 15 = 2.4\n\\]\nVariance (Assuming Independence): \\[\n\\operatorname{Var}(Z) = 56.44\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(Z) = \\sqrt{56.44} \\approx 7.52\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#board-problem-two-bags-of-balls",
    "href": "MA206-AY26-1/lesson-7.html#board-problem-two-bags-of-balls",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "You draw one ball from each bag, independently.\nBag A (5 balls total):\n- 2 red balls worth $3 each,\n- 2 blue balls worth $7 each,\n- 1 gold ball worth $15.\nLet \\(X_A\\) be the payout from Bag A.\nBag B (4 balls total):\n- 1 black ball worth $0,\n- 1 green ball worth $4,\n- 2 purple balls worth $8.\nLet \\(X_B\\) be the payout from Bag B.\n\nFind \\(E[X_A]\\) and \\(\\operatorname{Var}(X_A)\\) from the PDF definition.\n\nFind \\(E[X_B]\\) and \\(\\operatorname{Var}(X_B)\\) from the PDF definition.\n\nCompute the expected value of both games played together assuming independence.\n\nBonus round with scaling and a fee. In a special round, the payout is multiplied: you get three times the Bag A value but half the Bag B value, then pay a flat fee of $20.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n1) Bag A. Probabilities: \\(P(X_A=3)=\\tfrac{2}{5}\\), \\(P(X_A=7)=\\tfrac{2}{5}\\), \\(P(X_A=15)=\\tfrac{1}{5}\\).\n\\[\nE[X_A] = 3\\cdot\\tfrac{2}{5} + 7\\cdot\\tfrac{2}{5} + 15\\cdot\\tfrac{1}{5}\n= \\tfrac{6+14+15}{5} = 7.\n\\]\n\\[\n\\operatorname{Var}(X_A) = 0.4(3-7)^2 + 0.4(7-7)^2 + 0.2(15-7)^2\n= 0.4(16) + 0 + 0.2(64) = 6.4 + 12.8 = 19.2.\n\\]\n2) Bag B. Probabilities: \\(P(X_B=0)=\\tfrac{1}{4}\\), \\(P(X_B=4)=\\tfrac{1}{4}\\), \\(P(X_B=8)=\\tfrac{1}{2}\\).\n\\[\nE[X_B] = 0\\cdot\\tfrac{1}{4} + 4\\cdot\\tfrac{1}{4} + 8\\cdot\\tfrac{1}{2}\n= 1 + 4 = 5.\n\\]\n\\[\n\\operatorname{Var}(X_B) = 0.25(0-5)^2 + 0.25(4-5)^2 + 0.5(8-5)^2\n= 0.25(25) + 0.25(1) + 0.5(9)\n= 6.25 + 0.25 + 4.5 = 11.\n\\]\n3) Both bags together. Let \\(Y = X_A + X_B\\).\n\\[\nE[Y] = E[X_A] + E[X_B] = 7 + 5 = 12.\n\\]\n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(X_A) + \\operatorname{Var}(X_B)\n= 19.2 + 11 = 30.2.\n\\]\n\\[\n\\mathrm{SD}(Y) = \\sqrt{30.2} \\approx 5.50.\n\\]\n4) Bonus round with fee.\n\\[\nZ = 3X_A + \\tfrac{1}{2}X_B - 20.\n\\]\nExpectation: \\[\nE[Z] = 3E[X_A] + \\tfrac{1}{2}E[X_B] - 20\n= 3(7) + 0.5(5) - 20\n= 21 + 2.5 - 20 = 3.5.\n\\]\nVariance: \\[\n\\operatorname{Var}(Z) = 3^2 \\operatorname{Var}(X_A) + \\left(\\tfrac{1}{2}\\right)^2 \\operatorname{Var}(X_B)\n= 9(19.2) + 0.25(11) = 172.8 + 2.75 = 175.55.\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(Z) = \\sqrt{175.55} \\approx 13.25.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-7.html#before-you-leave",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\n\n\n\nWPR 1: Lesson 10\nExploration Exercise 11.3B\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-5.html",
    "href": "MA206-AY26-1/lesson-5.html",
    "title": "Lesson 5: Tidyverse Tutorial",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n ✏️ Edit this workbook (opens in Excel for the web) \n\n\n\n\n\nThis is an individual assignment. You SHOULD sit next to your partner and help each other. Collaboration in class does NOT need to be documented. Collaboration outside of this class (even with your partner) DOES need to be documented.\nLets go to the Course Project Page for specific instructions. You should refer to this as you do this milestone.\nWe are going to execute AY26_1Tidyverse-Lab.Rmd on wage data set\nReference the Tidyverse Tutorial as needed (a lot!)\nComplete the Tidyverse Lab and submit on Canvas NLT 0700 on 5 September 2025 for Day 1 cadets and 0700 on 4 September 2025 for Day 2 cadets\nMust include an Annex B (for addressing feedback) in this Canvas submission\nAdd the Tidyverse-Lab pdf and Annex B to your binder.\n\n\n\n\nSuite of packages\n\n\n\n\n\nTons of extentions\nggvfields\n\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 6\n\n\n\n\n\nMilestone 2 / Tidyverse Tutorial: Due 0700 Lesson 7\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-5.html#welcome",
    "href": "MA206-AY26-1/lesson-5.html#welcome",
    "title": "Lesson 5: Tidyverse Tutorial",
    "section": "",
    "text": "Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-5.html#tidyverse-tutorial",
    "href": "MA206-AY26-1/lesson-5.html#tidyverse-tutorial",
    "title": "Lesson 5: Tidyverse Tutorial",
    "section": "",
    "text": "✏️ Edit this workbook (opens in Excel for the web) \n\n\n\n\n\nThis is an individual assignment. You SHOULD sit next to your partner and help each other. Collaboration in class does NOT need to be documented. Collaboration outside of this class (even with your partner) DOES need to be documented.\nLets go to the Course Project Page for specific instructions. You should refer to this as you do this milestone.\nWe are going to execute AY26_1Tidyverse-Lab.Rmd on wage data set\nReference the Tidyverse Tutorial as needed (a lot!)\nComplete the Tidyverse Lab and submit on Canvas NLT 0700 on 5 September 2025 for Day 1 cadets and 0700 on 4 September 2025 for Day 2 cadets\nMust include an Annex B (for addressing feedback) in this Canvas submission\nAdd the Tidyverse-Lab pdf and Annex B to your binder.\n\n\n\n\nSuite of packages\n\n\n\n\n\nTons of extentions\nggvfields",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-5.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-5.html#before-you-leave",
    "title": "Lesson 5: Tidyverse Tutorial",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nLesson 6\n\n\n\n\n\nMilestone 2 / Tidyverse Tutorial: Due 0700 Lesson 7\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html",
    "href": "MA206-AY26-1/lesson-3.html",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\nAn experiment is a process or action that produces observable outcomes.\n\nIt must have at least two possible outcomes.\n\nEach repetition of the experiment under the same conditions may produce a different outcome.\n\nExample 1: Flipping a coin\nExample 2: Rolling a die\n\n\n\n\nThe sample space is the set of all possible outcomes of a random experiment.\n\\(S = \\{ \\text{Outcome 1}, \\text{Outcome 2}, \\dots, \\text{Outcome n} \\}\\)\n\n\n\n\n\n\nSample space for coin flip\n\n\n\n\n\n\\(S = \\{ \\text{Heads}, \\text{Tails} \\}\\)\n\n\n\n\n\n\n\n\n\nSample space for die roll\n\n\n\n\n\n\\(S = \\{ 1,2,3,4,5,6 \\}\\)\n\n\n\n\n\n\n\nAn event is a subset of the sample space \\(S\\) of an experiment. So… the outcome of an experiment.\nNotation: If \\(A\\) is an event, then \\(A \\subseteq S\\)\nExample 1: Coin lands heads\n\n\n\n\n\n\nEvent heads in the coin flip sample space\n\n\n\n\n\nEvent \\(A =\\) “coin lands heads”\n\\[A = \\{ \\text{Heads} \\}\\]\n\n\n\nExample 2: Die lands 1 or 2\n\n\n\n\n\n\nEvent 1 or 2 in the die roll sample space\n\n\n\n\n\nEvent \\(B =\\) “die lands 1 or 2”\n\\[B = \\{ 1, 2 \\}\\]\n\n\n\n\n\n\n\nThe complement of an event \\(A\\), denoted \\(A^c\\), is the event that \\(A\\) does not occur.\nExample 1: Coin flip lands not heads\n\n\n\n\n\n\nComplement of \\(A =\\) ‘coin lands heads’\n\n\n\n\n\n\\(A^c =\\) “coin lands tails”\n\\[A^c = \\{ \\text{Tails} \\}\\]\n\n\n\nExample 2: Die lands not 1 or 2\n\n\n\n\n\n\nComplement of \\(B =\\) ‘die lands 1 or 2’\n\n\n\n\n\n\\(B^c =\\) “die lands 3,4,5,6”\n\\[B^c = \\{ 3,4,5,6 \\}\\]\n\n\n\n\n\n\n\nThe intersection of two events \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is the event that both \\(A\\) and \\(B\\) occur at the same time.\nExample: Coin flip + die roll\n- \\(A =\\) coin lands heads\n- \\(B =\\) die shows 1 or 2\n\n\n\n\n\n\nWhat is \\(A \\cap B\\)\n\n\n\n\n\n\\(A \\cap B =\\) “coin lands heads and die shows 1 or 2”\n\\[A \\cap B = \\{ (H,1), (H,2) \\}\\]\n\n\n\n\n\n\n\nThe union of two events \\(A\\) and \\(B\\), denoted \\(A \\cup B\\), is the event that either \\(A\\) occurs, or \\(B\\) occurs, or both occur.\nExample: Coin flip + die roll\n- \\(A =\\) coin lands heads\n- \\(B =\\) die shows 1 or 2\n\n\n\n\n\n\nWhat is \\(A \\cup B\\)\n\n\n\n\n\n\\(A \\cup B = \\{ (H,1), (H,2), (H,3), (H,4), (H,5), (H,6), (T,1), (T,2) \\}\\)\n\n\n\n\n\n\n\n\n\n\nThe probability of an event \\(A\\), written \\(P(A)\\), is a number between \\(0\\) and \\(1\\) that measures the likelihood that \\(A\\) occurs.\nFor equally likely outcomes:\n\\[\nP(A) = \\frac{|A|}{|S|}\n\\]\nwhere:\n- \\(|A| =\\) number of outcomes in event \\(A\\)\n- \\(|S| =\\) number of outcomes in the sample space\nAll probabilities are between 0 and 1, inclusive, so the probability of an event \\(A\\) is \\(0 \\leq P(A) \\leq 1\\).\nExample 1: Coin flip\n\n\n\n\n\n\nWhat is the probability of heads\n\n\n\n\n\n\nSample space: \\(S = \\{\\text{Heads}, \\text{Tails}\\}\\), so \\(|S| = 2\\)\n\nEvent \\(A =\\) “coin lands heads” → \\(A = \\{\\text{Heads}\\}\\), so \\(|A| = 1\\)\n\n\\[\nP(A) = \\tfrac{|A|}{|S|} = \\tfrac{1}{2}\n\\]\n\n\n\nExample 2: Die roll\n\n\n\n\n\n\nWhat is the probability of rolling 1 or 2\n\n\n\n\n\n\nSample space: \\(S = \\{1,2,3,4,5,6\\}\\), so \\(|S| = 6\\)\n\nEvent \\(B =\\) “die shows 1 or 2” → \\(B = \\{1,2\\}\\), so \\(|B| = 2\\)\n\n\\[\nP(B) = \\tfrac{|B|}{|S|} = \\tfrac{2}{6} = \\tfrac{1}{3}\n\\]\n\n\n\n\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\nExample 1 (coin):\n\n\n\n\n\n\nWhat is the probability of not heads\n\n\n\n\n\n\n\\(P(\\text{A}) = \\tfrac{1}{2}\\)\n\nSo \\(P(A^c) = 1 - P(A) = 1 - \\tfrac{1}{2} = \\tfrac{1}{2}\\)\n\n\n\n\nExample 2 (die):\n\n\n\n\n\n\nWhat is the probability of not rolling 1 or 2\n\n\n\n\n\n\n\\(P(B) = \\tfrac{1}{3}\\)\n\nSo \\(P(B^c) = 1 - P(B) = 1 - \\tfrac{1}{3} = \\tfrac{2}{3}\\)\n\n\n\n\n\n\n\n\n\nIf all outcomes are equally likely: \\[\nP(A \\cap B) = \\frac{|A \\cap B|}{|S|}\n\\]\nExample 1 (coin + die):\n\n\n\n\n\n\nWhat is the probability the coin is heads AND the die is 1 or 2\n\n\n\n\n\n\nSample space size: \\(|S| = 12\\) (2 coin outcomes × 6 die outcomes)\n\n\\(A =\\) coin lands heads\n\n\\(B =\\) die shows 1 or 2\n\n\\(A \\cap B = \\{(H,1),(H,2)\\}\\), so \\(|A \\cap B| = 2\\)\n\n\\[\nP(A \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\n\\]\n\n\n\n\n\n\n\nIf all outcomes are equally likely: \\[\nP(A \\cup B) = \\frac{|A \\cup B|}{|S|}\n\\]\nExample 1 (coin + die):\n\n\n\n\n\n\nWhat is the probability the coin is heads OR the die is 1 or 2\n\n\n\n\n\n\nSample space size: \\(|S| = 12\\)\n\n\\(A =\\) coin lands heads\n\n\\(B =\\) die shows 1 or 2\n\n\\(A \\cup B = \\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2)\\}\\), so \\(|A \\cup B| = 8\\)\n\n\\[\nP(A \\cup B) = \\tfrac{8}{12} = \\tfrac{2}{3}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\n\n\n\n\n\\(A^c\\) (T)\n\n\n\n\n\nCol Total\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\\(P(A \\cap B)\\)\n\\(P(A \\cap B^c)\\)\n\\(P(A)\\)\n\n\n\\(A^c\\) (T)\n\\(P(A^c \\cap B)\\)\n\\(P(A^c \\cap B^c)\\)\n\\(P(A^c)\\)\n\n\nCol Total\n\\(P(B)\\)\n\\(P(B^c)\\)\n\\(1\\)\n\n\n\nReminder:\n- \\(A =\\) “coin lands heads”\n- \\(B =\\) “die shows 1 or 2”\n- \\(A^c =\\) “coin lands tails”\n- \\(B^c =\\) “die shows 3–6”\nThere are \\(|S|=12\\) equally likely outcomes for (coin, die).\n\n\n\n\n\\(A \\cap B\\) = \\(\\{(H,1),(H,2)\\}\\)\n\\(P(A \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\\)\n\\(A \\cap B^c\\) = \\(\\{(H,3),(H,4),(H,5),(H,6)\\}\\)\n\\(P(A \\cap B^c) = \\tfrac{4}{12} = \\tfrac{1}{3}\\)\n\\(A^c \\cap B\\) = \\(\\{(T,1),(T,2)\\}\\)\n\\(P(A^c \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\\)\n\\(A^c \\cap B^c\\) = \\(\\{(T,3),(T,4),(T,5),(T,6)\\}\\)\n\\(P(A^c \\cap B^c) = \\tfrac{4}{12} = \\tfrac{1}{3}\\)\n\n\n\n\n\n\\(P(A) = P(A \\cap B) + P(A \\cap B^c) = \\tfrac{1}{6} + \\tfrac{1}{3} = \\tfrac{1}{2}\\)\n\\(P(A^c) = P(A^c \\cap B) + P(A^c \\cap B^c) = \\tfrac{1}{6} + \\tfrac{1}{3} = \\tfrac{1}{2}\\)\n\\(P(B) = P(A \\cap B) + P(A^c \\cap B) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{1}{3}\\)\n\\(P(B^c) = P(A \\cap B^c) + P(A^c \\cap B^c) = \\tfrac{1}{3} + \\tfrac{1}{3} = \\tfrac{2}{3}\\)\nTotal: \\(P(S)=1\\)\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\\(\\tfrac{1}{6}\\)\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{1}{2}\\)\n\n\n\\(A^c\\) (T)\n\\(\\tfrac{1}{6}\\)\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{1}{2}\\)\n\n\nCol Total\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{2}{3}\\)\n\\(1\\)\n\n\n\n\n\n\n\n\n\n\n   A: Heads B: Die is 1 or 2 A ∩ B^c (H,3) (H,4) (H,5) (H,6) P = 1/3 A ∩ B (H,1) (H,2) P = 1/6 A^c ∩ B (T,1) (T,2) P = 1/6 (A^c ∩ B^c) (T,3) (T,4) (T,5) (T,6) P = 1/3\n\n\n\n\n\n\n\n\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n\nTwo events are mutually exclusive when it is impossible for both to happen at the same time.\nTherefore:\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\n\nExample 1 (coin + die):\n\n\n\n\n\n\nCompute \\(P(A \\cup B)\\) using the addition rule\n\n\n\n\n\n\n\\(P(A)=\\tfrac{1}{2}\\)\n\n\\(P(B)=\\tfrac{1}{3}\\)\n\\(P(A\\cap B)=\\tfrac{1}{6}\\)\n\nApply the rule: \\[\nP(A\\cup B)=\\tfrac{1}{2}+\\tfrac{1}{3}-\\tfrac{1}{6}=\\tfrac{2}{3}.\n\\]\nSet view: \\[\nA\\cup B=\\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2)\\},\\quad |A\\cup B|=8.\n\\]\n\n\n\n\n\n\n\n\n\n\nOut of 100 students:\n- 40 like pizza\n- 30 like burgers\n- 10 of those included above like both pizza and burgers\nExperiment: I select one student at random\nLet:\n- \\(A =\\) “student likes pizza”\n- \\(B =\\) “student likes burgers”\n\nWhat is \\(P(A)\\)?\n\nWhat is \\(P(B)\\)?\n\nWhat is \\(P(A \\cap B)\\)?\n\nWhat is \\(P(A \\cup B)\\) using the addition rule?\n\nWhat is \\(P(A^c)\\), the probability a student does not like pizza?\n\nWhat is \\(P(B^c)\\), the probability a student does not like burgers?\n\nWhat is \\(P(A^c \\cap B^c)\\), the probability a student likes neither?\n\nVerify your results using the 2×2 probability table.\n\nRepresent the results with a Venn diagram.\n\n\n\n\n\n\n\nAnswers Problem 1\n\n\n\n\n\n\n\\(P(A) = \\tfrac{40}{100} = 0.40\\)\n\n\\(P(B) = \\tfrac{30}{100} = 0.30\\)\n\n\\(P(A \\cap B) = \\tfrac{10}{100} = 0.10\\)\n\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = 0.40 + 0.30 - 0.10 = 0.60\\)\n\n\\(P(A^c) = 1 - P(A) = 1 - 0.40 = 0.60\\)\n\n\\(P(B^c) = 1 - P(B) = 1 - 0.30 = 0.70\\)\n\n\\(P(A^c \\cap B^c) = 1 - (A \\cup \\ B) = 1 - \\tfrac{60}{100} = \\tfrac{40}{100} = 0.40\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (burger)\n\\(B^c\\) (not burger)\nRow Total\n\n\n\n\n\\(A\\) (pizza)\n\\(\\tfrac{10}{100} = 0.10\\)\n\\(\\tfrac{30}{100} = 0.30\\)\n\\(\\tfrac{40}{100} = 0.40\\)\n\n\n\\(A^c\\) (not pizza)\n\\(\\tfrac{20}{100} = 0.20\\)\n\\(\\tfrac{40}{100} = 0.40\\)\n\\(\\tfrac{60}{100} = 0.60\\)\n\n\nCol Total\n\\(\\tfrac{30}{100} = 0.30\\)\n\\(\\tfrac{70}{100} = 0.70\\)\n\\(1\\)\n\n\n\n\n\n\n\n\n     A: Pizza B: Burger  A ∩ Bᶜ 30 students P = 0.30  A ∩ B 10 students P = 0.10  Aᶜ ∩ B 20 students P = 0.20  Aᶜ ∩ Bᶜ 40 students P = 0.40\n\n\n\n\n\n\n\n\n\nShade \\(A \\cap B^c\\)\n\n\n\n\n\n\nAnswer Problem 2\n\n\n\n\n\n\n\n\n         A B\n\n\n\n\n\n\n\n\nShade \\((A \\cup B)^c\\)\n\n\n\n\n\n\nAnswer Problem 3\n\n\n\n\n\n\n\n\n          A B\n\n\n\n\n\n\n\n\nShade \\(\\big( (A \\cup B)^c \\big) \\cup (A \\cap B)\\)\n\n\n\n\n\n\nAnswer Problem 4\n\n\n\n\n\n\n\n\n\n\n\n\nShade \\((A \\cup B)^c \\cap C\\)\n\n\n\n\n\n\nAnswer Problem 5\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s Simulate",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#welcome",
    "href": "MA206-AY26-1/lesson-3.html#welcome",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#events",
    "href": "MA206-AY26-1/lesson-3.html#events",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "An experiment is a process or action that produces observable outcomes.\n\nIt must have at least two possible outcomes.\n\nEach repetition of the experiment under the same conditions may produce a different outcome.\n\nExample 1: Flipping a coin\nExample 2: Rolling a die\n\n\n\n\nThe sample space is the set of all possible outcomes of a random experiment.\n\\(S = \\{ \\text{Outcome 1}, \\text{Outcome 2}, \\dots, \\text{Outcome n} \\}\\)\n\n\n\n\n\n\nSample space for coin flip\n\n\n\n\n\n\\(S = \\{ \\text{Heads}, \\text{Tails} \\}\\)\n\n\n\n\n\n\n\n\n\nSample space for die roll\n\n\n\n\n\n\\(S = \\{ 1,2,3,4,5,6 \\}\\)\n\n\n\n\n\n\n\nAn event is a subset of the sample space \\(S\\) of an experiment. So… the outcome of an experiment.\nNotation: If \\(A\\) is an event, then \\(A \\subseteq S\\)\nExample 1: Coin lands heads\n\n\n\n\n\n\nEvent heads in the coin flip sample space\n\n\n\n\n\nEvent \\(A =\\) “coin lands heads”\n\\[A = \\{ \\text{Heads} \\}\\]\n\n\n\nExample 2: Die lands 1 or 2\n\n\n\n\n\n\nEvent 1 or 2 in the die roll sample space\n\n\n\n\n\nEvent \\(B =\\) “die lands 1 or 2”\n\\[B = \\{ 1, 2 \\}\\]\n\n\n\n\n\n\n\nThe complement of an event \\(A\\), denoted \\(A^c\\), is the event that \\(A\\) does not occur.\nExample 1: Coin flip lands not heads\n\n\n\n\n\n\nComplement of \\(A =\\) ‘coin lands heads’\n\n\n\n\n\n\\(A^c =\\) “coin lands tails”\n\\[A^c = \\{ \\text{Tails} \\}\\]\n\n\n\nExample 2: Die lands not 1 or 2\n\n\n\n\n\n\nComplement of \\(B =\\) ‘die lands 1 or 2’\n\n\n\n\n\n\\(B^c =\\) “die lands 3,4,5,6”\n\\[B^c = \\{ 3,4,5,6 \\}\\]\n\n\n\n\n\n\n\nThe intersection of two events \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is the event that both \\(A\\) and \\(B\\) occur at the same time.\nExample: Coin flip + die roll\n- \\(A =\\) coin lands heads\n- \\(B =\\) die shows 1 or 2\n\n\n\n\n\n\nWhat is \\(A \\cap B\\)\n\n\n\n\n\n\\(A \\cap B =\\) “coin lands heads and die shows 1 or 2”\n\\[A \\cap B = \\{ (H,1), (H,2) \\}\\]\n\n\n\n\n\n\n\nThe union of two events \\(A\\) and \\(B\\), denoted \\(A \\cup B\\), is the event that either \\(A\\) occurs, or \\(B\\) occurs, or both occur.\nExample: Coin flip + die roll\n- \\(A =\\) coin lands heads\n- \\(B =\\) die shows 1 or 2\n\n\n\n\n\n\nWhat is \\(A \\cup B\\)\n\n\n\n\n\n\\(A \\cup B = \\{ (H,1), (H,2), (H,3), (H,4), (H,5), (H,6), (T,1), (T,2) \\}\\)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#probability",
    "href": "MA206-AY26-1/lesson-3.html#probability",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "The probability of an event \\(A\\), written \\(P(A)\\), is a number between \\(0\\) and \\(1\\) that measures the likelihood that \\(A\\) occurs.\nFor equally likely outcomes:\n\\[\nP(A) = \\frac{|A|}{|S|}\n\\]\nwhere:\n- \\(|A| =\\) number of outcomes in event \\(A\\)\n- \\(|S| =\\) number of outcomes in the sample space\nAll probabilities are between 0 and 1, inclusive, so the probability of an event \\(A\\) is \\(0 \\leq P(A) \\leq 1\\).\nExample 1: Coin flip\n\n\n\n\n\n\nWhat is the probability of heads\n\n\n\n\n\n\nSample space: \\(S = \\{\\text{Heads}, \\text{Tails}\\}\\), so \\(|S| = 2\\)\n\nEvent \\(A =\\) “coin lands heads” → \\(A = \\{\\text{Heads}\\}\\), so \\(|A| = 1\\)\n\n\\[\nP(A) = \\tfrac{|A|}{|S|} = \\tfrac{1}{2}\n\\]\n\n\n\nExample 2: Die roll\n\n\n\n\n\n\nWhat is the probability of rolling 1 or 2\n\n\n\n\n\n\nSample space: \\(S = \\{1,2,3,4,5,6\\}\\), so \\(|S| = 6\\)\n\nEvent \\(B =\\) “die shows 1 or 2” → \\(B = \\{1,2\\}\\), so \\(|B| = 2\\)\n\n\\[\nP(B) = \\tfrac{|B|}{|S|} = \\tfrac{2}{6} = \\tfrac{1}{3}\n\\]\n\n\n\n\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\nExample 1 (coin):\n\n\n\n\n\n\nWhat is the probability of not heads\n\n\n\n\n\n\n\\(P(\\text{A}) = \\tfrac{1}{2}\\)\n\nSo \\(P(A^c) = 1 - P(A) = 1 - \\tfrac{1}{2} = \\tfrac{1}{2}\\)\n\n\n\n\nExample 2 (die):\n\n\n\n\n\n\nWhat is the probability of not rolling 1 or 2\n\n\n\n\n\n\n\\(P(B) = \\tfrac{1}{3}\\)\n\nSo \\(P(B^c) = 1 - P(B) = 1 - \\tfrac{1}{3} = \\tfrac{2}{3}\\)\n\n\n\n\n\n\n\n\n\nIf all outcomes are equally likely: \\[\nP(A \\cap B) = \\frac{|A \\cap B|}{|S|}\n\\]\nExample 1 (coin + die):\n\n\n\n\n\n\nWhat is the probability the coin is heads AND the die is 1 or 2\n\n\n\n\n\n\nSample space size: \\(|S| = 12\\) (2 coin outcomes × 6 die outcomes)\n\n\\(A =\\) coin lands heads\n\n\\(B =\\) die shows 1 or 2\n\n\\(A \\cap B = \\{(H,1),(H,2)\\}\\), so \\(|A \\cap B| = 2\\)\n\n\\[\nP(A \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\n\\]\n\n\n\n\n\n\n\nIf all outcomes are equally likely: \\[\nP(A \\cup B) = \\frac{|A \\cup B|}{|S|}\n\\]\nExample 1 (coin + die):\n\n\n\n\n\n\nWhat is the probability the coin is heads OR the die is 1 or 2\n\n\n\n\n\n\nSample space size: \\(|S| = 12\\)\n\n\\(A =\\) coin lands heads\n\n\\(B =\\) die shows 1 or 2\n\n\\(A \\cup B = \\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2)\\}\\), so \\(|A \\cup B| = 8\\)\n\n\\[\nP(A \\cup B) = \\tfrac{8}{12} = \\tfrac{2}{3}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\n\n\n\n\n\\(A^c\\) (T)\n\n\n\n\n\nCol Total\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\\(P(A \\cap B)\\)\n\\(P(A \\cap B^c)\\)\n\\(P(A)\\)\n\n\n\\(A^c\\) (T)\n\\(P(A^c \\cap B)\\)\n\\(P(A^c \\cap B^c)\\)\n\\(P(A^c)\\)\n\n\nCol Total\n\\(P(B)\\)\n\\(P(B^c)\\)\n\\(1\\)\n\n\n\nReminder:\n- \\(A =\\) “coin lands heads”\n- \\(B =\\) “die shows 1 or 2”\n- \\(A^c =\\) “coin lands tails”\n- \\(B^c =\\) “die shows 3–6”\nThere are \\(|S|=12\\) equally likely outcomes for (coin, die).\n\n\n\n\n\\(A \\cap B\\) = \\(\\{(H,1),(H,2)\\}\\)\n\\(P(A \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\\)\n\\(A \\cap B^c\\) = \\(\\{(H,3),(H,4),(H,5),(H,6)\\}\\)\n\\(P(A \\cap B^c) = \\tfrac{4}{12} = \\tfrac{1}{3}\\)\n\\(A^c \\cap B\\) = \\(\\{(T,1),(T,2)\\}\\)\n\\(P(A^c \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\\)\n\\(A^c \\cap B^c\\) = \\(\\{(T,3),(T,4),(T,5),(T,6)\\}\\)\n\\(P(A^c \\cap B^c) = \\tfrac{4}{12} = \\tfrac{1}{3}\\)\n\n\n\n\n\n\\(P(A) = P(A \\cap B) + P(A \\cap B^c) = \\tfrac{1}{6} + \\tfrac{1}{3} = \\tfrac{1}{2}\\)\n\\(P(A^c) = P(A^c \\cap B) + P(A^c \\cap B^c) = \\tfrac{1}{6} + \\tfrac{1}{3} = \\tfrac{1}{2}\\)\n\\(P(B) = P(A \\cap B) + P(A^c \\cap B) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{1}{3}\\)\n\\(P(B^c) = P(A \\cap B^c) + P(A^c \\cap B^c) = \\tfrac{1}{3} + \\tfrac{1}{3} = \\tfrac{2}{3}\\)\nTotal: \\(P(S)=1\\)\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\\(\\tfrac{1}{6}\\)\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{1}{2}\\)\n\n\n\\(A^c\\) (T)\n\\(\\tfrac{1}{6}\\)\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{1}{2}\\)\n\n\nCol Total\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{2}{3}\\)\n\\(1\\)\n\n\n\n\n\n\n\n\n\n\n   A: Heads B: Die is 1 or 2 A ∩ B^c (H,3) (H,4) (H,5) (H,6) P = 1/3 A ∩ B (H,1) (H,2) P = 1/6 A^c ∩ B (T,1) (T,2) P = 1/6 (A^c ∩ B^c) (T,3) (T,4) (T,5) (T,6) P = 1/3",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#probability-rules",
    "href": "MA206-AY26-1/lesson-3.html#probability-rules",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n\nTwo events are mutually exclusive when it is impossible for both to happen at the same time.\nTherefore:\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\n\nExample 1 (coin + die):\n\n\n\n\n\n\nCompute \\(P(A \\cup B)\\) using the addition rule\n\n\n\n\n\n\n\\(P(A)=\\tfrac{1}{2}\\)\n\n\\(P(B)=\\tfrac{1}{3}\\)\n\\(P(A\\cap B)=\\tfrac{1}{6}\\)\n\nApply the rule: \\[\nP(A\\cup B)=\\tfrac{1}{2}+\\tfrac{1}{3}-\\tfrac{1}{6}=\\tfrac{2}{3}.\n\\]\nSet view: \\[\nA\\cup B=\\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2)\\},\\quad |A\\cup B|=8.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#board-problems",
    "href": "MA206-AY26-1/lesson-3.html#board-problems",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "Out of 100 students:\n- 40 like pizza\n- 30 like burgers\n- 10 of those included above like both pizza and burgers\nExperiment: I select one student at random\nLet:\n- \\(A =\\) “student likes pizza”\n- \\(B =\\) “student likes burgers”\n\nWhat is \\(P(A)\\)?\n\nWhat is \\(P(B)\\)?\n\nWhat is \\(P(A \\cap B)\\)?\n\nWhat is \\(P(A \\cup B)\\) using the addition rule?\n\nWhat is \\(P(A^c)\\), the probability a student does not like pizza?\n\nWhat is \\(P(B^c)\\), the probability a student does not like burgers?\n\nWhat is \\(P(A^c \\cap B^c)\\), the probability a student likes neither?\n\nVerify your results using the 2×2 probability table.\n\nRepresent the results with a Venn diagram.\n\n\n\n\n\n\n\nAnswers Problem 1\n\n\n\n\n\n\n\\(P(A) = \\tfrac{40}{100} = 0.40\\)\n\n\\(P(B) = \\tfrac{30}{100} = 0.30\\)\n\n\\(P(A \\cap B) = \\tfrac{10}{100} = 0.10\\)\n\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = 0.40 + 0.30 - 0.10 = 0.60\\)\n\n\\(P(A^c) = 1 - P(A) = 1 - 0.40 = 0.60\\)\n\n\\(P(B^c) = 1 - P(B) = 1 - 0.30 = 0.70\\)\n\n\\(P(A^c \\cap B^c) = 1 - (A \\cup \\ B) = 1 - \\tfrac{60}{100} = \\tfrac{40}{100} = 0.40\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (burger)\n\\(B^c\\) (not burger)\nRow Total\n\n\n\n\n\\(A\\) (pizza)\n\\(\\tfrac{10}{100} = 0.10\\)\n\\(\\tfrac{30}{100} = 0.30\\)\n\\(\\tfrac{40}{100} = 0.40\\)\n\n\n\\(A^c\\) (not pizza)\n\\(\\tfrac{20}{100} = 0.20\\)\n\\(\\tfrac{40}{100} = 0.40\\)\n\\(\\tfrac{60}{100} = 0.60\\)\n\n\nCol Total\n\\(\\tfrac{30}{100} = 0.30\\)\n\\(\\tfrac{70}{100} = 0.70\\)\n\\(1\\)\n\n\n\n\n\n\n\n\n     A: Pizza B: Burger  A ∩ Bᶜ 30 students P = 0.30  A ∩ B 10 students P = 0.10  Aᶜ ∩ B 20 students P = 0.20  Aᶜ ∩ Bᶜ 40 students P = 0.40\n\n\n\n\n\n\n\n\n\nShade \\(A \\cap B^c\\)\n\n\n\n\n\n\nAnswer Problem 2\n\n\n\n\n\n\n\n\n         A B\n\n\n\n\n\n\n\n\nShade \\((A \\cup B)^c\\)\n\n\n\n\n\n\nAnswer Problem 3\n\n\n\n\n\n\n\n\n          A B\n\n\n\n\n\n\n\n\nShade \\(\\big( (A \\cup B)^c \\big) \\cup (A \\cap B)\\)\n\n\n\n\n\n\nAnswer Problem 4\n\n\n\n\n\n\n\n\n\n\n\n\nShade \\((A \\cup B)^c \\cap C\\)\n\n\n\n\n\n\nAnswer Problem 5",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#simulation",
    "href": "MA206-AY26-1/lesson-3.html#simulation",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "Let’s Simulate",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-10.html",
    "href": "MA206-AY26-1/lesson-10.html",
    "title": "Lesson 10 — Test Day",
    "section": "",
    "text": "Lesson 10 — Test Day",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 10"
    ]
  },
  {
    "objectID": "MA206-AY26-1/assets/files/AY26-1_Tidyverse-Lab.html",
    "href": "MA206-AY26-1/assets/files/AY26-1_Tidyverse-Lab.html",
    "title": "MA206 Tidyverse Lab",
    "section": "",
    "text": "Save This File\nSave this .Rmd file with the title Lastname_Firstname_Tidyverse-Lab.Rmd into your RStudio folder.\n\n\nImporting Libraries\n\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n\nThe Research Question\nType your research question here.\n\n\nSummary of the ________ Data Set\nInsert your data set name above and type your summary here. You will fill out the table shortly.\n\n\nThe Data Set\nName and load your data set below.\n\nwage_data &lt;- read_csv(\"wage_data.csv\")\nhead(wage_data)\n\n# A tibble: 6 × 10\n  Education       Sex   Occupation   Age Earnings MaritalStatus Race  FamilySize\n  &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;      &lt;dbl&gt;\n1 Bachelors       M     40: Offic…    49   220000 Married       White          5\n2 Some College/A… F     53: Never…    51        0 Married       White          5\n3 Less than HS    F     39: Retai…    20     8000 Never Married White          5\n4 Less than HS    M     8: Comput…    16     4000 Never Married White          5\n5 Less than HS    F     53: Never…    80        0 Widowed       White          5\n6 Less than HS    M     32: Chefs…    27    17350 Never Married Black          2\n# ℹ 2 more variables: FamilyMakeup &lt;chr&gt;, Age_squared &lt;dbl&gt;\n\n\nThe output of the above code, in conjunction with any provided data dictionary, should enable you to complete the table below. Remove the information from the wage data set and use your own.\n\n\n\n\nVariable\nColumn Name\nUnits\nVariable Type\n\n\n\n\nEducation\nEducation\nN/A\nCategorical\n\n\nSex\nSex\nN/A\nCategorical\n\n\nOccupation\nOccupation\nN/A\nCategorical\n\n\nAge\nAge\nYears\nQuantitative\n\n\nEarnings\nEarnings\nDollars\nQuantitative\n\n\nMarital Status\nMaritalStatus\nN/A\nCategorical\n\n\nRace\nRace\nN/A\nCategorical\n\n\nFamily Size\nFamilySize\nN/A\nCategorical\n\n\nFamily Makeup\nFamilyMakeup\nN/A\nCategorical\n\n\nAge Squared\nAge_squared\nYears\nQuantitative\n\n\n\n\n\n\nPractice\nUse the below space to practice calling, selecting, filtering, summarizing, grouping by, and mutating variables.\n\n# call a variable\nwage_data$Age |&gt; head()\n\n[1] 49 51 20 16 80 27\n\n\n\n# select a variable\nwage_data |&gt; select(Age, Earnings)\n\n# A tibble: 180,084 × 2\n     Age Earnings\n   &lt;dbl&gt;    &lt;dbl&gt;\n 1    49   220000\n 2    51        0\n 3    20     8000\n 4    16     4000\n 5    80        0\n 6    27    17350\n 7    24    12000\n 8    62    25480\n 9    70        0\n10    53     6000\n# ℹ 180,074 more rows\n\n\n\n# filter a variable by two values\nwage_data |&gt; filter(Sex == \"M\", Age &gt; 40)\n\n# A tibble: 37,174 × 10\n   Education    Sex   Occupation     Age Earnings MaritalStatus Race  FamilySize\n   &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;      &lt;dbl&gt;\n 1 Bachelors    M     40: Office …    49   220000 Married       White          5\n 2 Bachelors    M     31: Animal …    62    25480 Never Married White          1\n 3 Bachelors    M     8: Computer…    52    70200 Married       Asian          6\n 4 Less than HS M     53: Never W…    50        0 Married       White          3\n 5 Less than HS M     53: Never W…    62        0 Married       White          2\n 6 Less than HS M     51: Transpo…    55    40000 Never Married White          1\n 7 Less than HS M     49: Product…    51    83000 Married       White          3\n 8 Less than HS M     40: Office …    47    35000 Married       White          4\n 9 Less than HS M     38: Retail …    70        0 Married       White          2\n10 Less than HS M     3: Educatio…    62        0 Divorced      White          1\n# ℹ 37,164 more rows\n# ℹ 2 more variables: FamilyMakeup &lt;chr&gt;, Age_squared &lt;dbl&gt;\n\n\n\n# summarize a variable\nwage_data |&gt; summarise(avg = mean(Age))\n\n# A tibble: 1 × 1\n    avg\n  &lt;dbl&gt;\n1  37.0\n\n\n\n# group a variable by a categorical variable\nwage_data |&gt; group_by(Sex) |&gt; summarise(ave = mean(Age))\n\n# A tibble: 2 × 2\n  Sex     ave\n  &lt;chr&gt; &lt;dbl&gt;\n1 F      37.9\n2 M      36.0\n\n\n\n# mutate a variable, using existing variables to create and name a new one\nwage_data |&gt; mutate(weird_age = Age * 2) |&gt; select(weird_age)\n\n# A tibble: 180,084 × 1\n   weird_age\n       &lt;dbl&gt;\n 1        98\n 2       102\n 3        40\n 4        32\n 5       160\n 6        54\n 7        48\n 8       124\n 9       140\n10       106\n# ℹ 180,074 more rows\n\n\n\n\n\nExplore Your Variables\n\n\n\nResponse Variable\nType the name, description, and units of your response variable here. Remember that this is a quantitative variable.\n\n# using quantitative variable techniques, create a summary table here\nwage_data$Earnings |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0   24813   35000 1609999 \n\n\n\n# using ggplot, create a visualization that shows the behavior of this variable here\nggplot(wage_data) +\n  geom_histogram(aes(x = Earnings))\n\n\n\n\n\n\n\n\n\n# using ggplot, create a second visualization that shows a different aspect of this variable\nggplot(wage_data) +\n  geom_point(aes(x = Age, y = Earnings, color = Sex)) +\n  labs(x = \"Age\", y = \"Earnings\")\n\n\n\n\n\n\n\n\nIn 1-2 sentences, describe this variable’s data. Which visualization is better, and why? Are there any questions that you have after exploring? Add code chunks below if you’d like to do some more exploration.\n\n\nQuantitative Explantory Variable #1\nType the name, description, and units of your quantitative variable here.\n\n# using quantitative variable techniques, create a summary table here. Replace\n# the 'dataframe' and 'variable' with your dataframe name and variable name\n\nwage_data |&gt; \n  summarize(mean = mean(Earnings),\n            s = sd(Earnings),\n            n = n())\n\n# A tibble: 1 × 3\n    mean      s      n\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;\n1 24813. 54264. 180084\n\n\n\n# using ggplot, create a visualization that shows the behavior of this variable here\n\n\n# using ggplot, create a second visualization that shows a different aspect of this variable\n\nIn 1-2 sentences, describe this variable’s data. Which visualization is better, and why? Are there any questions that you have after exploring? Add code chunks below if you’d like to do some more exploration.\n\n\nQuantitative Explantory Variable #2\nType the name, description, and units of your quantitative variable here.\n\n# using quantitative variable techniques, create a summary table here. Replace\n# the 'dataframe' and 'variable' with your dataframe name and variable name\n\n# dataframe %&gt;%\n#   summarize(mean = mean(variable),\n#             s = sd(variable),\n#             n = n())\n\n\n# using ggplot, create a visualization that shows the behavior of this variable here\n\n\n# using ggplot, create a second visualization that shows a different aspect of this variable\n\nIn 1-2 sentences, describe this variable’s data. Which visualization is better, and why? Are there any questions that you have after exploring? Add code chunks below if you’d like to do some more exploration.\n\n\nCategorical Explanatory Variable #1\nType the name, description, and units of your categorical variable here. Remember that this will require different code than your quantitative variables.\n\n# using categorical variable techniques, create a summary table here\nwage_data$Sex |&gt; table()\n\n\n    F     M \n92693 87391 \n\n\n\n# using ggplot, create a visualization that shows the behavior of this variable here\nwage_data |&gt; \n  ggplot() +\n  geom_boxplot(aes(y = Earnings, x = Sex))\n\n\n\n\n\n\n\n\n\n# using ggplot, create a second visualization that shows a different aspect of this variable\nwage_data |&gt; \n  ggplot() +\n  geom_histogram(aes(x = Earnings)) +\n  facet_wrap(~Sex)\n\n\n\n\n\n\n\n\nIn 1-2 sentences, describe this variable’s data. Which visualization is better, and why? Are there any questions that you have after exploring? Add code chunks below if you’d like to do some more exploration.\n\n\nCategorical Explanatory Variable #2\nType the name, description, and units of your categorical variable here. Remember that this will require different code than your quantitative variables.\n\n# using categorical variable techniques, create a summary table here\n\n\n# using ggplot, create a visualization that shows the behavior of this variable here\n\n\n# using ggplot, create a second visualization that shows a different aspect of this variable\n\nIn 1-2 sentences, describe this variable’s data. Which visualization is better, and why? Are there any questions that you have after exploring? Add code chunks below if you’d like to do some more exploration.\n\n\nHow are the variables associated?\nUsing ggplot, create visualizations that show relationships between your variables below. Since you have five variables, you will need at minimum four plots so that each variable is visualized at least once. It is possible to display relationships between 3+ variables in one plot; at least one of your plots should demonstrate mastery of this skill. Create more code chunks as needed.\n\n\n\nFinish the tutorial\n\nTest your skills by working through the code after the ggplot section of the Tutorial. These examples will help you gain a basic understanding of what is happening with specific commands or data structures within R, which will be useful to you over the course of the semester. Create more code chunks as needed.\n\n\n\nGetting Ready to Submit!\n\nNow that you’re done, you need to save this file (if the title is red, it has unsaved changes). RStudio does NOT autosave while you work, so CTRL+S early and often. Next, press the Knit button up top with the yarn icon. This will create an HTML file, because that was specified in the header. Save your HTML file with the name Lastname_Firstname_Tidyverse-Lab.html. Then, open the HTML file and print, using the `Microsoft Print to PDF\" option to save asLastname_Firstname_Tidyverse-Lab.pdf`. This PDF file is what you will submit on Canvas for Milestone 2."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Administration",
    "section": "",
    "text": "This page contains my instructor notes and lesson resources for our class.\nFor complete details, schedules, and additional materials, please visit our Canvas course page here: Canvas Course.\nLTC Dusty Turner\n\n\n\nHere is the project quarto file we worked through in class as a starting point",
    "crumbs": [
      "MA206-AY26-1",
      "Course Information"
    ]
  },
  {
    "objectID": "index.html#instructor-notes-and-class-resources",
    "href": "index.html#instructor-notes-and-class-resources",
    "title": "Course Administration",
    "section": "",
    "text": "This page contains my instructor notes and lesson resources for our class.\nFor complete details, schedules, and additional materials, please visit our Canvas course page here: Canvas Course.\nLTC Dusty Turner",
    "crumbs": [
      "MA206-AY26-1",
      "Course Information"
    ]
  },
  {
    "objectID": "index.html#tidyverse-tutorial-from-class",
    "href": "index.html#tidyverse-tutorial-from-class",
    "title": "Course Administration",
    "section": "",
    "text": "Here is the project quarto file we worked through in class as a starting point",
    "crumbs": [
      "MA206-AY26-1",
      "Course Information"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html",
    "href": "MA206-AY26-1/lesson-1.html",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "https://xkcd.com/552/\n \n\n\n\n\n\n\n\n\n\n\nPlease Share:\n\nName\n\nHometown\n\nCompany\n\nBirthday\n\nDid you come directly from high school?\n\nAcademic Major\nWhat you do in the Corps (Sport, Club, etc)\n\nFavorite sports team\n\nPossible Branch\n\nWhy you picked your seat today\n\n\n\n\n\n\nSandhurst\nOCF\nF4\nDallas Cowboys, San Antonio Spurs, Texas Rangers\n\n\n\n12345\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2003-2007 BS, Operations Research: United States Military Academy (USMA)\n\n2007-2008 Engineer Basic Officer Course: Fort Leonard Wood, Missouri\n\n2008-2011 Platoon Leader / XO / AS3: Schofield Barracks, HI / Iraq\n\n2011-2012 Engineer Captain’s Career Course: Missouri S&T\n\n2012 MS, Engineering Management: Missouri S&T\n\n2012-2014 Company Commander: White Sands Missile Range, NM / Afghanistan\n\n2014-2016 MS, Integrated Systems Engineering: The Ohio State University\n\n2016-2019 Assistant Professor: United States Military Academy, West Point\n\n2019-2022 ORSA / Data Scientist: Center for Army Analysis, Ft. Belvoir\n\n2022-2025 PhD, Statistical Science: Baylor University (Waco, TX)\n\n2025-? Academy Professor: United States Military Academy, West Point\n\n\n\n\n\nlibrary(leaflet)\n\nplaces &lt;- data.frame(\n  place = c(\"USMA (West Point, NY)\",\n            \"Schofield Barracks, HI\",\n            \"Missouri S&T (Rolla, MO)\",\n            \"White Sands Missile Range, NM\",\n            \"The Ohio State University (Columbus, OH)\",\n            \"Baylor University (Waco, TX)\",\n            \"Center for Army Analysis (Ft. Belvoir, VA)\"),\n  lat = c(41.391, 21.483, 37.954, 32.389, 39.999, 31.549, 38.711),\n  lng = c(-73.959, -158.063, -91.774, -106.491, -83.018, -97.114, -77.147)\n)\n\nleaflet(places) |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(\n    ~lng, ~lat,\n    radius = 6,\n    color = \"black\",\n    fillColor = \"red\",\n    fillOpacity = 0.85\n  ) |&gt;\n  addLabelOnlyMarkers(\n    ~lng, ~lat,\n    # label = ~place,\n    labelOptions = labelOptions(\n      noHide = TRUE,\n      direction = \"top\",\n      textOnly = TRUE,\n      style = list(\"color\" = \"black\", \"font-size\" = \"12px\", \"font-weight\" = \"bold\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000–2004 BS, Economics: Michigan State University\n\n2004–2010 Project Manager: Epic Systems\n\n2011–2020 Consultant / Build Analyst (Epic Radiant & Cadence)\n\n2011–2013 Epic Radiant Build Consultant: Intellistar Consulting\n\n2014 Epic Radiant Build Analyst: Vonlay\n\n2016–2017 Epic Radiant Build Analyst: Huron\n\n2018–2020 Epic Cadence Build Analyst: Bluetree Network\n\n\n2020–Present Solutions & Application Architect / Principal Analyst: Mayo Clinic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n123456789101112131415\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1234567891011121314151617\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\nDevelop a Base of Knowledge\n\nLeverage Technology\n\nCommunicate Concepts and Results\n\nProblem Solving Techniques\n\nDevelop habits of mind\n\nDevelop an interdisciplinary perspective\n\n\n\n\n\n\n\n\nArrive prepared for each lesson\n\nEncourage independent thinking\nMaintain professionalism and respect at all times\n\nUphold the values of the Corps and our institution\n\nClear guidance and expectations for assignments\n\nBe a professional mentor\n\nMake Mistakes\n\n\n\n\n\n\n\n\n\n\nBe responsible for your learning\nArrive prepared for each lesson\n\nEngage actively in discussions and exercises\n\nMaintain professionalism and respect at all times\n\nUphold the values of the Corps and our institution - you are junior members of this profession\n\nCommunicate early if challenges arise\n\nMake mistakes\n\n\n\n\n\nComputers will only be used for course materials only\n\nNo food or gum allowed in the classroom\n\nOnly drinks in spill-proof containers are allowed\n\nLeave bags, backpacks, coats, and hats in the hallway\n\nStay awake in class. Stand up if you are tired\n\nArrive on time and do not start packing up before I dismiss the class\n\nBe respectful when others are speaking\n\nSupport the section marcher\n\n\n\n\n\n\n\n\nProbability\n\nFoundations: preliminaries, dataset exploration, tidyverse basics\n\nCore Probability: principles, conditional probability, rules of random variables\n\nRandom Variables: discrete, continuous, and named distributions\n\nStatistical Tests\n\nOne-sample tests: one proportion Z-test, one mean T-test\n\nConfidence Intervals: categorical and quantitative data\n\nComparative tests: two proportion Z-test, two mean T-test, paired data\n\nBroader Concepts: generalization, causation, and investigation labs\n\nRegression\n\nCorrelation & Simple Linear Regression\n\nMultiple Linear Regression (I–III)\n\nApplications: project work, presentations, writer’s workshop, course review\n\n\n\n\n\n\nCanvas\nCalendar (Day 1) (Day 2)\nBook: Introduction to Statistical Investigations (Digital or hard copy authorized)\n\nCourse Guide\n\nCourse Admin\n\nSpecific Help / Instructions\n\nR Code\n\n\nGraded Assignments\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nEvent\nPoints\n\n\n\n\nDay 0 Assignment\n15\n\n\nGenerative AI Certification\n25\n\n\nExploration Exercises (6 @ 10 points each)\n60\n\n\nStatistical Investigation Labs: - SIL 1 (25) - SIL 2 (25)\n50\n\n\nWPR 1\n125\n\n\nWPR 2\n140\n\n\nCourse Project: - Milestone 1 – Setup and Data (25) - Milestone 2 – EDA (25) - Milestone 3 – Intro & Academic Articles (30) - Milestone 4 – Literature Review (25) - Milestone 5 – Methodology (30) - Milestone 6 – Results, Discussion, Conclusion (60) - In-Progress Review (20) - Presentation (50) - Milestone 7 – Writer’s Workshop (25) - Final Turn In (30)\n300\n\n\nTEE\n285\n\n\nTOTAL\n1000\n\n\n\n\n\n\nProject\nA note on course grades\nWhere to get this presentation\n\n\n\n\n\n\n\nNote\n\n\n\nFor assignments worth less than 20 points that are turned in late:\n50% reduction if turned in before Lesson 30 For assignments turned in late worth 20 points or more:\n10% reduction per day until assignment is worth 0 points (10 days late)\n\n\n\n\n\n\n\nAcademic Integrity Brief\n\nAcademic Security\n\n\n\n\n\n\n\n\n\n\n\n\nGo to Lesson 1 on Canvas\nNote the objectives\nDo the reading\nWatch the videos\nNote the Course Guide\n\nDo the Homework (If applicable)\n\n\n\n\n\n\n\n\n\n\n\n\nAsk a question\nDo basketball cadets tend to be taller than cadets who aren’t on the basketball team?\nDesign a study and collect data\nMeasure the heights of some basketball cadets and some non-basketball cadets.\n\n\n\nBasketball cadets: 71, 72, 73, 74, 75, 75, 75, 75, 76, 76, 76, 77, 77, 77, 78 \n\n\nNon-basketball cadets: 68, 68, 70, 70, 70, 70, 70, 70, 72, 73\n\n\n\nExplore the data\nAverage basketball cadet height: 75.1 in\nAverage non-basketball cadet height: 70.1 in\n\n\n\n\n\n\n\n\n\n\n\nDraw inferences\nThe basketball group is taller by about 5 inches in this sample.\nFormulate conclusions\nLooks like basketball cadets are taller.\nLook back and ahead\nYou could improve this by measuring more cadets or using a statistical test.\n\n\n\n\n\nAsk a question\nIf the host opens a goat door, are you better off sticking with your first door or switching to the other unopened door?\nDesign a study and collect data\nWe simulate the game many times and record whether “stay” or “switch” wins the car.\nExplore the data\nDraw inferences\nFormulate conclusions\nLook back and ahead\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 2\n\n\n\n\n\nProject Milestone 1: Due 22 Aug (Friday) All Sections - Read through this and come to class with questions\nGenAI Certification: Due 25 August (Monday) All Sections",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#welcome",
    "href": "MA206-AY26-1/lesson-1.html#welcome",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "https://xkcd.com/552/",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#introductions",
    "href": "MA206-AY26-1/lesson-1.html#introductions",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Please Share:\n\nName\n\nHometown\n\nCompany\n\nBirthday\n\nDid you come directly from high school?\n\nAcademic Major\nWhat you do in the Corps (Sport, Club, etc)\n\nFavorite sports team\n\nPossible Branch\n\nWhy you picked your seat today\n\n\n\n\n\n\nSandhurst\nOCF\nF4\nDallas Cowboys, San Antonio Spurs, Texas Rangers\n\n\n\n12345\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2003-2007 BS, Operations Research: United States Military Academy (USMA)\n\n2007-2008 Engineer Basic Officer Course: Fort Leonard Wood, Missouri\n\n2008-2011 Platoon Leader / XO / AS3: Schofield Barracks, HI / Iraq\n\n2011-2012 Engineer Captain’s Career Course: Missouri S&T\n\n2012 MS, Engineering Management: Missouri S&T\n\n2012-2014 Company Commander: White Sands Missile Range, NM / Afghanistan\n\n2014-2016 MS, Integrated Systems Engineering: The Ohio State University\n\n2016-2019 Assistant Professor: United States Military Academy, West Point\n\n2019-2022 ORSA / Data Scientist: Center for Army Analysis, Ft. Belvoir\n\n2022-2025 PhD, Statistical Science: Baylor University (Waco, TX)\n\n2025-? Academy Professor: United States Military Academy, West Point\n\n\n\n\n\nlibrary(leaflet)\n\nplaces &lt;- data.frame(\n  place = c(\"USMA (West Point, NY)\",\n            \"Schofield Barracks, HI\",\n            \"Missouri S&T (Rolla, MO)\",\n            \"White Sands Missile Range, NM\",\n            \"The Ohio State University (Columbus, OH)\",\n            \"Baylor University (Waco, TX)\",\n            \"Center for Army Analysis (Ft. Belvoir, VA)\"),\n  lat = c(41.391, 21.483, 37.954, 32.389, 39.999, 31.549, 38.711),\n  lng = c(-73.959, -158.063, -91.774, -106.491, -83.018, -97.114, -77.147)\n)\n\nleaflet(places) |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(\n    ~lng, ~lat,\n    radius = 6,\n    color = \"black\",\n    fillColor = \"red\",\n    fillOpacity = 0.85\n  ) |&gt;\n  addLabelOnlyMarkers(\n    ~lng, ~lat,\n    # label = ~place,\n    labelOptions = labelOptions(\n      noHide = TRUE,\n      direction = \"top\",\n      textOnly = TRUE,\n      style = list(\"color\" = \"black\", \"font-size\" = \"12px\", \"font-weight\" = \"bold\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000–2004 BS, Economics: Michigan State University\n\n2004–2010 Project Manager: Epic Systems\n\n2011–2020 Consultant / Build Analyst (Epic Radiant & Cadence)\n\n2011–2013 Epic Radiant Build Consultant: Intellistar Consulting\n\n2014 Epic Radiant Build Analyst: Vonlay\n\n2016–2017 Epic Radiant Build Analyst: Huron\n\n2018–2020 Epic Cadence Build Analyst: Bluetree Network\n\n\n2020–Present Solutions & Application Architect / Principal Analyst: Mayo Clinic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n123456789101112131415\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1234567891011121314151617",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#expectations",
    "href": "MA206-AY26-1/lesson-1.html#expectations",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Develop a Base of Knowledge\n\nLeverage Technology\n\nCommunicate Concepts and Results\n\nProblem Solving Techniques\n\nDevelop habits of mind\n\nDevelop an interdisciplinary perspective\n\n\n\n\n\n\n\n\nArrive prepared for each lesson\n\nEncourage independent thinking\nMaintain professionalism and respect at all times\n\nUphold the values of the Corps and our institution\n\nClear guidance and expectations for assignments\n\nBe a professional mentor\n\nMake Mistakes\n\n\n\n\n\n\n\n\n\n\nBe responsible for your learning\nArrive prepared for each lesson\n\nEngage actively in discussions and exercises\n\nMaintain professionalism and respect at all times\n\nUphold the values of the Corps and our institution - you are junior members of this profession\n\nCommunicate early if challenges arise\n\nMake mistakes\n\n\n\n\n\nComputers will only be used for course materials only\n\nNo food or gum allowed in the classroom\n\nOnly drinks in spill-proof containers are allowed\n\nLeave bags, backpacks, coats, and hats in the hallway\n\nStay awake in class. Stand up if you are tired\n\nArrive on time and do not start packing up before I dismiss the class\n\nBe respectful when others are speaking\n\nSupport the section marcher",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#course-overview-admin",
    "href": "MA206-AY26-1/lesson-1.html#course-overview-admin",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Probability\n\nFoundations: preliminaries, dataset exploration, tidyverse basics\n\nCore Probability: principles, conditional probability, rules of random variables\n\nRandom Variables: discrete, continuous, and named distributions\n\nStatistical Tests\n\nOne-sample tests: one proportion Z-test, one mean T-test\n\nConfidence Intervals: categorical and quantitative data\n\nComparative tests: two proportion Z-test, two mean T-test, paired data\n\nBroader Concepts: generalization, causation, and investigation labs\n\nRegression\n\nCorrelation & Simple Linear Regression\n\nMultiple Linear Regression (I–III)\n\nApplications: project work, presentations, writer’s workshop, course review\n\n\n\n\n\n\nCanvas\nCalendar (Day 1) (Day 2)\nBook: Introduction to Statistical Investigations (Digital or hard copy authorized)\n\nCourse Guide\n\nCourse Admin\n\nSpecific Help / Instructions\n\nR Code\n\n\nGraded Assignments\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nEvent\nPoints\n\n\n\n\nDay 0 Assignment\n15\n\n\nGenerative AI Certification\n25\n\n\nExploration Exercises (6 @ 10 points each)\n60\n\n\nStatistical Investigation Labs: - SIL 1 (25) - SIL 2 (25)\n50\n\n\nWPR 1\n125\n\n\nWPR 2\n140\n\n\nCourse Project: - Milestone 1 – Setup and Data (25) - Milestone 2 – EDA (25) - Milestone 3 – Intro & Academic Articles (30) - Milestone 4 – Literature Review (25) - Milestone 5 – Methodology (30) - Milestone 6 – Results, Discussion, Conclusion (60) - In-Progress Review (20) - Presentation (50) - Milestone 7 – Writer’s Workshop (25) - Final Turn In (30)\n300\n\n\nTEE\n285\n\n\nTOTAL\n1000\n\n\n\n\n\n\nProject\nA note on course grades\nWhere to get this presentation\n\n\n\n\n\n\n\nNote\n\n\n\nFor assignments worth less than 20 points that are turned in late:\n50% reduction if turned in before Lesson 30 For assignments turned in late worth 20 points or more:\n10% reduction per day until assignment is worth 0 points (10 days late)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#mandatory-and-important-briefings",
    "href": "MA206-AY26-1/lesson-1.html#mandatory-and-important-briefings",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Academic Integrity Brief\n\nAcademic Security",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#lesson-1",
    "href": "MA206-AY26-1/lesson-1.html#lesson-1",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Go to Lesson 1 on Canvas\nNote the objectives\nDo the reading\nWatch the videos\nNote the Course Guide\n\nDo the Homework (If applicable)\n\n\n\n\n\n\n\n\n\n\n\n\nAsk a question\nDo basketball cadets tend to be taller than cadets who aren’t on the basketball team?\nDesign a study and collect data\nMeasure the heights of some basketball cadets and some non-basketball cadets.\n\n\n\nBasketball cadets: 71, 72, 73, 74, 75, 75, 75, 75, 76, 76, 76, 77, 77, 77, 78 \n\n\nNon-basketball cadets: 68, 68, 70, 70, 70, 70, 70, 70, 72, 73\n\n\n\nExplore the data\nAverage basketball cadet height: 75.1 in\nAverage non-basketball cadet height: 70.1 in\n\n\n\n\n\n\n\n\n\n\n\nDraw inferences\nThe basketball group is taller by about 5 inches in this sample.\nFormulate conclusions\nLooks like basketball cadets are taller.\nLook back and ahead\nYou could improve this by measuring more cadets or using a statistical test.\n\n\n\n\n\nAsk a question\nIf the host opens a goat door, are you better off sticking with your first door or switching to the other unopened door?\nDesign a study and collect data\nWe simulate the game many times and record whether “stay” or “switch” wins the car.\nExplore the data\nDraw inferences\nFormulate conclusions\nLook back and ahead",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-1.html#before-you-leave",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nLesson 2\n\n\n\n\n\nProject Milestone 1: Due 22 Aug (Friday) All Sections - Read through this and come to class with questions\nGenAI Certification: Due 25 August (Monday) All Sections",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html",
    "href": "MA206-AY26-1/lesson-2.html",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\nDon’t forget\n\n\n\n\n\n\n\n\n\n\nGenAI Assignment\n\n\n\n\n\n\n\n\n\n\n\nLet’s go to Canvas\n\n\n\n\n\n\nYour work will result in a presentation and a technical report.\nBoth these files are found on Canvas.\nUltimately, you are going to conduct a linear regression where you determine how much one variable impacted by other variables. For example:\n\n\n\n\n\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i = 1, 2, \\dots, n\\)\n\n\n\n\n\n\n\n\n\nBut there might be multiple things that can impact our dependent variable.\n\n\n\n\n\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i,\n\\quad i = 1, 2, \\dots, n\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis milestone sets up your binder and makes sure you have acceptable data.\nLets navigate to it on Canvas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis are just ideas on where to get data\n\nWe’ll talk about the Dataset Worksheet in a moment\n\n\n\n\n\n\nWhat is synthetic data?\n\n\n# A tibble: 10 × 6\n      id sport    gender height_cm training_hours run_time_min\n   &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1     1 Handball Female      171.            5.7         18.3\n 2     2 Track    Male        183.            7.9         14.5\n 3     3 Handball Female      152.            6.5         18.7\n 4     4 Lacrosse Male        187.            5.8         19.3\n 5     5 Lacrosse Male        179.            5.8         19.2\n 6     6 Lacrosse Female      170.            6.8         19.1\n 7     7 Track    Female      160.            6.4         17.2\n 8     8 Handball Male        172.            6.6         19.6\n 9     9 Track    Male        165.            8.7         14.6\n10    10 Handball Female      160.            3.9         19.3\n\n\n\n\n\n\n\nDid we satisfy this?\n\n\n\n\n\nHow about this?\n\n\n\n\n\nOkay, now this?\n\n\n\n\n\nAnd finally, this!\n\n\n\n\n\n\n\n\nThis is where you show me you’ve met the criteria.\n\n\n\n\n\n\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n     carat               cut        color        clarity          depth      \n Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065   Min.   :43.00  \n 1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258   1st Qu.:61.00  \n Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194   Median :61.80  \n Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171   Mean   :61.75  \n 3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066   3rd Qu.:62.50  \n Max.   :5.0100                     I: 5422   VVS1   : 3655   Max.   :79.00  \n                                    J: 2808   (Other): 2531                  \n     table           price             x                y         \n Min.   :43.00   Min.   :  326   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710   1st Qu.: 4.720  \n Median :57.00   Median : 2401   Median : 5.700   Median : 5.710  \n Mean   :57.46   Mean   : 3933   Mean   : 5.731   Mean   : 5.735  \n 3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540   3rd Qu.: 6.540  \n Max.   :95.00   Max.   :18823   Max.   :10.740   Max.   :58.900  \n                                                                  \n       z         \n Min.   : 0.000  \n 1st Qu.: 2.910  \n Median : 3.530  \n Mean   : 3.539  \n 3rd Qu.: 4.040  \n Max.   :31.800  \n                 \n\n\n\n\n\n\nI’m glad you asked!\n\n\n\n\n\n\n\nAnnex B Milestone 2 has you do the Tidyverse Tutorial on your own data\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 3\n\n\n\n\n\nProject Milestone 1: Due 22 Aug (Friday) All Sections\nGenAI Certification: Due 25 August (Monday) All Sections",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#welcome",
    "href": "MA206-AY26-1/lesson-2.html#welcome",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\nDon’t forget",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#genai-assignment-due-25-aug",
    "href": "MA206-AY26-1/lesson-2.html#genai-assignment-due-25-aug",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "GenAI Assignment",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#project-the-long-view",
    "href": "MA206-AY26-1/lesson-2.html#project-the-long-view",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "Let’s go to Canvas\n\n\n\n\n\n\nYour work will result in a presentation and a technical report.\nBoth these files are found on Canvas.\nUltimately, you are going to conduct a linear regression where you determine how much one variable impacted by other variables. For example:\n\n\n\n\n\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i = 1, 2, \\dots, n\\)\n\n\n\n\n\n\n\n\n\nBut there might be multiple things that can impact our dependent variable.\n\n\n\n\n\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i,\n\\quad i = 1, 2, \\dots, n\\)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#project-milestone-1",
    "href": "MA206-AY26-1/lesson-2.html#project-milestone-1",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "This milestone sets up your binder and makes sure you have acceptable data.\nLets navigate to it on Canvas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis are just ideas on where to get data\n\nWe’ll talk about the Dataset Worksheet in a moment\n\n\n\n\n\n\nWhat is synthetic data?\n\n\n# A tibble: 10 × 6\n      id sport    gender height_cm training_hours run_time_min\n   &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1     1 Handball Female      171.            5.7         18.3\n 2     2 Track    Male        183.            7.9         14.5\n 3     3 Handball Female      152.            6.5         18.7\n 4     4 Lacrosse Male        187.            5.8         19.3\n 5     5 Lacrosse Male        179.            5.8         19.2\n 6     6 Lacrosse Female      170.            6.8         19.1\n 7     7 Track    Female      160.            6.4         17.2\n 8     8 Handball Male        172.            6.6         19.6\n 9     9 Track    Male        165.            8.7         14.6\n10    10 Handball Female      160.            3.9         19.3\n\n\n\n\n\n\n\nDid we satisfy this?\n\n\n\n\n\nHow about this?\n\n\n\n\n\nOkay, now this?\n\n\n\n\n\nAnd finally, this!\n\n\n\n\n\n\n\n\nThis is where you show me you’ve met the criteria.\n\n\n\n\n\n\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n     carat               cut        color        clarity          depth      \n Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065   Min.   :43.00  \n 1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258   1st Qu.:61.00  \n Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194   Median :61.80  \n Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171   Mean   :61.75  \n 3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066   3rd Qu.:62.50  \n Max.   :5.0100                     I: 5422   VVS1   : 3655   Max.   :79.00  \n                                    J: 2808   (Other): 2531                  \n     table           price             x                y         \n Min.   :43.00   Min.   :  326   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710   1st Qu.: 4.720  \n Median :57.00   Median : 2401   Median : 5.700   Median : 5.710  \n Mean   :57.46   Mean   : 3933   Mean   : 5.731   Mean   : 5.735  \n 3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540   3rd Qu.: 6.540  \n Max.   :95.00   Max.   :18823   Max.   :10.740   Max.   :58.900  \n                                                                  \n       z         \n Min.   : 0.000  \n 1st Qu.: 2.910  \n Median : 3.530  \n Mean   : 3.539  \n 3rd Qu.: 4.040  \n Max.   :31.800",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#so-where-can-i-get-data",
    "href": "MA206-AY26-1/lesson-2.html#so-where-can-i-get-data",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "I’m glad you asked!",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-2.html#before-you-leave",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "Annex B Milestone 2 has you do the Tidyverse Tutorial on your own data\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 3\n\n\n\n\n\nProject Milestone 1: Due 22 Aug (Friday) All Sections\nGenAI Certification: Due 25 August (Monday) All Sections",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html",
    "href": "MA206-AY26-1/lesson-4.html",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Project Milestone 2\n\nTidyverse Tutorial applied to your team’s data\nAnnex B (for addressing feedback)\n\nExploration 11.3B\n\n\n\n\n\n\nNot Cal, but….\n\n  Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\nPreviously 0-0\n\n\n\n\n\n\n1-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy I become an Engineer\nWhat I think you should do\nWhat do I do as an Engineer\nWhat is my branch now?\n\n\n\n\n\n\nRemember this?\nOut of 100 students:\n- 40 like pizza\n- 30 like burgers\n- 10 of those included above like both pizza and burgers\nExperiment: I select one student at random\nLet:\n- \\(A =\\) “student likes pizza”\n- \\(B =\\) “student likes burgers”\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\nWhat is the probablity that a randomly selected student doesn’t like pizza (\\(P(A^c)\\))?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe know \\(P(A) = 40/100 = 0.40\\).\nBy the complement rule:\n\\[\nP(A^c) = 1 - P(A) = 1 - 0.40 = 0.60\n\\]\n\n\n\n\n\n\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n   A B\n\n\nWhat is the probability a randomly selected cadet likes pizza or burgers? \\(P(A \\cup B)\\)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy the addition rule:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\nSubstitute values:\n\\[\nP(A \\cup B) = 0.40 + 0.30 - 0.10 = 0.60\n\\]\n\n\n\n\n\n\nTwo events are mutually exclusive when it is impossible for both to happen at the same time.\nTherefore:\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n   A B\n\n\nAre events A and B mutually exclusive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNo because:\n\\[\nP(A \\cup B) \\neq P(A) + P(B)\n\\]\n\n\n\n\n\n\nTwo events \\(A\\) and \\(B\\) are independent if knowing that one occurs does not change the probability of the other.\n\\[\nP(A \\cap B) = P(A)P(B)\n\\]\nEquivalently (if \\(P(B) &gt; 0\\)):\n\\[\nP(A \\mid B) = P(A)\n\\]\nIntuition: The outcome of one event gives no information about the other.\nDoes Mutually Exclusive imply independence or no or undetermined?\nAre \\(A =\\) “likes pizza” and \\(B =\\) “likes burgers” independent?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe check using the multiplication rule for independence:\n\\[\nP(A)P(B) = (0.40)(0.30) = 0.12\n\\]\nBut from the data:\n\\[\nP(A \\cap B) = 0.10\n\\]\nSince \\(0.10 \\neq 0.12\\), the events are not independent.\nAlso, they are not mutually exclusive either, because\n\\[\nP(A \\cap B) = 0.10 &gt; 0.\n\\]\nSo in this example, \\(A\\) and \\(B\\) are neither independent nor mutually exclusive.\nGeneral Note:\nIf \\(A\\) and \\(B\\) are mutually exclusive with nonzero probabilities, they cannot be independent.\n\n\n\n\n\n\n\nWe’ll use a single Factory Machines scenario for all concepts in this section.\nFactory Setup (used for all examples)\nA factory produces 1,000 items per day using two machines:\n\nMachine \\(M_A\\) produces 400 items, of which 40 are defective.\n\nMachine \\(M_B\\) produces 600 items, of which 24 are defective.\n\nDefine events:\n- \\(M_A =\\) “item came from Machine A”\n- \\(M_B =\\) “item came from Machine B”\n- \\(D =\\) “item is defective”\n- \\(D^c =\\) “item is not defective”\nFrom the counts:\n- \\(P(M_A) = 400/1000 = 0.40\\)\n- \\(P(M_B) = 600/1000 = 0.60\\)\n- \\(P(D \\cap M_A) = 40/1000 = 0.04\\)\n- \\(P(D \\cap M_B) = 24/1000 = 0.024\\)\n- \\(P(D) = (40+24)/1000 = 0.064\\)\n\n\nThe probability that event \\(A\\) occurs given that event \\(B\\) occurs:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\n\\]\nIntuition: Restrict the sample space to \\(B\\); ask what fraction of those outcomes also fall in \\(A\\).\n\n\n      A B\n\n\nExample (Factory Machines)\nWhat is \\(P(D \\mid M_A)\\), the probability an item is defective given it was made by Machine \\(A\\)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy definition:\n\\[\nP(D \\mid M_A) = \\frac{P(D \\cap M_A)}{P(M_A)}.\n\\]\nFrom the setup:\n- \\(P(D \\cap M_A) = 40/1000 = 0.04\\)\n- \\(P(M_A) = 400/1000 = 0.40\\)\nSo:\n\\[\nP(D \\mid M_A) = \\frac{0.04}{0.40} = 0.10.\n\\]\n\n\n\n\n\n\nRelates intersections to conditional probabilities.\n\\[\\begin{align}\nP(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\n&& \\text{definition of conditional probability} \\\\[24pt]\n\nP(A \\mid B)\\,P(B) &= \\frac{P(A \\cap B)}{P(B)} \\cdot P(B)\n&& \\text{multiply both sides by $P(B)$} \\\\[24pt]\n\nP(A \\mid B)\\,P(B) &= P(A \\cap B)\n&& \\text{simplify} \\\\[24pt]\n\nP(B \\mid A)\\,P(A) &= P(A \\cap B)\n&& \\text{swap $A$ and $B$} \\\\[24pt]\n\nP(A \\cap B) &= P(A \\mid B)P(B) = P(B \\mid A)P(A)\n&& \\text{final multiplication rule}\n\\end{align}\\]\nIntuition: To find the chance that both happen, compute the chance that one happens, then multiply by the chance the other happens given that.\nExample (Factory Machines)\nUsing the rule, compute \\(P(D \\cap M_B)\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy the multiplication rule:\n\\[\nP(D \\cap M_B) = P(D \\mid M_B)\\,P(M_B).\n\\]\nFrom counts:\n- \\(P(D \\mid M_B) = 24/600 = 0.04\\)\n- \\(P(M_B) = 600/1000 = 0.60\\)\nSo:\n\\[\nP(D \\cap M_B) = (0.04)(0.60) = 0.024.\n\\]\nThis matches the direct count calculation \\(24/1000=0.024\\).\n\n\n\n\n\nThe multiplication rule shows us how to express the probability of two events happening together in terms of a conditional probability:\n\\[\nP(A \\cap B) = P(A \\mid B)\\, P(B).\n\\]\nNow imagine that the whole sample space can be split into two non-overlapping cases (aka mutually exclusive) \\(B_1\\) and \\(B_2\\) (for example: “athlete” vs. “not an athlete”).\nSince \\(B_1\\) and \\(B_2\\) cover all possibilities, any time \\(A\\) happens, it must happen either with \\(B_1\\) or with \\(B_2\\).\n\n\n\n\n\nTo find \\(P(A)\\), we know that it is the union of where A intersects B in all ways that B can happen. so:\n\\[\nP(A) = P((A \\cap B_1) \\cup (A \\cap B_2))\n\\]\nBecause \\((A \\cap B_1)\\) and \\((A \\cap B_2)\\) are mutually exclusive, we can use the mutually exclusive version of the addition rule \\(P(E_1 \\cup E_2) = P(E_1) + P(E_2)\\) to get:\n\\[\nP((A \\cap B_1) \\cup (A \\cap B_2)) = P(A \\cap B_1) + P(A \\cap B_2).\n\\]\nAnd if we apply the multiplication rule (\\(P(E_1 \\cap E_2) = P(E_1 \\mid E_2)\\, P(E_2)\\)) to each intersection, we get exactly the Law of Total Probability.\n\\[\nP(A) = P(A | B_1)P(B_1) + P(A | B_2)P(B_2).\n\\]\n\n\n\n\nWhat we just derived for two outcomes extends to any partition of the sample space.\nIf \\(B_1, B_2, \\ldots, B_k\\) are mutually exclusive and exhaustive events, then for any event \\(A\\):\n\\[\nP(A) = \\sum_{i=1}^{k} P(A \\mid B_i)\\, P(B_i).\n\\]\nFor two outcomes:\n\\[\nP(A) = P(A \\mid B_1)\\,P(B_1) \\;+\\; P(A \\mid B_2)\\,P(B_2).\n\\]\nIntuition:\nInstead of calculating \\(P(A)\\) directly, we “partition” the sample space into simpler pieces \\(B_1, B_2, \\ldots, B_k\\). We compute \\(P(A)\\) by adding up the contributions from each path.\nExample (Factory Machines)\nWhat is the overall probability that a randomly chosen item is defective (\\(P(D)\\))?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPartition by which machine made the item:\n\\[\nP(D) = P(D \\mid M_A)P(M_A) + P(D \\mid M_B)P(M_B).\n\\]\nFrom counts:\n- \\(P(D \\mid M_A)=40/400=0.10\\), \\(P(M_A)=0.40\\)\n- \\(P(D \\mid M_B)=24/600=0.04\\), \\(P(M_B)=0.60\\)\nSo:\n\\[\nP(D) = (0.10)(0.40) + (0.04)(0.60) = 0.04 + 0.024 = 0.064.\n\\]\nSo 6.4% of all items are defective.\n\n\n\n\n\n\nBayes’ Theorem lets us “flip” conditional probabilities. It tells us how to update beliefs about a cause (\\(B\\)) when we observe some evidence (\\(A\\)).\nFor events \\(A\\) and \\(B\\) with \\(P(B) &gt; 0\\):\n\\[\nP(B \\mid A) = \\frac{P(A \\mid B)\\, P(B)}{P(A)}.\n\\]\nUsing the Law of Total Probability for \\(P(A)\\):\n\\[\nP(B_j \\mid A) = \\frac{P(A \\mid B_j)\\, P(B_j)}{\\sum_{i=1}^{k} P(A \\mid B_i)\\, P(B_i)}.\n\\]\n\\[\nP(B_2 \\mid A) = \\frac{P(A \\mid B_2)\\, P(B_2)}{P(A \\mid B_1)P(B_1) + P(A \\mid B_2)P(B_2)}.\n\\]\nIntuition:\nThink of Bayes’ theorem as a way to reverse the condition. If we know how likely \\(A\\) is when \\(B\\) happens, Bayes tells us how likely \\(B\\) is given that we saw \\(A\\).\nExample (Factory Machines)\nIf an item is defective, what is the probability it came from Machine A?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe want \\(P(M_A \\mid D)\\).\nBy Bayes’ theorem:\n\\[\nP(M_A \\mid D) = \\frac{P(D \\mid M_A)\\, P(M_A)}{P(D)}.\n\\]\nThe denominator \\(P(D)\\) comes from the Law of Total Probability:\n\\[\nP(D) = P(D \\mid M_A)P(M_A) + P(D \\mid M_B)P(M_B).\n\\]\nFrom counts:\n- \\(P(D \\mid M_A)=40/400=0.10\\), \\(P(M_A)=400/1000=0.40\\)\n- \\(P(D \\mid M_B)=24/600=0.04\\), \\(P(M_B)=600/1000=0.60\\)\nSo:\n\\[\nP(D) = (0.10)(0.40) + (0.04)(0.60) = 0.064\n\\]\nNow substitute into Bayes’ theorem:\n\\[\nP(M_A \\mid D) = \\frac{0.10 \\cdot 0.40}{0.064} = \\frac{0.04}{0.064} = 0.625.\n\\]\nThus, if an item is defective, there is a 62.5% chance it was made by Machine A.\n\n\n\n\n\n\nAnother way to picture the Law of Total Probability is to start with the partition (\\(M_A\\) vs \\(M_B\\)), then branch into whether \\(D\\) happens or not under each case.\nStart\n├── M_A (0.40)\n│   ├── D (40/400 = 0.10)   ⇒ 0.40 · 0.10 = 0.04\n│   └── D^c (360/400 = 0.90) ⇒ 0.40 · 0.90 = 0.36\n└── M_B (0.60)\n    ├── D (24/600 = 0.04)   ⇒ 0.60 · 0.04 = 0.024\n    └── D^c (576/600 = 0.96) ⇒ 0.60 · 0.96 = 0.576\nAdding the two disjoint paths where \\(D\\) occurs:\n\\[\nP(D) = 0.04 + 0.024 = 0.064.\n\\]\nThis confirms the Law of Total Probability result and sets up Bayes’ theorem\n\n\n\n\nA certain disease affects 1% of a population.\nA test is used to detect the disease:\n\nIf a person has the disease, the test is positive 92% of the time.\n\nIf a person does not have the disease, the test is positive 7% of the time.\n\nLet’s define events:\n\n\\(D =\\) person has the disease.\n\n\\(D^c =\\) person does not have the disease.\n\n\\(+\\) = test is positive.\n\n\\(-\\) = test is negative.\n\n\n\nWhat is \\(P(+ \\mid D)\\), the probability that a person tests positive given they have the disease?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFrom the problem statement:\n\\[\nP(+ \\mid D) = 0.92\n\\]\n\n\n\n\n\n\nWhat is \\(P(D \\cap +)\\), the probability that a randomly chosen person both has the disease and tests positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy the multiplication rule:\n\\[\nP(D \\cap +) = P(+ \\mid D)\\, P(D).\n\\]\nSubstitute the known values:\n\\[\nP(D \\cap +) = (0.92)(0.01).\n\\]\nSimplify:\n\\[\nP(D \\cap +) = 0.0092\n\\]\n\n\n\n\n\n\nDraw a tree diagram for this situation.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBranch probabilities\n\n\\(P(D)=0.01,\\quad P(D^c)=0.99\\)\n\\(P(+\\mid D)=0.92,\\quad P(-\\mid D)=1-0.92=0.08\\)\n\\(P(+\\mid D^c)=0.07,\\quad P(-\\mid D^c)=1-0.07=0.93\\)\n\nTree with path (leaf) probabilities\nStart\n├── D (0.01)\n│   ├── + (0.92)  ⇒ P(D ∩ +)  = 0.01 · 0.92  = 0.0092\n│   └── − (0.08)  ⇒ P(D ∩ −)  = 0.01 · 0.08  = 0.0008\n└── D^c (0.99)\n    ├── + (0.07)  ⇒ P(D^c ∩ +) = 0.99 · 0.07  = 0.0693\n    └── − (0.93)  ⇒ P(D^c ∩ −) = 0.99 · 0.93  = 0.9207\n\n\n\n\n\n\nWhat is \\(P(+)\\), the probability that a randomly chosen person tests positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPartition into whether the person has the disease (\\(D\\)) or not (\\(D^c\\)):\n\\[\nP(+) = P(+ \\mid D)\\,P(D) + P(+ \\mid D^c)\\,P(D^c).\n\\]\nSubstitute values:\n\\[\nP(+) = (0.92)(0.01) + (0.07)(0.99).\n\\]\nSimplify:\n\\[\nP(+) = 0.0092 + 0.0693 = 0.0785\n\\]\n\n\n\n\n\n\nWhat is \\(P(D \\mid +)\\), the probability that a person actually has the disease given that their test is positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy Bayes’ theorem:\n\\[\nP(D \\mid +) = \\frac{P(+ \\mid D)\\, P(D)}{P(+)}.\n\\]\nSubstitute values:\n\\[\nP(D \\mid +) = \\frac{0.92 \\cdot 0.01}{0.0785}.\n\\]\nSimplify:\n\\[\nP(D \\mid +) \\approx 0.117\n\\]\n\n\n\n\n\n\n\nYour campus email system uses an automated spam filter. From long-run monitoring. About 16% of incoming messages are spam. When a message is spam, the filter flags it about 90% of the time. When a message is not spam the filter still flags it (a false positive) about 8% of the time.\nAnswer the questions below.\n\n\n\nDefine, in words, the four basic events used in this problem:\n• the message is spam,\n• the message is not spam,\n• the message is flagged by the filter,\n• the message is not flagged.\n(Choose symbols for each and state your choices.)\nFrom the story, write down the numerical values for all probabilities needed to solve the problem set, including:\n• the probability a message is spam,\n• the probability a message is not spam,\n• the probability a message is flagged given it is spam,\n• the probability a message is not flagged given it is spam,\n• the probability a message is flagged given it is not spam,\n• the probability a message is not flagged given it is not spam.\n(Express each using the symbols you chose in part 1.)\nDraw a tree diagram that first branches on whether the message is spam vs. not spam, and then on flagged vs. not flagged under each branch. Label every branch with the appropriate probability.\nWhat is the probability that a message is flagged?\nWhat is the probability that a message is both spam and flagged?\nGiven that a message is flagged, what is the probability that it is spam?\nFind the probability that a message is not flagged, and the conditional probabilities of not flagged given spam and given not spam.\nCompute the probabilities of each joint outcome:\n• spam and not flagged,\n• not spam and flagged,\n• not spam and not flagged.\nAre the events “spam” and “flagged” independent? Justify with a numerical check using your probabilities.\nWhat is the probability that a message is either spam or flagged (or both)?\nBuild the 2×2 probability table with rows “spam / not spam” and columns “flagged / not flagged.” Fill in each cell with the corresponding probability, and verify that row and column totals match your earlier results and that the four cells sum to 1.\n\n\n\n\n\n\n\n\nWorked solutions\n\n\n\n\n\n1) Definitions (words)\n\\(S\\): “message is spam.”\n\\(S^c\\): “message is not spam.”\n\\(+\\): “message is flagged by the filter.”\n\\(-\\): “message is not flagged.”\n2) Defined probabilities (from the story and complements)\n\\[\nP(S)=0.16 \\qquad P(S^c)=1-P(S)=0.84\n\\] \\[\nP(+\\mid S)=0.90 \\qquad P(-\\mid S)=1-P(+\\mid S)=0.10\n\\] \\[\nP(+\\mid S^c)=0.08 \\qquad P(-\\mid S^c)=1-P(+\\mid S^c)=0.92\n\\]\n3) Tree diagram (branches & path probabilities)\nStart\n├── S (0.16)\n│   ├── + (0.90)  ⇒ P(S ∩ +)  = 0.16 · 0.90 = 0.144\n│   └── − (0.10)  ⇒ P(S ∩ −)  = 0.16 · 0.10 = 0.016\n└── S^c (0.84)\n    ├── + (0.08)  ⇒ P(S^c ∩ +) = 0.84 · 0.08 = 0.0672\n    └── − (0.92)  ⇒ P(S^c ∩ −) = 0.84 · 0.92 = 0.7728\n4) \\(P(+)\\)\nUses Law of Total Probability\n\\[\nP(+)=P(+\\mid S)P(S)+P(+\\mid S^c)P(S^c)\n=0.90(0.16)+0.08(0.84)\n=0.144+0.0672\n=0.2112\n\\]\n5) \\(P(S\\cap +)\\)\n\\[\nP(S\\cap +)=P(+\\mid S)P(S)=0.90\\cdot 0.16=0.144\n\\]\n6) \\(P(S\\mid +)\\)\nUsing earlier parts:\nFrom part 5, \\(P(S\\cap +)=0.144\\).\nFrom part 4, \\(P(+)=0.2112\\).\n\\[\nP(S\\mid +)=\\frac{P(S\\cap +)}{P(+)}=\\frac{0.144}{0.2112}\n=\\frac{15}{22}\\approx 0.6818\n\\]\nBayes’ Theorem (full form)\n\\[\nP(S\\mid +)=\\frac{P(+\\mid S)\\,P(S)}{P(+\\mid S)\\,P(S)\\;+\\;P(+\\mid S^c)\\,P(S^c)}\n\\]\nSubstitute the values (from the story / part 2): \\(P(+\\mid S)=0.90\\), \\(P(S)=0.16\\), \\(P(+\\mid S^c)=0.08\\), \\(P(S^c)=0.84\\).\n\\[\nP(S\\mid +)=\\frac{0.90\\cdot 0.16}{0.90\\cdot 0.16+0.08\\cdot 0.84}\n=\\frac{0.144}{0.144+0.0672}\n=\\frac{0.144}{0.2112}\n=\\frac{15}{22}\\approx 0.6818\n\\]\n7) \\(P(-)\\), \\(P(-\\mid S)\\), \\(P(-\\mid S^c)\\)\n\\[\nP(-)=1-P(+)=1-0.2112=0.7888.\n\\]\nFrom part 2, \\(P(+\\mid S)=0.90\\) and \\(P(+\\mid S^c)=0.08\\). Therefore \\[\nP(-\\mid S)=1-P(+\\mid S)=1-0.90=0.10,\\qquad\nP(-\\mid S^c)=1-P(+\\mid S^c)=1-0.08=0.92.\n\\]\nOptional cross-check via paths (from parts 3 & 5): \\[\nP(-\\mid S)=\\frac{P(S\\cap -)}{P(S)}=\\frac{0.016}{0.16}=0.10,\\qquad\nP(-\\mid S^c)=\\frac{P(S^c\\cap -)}{P(S^c)}=\\frac{0.7728}{0.84}=0.92.\n\\]\n8) \\(P(S\\cap -)\\), \\(P(S^c\\cap +)\\), \\(P(S^c\\cap -)\\)\nUsing the multiplication rule and earlier parts:\n\nFrom part 7 and part 2: \\[\nP(S\\cap -)=P(-\\mid S)\\,P(S)=(0.10)(0.16)=0.016.\n\\] (Cross-check from part 5 and part 2: (P(S-)=P(S)-P(S+)=0.16-0.144=0.016).)\nFrom part 2: \\[\nP(S^c\\cap +)=P(+\\mid S^c)\\,P(S^c)=(0.08)(0.84)=0.0672.\n\\]\nFrom part 7 and part 2: \\[\nP(S^c\\cap -)=P(-\\mid S^c)\\,P(S^c)=(0.92)(0.84)=0.7728.\n\\]\n\nCheck (with (P(S+)) from part 5): \\[\n0.144+0.016+0.0672+0.7728=1.\n\\]\n9) Independence check (\\(S\\) vs \\(+\\))\nIndependent would require \\(P(S\\cap +)=P(S)\\,P(+)\\).\n\\[\nP(S)\\,P(+)=0.16\\cdot 0.2112=0.033792\\neq 0.144\n\\] So \\(S\\) and \\(+\\) are not independent.\n10) \\(P(S\\cup +)\\)\n\\[\nP(S\\cup +)=P(S)+P(+)-P(S\\cap +)\n=0.16+0.2112-0.144\n=0.2272\n\\]\n11) \\(2\\times 2\\) probability table\n\n\n\n\n\\(+\\)\n\\(-\\)\nRow total\n\n\n\n\n\\(S\\)\n\\(0.144\\)\n\\(0.016\\)\n\\(0.160\\)\n\n\n\\(S^c\\)\n\\(0.0672\\)\n\\(0.7728\\)\n\\(0.840\\)\n\n\nCol total\n0.2112\n0.7888\n1.000\n\n\n\nRow/column totals match; the four cells sum to \\(1\\).\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\nProject Milestone 2\nAnnex B\n\n\n\n\nLesson 5\n\n\n\n\n\nTidyverse Tutorial: In Class Lesson 5\nProject Milestone 2: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#welcome",
    "href": "MA206-AY26-1/lesson-4.html#welcome",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Project Milestone 2\n\nTidyverse Tutorial applied to your team’s data\nAnnex B (for addressing feedback)\n\nExploration 11.3B\n\n\n\n\n\n\nNot Cal, but….\n\n  Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\nPreviously 0-0\n\n\n\n\n\n\n1-0",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#branch-week",
    "href": "MA206-AY26-1/lesson-4.html#branch-week",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Why I become an Engineer\nWhat I think you should do\nWhat do I do as an Engineer\nWhat is my branch now?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#probability-review",
    "href": "MA206-AY26-1/lesson-4.html#probability-review",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Remember this?\nOut of 100 students:\n- 40 like pizza\n- 30 like burgers\n- 10 of those included above like both pizza and burgers\nExperiment: I select one student at random\nLet:\n- \\(A =\\) “student likes pizza”\n- \\(B =\\) “student likes burgers”\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\nWhat is the probablity that a randomly selected student doesn’t like pizza (\\(P(A^c)\\))?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe know \\(P(A) = 40/100 = 0.40\\).\nBy the complement rule:\n\\[\nP(A^c) = 1 - P(A) = 1 - 0.40 = 0.60\n\\]\n\n\n\n\n\n\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n   A B\n\n\nWhat is the probability a randomly selected cadet likes pizza or burgers? \\(P(A \\cup B)\\)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy the addition rule:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\nSubstitute values:\n\\[\nP(A \\cup B) = 0.40 + 0.30 - 0.10 = 0.60\n\\]\n\n\n\n\n\n\nTwo events are mutually exclusive when it is impossible for both to happen at the same time.\nTherefore:\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n   A B\n\n\nAre events A and B mutually exclusive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNo because:\n\\[\nP(A \\cup B) \\neq P(A) + P(B)\n\\]\n\n\n\n\n\n\nTwo events \\(A\\) and \\(B\\) are independent if knowing that one occurs does not change the probability of the other.\n\\[\nP(A \\cap B) = P(A)P(B)\n\\]\nEquivalently (if \\(P(B) &gt; 0\\)):\n\\[\nP(A \\mid B) = P(A)\n\\]\nIntuition: The outcome of one event gives no information about the other.\nDoes Mutually Exclusive imply independence or no or undetermined?\nAre \\(A =\\) “likes pizza” and \\(B =\\) “likes burgers” independent?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe check using the multiplication rule for independence:\n\\[\nP(A)P(B) = (0.40)(0.30) = 0.12\n\\]\nBut from the data:\n\\[\nP(A \\cap B) = 0.10\n\\]\nSince \\(0.10 \\neq 0.12\\), the events are not independent.\nAlso, they are not mutually exclusive either, because\n\\[\nP(A \\cap B) = 0.10 &gt; 0.\n\\]\nSo in this example, \\(A\\) and \\(B\\) are neither independent nor mutually exclusive.\nGeneral Note:\nIf \\(A\\) and \\(B\\) are mutually exclusive with nonzero probabilities, they cannot be independent.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#new-probability-material",
    "href": "MA206-AY26-1/lesson-4.html#new-probability-material",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "We’ll use a single Factory Machines scenario for all concepts in this section.\nFactory Setup (used for all examples)\nA factory produces 1,000 items per day using two machines:\n\nMachine \\(M_A\\) produces 400 items, of which 40 are defective.\n\nMachine \\(M_B\\) produces 600 items, of which 24 are defective.\n\nDefine events:\n- \\(M_A =\\) “item came from Machine A”\n- \\(M_B =\\) “item came from Machine B”\n- \\(D =\\) “item is defective”\n- \\(D^c =\\) “item is not defective”\nFrom the counts:\n- \\(P(M_A) = 400/1000 = 0.40\\)\n- \\(P(M_B) = 600/1000 = 0.60\\)\n- \\(P(D \\cap M_A) = 40/1000 = 0.04\\)\n- \\(P(D \\cap M_B) = 24/1000 = 0.024\\)\n- \\(P(D) = (40+24)/1000 = 0.064\\)\n\n\nThe probability that event \\(A\\) occurs given that event \\(B\\) occurs:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\n\\]\nIntuition: Restrict the sample space to \\(B\\); ask what fraction of those outcomes also fall in \\(A\\).\n\n\n      A B\n\n\nExample (Factory Machines)\nWhat is \\(P(D \\mid M_A)\\), the probability an item is defective given it was made by Machine \\(A\\)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy definition:\n\\[\nP(D \\mid M_A) = \\frac{P(D \\cap M_A)}{P(M_A)}.\n\\]\nFrom the setup:\n- \\(P(D \\cap M_A) = 40/1000 = 0.04\\)\n- \\(P(M_A) = 400/1000 = 0.40\\)\nSo:\n\\[\nP(D \\mid M_A) = \\frac{0.04}{0.40} = 0.10.\n\\]\n\n\n\n\n\n\nRelates intersections to conditional probabilities.\n\\[\\begin{align}\nP(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\n&& \\text{definition of conditional probability} \\\\[24pt]\n\nP(A \\mid B)\\,P(B) &= \\frac{P(A \\cap B)}{P(B)} \\cdot P(B)\n&& \\text{multiply both sides by $P(B)$} \\\\[24pt]\n\nP(A \\mid B)\\,P(B) &= P(A \\cap B)\n&& \\text{simplify} \\\\[24pt]\n\nP(B \\mid A)\\,P(A) &= P(A \\cap B)\n&& \\text{swap $A$ and $B$} \\\\[24pt]\n\nP(A \\cap B) &= P(A \\mid B)P(B) = P(B \\mid A)P(A)\n&& \\text{final multiplication rule}\n\\end{align}\\]\nIntuition: To find the chance that both happen, compute the chance that one happens, then multiply by the chance the other happens given that.\nExample (Factory Machines)\nUsing the rule, compute \\(P(D \\cap M_B)\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy the multiplication rule:\n\\[\nP(D \\cap M_B) = P(D \\mid M_B)\\,P(M_B).\n\\]\nFrom counts:\n- \\(P(D \\mid M_B) = 24/600 = 0.04\\)\n- \\(P(M_B) = 600/1000 = 0.60\\)\nSo:\n\\[\nP(D \\cap M_B) = (0.04)(0.60) = 0.024.\n\\]\nThis matches the direct count calculation \\(24/1000=0.024\\).\n\n\n\n\n\nThe multiplication rule shows us how to express the probability of two events happening together in terms of a conditional probability:\n\\[\nP(A \\cap B) = P(A \\mid B)\\, P(B).\n\\]\nNow imagine that the whole sample space can be split into two non-overlapping cases (aka mutually exclusive) \\(B_1\\) and \\(B_2\\) (for example: “athlete” vs. “not an athlete”).\nSince \\(B_1\\) and \\(B_2\\) cover all possibilities, any time \\(A\\) happens, it must happen either with \\(B_1\\) or with \\(B_2\\).\n\n\n\n\n\nTo find \\(P(A)\\), we know that it is the union of where A intersects B in all ways that B can happen. so:\n\\[\nP(A) = P((A \\cap B_1) \\cup (A \\cap B_2))\n\\]\nBecause \\((A \\cap B_1)\\) and \\((A \\cap B_2)\\) are mutually exclusive, we can use the mutually exclusive version of the addition rule \\(P(E_1 \\cup E_2) = P(E_1) + P(E_2)\\) to get:\n\\[\nP((A \\cap B_1) \\cup (A \\cap B_2)) = P(A \\cap B_1) + P(A \\cap B_2).\n\\]\nAnd if we apply the multiplication rule (\\(P(E_1 \\cap E_2) = P(E_1 \\mid E_2)\\, P(E_2)\\)) to each intersection, we get exactly the Law of Total Probability.\n\\[\nP(A) = P(A | B_1)P(B_1) + P(A | B_2)P(B_2).\n\\]\n\n\n\n\nWhat we just derived for two outcomes extends to any partition of the sample space.\nIf \\(B_1, B_2, \\ldots, B_k\\) are mutually exclusive and exhaustive events, then for any event \\(A\\):\n\\[\nP(A) = \\sum_{i=1}^{k} P(A \\mid B_i)\\, P(B_i).\n\\]\nFor two outcomes:\n\\[\nP(A) = P(A \\mid B_1)\\,P(B_1) \\;+\\; P(A \\mid B_2)\\,P(B_2).\n\\]\nIntuition:\nInstead of calculating \\(P(A)\\) directly, we “partition” the sample space into simpler pieces \\(B_1, B_2, \\ldots, B_k\\). We compute \\(P(A)\\) by adding up the contributions from each path.\nExample (Factory Machines)\nWhat is the overall probability that a randomly chosen item is defective (\\(P(D)\\))?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPartition by which machine made the item:\n\\[\nP(D) = P(D \\mid M_A)P(M_A) + P(D \\mid M_B)P(M_B).\n\\]\nFrom counts:\n- \\(P(D \\mid M_A)=40/400=0.10\\), \\(P(M_A)=0.40\\)\n- \\(P(D \\mid M_B)=24/600=0.04\\), \\(P(M_B)=0.60\\)\nSo:\n\\[\nP(D) = (0.10)(0.40) + (0.04)(0.60) = 0.04 + 0.024 = 0.064.\n\\]\nSo 6.4% of all items are defective.\n\n\n\n\n\n\nBayes’ Theorem lets us “flip” conditional probabilities. It tells us how to update beliefs about a cause (\\(B\\)) when we observe some evidence (\\(A\\)).\nFor events \\(A\\) and \\(B\\) with \\(P(B) &gt; 0\\):\n\\[\nP(B \\mid A) = \\frac{P(A \\mid B)\\, P(B)}{P(A)}.\n\\]\nUsing the Law of Total Probability for \\(P(A)\\):\n\\[\nP(B_j \\mid A) = \\frac{P(A \\mid B_j)\\, P(B_j)}{\\sum_{i=1}^{k} P(A \\mid B_i)\\, P(B_i)}.\n\\]\n\\[\nP(B_2 \\mid A) = \\frac{P(A \\mid B_2)\\, P(B_2)}{P(A \\mid B_1)P(B_1) + P(A \\mid B_2)P(B_2)}.\n\\]\nIntuition:\nThink of Bayes’ theorem as a way to reverse the condition. If we know how likely \\(A\\) is when \\(B\\) happens, Bayes tells us how likely \\(B\\) is given that we saw \\(A\\).\nExample (Factory Machines)\nIf an item is defective, what is the probability it came from Machine A?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe want \\(P(M_A \\mid D)\\).\nBy Bayes’ theorem:\n\\[\nP(M_A \\mid D) = \\frac{P(D \\mid M_A)\\, P(M_A)}{P(D)}.\n\\]\nThe denominator \\(P(D)\\) comes from the Law of Total Probability:\n\\[\nP(D) = P(D \\mid M_A)P(M_A) + P(D \\mid M_B)P(M_B).\n\\]\nFrom counts:\n- \\(P(D \\mid M_A)=40/400=0.10\\), \\(P(M_A)=400/1000=0.40\\)\n- \\(P(D \\mid M_B)=24/600=0.04\\), \\(P(M_B)=600/1000=0.60\\)\nSo:\n\\[\nP(D) = (0.10)(0.40) + (0.04)(0.60) = 0.064\n\\]\nNow substitute into Bayes’ theorem:\n\\[\nP(M_A \\mid D) = \\frac{0.10 \\cdot 0.40}{0.064} = \\frac{0.04}{0.064} = 0.625.\n\\]\nThus, if an item is defective, there is a 62.5% chance it was made by Machine A.\n\n\n\n\n\n\nAnother way to picture the Law of Total Probability is to start with the partition (\\(M_A\\) vs \\(M_B\\)), then branch into whether \\(D\\) happens or not under each case.\nStart\n├── M_A (0.40)\n│   ├── D (40/400 = 0.10)   ⇒ 0.40 · 0.10 = 0.04\n│   └── D^c (360/400 = 0.90) ⇒ 0.40 · 0.90 = 0.36\n└── M_B (0.60)\n    ├── D (24/600 = 0.04)   ⇒ 0.60 · 0.04 = 0.024\n    └── D^c (576/600 = 0.96) ⇒ 0.60 · 0.96 = 0.576\nAdding the two disjoint paths where \\(D\\) occurs:\n\\[\nP(D) = 0.04 + 0.024 = 0.064.\n\\]\nThis confirms the Law of Total Probability result and sets up Bayes’ theorem",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#example-medical-test-and-probability-rules",
    "href": "MA206-AY26-1/lesson-4.html#example-medical-test-and-probability-rules",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "A certain disease affects 1% of a population.\nA test is used to detect the disease:\n\nIf a person has the disease, the test is positive 92% of the time.\n\nIf a person does not have the disease, the test is positive 7% of the time.\n\nLet’s define events:\n\n\\(D =\\) person has the disease.\n\n\\(D^c =\\) person does not have the disease.\n\n\\(+\\) = test is positive.\n\n\\(-\\) = test is negative.\n\n\n\nWhat is \\(P(+ \\mid D)\\), the probability that a person tests positive given they have the disease?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFrom the problem statement:\n\\[\nP(+ \\mid D) = 0.92\n\\]\n\n\n\n\n\n\nWhat is \\(P(D \\cap +)\\), the probability that a randomly chosen person both has the disease and tests positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy the multiplication rule:\n\\[\nP(D \\cap +) = P(+ \\mid D)\\, P(D).\n\\]\nSubstitute the known values:\n\\[\nP(D \\cap +) = (0.92)(0.01).\n\\]\nSimplify:\n\\[\nP(D \\cap +) = 0.0092\n\\]\n\n\n\n\n\n\nDraw a tree diagram for this situation.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBranch probabilities\n\n\\(P(D)=0.01,\\quad P(D^c)=0.99\\)\n\\(P(+\\mid D)=0.92,\\quad P(-\\mid D)=1-0.92=0.08\\)\n\\(P(+\\mid D^c)=0.07,\\quad P(-\\mid D^c)=1-0.07=0.93\\)\n\nTree with path (leaf) probabilities\nStart\n├── D (0.01)\n│   ├── + (0.92)  ⇒ P(D ∩ +)  = 0.01 · 0.92  = 0.0092\n│   └── − (0.08)  ⇒ P(D ∩ −)  = 0.01 · 0.08  = 0.0008\n└── D^c (0.99)\n    ├── + (0.07)  ⇒ P(D^c ∩ +) = 0.99 · 0.07  = 0.0693\n    └── − (0.93)  ⇒ P(D^c ∩ −) = 0.99 · 0.93  = 0.9207\n\n\n\n\n\n\nWhat is \\(P(+)\\), the probability that a randomly chosen person tests positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPartition into whether the person has the disease (\\(D\\)) or not (\\(D^c\\)):\n\\[\nP(+) = P(+ \\mid D)\\,P(D) + P(+ \\mid D^c)\\,P(D^c).\n\\]\nSubstitute values:\n\\[\nP(+) = (0.92)(0.01) + (0.07)(0.99).\n\\]\nSimplify:\n\\[\nP(+) = 0.0092 + 0.0693 = 0.0785\n\\]\n\n\n\n\n\n\nWhat is \\(P(D \\mid +)\\), the probability that a person actually has the disease given that their test is positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy Bayes’ theorem:\n\\[\nP(D \\mid +) = \\frac{P(+ \\mid D)\\, P(D)}{P(+)}.\n\\]\nSubstitute values:\n\\[\nP(D \\mid +) = \\frac{0.92 \\cdot 0.01}{0.0785}.\n\\]\nSimplify:\n\\[\nP(D \\mid +) \\approx 0.117\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#take-home-problem-spam-filter",
    "href": "MA206-AY26-1/lesson-4.html#take-home-problem-spam-filter",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Your campus email system uses an automated spam filter. From long-run monitoring. About 16% of incoming messages are spam. When a message is spam, the filter flags it about 90% of the time. When a message is not spam the filter still flags it (a false positive) about 8% of the time.\nAnswer the questions below.\n\n\n\nDefine, in words, the four basic events used in this problem:\n• the message is spam,\n• the message is not spam,\n• the message is flagged by the filter,\n• the message is not flagged.\n(Choose symbols for each and state your choices.)\nFrom the story, write down the numerical values for all probabilities needed to solve the problem set, including:\n• the probability a message is spam,\n• the probability a message is not spam,\n• the probability a message is flagged given it is spam,\n• the probability a message is not flagged given it is spam,\n• the probability a message is flagged given it is not spam,\n• the probability a message is not flagged given it is not spam.\n(Express each using the symbols you chose in part 1.)\nDraw a tree diagram that first branches on whether the message is spam vs. not spam, and then on flagged vs. not flagged under each branch. Label every branch with the appropriate probability.\nWhat is the probability that a message is flagged?\nWhat is the probability that a message is both spam and flagged?\nGiven that a message is flagged, what is the probability that it is spam?\nFind the probability that a message is not flagged, and the conditional probabilities of not flagged given spam and given not spam.\nCompute the probabilities of each joint outcome:\n• spam and not flagged,\n• not spam and flagged,\n• not spam and not flagged.\nAre the events “spam” and “flagged” independent? Justify with a numerical check using your probabilities.\nWhat is the probability that a message is either spam or flagged (or both)?\nBuild the 2×2 probability table with rows “spam / not spam” and columns “flagged / not flagged.” Fill in each cell with the corresponding probability, and verify that row and column totals match your earlier results and that the four cells sum to 1.\n\n\n\n\n\n\n\n\nWorked solutions\n\n\n\n\n\n1) Definitions (words)\n\\(S\\): “message is spam.”\n\\(S^c\\): “message is not spam.”\n\\(+\\): “message is flagged by the filter.”\n\\(-\\): “message is not flagged.”\n2) Defined probabilities (from the story and complements)\n\\[\nP(S)=0.16 \\qquad P(S^c)=1-P(S)=0.84\n\\] \\[\nP(+\\mid S)=0.90 \\qquad P(-\\mid S)=1-P(+\\mid S)=0.10\n\\] \\[\nP(+\\mid S^c)=0.08 \\qquad P(-\\mid S^c)=1-P(+\\mid S^c)=0.92\n\\]\n3) Tree diagram (branches & path probabilities)\nStart\n├── S (0.16)\n│   ├── + (0.90)  ⇒ P(S ∩ +)  = 0.16 · 0.90 = 0.144\n│   └── − (0.10)  ⇒ P(S ∩ −)  = 0.16 · 0.10 = 0.016\n└── S^c (0.84)\n    ├── + (0.08)  ⇒ P(S^c ∩ +) = 0.84 · 0.08 = 0.0672\n    └── − (0.92)  ⇒ P(S^c ∩ −) = 0.84 · 0.92 = 0.7728\n4) \\(P(+)\\)\nUses Law of Total Probability\n\\[\nP(+)=P(+\\mid S)P(S)+P(+\\mid S^c)P(S^c)\n=0.90(0.16)+0.08(0.84)\n=0.144+0.0672\n=0.2112\n\\]\n5) \\(P(S\\cap +)\\)\n\\[\nP(S\\cap +)=P(+\\mid S)P(S)=0.90\\cdot 0.16=0.144\n\\]\n6) \\(P(S\\mid +)\\)\nUsing earlier parts:\nFrom part 5, \\(P(S\\cap +)=0.144\\).\nFrom part 4, \\(P(+)=0.2112\\).\n\\[\nP(S\\mid +)=\\frac{P(S\\cap +)}{P(+)}=\\frac{0.144}{0.2112}\n=\\frac{15}{22}\\approx 0.6818\n\\]\nBayes’ Theorem (full form)\n\\[\nP(S\\mid +)=\\frac{P(+\\mid S)\\,P(S)}{P(+\\mid S)\\,P(S)\\;+\\;P(+\\mid S^c)\\,P(S^c)}\n\\]\nSubstitute the values (from the story / part 2): \\(P(+\\mid S)=0.90\\), \\(P(S)=0.16\\), \\(P(+\\mid S^c)=0.08\\), \\(P(S^c)=0.84\\).\n\\[\nP(S\\mid +)=\\frac{0.90\\cdot 0.16}{0.90\\cdot 0.16+0.08\\cdot 0.84}\n=\\frac{0.144}{0.144+0.0672}\n=\\frac{0.144}{0.2112}\n=\\frac{15}{22}\\approx 0.6818\n\\]\n7) \\(P(-)\\), \\(P(-\\mid S)\\), \\(P(-\\mid S^c)\\)\n\\[\nP(-)=1-P(+)=1-0.2112=0.7888.\n\\]\nFrom part 2, \\(P(+\\mid S)=0.90\\) and \\(P(+\\mid S^c)=0.08\\). Therefore \\[\nP(-\\mid S)=1-P(+\\mid S)=1-0.90=0.10,\\qquad\nP(-\\mid S^c)=1-P(+\\mid S^c)=1-0.08=0.92.\n\\]\nOptional cross-check via paths (from parts 3 & 5): \\[\nP(-\\mid S)=\\frac{P(S\\cap -)}{P(S)}=\\frac{0.016}{0.16}=0.10,\\qquad\nP(-\\mid S^c)=\\frac{P(S^c\\cap -)}{P(S^c)}=\\frac{0.7728}{0.84}=0.92.\n\\]\n8) \\(P(S\\cap -)\\), \\(P(S^c\\cap +)\\), \\(P(S^c\\cap -)\\)\nUsing the multiplication rule and earlier parts:\n\nFrom part 7 and part 2: \\[\nP(S\\cap -)=P(-\\mid S)\\,P(S)=(0.10)(0.16)=0.016.\n\\] (Cross-check from part 5 and part 2: (P(S-)=P(S)-P(S+)=0.16-0.144=0.016).)\nFrom part 2: \\[\nP(S^c\\cap +)=P(+\\mid S^c)\\,P(S^c)=(0.08)(0.84)=0.0672.\n\\]\nFrom part 7 and part 2: \\[\nP(S^c\\cap -)=P(-\\mid S^c)\\,P(S^c)=(0.92)(0.84)=0.7728.\n\\]\n\nCheck (with (P(S+)) from part 5): \\[\n0.144+0.016+0.0672+0.7728=1.\n\\]\n9) Independence check (\\(S\\) vs \\(+\\))\nIndependent would require \\(P(S\\cap +)=P(S)\\,P(+)\\).\n\\[\nP(S)\\,P(+)=0.16\\cdot 0.2112=0.033792\\neq 0.144\n\\] So \\(S\\) and \\(+\\) are not independent.\n10) \\(P(S\\cup +)\\)\n\\[\nP(S\\cup +)=P(S)+P(+)-P(S\\cap +)\n=0.16+0.2112-0.144\n=0.2272\n\\]\n11) \\(2\\times 2\\) probability table\n\n\n\n\n\\(+\\)\n\\(-\\)\nRow total\n\n\n\n\n\\(S\\)\n\\(0.144\\)\n\\(0.016\\)\n\\(0.160\\)\n\n\n\\(S^c\\)\n\\(0.0672\\)\n\\(0.7728\\)\n\\(0.840\\)\n\n\nCol total\n0.2112\n0.7888\n1.000\n\n\n\nRow/column totals match; the four cells sum to \\(1\\).",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-4.html#before-you-leave",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\nProject Milestone 2\nAnnex B\n\n\n\n\nLesson 5\n\n\n\n\n\nTidyverse Tutorial: In Class Lesson 5\nProject Milestone 2: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-6.html",
    "href": "MA206-AY26-1/lesson-6.html",
    "title": "Lesson 6: Discrete Distributions of Random Variables",
    "section": "",
    "text": "Math 1 vs Math 2\n\n\n\n\n\n\nPreviously 1-0\n\n\n\n\n\n\n2-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLets render a document\nDon’t forget Annex B\n\n\n\n\n\n\n\n\n\nNot that type of discrete random variable…\nA random variable is a rule (or function) that assigns a number to each possible outcome of a random process.\n\nDiscrete random variable: takes on a countable set of values (like dice rolls, coin flips, number of emails received in a day).\n\nContinuous random variable: takes on values from an interval or continuum (like height, time, or temperature).\n\n\n\nLet \\(X\\in\\{1,2,3,4,5,6\\}\\) be the points shown on one fair die roll.\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P(X=x)\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\n\n\n\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean or expected value of a discrete random variable refers to the long-run average value of the random process if the process is repeated indefinitely under identical conditions. The mean of the random variable is a weighted average of the possible values the random variable can take, weighting each outcome according to its probability:\n\\[\n\\mu_X = E[X] = \\sum_x x \\cdot P(X=x).\n\\]\n\\[\nE[X] = 1\\cdot\\tfrac16 + 2\\cdot\\tfrac16 + 3\\cdot\\tfrac16 + 4\\cdot\\tfrac16 + 5\\cdot\\tfrac16 + 6\\cdot\\tfrac16\n= \\tfrac{21}{6} = 3.5.\n\\]\n\n\n\nOver many rolls, the long-run average face value is \\(3.5\\) (even though no single roll shows \\(3.5\\)).\n\n\n\nThe variance of a discrete random variable measures the average squared deviation from its mean: \\[\n\\mathrm{Var}(X)=\\sum_x (x-\\mu_X)^2\\,P(X=x)\n\\] and the standard deviation is \\[\n\\mathrm{SD}(X)=\\sqrt{\\mathrm{Var}(X)}.\n\\]\n\\[\n\\mathrm{Var}(X)=\\left(1-3.5\\right)^2\\cdot\\tfrac16\n+\\left(2-3.5\\right)^2\\cdot\\tfrac16\n+\\left(3-3.5\\right)^2\\cdot\\tfrac16\n+\\left(4-3.5\\right)^2\\cdot\\tfrac16\n+\\left(5-3.5\\right)^2\\cdot\\tfrac16\n+\\left(6-3.5\\right)^2\\cdot\\tfrac16\n=\\tfrac{35}{12}\n\\]\n\\[\n\\mathrm{SD}(X)=\\sqrt{\\tfrac{35}{12}}\\approx1.7078.\n\\]\n\n\n\nExample A — Fair die\nLarger variance/SD indicates outcomes are more spread out around the mean; smaller variance/SD indicates the outcomes are more tightly clustered.\nBecause most values of a distribution tend to fall within about two standard deviations of the mean, we would expect nearly all rolls to land between about \\(3.5 - 2(1.71)\\approx0\\) and \\(3.5 + 2(1.71)\\approx7.0\\). Since the die only takes values \\(1\\) through \\(6\\), this tells us practically all the possible outcomes fall within that range around the mean.\n\n\n\n\nProbability that \\(X \\leq 2\\)\n\\[\nP(X \\leq 2) = P(X=1) + P(X=2) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]\nProbability that \\(X\\) is less than one standard deviation above the mean\n\n\n\nMean: \\(\\mu = 3.5\\)\n\nStandard deviation: \\(\\sigma \\approx 1.71\\)\n\nOne standard deviation above the mean: \\(\\mu + \\sigma \\approx 3.5 + 1.71 = 5.21\\)\n\nSo we want \\(P(X &lt; 5.21)\\), i.e. \\(P(X \\in \\{1,2,3,4,5\\})\\).\n\\[\nP(X &lt; 5.21) = 5 \\cdot \\tfrac{1}{6} = \\tfrac{5}{6}.\n\\]\n\nProbability that \\(X &gt; 4\\)\n\\[\nP(X &gt; 4) = P(X=5) + P(X=6) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]\n\n\n\n\n\n4 Basketball Shots, \\(p=\\tfrac12\\)\nA player takes 4 shots. Each shot has probability \\(p=\\tfrac12\\) of going in.\nLet \\(Y =\\) the number of made shots in 4 independent attempts.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nLet M = make, X = miss.\n\n\\(Y=0\\) (no makes): \\(XXXX\\)\n\n\\(Y=1\\) (exactly one make): \\(MXXX, XMXX, XXMX, XXXM\\)\n\n\\(Y=2\\) (exactly two makes): \\(MMXX, MXMX, MXXM, XMMX, XMXM, XXMM\\)\n\n\\(Y=3\\) (exactly three makes): \\(MMMX, MMXM, MXMM, XMMM\\)\n\n\\(Y=4\\) (all makes): \\(MMMM\\)\n\nSo:\n\\(P(Y=0)=\\tfrac{1}{16}, \\quad\nP(Y=1)=\\tfrac{4}{16}, \\quad\nP(Y=2)=\\tfrac{6}{16}, \\quad\nP(Y=3)=\\tfrac{4}{16}, \\quad\nP(Y=4)=\\tfrac{1}{16}.\\)\n\n\n\n\\(k\\) (makes)\n0\n1\n2\n3\n4\n\n\n\n\n\\(P(Y=k)\\)\n\\(1/16\\)\n\\(4/16\\)\n\\(6/16\\)\n\\(4/16\\)\n\\(1/16\\)\n\n\n\nAnd in function form:\n\\[\nP(Y=k) =\n\\begin{cases}\n\\dfrac{1}{16}, & k=0, \\\\[6pt]\n\\dfrac{4}{16}, & k=1, \\\\[6pt]\n\\dfrac{6}{16}, & k=2, \\\\[6pt]\n\\dfrac{4}{16}, & k=3, \\\\[6pt]\n\\dfrac{1}{16}, & k=4, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\\(E[Y] = \\sum_{k=0}^4 k \\, P(Y=k)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(E[Y] = 0 \\, P(Y=0) + 1 \\, P(Y=1) + 2 \\, P(Y=2) + 3 \\, P(Y=3) + 4 \\, P(Y=4)\\)\n\\(= 0 \\, \\tfrac{1}{16} + 1 \\, \\tfrac{4}{16} + 2 \\, \\tfrac{6}{16} + 3 \\, \\tfrac{4}{16} + 4 \\, \\tfrac{1}{16}\\)\n\\(= \\tfrac{32}{16} = 2\\)\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOn average, the player makes 2 shots out of 4.\nThis is the long-run average across many 4-shot sequences, even though most sequences are not exactly 2 makes.\n\n\n\n\n\n\n\\(\\mathrm{Var}(Y) = \\sum_{k=0}^4 (k - E[Y])^2 \\cdot P(Y=k).\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\mathrm{Var}(Y)\n= (0-2)^2\\cdot\\tfrac{1}{16}\n+ (1-2)^2\\cdot\\tfrac{4}{16}\n+ (2-2)^2\\cdot\\tfrac{6}{16}\n+ (3-2)^2\\cdot\\tfrac{4}{16}\n+ (4-2)^2\\cdot\\tfrac{1}{16}.\\)\n\\(= 4\\cdot\\tfrac{1}{16} + 1\\cdot\\tfrac{4}{16} + 0\\cdot\\tfrac{6}{16} + 1\\cdot\\tfrac{4}{16} + 4\\cdot\\tfrac{1}{16}\n= \\tfrac{16}{16} = 1.\\)\n\\(\\mathrm{SD}(Y) = \\sqrt{1} = 1.\\)\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe standard deviation of 1 means that the number of made shots is typically within about 1 of the mean.\nSo in most 4-shot sequences, the player makes about 1 to 3 shots.\n\n\n\n\n\n\n\nProbability that \\(Y \\leq 1\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(Y \\leq 1) = P(Y=0) + P(Y=1) = \\tfrac{1}{16} + \\tfrac{4}{16} = \\tfrac{5}{16}.\n\\]\n\n\n\n\nProbability that \\(Y\\) is less than one standard deviation above the mean\n\n\nMean: \\(E[Y] = 2\\)\n\nStandard deviation: \\(\\mathrm{SD}(Y) = 1\\)\n\nOne standard deviation above the mean: \\(2 + 1 = 3\\)\n\nSo we want \\(P(Y &lt; 3)\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(Y &lt; 3) = P(Y=0) + P(Y=1) + P(Y=2)  \n= \\tfrac{1}{16} + \\tfrac{4}{16} + \\tfrac{6}{16}  \n= \\tfrac{11}{16}.\n\\]\n\n\n\n\nProbability that \\(Y &gt; 2\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(Y &gt; 2) = P(Y=3) + P(Y=4) = \\tfrac{4}{16} + \\tfrac{1}{16} = \\tfrac{5}{16}.\n\\]\n\n\n\n\n\n\n\nA bag contains 3 balls: one red, one blue, and one green.\nYou draw one ball at random. Let\n\\(X =\\) the number of points you score, where\n- red = 3 points\n- blue = 6 points\n- green = 12 points\n\nWhat is the probability distribution of \\(X\\)?\n\nPlot the probability distribution.\nFind the expected value \\(E[X]\\).\n\nInterpret the expected value.\n\nFind the variance and standard deviation of \\(X\\).\n\nInterpret the variance and standard deviation.\n\nWhat is the probability that \\(X \\leq 6\\)?\n\nWhat is the probability that \\(X\\) is less than one standard deviation above the mean?\n\nWhat is the probability that \\(X &gt; 6\\)?\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nEach outcome is equally likely:\n\n\\(P(X=3) = \\tfrac{1}{3}, \\quad P(X=6) = \\tfrac{1}{3}, \\quad P(X=12) = \\tfrac{1}{3}\\)\n\n\n\n\\(x\\)\n3\n6\n12\n\n\n\n\n\\(P(X=x)\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1/3\\)\n\n\n\n\nPlot the probability distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(E[X] = 3 \\, P(X=3) + 6 \\, P(X=6) + 12 \\, P(X=12)\\)\n\\(= 3 \\cdot \\tfrac{1}{3} + 6 \\cdot \\tfrac{1}{3} + 12 \\cdot \\tfrac{1}{3}\\)\n\\(= \\tfrac{21}{3} = 7\\)\n\n\n\nOn average, you score 7 points per draw.\nThis doesn’t mean you ever score exactly 7, but across many draws, the outcomes balance to that average.\n\n\n\n\\(\\mathrm{Var}(X) = (3-7)^2 \\, \\tfrac{1}{3} + (6-7)^2 \\, \\tfrac{1}{3} + (12-7)^2 \\, \\tfrac{1}{3}\\)\n\\(= (16)\\tfrac{1}{3} + (1)\\tfrac{1}{3} + (25)\\tfrac{1}{3}\\)\n\\(= \\tfrac{42}{3} = 14\\)\n\\(\\mathrm{SD}(X) = \\sqrt{14} \\approx 3.742\\)\n\n\n\nMost outcomes fall within about 3.7 points of the mean of 7.\nThat means the scores vary moderately, depending on whether you draw the low (3), medium (6), or high (12) ball.\n\n\n\n\\[\nP(X \\leq 6) = P(X=3) + P(X=6) = \\tfrac{1}{3} + \\tfrac{1}{3} = \\tfrac{2}{3}.\n\\]\n\n\n\nMean \\(= 7\\), \\(\\ \\mathrm{SD}(X) \\approx 3.742\\), so one standard deviation above the mean is about \\(10.742\\).\n\\[\nP(X &lt; 10.742) = P(X=3) + P(X=6) = \\tfrac{2}{3}.\n\\]\n\n\n\n\\[\nP(X &gt; 6) = P(X=12) = \\tfrac{1}{3}.\n\\]\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 7\n\n\n\n\n\nMilestone 2 / Tidyverse Tutorial: Due 0700 Lesson 7\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 6"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-6.html#discrete-distributions-of-random-variables",
    "href": "MA206-AY26-1/lesson-6.html#discrete-distributions-of-random-variables",
    "title": "Lesson 6: Discrete Distributions of Random Variables",
    "section": "",
    "text": "Not that type of discrete random variable…\nA random variable is a rule (or function) that assigns a number to each possible outcome of a random process.\n\nDiscrete random variable: takes on a countable set of values (like dice rolls, coin flips, number of emails received in a day).\n\nContinuous random variable: takes on values from an interval or continuum (like height, time, or temperature).\n\n\n\nLet \\(X\\in\\{1,2,3,4,5,6\\}\\) be the points shown on one fair die roll.\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P(X=x)\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\n\n\n\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean or expected value of a discrete random variable refers to the long-run average value of the random process if the process is repeated indefinitely under identical conditions. The mean of the random variable is a weighted average of the possible values the random variable can take, weighting each outcome according to its probability:\n\\[\n\\mu_X = E[X] = \\sum_x x \\cdot P(X=x).\n\\]\n\\[\nE[X] = 1\\cdot\\tfrac16 + 2\\cdot\\tfrac16 + 3\\cdot\\tfrac16 + 4\\cdot\\tfrac16 + 5\\cdot\\tfrac16 + 6\\cdot\\tfrac16\n= \\tfrac{21}{6} = 3.5.\n\\]\n\n\n\nOver many rolls, the long-run average face value is \\(3.5\\) (even though no single roll shows \\(3.5\\)).\n\n\n\nThe variance of a discrete random variable measures the average squared deviation from its mean: \\[\n\\mathrm{Var}(X)=\\sum_x (x-\\mu_X)^2\\,P(X=x)\n\\] and the standard deviation is \\[\n\\mathrm{SD}(X)=\\sqrt{\\mathrm{Var}(X)}.\n\\]\n\\[\n\\mathrm{Var}(X)=\\left(1-3.5\\right)^2\\cdot\\tfrac16\n+\\left(2-3.5\\right)^2\\cdot\\tfrac16\n+\\left(3-3.5\\right)^2\\cdot\\tfrac16\n+\\left(4-3.5\\right)^2\\cdot\\tfrac16\n+\\left(5-3.5\\right)^2\\cdot\\tfrac16\n+\\left(6-3.5\\right)^2\\cdot\\tfrac16\n=\\tfrac{35}{12}\n\\]\n\\[\n\\mathrm{SD}(X)=\\sqrt{\\tfrac{35}{12}}\\approx1.7078.\n\\]\n\n\n\nExample A — Fair die\nLarger variance/SD indicates outcomes are more spread out around the mean; smaller variance/SD indicates the outcomes are more tightly clustered.\nBecause most values of a distribution tend to fall within about two standard deviations of the mean, we would expect nearly all rolls to land between about \\(3.5 - 2(1.71)\\approx0\\) and \\(3.5 + 2(1.71)\\approx7.0\\). Since the die only takes values \\(1\\) through \\(6\\), this tells us practically all the possible outcomes fall within that range around the mean.\n\n\n\n\nProbability that \\(X \\leq 2\\)\n\\[\nP(X \\leq 2) = P(X=1) + P(X=2) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]\nProbability that \\(X\\) is less than one standard deviation above the mean\n\n\n\nMean: \\(\\mu = 3.5\\)\n\nStandard deviation: \\(\\sigma \\approx 1.71\\)\n\nOne standard deviation above the mean: \\(\\mu + \\sigma \\approx 3.5 + 1.71 = 5.21\\)\n\nSo we want \\(P(X &lt; 5.21)\\), i.e. \\(P(X \\in \\{1,2,3,4,5\\})\\).\n\\[\nP(X &lt; 5.21) = 5 \\cdot \\tfrac{1}{6} = \\tfrac{5}{6}.\n\\]\n\nProbability that \\(X &gt; 4\\)\n\\[\nP(X &gt; 4) = P(X=5) + P(X=6) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 6"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-6.html#practice-problem",
    "href": "MA206-AY26-1/lesson-6.html#practice-problem",
    "title": "Lesson 6: Discrete Distributions of Random Variables",
    "section": "",
    "text": "4 Basketball Shots, \\(p=\\tfrac12\\)\nA player takes 4 shots. Each shot has probability \\(p=\\tfrac12\\) of going in.\nLet \\(Y =\\) the number of made shots in 4 independent attempts.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nLet M = make, X = miss.\n\n\\(Y=0\\) (no makes): \\(XXXX\\)\n\n\\(Y=1\\) (exactly one make): \\(MXXX, XMXX, XXMX, XXXM\\)\n\n\\(Y=2\\) (exactly two makes): \\(MMXX, MXMX, MXXM, XMMX, XMXM, XXMM\\)\n\n\\(Y=3\\) (exactly three makes): \\(MMMX, MMXM, MXMM, XMMM\\)\n\n\\(Y=4\\) (all makes): \\(MMMM\\)\n\nSo:\n\\(P(Y=0)=\\tfrac{1}{16}, \\quad\nP(Y=1)=\\tfrac{4}{16}, \\quad\nP(Y=2)=\\tfrac{6}{16}, \\quad\nP(Y=3)=\\tfrac{4}{16}, \\quad\nP(Y=4)=\\tfrac{1}{16}.\\)\n\n\n\n\\(k\\) (makes)\n0\n1\n2\n3\n4\n\n\n\n\n\\(P(Y=k)\\)\n\\(1/16\\)\n\\(4/16\\)\n\\(6/16\\)\n\\(4/16\\)\n\\(1/16\\)\n\n\n\nAnd in function form:\n\\[\nP(Y=k) =\n\\begin{cases}\n\\dfrac{1}{16}, & k=0, \\\\[6pt]\n\\dfrac{4}{16}, & k=1, \\\\[6pt]\n\\dfrac{6}{16}, & k=2, \\\\[6pt]\n\\dfrac{4}{16}, & k=3, \\\\[6pt]\n\\dfrac{1}{16}, & k=4, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\\(E[Y] = \\sum_{k=0}^4 k \\, P(Y=k)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(E[Y] = 0 \\, P(Y=0) + 1 \\, P(Y=1) + 2 \\, P(Y=2) + 3 \\, P(Y=3) + 4 \\, P(Y=4)\\)\n\\(= 0 \\, \\tfrac{1}{16} + 1 \\, \\tfrac{4}{16} + 2 \\, \\tfrac{6}{16} + 3 \\, \\tfrac{4}{16} + 4 \\, \\tfrac{1}{16}\\)\n\\(= \\tfrac{32}{16} = 2\\)\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOn average, the player makes 2 shots out of 4.\nThis is the long-run average across many 4-shot sequences, even though most sequences are not exactly 2 makes.\n\n\n\n\n\n\n\\(\\mathrm{Var}(Y) = \\sum_{k=0}^4 (k - E[Y])^2 \\cdot P(Y=k).\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\mathrm{Var}(Y)\n= (0-2)^2\\cdot\\tfrac{1}{16}\n+ (1-2)^2\\cdot\\tfrac{4}{16}\n+ (2-2)^2\\cdot\\tfrac{6}{16}\n+ (3-2)^2\\cdot\\tfrac{4}{16}\n+ (4-2)^2\\cdot\\tfrac{1}{16}.\\)\n\\(= 4\\cdot\\tfrac{1}{16} + 1\\cdot\\tfrac{4}{16} + 0\\cdot\\tfrac{6}{16} + 1\\cdot\\tfrac{4}{16} + 4\\cdot\\tfrac{1}{16}\n= \\tfrac{16}{16} = 1.\\)\n\\(\\mathrm{SD}(Y) = \\sqrt{1} = 1.\\)\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe standard deviation of 1 means that the number of made shots is typically within about 1 of the mean.\nSo in most 4-shot sequences, the player makes about 1 to 3 shots.\n\n\n\n\n\n\n\nProbability that \\(Y \\leq 1\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(Y \\leq 1) = P(Y=0) + P(Y=1) = \\tfrac{1}{16} + \\tfrac{4}{16} = \\tfrac{5}{16}.\n\\]\n\n\n\n\nProbability that \\(Y\\) is less than one standard deviation above the mean\n\n\nMean: \\(E[Y] = 2\\)\n\nStandard deviation: \\(\\mathrm{SD}(Y) = 1\\)\n\nOne standard deviation above the mean: \\(2 + 1 = 3\\)\n\nSo we want \\(P(Y &lt; 3)\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(Y &lt; 3) = P(Y=0) + P(Y=1) + P(Y=2)  \n= \\tfrac{1}{16} + \\tfrac{4}{16} + \\tfrac{6}{16}  \n= \\tfrac{11}{16}.\n\\]\n\n\n\n\nProbability that \\(Y &gt; 2\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(Y &gt; 2) = P(Y=3) + P(Y=4) = \\tfrac{4}{16} + \\tfrac{1}{16} = \\tfrac{5}{16}.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 6"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-6.html#board-problem",
    "href": "MA206-AY26-1/lesson-6.html#board-problem",
    "title": "Lesson 6: Discrete Distributions of Random Variables",
    "section": "",
    "text": "A bag contains 3 balls: one red, one blue, and one green.\nYou draw one ball at random. Let\n\\(X =\\) the number of points you score, where\n- red = 3 points\n- blue = 6 points\n- green = 12 points\n\nWhat is the probability distribution of \\(X\\)?\n\nPlot the probability distribution.\nFind the expected value \\(E[X]\\).\n\nInterpret the expected value.\n\nFind the variance and standard deviation of \\(X\\).\n\nInterpret the variance and standard deviation.\n\nWhat is the probability that \\(X \\leq 6\\)?\n\nWhat is the probability that \\(X\\) is less than one standard deviation above the mean?\n\nWhat is the probability that \\(X &gt; 6\\)?\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nEach outcome is equally likely:\n\n\\(P(X=3) = \\tfrac{1}{3}, \\quad P(X=6) = \\tfrac{1}{3}, \\quad P(X=12) = \\tfrac{1}{3}\\)\n\n\n\n\\(x\\)\n3\n6\n12\n\n\n\n\n\\(P(X=x)\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1/3\\)\n\n\n\n\nPlot the probability distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(E[X] = 3 \\, P(X=3) + 6 \\, P(X=6) + 12 \\, P(X=12)\\)\n\\(= 3 \\cdot \\tfrac{1}{3} + 6 \\cdot \\tfrac{1}{3} + 12 \\cdot \\tfrac{1}{3}\\)\n\\(= \\tfrac{21}{3} = 7\\)\n\n\n\nOn average, you score 7 points per draw.\nThis doesn’t mean you ever score exactly 7, but across many draws, the outcomes balance to that average.\n\n\n\n\\(\\mathrm{Var}(X) = (3-7)^2 \\, \\tfrac{1}{3} + (6-7)^2 \\, \\tfrac{1}{3} + (12-7)^2 \\, \\tfrac{1}{3}\\)\n\\(= (16)\\tfrac{1}{3} + (1)\\tfrac{1}{3} + (25)\\tfrac{1}{3}\\)\n\\(= \\tfrac{42}{3} = 14\\)\n\\(\\mathrm{SD}(X) = \\sqrt{14} \\approx 3.742\\)\n\n\n\nMost outcomes fall within about 3.7 points of the mean of 7.\nThat means the scores vary moderately, depending on whether you draw the low (3), medium (6), or high (12) ball.\n\n\n\n\\[\nP(X \\leq 6) = P(X=3) + P(X=6) = \\tfrac{1}{3} + \\tfrac{1}{3} = \\tfrac{2}{3}.\n\\]\n\n\n\nMean \\(= 7\\), \\(\\ \\mathrm{SD}(X) \\approx 3.742\\), so one standard deviation above the mean is about \\(10.742\\).\n\\[\nP(X &lt; 10.742) = P(X=3) + P(X=6) = \\tfrac{2}{3}.\n\\]\n\n\n\n\\[\nP(X &gt; 6) = P(X=12) = \\tfrac{1}{3}.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 6"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-6.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-6.html#before-you-leave",
    "title": "Lesson 6: Discrete Distributions of Random Variables",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nLesson 7\n\n\n\n\n\nMilestone 2 / Tidyverse Tutorial: Due 0700 Lesson 7\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 6"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html",
    "href": "MA206-AY26-1/lesson-8.html",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "Math 1 vs DPE\n\n\n\n\n\n\nPreviously 2-0\n\n\n\n\n\n\n3-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nThis is the probability mass function of a fair die.\nWhat do we know about it?\n\nProbabilities are non-negative: \\(P(X=x) \\geq 0\\).\n\nThe total probability is 1:\n\\[\n\\sum_x P(X=x) = 1\n\\]\nWe can compute probabilities of events by adding up the masses.\n\nExample: \\(P(X \\leq 3) = \\tfrac{1}{6} + \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{1}{2}\\).\n\n\n\n\n\nThis probability distribution function isn’t all that different from the discrete version.\nHere are the rules that all PDFs must follow:\n\n\\(f(x) \\geq 0\\) for all \\(x\\).\n\nThe total area under the curve is 1:\n\\[\n\\int_{-\\infty}^{\\infty} f(x)\\,dx = 1\n\\]\nProbabilities are found by areas, not points:\n\\[\nP(a \\leq X \\leq b) = \\int_a^b f(x)\\,dx\n\\]\nFor any single point,\n\\[\nP(X=c) = 0\n\\]\n\n\n\n\nConsider the function\n\\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\leq x \\leq 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nCheck the properties:\n\nNonnegative?\n\\(f(x) = 2x \\geq 0\\) on \\([0,1]\\). ✅\nTotal area?\n\\[\n\\int_{-\\infty}^\\infty f(x)\\,dx = \\int_0^1 2x\\,dx = \\left[x^2\\right]_0^1 = 1\n\\] So it integrates to 1. ✅\n\n\n\n\n\n\n\n\n\n\n\nProbability of an interval?\n\\[\nP(0.2 \\leq X \\leq 0.5) = \\int_{0.2}^{0.5} 2x \\, dx\n= \\left[x^2\\right]_{0.2}^{0.5}\n= (0.25 - 0.04) = 0.21\n\\]\n\n\n\n\n\n\n\n\n\n\nSo this is a valid PDF! It’s a “triangle-shaped” distribution on \\([0,1]\\) that places more weight near 1 than near 0.\n\nProbability of a single point is zero: \\(P(X=c)=0\\).\nFor a continuous random variable with PDF \\(f(x)\\), probability comes from area, so any single point has zero width: \\[\nP(X=c) \\;=\\; \\int_{c}^{c} f(x)\\,dx \\;=\\; 0.\n\\]\n\nFor example: \\(P(X=.6)=0\\)\n\\[\nP(X=.6) \\;=\\; \\int_{.6}^{.6} f(x)\\,dx \\;=\\; 0.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 1, \\\\[6pt]\n\\dfrac{1}{6}, & 1 \\leq x &lt; 2, \\\\[6pt]\n\\dfrac{2}{6}, & 2 \\leq x &lt; 3, \\\\[6pt]\n\\dfrac{3}{6}, & 3 \\leq x &lt; 4, \\\\[6pt]\n\\dfrac{4}{6}, & 4 \\leq x &lt; 5, \\\\[6pt]\n\\dfrac{5}{6}, & 5 \\leq x &lt; 6, \\\\[6pt]\n1, & x \\geq 6.\n\\end{cases}\n\\]\nThis step function is the CDF of the discrete die. It accumulates probability as we move to the right.\n\n\n\nFor any random variable \\(X\\) (discrete or continuous), the cumulative distribution function is\n\\[\nF(x) \\;=\\; P(X \\le x).\n\\]\nThings that are true of all CDFs:\n- Nondecreasing: if \\(a &lt; b\\) then \\(F(a) \\le F(b)\\).\n- Right-continuous: \\(\\lim_{x \\downarrow c} F(x) = F(c)\\).\n - Event probabilities from \\(F\\): for any \\(a &lt; b\\),\n\\[\nP(a \\le X \\le b) \\;=\\; F(b) - F(a).\n\\]\n\n\n\nRecall our PDF: \\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\le x \\le 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nIntegrate to get \\(F\\):\n\\[\nF(x) \\;=\\; \\int_{0}^{x} 2t\\,dt \\;=\\; x^{2}.\n\\]\nSo for any \\(x\\) in \\([0,1]\\),\n\\[\nF(x) = x^2.\n\\]\nQuick checks:\n- \\(F\\) is nondecreasing and continuous (no jumps).\n- \\(F(0)=0\\), \\(F(1)=1\\).\n- Example reads: \\(F(0.5)=0.5^2=0.25\\), \\(F(0.7)=0.49\\).\n- Interval probability via CDF:\n\\[\nP(0.2 \\le X \\le 0.5) = F(0.5) - F(0.2) = 0.25 - 0.04 = 0.21.\n\\]\n\n\n\n\n\n\n\n\n\nReading probabilities from the CDF:\n\n\\(P(X \\le 0.5) = F(0.5) = 0.25\\)\n\\[\nP(X \\le 0.5) = \\int_{0}^{0.5} 2t\\,dt = 0.25\n\\]\n\\(P(X &gt; 0.7) = 1 - F(0.7) = 1 - 0.49 = 0.51\\)\n\\[\nP(X &gt; 0.7) = \\int_{0.7}^{1} 2t\\,dt = 0.51\n\\]\n\\(P(0.2 \\le X \\le 0.5) = F(0.5) - F(0.2) = 0.21\\)\n\\[\nP(0.2 \\le X \\le 0.5) = \\int_{0.2}^{0.5} 2t\\,dt = 0.21\n\\]\n\n\n\n\n\n\n\nFor a discrete random variable (like a fair die), the expected value is the weighted average of the possible outcomes:\n\\[\nE[X] = \\sum_x x \\cdot P(X=x).\n\\]\nFor the fair die: \\[\nE[X] = 1\\cdot \\tfrac{1}{6} + 2\\cdot \\tfrac{1}{6} + 3\\cdot \\tfrac{1}{6} + 4\\cdot \\tfrac{1}{6} + 5\\cdot \\tfrac{1}{6} + 6\\cdot \\tfrac{1}{6} = 3.5\n\\]\n\n\n\nFor a continuous random variable with PDF \\(f(x)\\), the expected value is defined by an integral:\n\\[\nE[X] = \\int_{-\\infty}^{\\infty} x \\, f(x)\\,dx.\n\\]\nThis is the continuous version of the same weighted average idea — instead of summing over points, we integrate over the real line.\n\n\n\nRecall our PDF:\n\\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\leq x \\leq 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nCompute the expected value:\n\\[\nE[X] = \\int_{0}^{1} x \\cdot (2x)\\,dx = \\int_{0}^{1} 2x^{2}\\,dx.\n\\]\n\\[\nE[X] = \\left[\\tfrac{2}{3}x^{3}\\right]_{0}^{1} = \\tfrac{2}{3}.\n\\]\nSo the mean of this distribution is \\(\\tfrac{2}{3} \\approx 0.667\\).\n\n\n\n\n\n\nFor a discrete random variable, variance measures how spread out the values are around the mean:\n\\[\n\\text{Var}(X) = \\sum_x (x - \\mu)^2 \\, P(X=x),\n\\]\nwhere \\(\\mu = E[X]\\).\nThe standard deviation is just the square root of the variance:\n\\[\n\\sigma = \\sqrt{\\text{Var}(X)}.\n\\]\nFor a fair six-sided die:\n\nWe already know the mean is\n\\[\nE[X] = \\frac{1+2+3+4+5+6}{6} = 3.5.\n\\]\nCompute the variance:\n\\[\n\\text{Var}(X) = \\sum_{x=1}^{6} (x - 3.5)^2 \\cdot \\tfrac{1}{6}.\n\\]\n\n\\[\n= \\frac{(1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-3.5)^2}{6}\n= \\frac{35}{12} \\approx 2.92.\n\\]\n\nThen the standard deviation is\n\\[\n\\sigma = \\sqrt{\\tfrac{35}{12}} \\approx 1.71.\n\\]\n\nSo the die’s outcomes are typically about 1.7 away from the mean value of 3.5.\n\n\n\nFor a continuous random variable with PDF \\(f(x)\\), we replace the sum with an integral:\n\\[\n\\text{Var}(X) = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x)\\,dx,\n\\]\nwhere \\(\\mu = E[X]\\).\nBut this integral is often hard to compute directly, so there is a shortcut formula we can use:\n\\[\n\\text{Var}(X) = E[X^2] - \\big(E[X]\\big)^2,\n\\]\nwhere\n\\[\nE[X^2] = \\int_{-\\infty}^{\\infty} x^2 f(x)\\,dx.\n\\]\n\n\n\n\\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\leq x \\leq 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nFrom before, the mean is \\(\\mu = E[X] = \\tfrac{2}{3}\\).\nCompute \\(E[X^2]\\):\n\\[\nE[X^2] = \\int_{0}^{1} x^2 \\cdot (2x)\\,dx\n= \\int_{0}^{1} 2x^3\\,dx\n= \\left[\\tfrac{1}{2}x^4\\right]_{0}^{1} = \\tfrac{1}{2}.\n\\]\nNow variance:\n\\[\n\\text{Var}(X) = \\tfrac{1}{2} - \\left(\\tfrac{2}{3}\\right)^2\n= \\tfrac{1}{2} - \\tfrac{4}{9}\n= \\tfrac{1}{18}.\n\\]\nStandard deviation:\n\\[\n\\sigma = \\sqrt{\\tfrac{1}{18}} \\approx 0.236.\n\\]\n\n\n\n\n\nConsider the random variable \\(X\\) with PDF\n\\[\nf(x) =\n\\begin{cases}\n2(1-x), & 0 \\le x \\le 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n1. Plot the PDF\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Find \\(P(X = 0.4)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor continuous random variables:\n\\[\nP(X=0.4) = \\int_{0.4}^{0.4} f(x)\\,dx = 0.\n\\]\n\n\n\n3. Find \\(P(0.2 \\le X \\le 0.6)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(0.2 \\le X \\le 0.6) = \\int_{0.2}^{0.6} 2(1-x)\\,dx\n= \\Big[2x - x^2\\Big]_{0.2}^{0.6}.\n\\]\nAt \\(0.6\\): \\(2(0.6)-0.6^2=0.84\\).\nAt \\(0.2\\): \\(0.36\\).\nDifference: \\(0.48\\).\n\n\n\n4. Find the CDF \\(F(x)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor \\(0 \\le x \\le 1\\):\n\\[\nF(x) = \\int_{0}^{x} 2(1-t)\\,dt = 2x - x^2.\n\\]\nSo\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 0, \\\\\n2x - x^2, & 0 \\le x \\le 1, \\\\\n1, & x \\ge 1.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n5. Find \\(P(0.2 \\le X \\le 0.6)\\) using the CDF\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(0.2 \\le X \\le 0.6) = F(0.6) - F(0.2).\n\\]\n\\(F(0.6)=0.84\\), \\(F(0.2)=0.36\\), difference = \\(0.48\\).\n✅ Same as before.\n\n\n\n6. Find the expected value \\(E[X]\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nE[X] = \\int_{0}^{1} x \\cdot 2(1-x)\\,dx\n= \\int_{0}^{1} (2x - 2x^2)\\,dx.\n\\]\n\\[\n= \\Big[x^2 - \\tfrac{2}{3}x^3\\Big]_0^1\n= \\tfrac{1}{3}.\n\\]\n\n\n\n7. Find the variance \\(\\text{Var}(X)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCompute \\(E[X^2]\\):\n\\[\nE[X^2] = \\int_{0}^{1} x^2 \\cdot 2(1-x)\\,dx\n= \\tfrac{1}{6}.\n\\]\nSo\n\\[\n\\text{Var}(X) = E[X^2] - (E[X])^2\n= \\tfrac{1}{6} - \\left(\\tfrac{1}{3}\\right)^2\n= \\tfrac{1}{18}.\n\\]\n\n\n\n8. Find the standard deviation\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\n\\sigma = \\sqrt{\\tfrac{1}{18}} \\approx 0.236.\n\\]\n\n\n\n9. Find \\(P(\\mu - 2\\sigma \\le X \\le \\mu + 2\\sigma)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\mu=\\tfrac{1}{3}\\), \\(\\sigma \\approx 0.236\\), so \\(2\\sigma \\approx 0.472\\).\nRange: \\([\\tfrac{1}{3} - 0.472,\\;\\tfrac{1}{3} + 0.472] = [-0.139,\\;0.805]\\).\nSince the support is \\([0,1]\\), we use \\([0,0.805]\\).\n\\[\nP(0 \\le X \\le 0.805) = F(0.805) - F(0) = F(0.805).\n\\]\n\\(F(0.805) = 2(0.805) - (0.805)^2 \\approx 0.962\\).\n\n\n\n10. Find \\(P(X &lt; \\mu - \\sigma \\;\\text{ or }\\; X &gt; \\mu + \\sigma)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\mu=\\tfrac{1}{3}\\), \\(\\sigma \\approx 0.236\\).\nInterval within 1 SD: \\([0.097,\\,0.569]\\).\nSo the probability outside is\n\\[\nP(X &lt; 0.097) + P(X &gt; 0.569).\n\\]\nUsing the CDF:\n\\[\nP(X &lt; 0.097) = F(0.097) \\approx 0.185,\n\\] \\[\nP(X &gt; 0.569) = 1 - F(0.569) \\approx 1 - 0.814 = 0.186.\n\\]\nTotal = \\(0.185 + 0.186 = 0.371\\).\n\n\n\n\n\n\nConsider the random variable \\(X\\) with PDF\n\\[\nf(x) =\n\\begin{cases}\n\\dfrac{1}{4}, & 2 \\le x \\le 6,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nPlot the PDF\n\nFind \\(P(X = 3)\\)\n\nFind \\(P(2.5 \\le X \\le 4.5)\\)\n\nFind the CDF \\(F(x)\\)\n\nFind \\(P(2.5 \\le X \\le 4.5)\\) using the CDF. Is it the same as before?\n\nFind the expected value \\(E[X]\\)\n\nFind the variance \\(\\mathrm{Var}(X)\\)\n\nFind the standard deviation\n\nFind \\(P(\\mu - 2\\sigma \\le X \\le \\mu + 2\\sigma)\\)\n\nFind \\(P(X &lt; \\mu - \\sigma \\;\\text{ or }\\; X &gt; \\mu + \\sigma)\\)\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n1. Plot the PDF\n\n\n\n\n\n\n\n\n\n2. \\(P(X=3)\\)\nFor a continuous RV:\n\\[\nP(X=3) = \\int_{3}^{3} f(x)\\,dx = 0.\n\\]\n3. \\(P(2.5 \\le X \\le 4.5)\\)\n\\[\nP(2.5 \\le X \\le 4.5) = \\int_{2.5}^{4.5} \\tfrac{1}{4}\\,dx\n= \\tfrac{1}{4}\\,(4.5-2.5) = \\tfrac{1}{2}.\n\\]\n4. CDF \\(F(x)\\)\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 2,\\\\[4pt]\n\\tfrac{x-2}{4}, & 2 \\le x \\le 6,\\\\[8pt]\n1, & x \\ge 6.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n5. \\(P(2.5 \\le X \\le 4.5)\\) using \\(F\\)\n\\[\nP(2.5 \\le X \\le 4.5) = F(4.5) - F(2.5).\n\\]\n\\(F(4.5) = (4.5-2)/4 = 0.625\\),\n\\(F(2.5) = (2.5-2)/4 = 0.125\\).\nDifference = \\(0.5\\). ✅ Same as before.\n6. Expected value \\(E[X]\\)\n\\[\nE[X] = \\int_{2}^{6} x \\cdot \\tfrac{1}{4}\\,dx\n= \\tfrac{1}{4}\\,\\left[\\tfrac{x^2}{2}\\right]_{2}^{6}\n= \\tfrac{1}{4}\\,\\Big(\\tfrac{36}{2}-\\tfrac{4}{2}\\Big)\n= \\tfrac{1}{4}\\,(18-2) = 4.\n\\]\n7. Variance \\(\\mathrm{Var}(X)\\)\nFirst compute\n\\[\nE[X^2] = \\int_{2}^{6} x^2 \\cdot \\tfrac{1}{4}\\,dx\n= \\tfrac{1}{4}\\,\\left[\\tfrac{x^3}{3}\\right]_{2}^{6}\n= \\tfrac{1}{4}\\,\\Big(\\tfrac{216}{3}-\\tfrac{8}{3}\\Big)\n= \\tfrac{1}{4}\\cdot \\tfrac{208}{3} = \\tfrac{52}{3}.\n\\]\nSo\n\\[\n\\mathrm{Var}(X) = E[X^2] - (E[X])^2\n= \\tfrac{52}{3} - 4^2 = \\tfrac{52}{3} - 16 = \\tfrac{4}{3}.\n\\]\n8. Standard deviation\n\\[\n\\sigma = \\sqrt{\\tfrac{4}{3}} = \\tfrac{2}{\\sqrt{3}} \\approx 1.155.\n\\]\n9. Probability within \\(2\\sigma\\) of the mean\nHere \\(\\mu=4\\), \\(2\\sigma \\approx 2.309\\).\nInterval \\([4-2.309,\\,4+2.309] = [1.691,\\,6.309]\\).\nIntersect with support \\([2,6]\\) gives \\([2,6]\\).\nSo\n\\[\nP(\\mu-2\\sigma \\le X \\le \\mu+2\\sigma) = 1.\n\\]\n10. Probability more than \\(1\\sigma\\) from the mean\nInterval within \\(1\\sigma\\):\n\\[\n[\\mu-\\sigma,\\;\\mu+\\sigma] = [4-1.155,\\,4+1.155] = [2.845,\\,5.155].\n\\]\nLength of this interval = \\(2.31\\).\nSince the PDF is flat on \\([2,6]\\) (length 4):\n\\[\nP(|X-\\mu|\\le\\sigma) = \\tfrac{2.31}{4} \\approx 0.577.\n\\]\nTherefore\n\\[\nP(|X-\\mu| &gt; \\sigma) = 1 - 0.577 = 0.423.\n\\]\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\n\n\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#understand-the-purpose-of-a-probability-density-function-for-a-continuous-random-variable",
    "href": "MA206-AY26-1/lesson-8.html#understand-the-purpose-of-a-probability-density-function-for-a-continuous-random-variable",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nThis is the probability mass function of a fair die.\nWhat do we know about it?\n\nProbabilities are non-negative: \\(P(X=x) \\geq 0\\).\n\nThe total probability is 1:\n\\[\n\\sum_x P(X=x) = 1\n\\]\nWe can compute probabilities of events by adding up the masses.\n\nExample: \\(P(X \\leq 3) = \\tfrac{1}{6} + \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{1}{2}\\).\n\n\n\n\n\nThis probability distribution function isn’t all that different from the discrete version.\nHere are the rules that all PDFs must follow:\n\n\\(f(x) \\geq 0\\) for all \\(x\\).\n\nThe total area under the curve is 1:\n\\[\n\\int_{-\\infty}^{\\infty} f(x)\\,dx = 1\n\\]\nProbabilities are found by areas, not points:\n\\[\nP(a \\leq X \\leq b) = \\int_a^b f(x)\\,dx\n\\]\nFor any single point,\n\\[\nP(X=c) = 0\n\\]\n\n\n\n\nConsider the function\n\\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\leq x \\leq 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nCheck the properties:\n\nNonnegative?\n\\(f(x) = 2x \\geq 0\\) on \\([0,1]\\). ✅\nTotal area?\n\\[\n\\int_{-\\infty}^\\infty f(x)\\,dx = \\int_0^1 2x\\,dx = \\left[x^2\\right]_0^1 = 1\n\\] So it integrates to 1. ✅\n\n\n\n\n\n\n\n\n\n\n\nProbability of an interval?\n\\[\nP(0.2 \\leq X \\leq 0.5) = \\int_{0.2}^{0.5} 2x \\, dx\n= \\left[x^2\\right]_{0.2}^{0.5}\n= (0.25 - 0.04) = 0.21\n\\]\n\n\n\n\n\n\n\n\n\n\nSo this is a valid PDF! It’s a “triangle-shaped” distribution on \\([0,1]\\) that places more weight near 1 than near 0.\n\nProbability of a single point is zero: \\(P(X=c)=0\\).\nFor a continuous random variable with PDF \\(f(x)\\), probability comes from area, so any single point has zero width: \\[\nP(X=c) \\;=\\; \\int_{c}^{c} f(x)\\,dx \\;=\\; 0.\n\\]\n\nFor example: \\(P(X=.6)=0\\)\n\\[\nP(X=.6) \\;=\\; \\int_{.6}^{.6} f(x)\\,dx \\;=\\; 0.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#understand-the-purpose-of-a-cumulative-density-function-for-a-continuous-random-variable",
    "href": "MA206-AY26-1/lesson-8.html#understand-the-purpose-of-a-cumulative-density-function-for-a-continuous-random-variable",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 1, \\\\[6pt]\n\\dfrac{1}{6}, & 1 \\leq x &lt; 2, \\\\[6pt]\n\\dfrac{2}{6}, & 2 \\leq x &lt; 3, \\\\[6pt]\n\\dfrac{3}{6}, & 3 \\leq x &lt; 4, \\\\[6pt]\n\\dfrac{4}{6}, & 4 \\leq x &lt; 5, \\\\[6pt]\n\\dfrac{5}{6}, & 5 \\leq x &lt; 6, \\\\[6pt]\n1, & x \\geq 6.\n\\end{cases}\n\\]\nThis step function is the CDF of the discrete die. It accumulates probability as we move to the right.\n\n\n\nFor any random variable \\(X\\) (discrete or continuous), the cumulative distribution function is\n\\[\nF(x) \\;=\\; P(X \\le x).\n\\]\nThings that are true of all CDFs:\n- Nondecreasing: if \\(a &lt; b\\) then \\(F(a) \\le F(b)\\).\n- Right-continuous: \\(\\lim_{x \\downarrow c} F(x) = F(c)\\).\n - Event probabilities from \\(F\\): for any \\(a &lt; b\\),\n\\[\nP(a \\le X \\le b) \\;=\\; F(b) - F(a).\n\\]\n\n\n\nRecall our PDF: \\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\le x \\le 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nIntegrate to get \\(F\\):\n\\[\nF(x) \\;=\\; \\int_{0}^{x} 2t\\,dt \\;=\\; x^{2}.\n\\]\nSo for any \\(x\\) in \\([0,1]\\),\n\\[\nF(x) = x^2.\n\\]\nQuick checks:\n- \\(F\\) is nondecreasing and continuous (no jumps).\n- \\(F(0)=0\\), \\(F(1)=1\\).\n- Example reads: \\(F(0.5)=0.5^2=0.25\\), \\(F(0.7)=0.49\\).\n- Interval probability via CDF:\n\\[\nP(0.2 \\le X \\le 0.5) = F(0.5) - F(0.2) = 0.25 - 0.04 = 0.21.\n\\]\n\n\n\n\n\n\n\n\n\nReading probabilities from the CDF:\n\n\\(P(X \\le 0.5) = F(0.5) = 0.25\\)\n\\[\nP(X \\le 0.5) = \\int_{0}^{0.5} 2t\\,dt = 0.25\n\\]\n\\(P(X &gt; 0.7) = 1 - F(0.7) = 1 - 0.49 = 0.51\\)\n\\[\nP(X &gt; 0.7) = \\int_{0.7}^{1} 2t\\,dt = 0.51\n\\]\n\\(P(0.2 \\le X \\le 0.5) = F(0.5) - F(0.2) = 0.21\\)\n\\[\nP(0.2 \\le X \\le 0.5) = \\int_{0.2}^{0.5} 2t\\,dt = 0.21\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#calculate-and-interpret-the-expected-value-of-a-continuous-random-variable",
    "href": "MA206-AY26-1/lesson-8.html#calculate-and-interpret-the-expected-value-of-a-continuous-random-variable",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "For a discrete random variable (like a fair die), the expected value is the weighted average of the possible outcomes:\n\\[\nE[X] = \\sum_x x \\cdot P(X=x).\n\\]\nFor the fair die: \\[\nE[X] = 1\\cdot \\tfrac{1}{6} + 2\\cdot \\tfrac{1}{6} + 3\\cdot \\tfrac{1}{6} + 4\\cdot \\tfrac{1}{6} + 5\\cdot \\tfrac{1}{6} + 6\\cdot \\tfrac{1}{6} = 3.5\n\\]\n\n\n\nFor a continuous random variable with PDF \\(f(x)\\), the expected value is defined by an integral:\n\\[\nE[X] = \\int_{-\\infty}^{\\infty} x \\, f(x)\\,dx.\n\\]\nThis is the continuous version of the same weighted average idea — instead of summing over points, we integrate over the real line.\n\n\n\nRecall our PDF:\n\\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\leq x \\leq 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nCompute the expected value:\n\\[\nE[X] = \\int_{0}^{1} x \\cdot (2x)\\,dx = \\int_{0}^{1} 2x^{2}\\,dx.\n\\]\n\\[\nE[X] = \\left[\\tfrac{2}{3}x^{3}\\right]_{0}^{1} = \\tfrac{2}{3}.\n\\]\nSo the mean of this distribution is \\(\\tfrac{2}{3} \\approx 0.667\\).",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#calculate-and-interpret-the-variance-and-standard-deviation-of-a-continuous-random-variable",
    "href": "MA206-AY26-1/lesson-8.html#calculate-and-interpret-the-variance-and-standard-deviation-of-a-continuous-random-variable",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "For a discrete random variable, variance measures how spread out the values are around the mean:\n\\[\n\\text{Var}(X) = \\sum_x (x - \\mu)^2 \\, P(X=x),\n\\]\nwhere \\(\\mu = E[X]\\).\nThe standard deviation is just the square root of the variance:\n\\[\n\\sigma = \\sqrt{\\text{Var}(X)}.\n\\]\nFor a fair six-sided die:\n\nWe already know the mean is\n\\[\nE[X] = \\frac{1+2+3+4+5+6}{6} = 3.5.\n\\]\nCompute the variance:\n\\[\n\\text{Var}(X) = \\sum_{x=1}^{6} (x - 3.5)^2 \\cdot \\tfrac{1}{6}.\n\\]\n\n\\[\n= \\frac{(1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-3.5)^2}{6}\n= \\frac{35}{12} \\approx 2.92.\n\\]\n\nThen the standard deviation is\n\\[\n\\sigma = \\sqrt{\\tfrac{35}{12}} \\approx 1.71.\n\\]\n\nSo the die’s outcomes are typically about 1.7 away from the mean value of 3.5.\n\n\n\nFor a continuous random variable with PDF \\(f(x)\\), we replace the sum with an integral:\n\\[\n\\text{Var}(X) = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x)\\,dx,\n\\]\nwhere \\(\\mu = E[X]\\).\nBut this integral is often hard to compute directly, so there is a shortcut formula we can use:\n\\[\n\\text{Var}(X) = E[X^2] - \\big(E[X]\\big)^2,\n\\]\nwhere\n\\[\nE[X^2] = \\int_{-\\infty}^{\\infty} x^2 f(x)\\,dx.\n\\]\n\n\n\n\\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\leq x \\leq 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nFrom before, the mean is \\(\\mu = E[X] = \\tfrac{2}{3}\\).\nCompute \\(E[X^2]\\):\n\\[\nE[X^2] = \\int_{0}^{1} x^2 \\cdot (2x)\\,dx\n= \\int_{0}^{1} 2x^3\\,dx\n= \\left[\\tfrac{1}{2}x^4\\right]_{0}^{1} = \\tfrac{1}{2}.\n\\]\nNow variance:\n\\[\n\\text{Var}(X) = \\tfrac{1}{2} - \\left(\\tfrac{2}{3}\\right)^2\n= \\tfrac{1}{2} - \\tfrac{4}{9}\n= \\tfrac{1}{18}.\n\\]\nStandard deviation:\n\\[\n\\sigma = \\sqrt{\\tfrac{1}{18}} \\approx 0.236.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#practice-problem",
    "href": "MA206-AY26-1/lesson-8.html#practice-problem",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "Consider the random variable \\(X\\) with PDF\n\\[\nf(x) =\n\\begin{cases}\n2(1-x), & 0 \\le x \\le 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n1. Plot the PDF\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Find \\(P(X = 0.4)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor continuous random variables:\n\\[\nP(X=0.4) = \\int_{0.4}^{0.4} f(x)\\,dx = 0.\n\\]\n\n\n\n3. Find \\(P(0.2 \\le X \\le 0.6)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(0.2 \\le X \\le 0.6) = \\int_{0.2}^{0.6} 2(1-x)\\,dx\n= \\Big[2x - x^2\\Big]_{0.2}^{0.6}.\n\\]\nAt \\(0.6\\): \\(2(0.6)-0.6^2=0.84\\).\nAt \\(0.2\\): \\(0.36\\).\nDifference: \\(0.48\\).\n\n\n\n4. Find the CDF \\(F(x)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor \\(0 \\le x \\le 1\\):\n\\[\nF(x) = \\int_{0}^{x} 2(1-t)\\,dt = 2x - x^2.\n\\]\nSo\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 0, \\\\\n2x - x^2, & 0 \\le x \\le 1, \\\\\n1, & x \\ge 1.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n5. Find \\(P(0.2 \\le X \\le 0.6)\\) using the CDF\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(0.2 \\le X \\le 0.6) = F(0.6) - F(0.2).\n\\]\n\\(F(0.6)=0.84\\), \\(F(0.2)=0.36\\), difference = \\(0.48\\).\n✅ Same as before.\n\n\n\n6. Find the expected value \\(E[X]\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nE[X] = \\int_{0}^{1} x \\cdot 2(1-x)\\,dx\n= \\int_{0}^{1} (2x - 2x^2)\\,dx.\n\\]\n\\[\n= \\Big[x^2 - \\tfrac{2}{3}x^3\\Big]_0^1\n= \\tfrac{1}{3}.\n\\]\n\n\n\n7. Find the variance \\(\\text{Var}(X)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCompute \\(E[X^2]\\):\n\\[\nE[X^2] = \\int_{0}^{1} x^2 \\cdot 2(1-x)\\,dx\n= \\tfrac{1}{6}.\n\\]\nSo\n\\[\n\\text{Var}(X) = E[X^2] - (E[X])^2\n= \\tfrac{1}{6} - \\left(\\tfrac{1}{3}\\right)^2\n= \\tfrac{1}{18}.\n\\]\n\n\n\n8. Find the standard deviation\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\n\\sigma = \\sqrt{\\tfrac{1}{18}} \\approx 0.236.\n\\]\n\n\n\n9. Find \\(P(\\mu - 2\\sigma \\le X \\le \\mu + 2\\sigma)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\mu=\\tfrac{1}{3}\\), \\(\\sigma \\approx 0.236\\), so \\(2\\sigma \\approx 0.472\\).\nRange: \\([\\tfrac{1}{3} - 0.472,\\;\\tfrac{1}{3} + 0.472] = [-0.139,\\;0.805]\\).\nSince the support is \\([0,1]\\), we use \\([0,0.805]\\).\n\\[\nP(0 \\le X \\le 0.805) = F(0.805) - F(0) = F(0.805).\n\\]\n\\(F(0.805) = 2(0.805) - (0.805)^2 \\approx 0.962\\).\n\n\n\n10. Find \\(P(X &lt; \\mu - \\sigma \\;\\text{ or }\\; X &gt; \\mu + \\sigma)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\mu=\\tfrac{1}{3}\\), \\(\\sigma \\approx 0.236\\).\nInterval within 1 SD: \\([0.097,\\,0.569]\\).\nSo the probability outside is\n\\[\nP(X &lt; 0.097) + P(X &gt; 0.569).\n\\]\nUsing the CDF:\n\\[\nP(X &lt; 0.097) = F(0.097) \\approx 0.185,\n\\] \\[\nP(X &gt; 0.569) = 1 - F(0.569) \\approx 1 - 0.814 = 0.186.\n\\]\nTotal = \\(0.185 + 0.186 = 0.371\\).",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#board-problem",
    "href": "MA206-AY26-1/lesson-8.html#board-problem",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "Consider the random variable \\(X\\) with PDF\n\\[\nf(x) =\n\\begin{cases}\n\\dfrac{1}{4}, & 2 \\le x \\le 6,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nPlot the PDF\n\nFind \\(P(X = 3)\\)\n\nFind \\(P(2.5 \\le X \\le 4.5)\\)\n\nFind the CDF \\(F(x)\\)\n\nFind \\(P(2.5 \\le X \\le 4.5)\\) using the CDF. Is it the same as before?\n\nFind the expected value \\(E[X]\\)\n\nFind the variance \\(\\mathrm{Var}(X)\\)\n\nFind the standard deviation\n\nFind \\(P(\\mu - 2\\sigma \\le X \\le \\mu + 2\\sigma)\\)\n\nFind \\(P(X &lt; \\mu - \\sigma \\;\\text{ or }\\; X &gt; \\mu + \\sigma)\\)\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n1. Plot the PDF\n\n\n\n\n\n\n\n\n\n2. \\(P(X=3)\\)\nFor a continuous RV:\n\\[\nP(X=3) = \\int_{3}^{3} f(x)\\,dx = 0.\n\\]\n3. \\(P(2.5 \\le X \\le 4.5)\\)\n\\[\nP(2.5 \\le X \\le 4.5) = \\int_{2.5}^{4.5} \\tfrac{1}{4}\\,dx\n= \\tfrac{1}{4}\\,(4.5-2.5) = \\tfrac{1}{2}.\n\\]\n4. CDF \\(F(x)\\)\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 2,\\\\[4pt]\n\\tfrac{x-2}{4}, & 2 \\le x \\le 6,\\\\[8pt]\n1, & x \\ge 6.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n5. \\(P(2.5 \\le X \\le 4.5)\\) using \\(F\\)\n\\[\nP(2.5 \\le X \\le 4.5) = F(4.5) - F(2.5).\n\\]\n\\(F(4.5) = (4.5-2)/4 = 0.625\\),\n\\(F(2.5) = (2.5-2)/4 = 0.125\\).\nDifference = \\(0.5\\). ✅ Same as before.\n6. Expected value \\(E[X]\\)\n\\[\nE[X] = \\int_{2}^{6} x \\cdot \\tfrac{1}{4}\\,dx\n= \\tfrac{1}{4}\\,\\left[\\tfrac{x^2}{2}\\right]_{2}^{6}\n= \\tfrac{1}{4}\\,\\Big(\\tfrac{36}{2}-\\tfrac{4}{2}\\Big)\n= \\tfrac{1}{4}\\,(18-2) = 4.\n\\]\n7. Variance \\(\\mathrm{Var}(X)\\)\nFirst compute\n\\[\nE[X^2] = \\int_{2}^{6} x^2 \\cdot \\tfrac{1}{4}\\,dx\n= \\tfrac{1}{4}\\,\\left[\\tfrac{x^3}{3}\\right]_{2}^{6}\n= \\tfrac{1}{4}\\,\\Big(\\tfrac{216}{3}-\\tfrac{8}{3}\\Big)\n= \\tfrac{1}{4}\\cdot \\tfrac{208}{3} = \\tfrac{52}{3}.\n\\]\nSo\n\\[\n\\mathrm{Var}(X) = E[X^2] - (E[X])^2\n= \\tfrac{52}{3} - 4^2 = \\tfrac{52}{3} - 16 = \\tfrac{4}{3}.\n\\]\n8. Standard deviation\n\\[\n\\sigma = \\sqrt{\\tfrac{4}{3}} = \\tfrac{2}{\\sqrt{3}} \\approx 1.155.\n\\]\n9. Probability within \\(2\\sigma\\) of the mean\nHere \\(\\mu=4\\), \\(2\\sigma \\approx 2.309\\).\nInterval \\([4-2.309,\\,4+2.309] = [1.691,\\,6.309]\\).\nIntersect with support \\([2,6]\\) gives \\([2,6]\\).\nSo\n\\[\nP(\\mu-2\\sigma \\le X \\le \\mu+2\\sigma) = 1.\n\\]\n10. Probability more than \\(1\\sigma\\) from the mean\nInterval within \\(1\\sigma\\):\n\\[\n[\\mu-\\sigma,\\;\\mu+\\sigma] = [4-1.155,\\,4+1.155] = [2.845,\\,5.155].\n\\]\nLength of this interval = \\(2.31\\).\nSince the PDF is flat on \\([2,6]\\) (length 4):\n\\[\nP(|X-\\mu|\\le\\sigma) = \\tfrac{2.31}{4} \\approx 0.577.\n\\]\nTherefore\n\\[\nP(|X-\\mu| &gt; \\sigma) = 1 - 0.577 = 0.423.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-8.html#before-you-leave",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\n\n\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html",
    "href": "MA206-AY26-1/lesson-11.html",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "📄 General Instructions\n\n\n\nAcademic Articles Worksheet\n\n📑 Use: Academic Articles Worksheet\n\n✅ Worth: 10 points\n\n⏰ Due: 0700 Friday, 19 Sept 2025\n\n🔗 Submit on Canvas: https://westpoint.instructure.com/courses/10295/assignments/223751\n\nIntroduction & Data Section\n\n📑 Use: Math Writing Template\n\n✅ Worth: 20 points\n\n⏰ Due: 0700 Sunday, 21 Sept 2025\n\n🔗 Submit on Canvas: https://westpoint.instructure.com/courses/10295/assignments/223738\n\n\n\n\n\n\n\n\nNote\n\n\n\nReminder: Also add both items to your binder with an updated Annex B (not graded yet).\n\n\n\n\n\n\n⏰ Due 0700 ET on Lesson 13\n\nDay 1: Wednesday, 24 Sept 2025\n\nDay 2: Thursday, 25 Sept 2025\n\n\n📑 Worksheet: https://westpoint.instructure.com/courses/10295/assignments/216497 — don’t sleep on this!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\nDuring my son and his team’s Little League World Series run, Cal got on base \\(18\\) out of \\(35\\) times. Across all of Texas East Little Leagues, the average on-base percentage was about \\(40\\%\\).\nQuestion: Is Cal really better than average, or is this just by chance?\n\n\nIf the average player gets on base at a \\(40\\%\\) rate, what is the probability that we would observe someone get on base \\(18/35 = 0.514\\) or higher?\nIn other words: if Cal were truly a 40% hitter, how often would we see a season this good (or better) just by chance?\n\n\n\nLet’s pretend we could replay Cal’s season many times under the assumption he is a \\(40\\%\\) hitter.\n\nlibrary(tidyverse)\n\n# simulate one season\none_season &lt;- rbinom(n = 35, size = 1, prob = 0.4)\none_season\n\n [1] 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0\n\n\n\n# total times on base\non_base_at_bats &lt;- sum(one_season)\non_base_at_bats\n\n[1] 16\n\n# as a proportion\non_base_at_bats / length(one_season)\n\n[1] 0.4571429\n\n\n\n\n\nNow let’s repeat this process \\(10{,}000\\) times.\n\nn &lt;- 10000\n\nsim_results &lt;- tibble(\n  trial = 1:n,\n  rocks = rbinom(n, size = 35, prob = 0.4)\n) |&gt; \n  mutate(proportion = rocks / 35)\n\nhead(sim_results)\n\n# A tibble: 6 × 3\n  trial rocks proportion\n  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1     1    18      0.514\n2     2     5      0.143\n3     3    10      0.286\n4     4    12      0.343\n5     5    15      0.429\n6     6     8      0.229\n\n\n\n\n\nHere’s the distribution of on-base proportions from the simulations.\n\non_base_hist &lt;- sim_results |&gt;\n  ggplot(aes(x = proportion)) +\n  geom_histogram(binwidth = 1/35, boundary = 0, fill = \"skyblue\", color = \"white\") \n\non_base_hist\n\n\n\n\n\n\n\n\n\n\n\nNow let’s add Cal’s observed rate (\\(0.514\\)):\n\non_base_hist +\n  geom_vline(xintercept = 0.514, color = \"firebrick\", linetype = 5, linewidth = 2)\n\n\n\n\n\n\n\n\n\n\n\nFinally, what proportion of simulated seasons were at least this extreme?\n\nsim_results |&gt; \n  summarise(prob_more_extreme = mean(proportion &gt;= 0.514))\n\n# A tibble: 1 × 1\n  prob_more_extreme\n              &lt;dbl&gt;\n1             0.114\n\n\n\n\n\nIn our simulation, only about X% of seasons produced an on-base percentage this high or higher if Cal were truly a 40% hitter.\n➡️ This suggests his observed \\(0.514\\) season is possibly to be due to chance alone — the evidence is not convincing that he might might be better than average.\n\n\n\n\n\n\n\nIs Cal’s on-base percentage higher than the Texas East Little League average of 40%?\n\n\n\nWe have observational data from Cal’s 35 plate appearances during the Little League World Series run.\n- \\(n = 35\\) plate appearances\n- \\(x = 18\\) times on base\n- Observed proportion: \\(\\hat \\pi = \\tfrac{18}{35} \\approx 0.514\\)\nWe treat these 35 at-bats as a random sample from his true underlying ability.\n\n\n\nThe observed proportion of \\(0.514\\) is above the reference average of \\(0.40\\). This is about 11 percentage points higher. The key question is whether this difference is large enough to be unlikely by chance.\n\n\n\nWe set up hypotheses:\n\nNull hypothesis: \\(H_0 : \\pi = 0.40\\)\n\nAlternative hypothesis: \\(H_A : \\pi &gt; 0.40\\)\n\nCompute the standard error and test statistic:\n\\[\nSE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\n= \\sqrt{\\frac{0.40(0.60)}{35}}\n\\approx 0.083\n\\]\n\\[\nz = \\frac{\\hat \\pi - \\pi_0}{SE}\n= \\frac{0.514 - 0.40}{0.083}\n\\approx 1.37\n\\]\n\n\n\nThe one-tailed \\(p\\)-value is:\n\\[\np = P(Z \\geq 1.37) \\approx 0.085\n\\]\n\n1 - pnorm(q = 1.37, mean = 0, sd = 1)\n\n[1] 0.08534345\n\n\nAt \\(\\alpha = 0.05\\), this result is not statistically significant. We do not have strong enough evidence to conclude Cal’s on-base percentage is truly higher than 40%.\n\n\n\n\nThe observed rate of \\(0.514\\) is higher than average and suggestive (\\(p \\approx 0.085\\)).\n\nWith only 35 plate appearances, there is considerable variability — a larger sample would provide a clearer answer.\n\nContext matters: 35 at bats may not represent long-term ability. Future seasons with more data or more at-bats in a season could confirm or refute this pattern.\n\n\n\n\n\n\n\nThe null hypothesis (\\(H_0\\)) is the starting assumption — usually that there is “no difference” or “no effect.”\n\nIn our example: \\(H_0 : \\pi = 0.40\\)\n\nThis means we assume Cal’s true on-base probability is the same as the Texas East Little League average of 40%.\n\n\n\n\nThe alternative hypothesis (\\(H_A\\)) is what we want to investigate. Depending on the research question, there are three common forms:\n\nRight-tailed (greater than)\n\n\\(H_A : \\pi &gt; 0.40\\)\n\nIn context: Is Cal’s true on-base percentage higher than 40%?\n\nThis is the version we are using, because the natural question is whether he’s better than average.\n\nLeft-tailed (less than)\n\n\\(H_A : \\pi &lt; 0.40\\)\n\nIn context: Is Cal’s true on-base percentage lower than 40%?\n\nYou would ask this if you suspected Cal might actually be worse than average at getting on base.\n\nTwo-tailed (not equal)\n\n\\(H_A : \\pi \\neq 0.40\\)\n\nIn context: Is Cal’s true on-base percentage different from 40% (either higher or lower)?\n\nYou would use this if you want to know whether Cal performs differently than average, without assuming in advance which direction.\n\n\n\n\n\n\nIf you are asking, “Is Cal better than average?”, the right-tailed test is appropriate.\n\nIf the concern were that Cal struggles at the plate, you’d use a left-tailed test.\n\nIf you only care whether Cal is different from average in either direction, the two-tailed test is the right choice.\n\n\n\n\n\n\\(\\pi\\) (the Greek letter pi) represents the population proportion of average of getting on base.\n\nIn context: \\(\\pi\\) is Cal’s true long-run probability of getting on base each plate appearance.\n\nWe never observe \\(\\pi\\) directly — we estimate it with \\(\\hat{\\pi}\\) (the sample proportion).\n\n\n\n\n\n\\(\\alpha\\) (alpha) is the threshold for evidence against the null hypothesis.\n\nCommon choices: \\(\\alpha = 0.05\\) (5%) or \\(\\alpha = 0.01\\) (1%).\n\nInterpretation: If \\(p \\leq \\alpha\\), the result is considered statistically significant — unlikely to occur just by chance if \\(H_0\\) were true.\n\n\n\n\nThe \\(z\\) distribution (also called the standard normal distribution) is a bell-shaped curve with:\n- Mean = \\(0\\)\n- Standard deviation = \\(1\\)\nSo how do we get from our sample proportion \\(\\hat{\\pi}\\) to this special distribution?\n\n\nUnder the null hypothesis \\(H_0 : \\pi = 0.40\\), the sample proportion \\(\\hat{\\pi}\\) has a sampling distribution that is approximately normal (by the Central Limit Theorem) with:\n\nMean = \\(\\pi_0 = 0.40\\)\n\nStandard deviation = \\(SE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\)\n\nThis tells us what values of \\(\\hat{\\pi}\\) we would expect just by chance if the null were true.\n\n\n\nTo compare our observed \\(\\hat{\\pi}\\) to this distribution, we standardize it:\n\nSubtract the mean under \\(H_0\\)\n\nThis centers the distribution at \\(0\\) by measuring how far away our observed statistic is from the null.\n\n\\((\\hat{\\pi} - \\pi_0)\\)\n\nDivide by the standard deviation\n\nThis rescales differences into standard deviation units, so we can judge how unusual they are.\n\n\\(\\dfrac{\\hat{\\pi} - \\pi_0}{SE}\\)\n\n\n\n\n\nAfter centering and rescaling, the new standardized statistic follows (approximately) the standard normal distribution, \\(N(0,1)\\).\nThat’s why we call it the \\(z\\) statistic:\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n\\]\nNow we can use the \\(z\\) distribution to calculate probabilities (like \\(p\\)-values) for how extreme our observed result is relative to the null hypothesis.\n\nggplot() +\n  geom_function(fun = dnorm, xlim = c(-4,4))\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Activity: With your eyes closed, on the count of three, everyone will make their first throw of Rock–Paper–Scissors.\n- 👊 = Rock\n- ✋ = Paper\n- ✌️ = Scissors\nHold your choice steady so we can tally the results. Closing your eyes helps reduce herding or copying from neighbors.\n\n\nDo students tend to choose Rock more or less often than random chance (\\(33\\%\\)) on their first throw?\n\n\n\nWith eyes closed, on the count of 3 each student chooses one option (Rock/Paper/Scissors) as if starting a game. We then tally the class counts:\n- \\(R\\) = number of Rock\n- \\(P\\) = number of Paper\n- \\(S\\) = number of Scissors\n- \\(n = R+P+S\\)\n\n# Enter the tallies you just collected:\nR &lt;- 2   # Rock count\nP &lt;- 10   # Paper count\nS &lt;- 7   # Scissors count\n\nn &lt;- R + P + S\nc(R = R, P = P, S = S, n = n)\n\n R  P  S  n \n 2 10  7 19 \n\n\n\n\n\nCompute the sample proportion choosing Rock and make a quick bar chart. Under complete randomness we’d expect each to be near \\(n/3\\).\n\npihat &lt;- R / n\npihat\n\n[1] 0.1052632\n\ndf &lt;- tibble(option = c(\"Rock\",\"Paper\",\"Scissors\"),\n             count  = c(R, P, S))\n\nggplot(df, aes(option, count)) +\n  geom_col() +\n  geom_hline(yintercept = n/3, linetype = 2) +\n  labs(title = \"Class First-Throw Choices\",\n       subtitle = \"Dashed line = expected count if choices were uniform (n/3)\",\n       x = NULL, y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\nWe’ll do two things:\n\nSimulate many samples of size \\(n\\) assuming \\(p_{Rock}=1/3\\), and estimate a two-sided \\(p\\)-value by comparing simulated proportions to the observed.\n\nDo the math version using the \\(z\\)-distribution.\n\n\n\n\npi0 &lt;- 1/3\nN  &lt;- 10000               # number of simulations\nR_obs &lt;- R                # keep observed Rock consistent with Step 2\npihat_obs &lt;- R_obs / n\n\n# Simulate many samples under H0\nsim_results &lt;- tibble(\n  trial = 1:N,\n  rocks = rbinom(N, size = n, prob = pi0)   # Rock counts in each simulated sample\n) |&gt;\n  mutate(pihat = rocks / n)                  # simulated sample proportions\n\nsim_results\n\n# A tibble: 10,000 × 3\n   trial rocks pihat\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     1     5 0.263\n 2     2     8 0.421\n 3     3     9 0.474\n 4     4     4 0.211\n 5     5     4 0.211\n 6     6     4 0.211\n 7     7     2 0.105\n 8     8     5 0.263\n 9     9     6 0.316\n10    10     4 0.211\n# ℹ 9,990 more rows\n\n\n\n# Plot the simulated sampling distribution with observed and null marked\nsim_results |&gt;\n  ggplot(aes(x = pihat)) +\n  geom_histogram(binwidth = 1/n, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = pihat_obs, color = \"firebrick\", linetype = 2, linewidth = 1.2) +\n  geom_vline(xintercept = pi0, color = \"gray40\", linewidth = 1.2) +\n  labs(title = paste0(\"Sampling Distribution of p̂i under H0 (n = \", n, \")\"),\n       subtitle = paste0(\"Observed p̂ = \", round(pihat_obs,3), \n                         \" | Null pi0 = \", round(pi0,3)),\n       x = \"p̂ (proportion Rock)\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Two-sided simulation p-value\nobs_abs_diff &lt;- abs(pihat_obs - pi0)\n\nsim_p_two_sided &lt;- sim_results |&gt; \n  mutate(pihat_different_than_null = pihat - pi0) |&gt;          # difference from null\n  mutate(abs_of_difference = abs(pihat_different_than_null)) |&gt; \n  mutate(is_extreme = abs_of_difference &gt;= obs_abs_diff) |&gt; # flag extremes\n  summarise(p_val = mean(is_extreme))                       # proportion of extremes = p-value\n\nsim_p_two_sided\n\n# A tibble: 1 × 1\n   p_val\n   &lt;dbl&gt;\n1 0.0503\n\n\n\n\n\n\n\\(H_0: p_{Rock} = 1/3\\)\n\n\\(H_A: p_{Rock} \\neq 1/3\\)\n\n\\(SE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\)\n\n# Standard Error under H0\nSE &lt;- sqrt(pi0 * (1 - pi0) / n)\nSE\n\n[1] 0.1081476\n\n\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n\\]\n\n# Test Statistic (z)\nz_stat &lt;- (pihat_obs - pi0) / SE\nz_stat\n\n[1] -2.108878\n\n\n\\[\np = 2 \\times P(Z \\geq |z|)\n\\]\n\n# Two-Sided p-value\np_val_two_sided &lt;- 2 * (1 - pnorm(abs(z_stat)))\np_val_two_sided\n\n[1] 0.03495507\n\n\n\n\n\n\nNow we bring the two approaches together:\n\nSimulation gave us an empirical \\(p\\)-value by resampling under \\(H_0\\).\n\nMath/analytic (\\(z\\)-test) gave us an approximate \\(p\\)-value using the standard normal.\n\n\n\n              n           R_obs       pihat_obs             pi0          z_stat \n        19.0000          2.0000          0.1050          0.3330         -2.1090 \nsim_p_two_sided   z_p_two_sided           alpha \n         0.0503          0.0350          0.0500 \n\n\nSimulation-based decision: Fail to reject H0 \n\n\nZ-approximation decision: Reject H0 (evidence of ≠ 1/3) \n\n\nInterpretation:\nWith \\(n\\) throws and observed \\(\\hat p\\), the simulation \\(p\\)-value tells us how unusual the result is if \\(p=1/3\\) were true. The \\(z\\) test gives a similar answer using a theoretical normal curve. At \\(\\alpha = 0.05\\), compare both to decide whether to reject \\(H_0\\).\n\n\n\n\n\nCode\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tibble)\n\nui &lt;- fluidPage(\n  titlePanel(\"One-Proportion Test: z-formulas + Live Simulation\"),\n  withMathJax(),\n  tags$hr(),\n  \n  fluidRow(\n    column(\n      width = 4,\n      h4(\"Inputs\"),\n      numericInput(\"x\", \"Observed successes (x)\", value = 2, min = 0, step = 1),\n      numericInput(\"n\", \"Sample size (n)\", value = 19, min = 1, step = 1),\n      sliderInput(\"pi0\", HTML(\"&pi;&lt;sub&gt;0&lt;/sub&gt; (null proportion)\"), min = 0, max = 1,\n                  value = 1/3, step = 0.01),\n      numericInput(\"B\", \"Simulations (B)\", value = 10000, min = 100, step = 100),\n      numericInput(\"seed\", \"Random seed\", value = 26, min = 1, step = 1),\n      helpText(\"Tip: This defaults to the R/P/S example: x = 2 (Rock), n = 19, π0 = 1/3.\")\n    ),\n    column(\n      width = 8,\n      h4(\"Formulas\"),\n      # Show the formulas via MathJax\n      div(style = \"font-size: 1.15em; margin-bottom: 8px;\",\n          \"$$ SE = \\\\sqrt{\\\\frac{\\\\pi_0(1 - \\\\pi_0)}{n}}, \\\\qquad z = \\\\frac{\\\\hat{p} - \\\\pi_0}{SE} $$\"\n      ),\n      h4(\"Computed Values\"),\n      tableOutput(\"value_table\"),\n      tags$br(),\n      h4(\"Sampling Distribution under H0 (Simulated)\"),\n      plotOutput(\"hist_plot\", height = \"330px\"),\n      helpText(\"Histogram shows simulated \\\\(\\\\hat p\\\\) under H0. Red line = observed \\\\(\\\\hat p\\\\). Gray line = \\\\(\\\\pi_0\\\\).\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  # Basic reactives\n  phat &lt;- reactive({\n    req(input$n &gt; 0)\n    input$x / input$n\n  })\n  \n  SE &lt;- reactive({\n    sqrt(input$pi0 * (1 - input$pi0) / input$n)\n  })\n  \n  z_stat &lt;- reactive({\n    (phat() - input$pi0) / SE()\n  })\n  \n  # p-values for three alternatives\n  p_right &lt;- reactive({ 1 - pnorm(z_stat()) })                 # H_A: p &gt; pi0\n  p_left  &lt;- reactive({ pnorm(z_stat()) })                     # H_A: p &lt; pi0\n  p_two   &lt;- reactive({ 2 * (1 - pnorm(abs(z_stat()))) })      # H_A: p != pi0\n  \n  # Simulation under H0\n  sim_df &lt;- reactive({\n    req(input$B &gt;= 100)\n    set.seed(input$seed)\n    rocks &lt;- rbinom(input$B, size = input$n, prob = input$pi0)\n    tibble(\n      phat = rocks / input$n\n    )\n  })\n  \n  # Output: table of computed values\n  output$value_table &lt;- renderTable({\n    tibble::tibble(\n      `x (successes)` = input$x,\n      `n (trials)`    = input$n,\n      `π0 (null)`     = round(input$pi0, 4),\n      `p̂ = x/n`      = round(phat(), 4),\n      `SE`            = round(SE(), 5),\n      `z`             = round(z_stat(), 4),\n      `p (right)`     = signif(p_right(), 4),\n      `p (left)`      = signif(p_left(), 4),\n      `p (two-sided)` = signif(p_two(), 4)\n    )\n  }, striped = TRUE, bordered = TRUE, spacing = \"s\", digits = 6)\n  \n  # Output: histogram with vertical lines at phat and pi0\n  output$hist_plot &lt;- renderPlot({\n    df &lt;- sim_df()\n    ggplot(df, aes(x = phat)) +\n      geom_histogram(binwidth = 1 / input$n, color = \"white\") +\n      geom_vline(xintercept = phat(), color = \"firebrick\", linetype = 2, linewidth = 1.2) +\n      geom_vline(xintercept = input$pi0, color = \"gray40\", linewidth = 1.2) +\n      labs(\n        x = expression(hat(p) ~ \"(proportion)\"),\n        y = \"Count\",\n        title = paste0(\"Simulated Sampling Distribution of \", expression(hat(p)), \" under H0\"),\n        subtitle = paste0(\"n = \", input$n, \", π0 = \", round(input$pi0, 3),\n                          \", observed p̂ = \", round(phat(), 3),\n                          \"; B = \", input$B)\n      ) +\n      theme_minimal(base_size = 12) +\n      xlim(c(0,1))\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\n\n\nFor all cases:\n\\[\nSE = \\sqrt{\\frac{\\pi_0(1 - \\pi_0)}{n}}, \\quad\nz = \\frac{\\hat{p} - \\pi_0}{SE}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.\n\n\n\n\nA hospital claims that 85% of discharge summaries are finalized within 24 hours.\nIn an audit of 60 summaries, 46 were finalized within 24 hours.\nResearch Question: Is the true proportion finalized within 24 hours less than 85%?\n\nState the hypotheses.\n\nExplain (in words) how you would simulate this test (do not actually simulate).\n\nThen, perform the mathematical one-proportion \\(z\\) test.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nDo fewer than 85% of discharge summaries get finalized within 24 hours?\n\n\n\n\n\\(H_0 : \\pi = 0.85\\)\n\n\\(H_A : \\pi &lt; 0.85\\)\n\n\n\n\n\n\\(n = 60\\) summaries\n\n\\(x = 46\\) finalized within 24h\n\n\\(\\hat{\\pi} = \\tfrac{x}{n} = \\tfrac{46}{60} = 0.767\\)\n\n\n\n\n\n\n\n\nUnder \\(H_0\\), simulate many samples of size \\(n=60\\) with \\(\\pi_0=0.85\\).\n\nFor each, compute \\(\\hat{\\pi}_{sim}\\).\n\nEstimate the left-tailed \\(p\\)-value as the fraction of simulations with \\(\\hat{\\pi}_{sim} \\le \\hat{\\pi}_{obs} = 0.767\\).\n\n\n\n\nStandard error under \\(H_0\\):\n\\[\nSE = \\sqrt{\\frac{\\pi_0 (1-\\pi_0)}{n}}\n   = \\sqrt{\\frac{0.85 \\cdot 0.15}{60}}\n   \\approx 0.046\n\\]\nTest statistic:\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n  = \\frac{0.767 - 0.85}{0.046}\n  \\approx -1.80\n\\]\n\\(p\\)-value (left-tailed):\n\\[\np = \\Phi(z) = \\Phi(-1.80) \\approx 0.036\n\\]\n\npnorm(-1.8)\n\n[1] 0.03593032\n\n\n\n\n\nAt \\(\\alpha = 0.05\\), since \\(p \\approx 0.036 &lt; 0.05\\), we reject \\(H_0\\).\n\n\n\nThere is statistical evidence that fewer than 85% of discharge summaries are completed within 24 hours.\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nProject Milestone 3: Due Canvas 22 Sept\nExploration Exercise 1.5: Due at 0700 on Lesson 13\n24 September 2025 for Day 1\n\n25 September 2025 for Day 2)\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#lets-talk-milestone-3",
    "href": "MA206-AY26-1/lesson-11.html#lets-talk-milestone-3",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "General Instructions\n2 Things Due\nAcademic Articles Worksheet - Use the Math Writing Template: - 10 Points\n- 0700 Friday, 19 Sept\nAnnex B \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-11.html#before-you-leave",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nProject Milestone 3: Due Canvas 22 Sept\nExploration Exercise 1.5: Due at 0700 on Lesson 13\n24 September 2025 for Day 1\n\n25 September 2025 for Day 2)\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#little-league-on-base-percentage",
    "href": "MA206-AY26-1/lesson-11.html#little-league-on-base-percentage",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "During my son and his team’s Little League World Series Run, Cal got on base \\(18\\) out of \\(35\\) times. I did some research and noticed that across all of Texas East Little Leagues, the average on base percentage was \\(40\\%\\).\nIs he really better at this or is this just by chance?\nHow can we approach this?\nLets rephrase the questions. If the average player gets on base at a \\(40\\%\\) rate, what is the probably that you would observe someone get on base \\(18/35=.514\\) or more extreme?\nHow can we approach this?\nLets pretend we redid his season over and over again assuming he was a \\(40\\%\\) hitter. How unlikely are we to see him have a \\(.517\\) season?\n\nlibrary(tidyverse)\n\nrbinom(n = 35, size = 1, prob = .4)\nsum(rbinom(n = 35, size = 1, prob = .4))\nsum(rbinom(n = 35, size = 1, prob = .4))/35\n\nn &lt;- 10000\n\nsim_results &lt;- \n  tibble(trial = 1:n,\n         successes = replicate(n, sum(rbinom(n = 35, size = 1, prob = 0.4)))\n         ) |&gt;\n  mutate(proportion = successes / 35)\n\nsim_results\n\n# bins &lt;- sim_results$successes |&gt; range() |&gt; diff()\nbins &lt;- sim_results$successes |&gt; unique() |&gt; range() |&gt; diff()\n\nsim_results |&gt; \n  ggplot(aes(x = proportion)) +\n  geom_histogram(bins = bins+1) +\n  geom_vline(xintercept = .514, color = \"firebrick\", linetype = 5, linewidth = 2)\n\nsim_results |&gt; \n  mutate(greater = proportion &gt; .514) |&gt; \n  summarise(prop = sum(greater)/n())",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#problem-little-league-on-base-percentage",
    "href": "MA206-AY26-1/lesson-11.html#problem-little-league-on-base-percentage",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "In his All-Star run, Cal reached base \\(18\\) times in \\(35\\) plate appearances, for an on-base percentage of\n\\[\n\\hat p = \\frac{18}{35} \\approx 0.514.\n\\]\nSuppose the average kid in the league gets on base \\(40\\%\\) of the time.\nQuestion: Is there evidence that this player is better at getting on base than other kids, or could this just be due to random chance?\n\n\n\n\nNull hypothesis: \\(H_0 : p = 0.40\\)\n\nAlternative hypothesis: \\(H_A : p &gt; 0.40\\)\n\n\n\n\n\n\n\\(n = 35\\) plate appearances\n\n\\(x = 18\\) successes (on base)\n\n\\(\\hat p = \\tfrac{18}{35} \\approx 0.514\\)\n\n\n\n\n\n\\[\nSE = \\sqrt{\\frac{p_0(1-p_0)}{n}}\n= \\sqrt{\\frac{0.40(0.60)}{35}}\n\\approx 0.083\n\\]\n\n\n\n\n\\[\nz = \\frac{\\hat p - p_0}{SE}\n= \\frac{0.514 - 0.40}{0.083}\n\\approx 1.37\n\\]\n\n\n\n\nFor a one-tailed test:\n\\[\np = P(Z \\geq 1.37) \\approx 0.085\n\\]\n(using the standard Normal distribution).\n\n\n\n\nAt \\(\\alpha = 0.05\\), the \\(p\\)-value is larger than the cutoff.\n- We do not have strong enough evidence to conclude this player is truly better than average.\n- The result is suggestive (since \\(p \\approx 0.085\\)), but with only 35 plate appearances the evidence is not statistically significant.\n- A larger sample would give a clearer answer."
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#lets-formalize-this-a-little-bit",
    "href": "MA206-AY26-1/lesson-11.html#lets-formalize-this-a-little-bit",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "Null hypothesis: \\(H_0 : \\pi = 0.40\\)\n\nAlternative hypothesis: \\(H_A : \\pi &gt; 0.40\\)\n\n\n\n\n\n\\(n = 35\\) plate appearances\n\n\\(x = 18\\) successes (on base)\n\n\\(\\hat \\pi = \\tfrac{18}{35} \\approx 0.514\\)\n\n\n\n\n\\[\nSE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\n= \\sqrt{\\frac{0.40(0.60)}{35}}\n\\approx 0.083\n\\]\n\n\n\n\\[\nz = \\frac{\\hat \\pi - p_0}{SE}\n= \\frac{0.514 - 0.40}{0.083}\n\\approx 1.37\n\\]\n\n\n\nFor a one-tailed test:\n\\[\np = P(Z \\geq 1.37) \\approx 0.085\n\\]\n\n1 - pnorm(q = 1.37, mean = 0, sd = 1)\n\n[1] 0.08534345\n\n\n\n\n\nAt \\(\\alpha = 0.05\\), the \\(p\\)-value is larger than the cutoff.\n- We do not have strong enough evidence to conclude this player is truly better than average.\n- The result is suggestive (since \\(p \\approx 0.085\\)), but with only 35 plate appearances the evidence is not statistically significant.\n- A larger sample would give a clearer answer.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#milestone-3-lets-talk",
    "href": "MA206-AY26-1/lesson-11.html#milestone-3-lets-talk",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "📄 General Instructions\n\n\n\nAcademic Articles Worksheet\n\n📑 Use: Academic Articles Worksheet\n\n✅ Worth: 10 points\n\n⏰ Due: 0700 Friday, 19 Sept 2025\n\n🔗 Submit on Canvas: https://westpoint.instructure.com/courses/10295/assignments/223751\n\nIntroduction & Data Section\n\n📑 Use: Math Writing Template\n\n✅ Worth: 20 points\n\n⏰ Due: 0700 Sunday, 21 Sept 2025\n\n🔗 Submit on Canvas: https://westpoint.instructure.com/courses/10295/assignments/223738\n\n\n\n\n\n\n\n\nNote\n\n\n\nReminder: Also add both items to your binder with an updated Annex B (not graded yet).\n\n\n\n\n\n\n⏰ Due 0700 ET on Lesson 13\n\nDay 1: Wednesday, 24 Sept 2025\n\nDay 2: Thursday, 25 Sept 2025\n\n\n📑 Worksheet: https://westpoint.instructure.com/courses/10295/assignments/216497 — don’t sleep on this!",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#one-proportion-z-test",
    "href": "MA206-AY26-1/lesson-11.html#one-proportion-z-test",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\nDuring my son and his team’s Little League World Series run, Cal got on base \\(18\\) out of \\(35\\) times. Across all of Texas East Little Leagues, the average on-base percentage was about \\(40\\%\\).\nQuestion: Is Cal really better than average, or is this just by chance?\n\n\nIf the average player gets on base at a \\(40\\%\\) rate, what is the probability that we would observe someone get on base \\(18/35 = 0.514\\) or higher?\nIn other words: if Cal were truly a 40% hitter, how often would we see a season this good (or better) just by chance?\n\n\n\nLet’s pretend we could replay Cal’s season many times under the assumption he is a \\(40\\%\\) hitter.\n\nlibrary(tidyverse)\n\n# simulate one season\none_season &lt;- rbinom(n = 35, size = 1, prob = 0.4)\none_season\n\n [1] 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0\n\n\n\n# total times on base\non_base_at_bats &lt;- sum(one_season)\non_base_at_bats\n\n[1] 16\n\n# as a proportion\non_base_at_bats / length(one_season)\n\n[1] 0.4571429\n\n\n\n\n\nNow let’s repeat this process \\(10{,}000\\) times.\n\nn &lt;- 10000\n\nsim_results &lt;- tibble(\n  trial = 1:n,\n  rocks = rbinom(n, size = 35, prob = 0.4)\n) |&gt; \n  mutate(proportion = rocks / 35)\n\nhead(sim_results)\n\n# A tibble: 6 × 3\n  trial rocks proportion\n  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1     1    18      0.514\n2     2     5      0.143\n3     3    10      0.286\n4     4    12      0.343\n5     5    15      0.429\n6     6     8      0.229\n\n\n\n\n\nHere’s the distribution of on-base proportions from the simulations.\n\non_base_hist &lt;- sim_results |&gt;\n  ggplot(aes(x = proportion)) +\n  geom_histogram(binwidth = 1/35, boundary = 0, fill = \"skyblue\", color = \"white\") \n\non_base_hist\n\n\n\n\n\n\n\n\n\n\n\nNow let’s add Cal’s observed rate (\\(0.514\\)):\n\non_base_hist +\n  geom_vline(xintercept = 0.514, color = \"firebrick\", linetype = 5, linewidth = 2)\n\n\n\n\n\n\n\n\n\n\n\nFinally, what proportion of simulated seasons were at least this extreme?\n\nsim_results |&gt; \n  summarise(prob_more_extreme = mean(proportion &gt;= 0.514))\n\n# A tibble: 1 × 1\n  prob_more_extreme\n              &lt;dbl&gt;\n1             0.114\n\n\n\n\n\nIn our simulation, only about X% of seasons produced an on-base percentage this high or higher if Cal were truly a 40% hitter.\n➡️ This suggests his observed \\(0.514\\) season is possibly to be due to chance alone — the evidence is not convincing that he might might be better than average.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#lets-formalize-this-with-tintles-6-steps",
    "href": "MA206-AY26-1/lesson-11.html#lets-formalize-this-with-tintles-6-steps",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "Is Cal’s on-base percentage higher than the Texas East Little League average of 40%?\n\n\n\nWe have observational data from Cal’s 35 plate appearances during the Little League World Series run.\n- \\(n = 35\\) plate appearances\n- \\(x = 18\\) times on base\n- Observed proportion: \\(\\hat \\pi = \\tfrac{18}{35} \\approx 0.514\\)\nWe treat these 35 at-bats as a random sample from his true underlying ability.\n\n\n\nThe observed proportion of \\(0.514\\) is above the reference average of \\(0.40\\). This is about 11 percentage points higher. The key question is whether this difference is large enough to be unlikely by chance.\n\n\n\nWe set up hypotheses:\n\nNull hypothesis: \\(H_0 : \\pi = 0.40\\)\n\nAlternative hypothesis: \\(H_A : \\pi &gt; 0.40\\)\n\nCompute the standard error and test statistic:\n\\[\nSE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\n= \\sqrt{\\frac{0.40(0.60)}{35}}\n\\approx 0.083\n\\]\n\\[\nz = \\frac{\\hat \\pi - \\pi_0}{SE}\n= \\frac{0.514 - 0.40}{0.083}\n\\approx 1.37\n\\]\n\n\n\nThe one-tailed \\(p\\)-value is:\n\\[\np = P(Z \\geq 1.37) \\approx 0.085\n\\]\n\n1 - pnorm(q = 1.37, mean = 0, sd = 1)\n\n[1] 0.08534345\n\n\nAt \\(\\alpha = 0.05\\), this result is not statistically significant. We do not have strong enough evidence to conclude Cal’s on-base percentage is truly higher than 40%.\n\n\n\n\nThe observed rate of \\(0.514\\) is higher than average and suggestive (\\(p \\approx 0.085\\)).\n\nWith only 35 plate appearances, there is considerable variability — a larger sample would provide a clearer answer.\n\nContext matters: 35 at bats may not represent long-term ability. Future seasons with more data or more at-bats in a season could confirm or refute this pattern.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#lets-further-define-a-few-things",
    "href": "MA206-AY26-1/lesson-11.html#lets-further-define-a-few-things",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "The null hypothesis (\\(H_0\\)) is the starting assumption — usually that there is “no difference” or “no effect.”\n\nIn our example: \\(H_0 : \\pi = 0.40\\)\n\nThis means we assume Cal’s true on-base probability is the same as the Texas East Little League average of 40%.\n\n\n\n\nThe alternative hypothesis (\\(H_A\\)) is what we want to investigate. Depending on the research question, there are three common forms:\n\nRight-tailed (greater than)\n\n\\(H_A : \\pi &gt; 0.40\\)\n\nIn context: Is Cal’s true on-base percentage higher than 40%?\n\nThis is the version we are using, because the natural question is whether he’s better than average.\n\nLeft-tailed (less than)\n\n\\(H_A : \\pi &lt; 0.40\\)\n\nIn context: Is Cal’s true on-base percentage lower than 40%?\n\nYou would ask this if you suspected Cal might actually be worse than average at getting on base.\n\nTwo-tailed (not equal)\n\n\\(H_A : \\pi \\neq 0.40\\)\n\nIn context: Is Cal’s true on-base percentage different from 40% (either higher or lower)?\n\nYou would use this if you want to know whether Cal performs differently than average, without assuming in advance which direction.\n\n\n\n\n\n\nIf you are asking, “Is Cal better than average?”, the right-tailed test is appropriate.\n\nIf the concern were that Cal struggles at the plate, you’d use a left-tailed test.\n\nIf you only care whether Cal is different from average in either direction, the two-tailed test is the right choice.\n\n\n\n\n\n\\(\\pi\\) (the Greek letter pi) represents the population proportion of average of getting on base.\n\nIn context: \\(\\pi\\) is Cal’s true long-run probability of getting on base each plate appearance.\n\nWe never observe \\(\\pi\\) directly — we estimate it with \\(\\hat{\\pi}\\) (the sample proportion).\n\n\n\n\n\n\\(\\alpha\\) (alpha) is the threshold for evidence against the null hypothesis.\n\nCommon choices: \\(\\alpha = 0.05\\) (5%) or \\(\\alpha = 0.01\\) (1%).\n\nInterpretation: If \\(p \\leq \\alpha\\), the result is considered statistically significant — unlikely to occur just by chance if \\(H_0\\) were true.\n\n\n\n\nThe \\(z\\) distribution (also called the standard normal distribution) is a bell-shaped curve with:\n- Mean = \\(0\\)\n- Standard deviation = \\(1\\)\nSo how do we get from our sample proportion \\(\\hat{\\pi}\\) to this special distribution?\n\n\nUnder the null hypothesis \\(H_0 : \\pi = 0.40\\), the sample proportion \\(\\hat{\\pi}\\) has a sampling distribution that is approximately normal (by the Central Limit Theorem) with:\n\nMean = \\(\\pi_0 = 0.40\\)\n\nStandard deviation = \\(SE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\)\n\nThis tells us what values of \\(\\hat{\\pi}\\) we would expect just by chance if the null were true.\n\n\n\nTo compare our observed \\(\\hat{\\pi}\\) to this distribution, we standardize it:\n\nSubtract the mean under \\(H_0\\)\n\nThis centers the distribution at \\(0\\) by measuring how far away our observed statistic is from the null.\n\n\\((\\hat{\\pi} - \\pi_0)\\)\n\nDivide by the standard deviation\n\nThis rescales differences into standard deviation units, so we can judge how unusual they are.\n\n\\(\\dfrac{\\hat{\\pi} - \\pi_0}{SE}\\)\n\n\n\n\n\nAfter centering and rescaling, the new standardized statistic follows (approximately) the standard normal distribution, \\(N(0,1)\\).\nThat’s why we call it the \\(z\\) statistic:\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n\\]\nNow we can use the \\(z\\) distribution to calculate probabilities (like \\(p\\)-values) for how extreme our observed result is relative to the null hypothesis.\n\nggplot() +\n  geom_function(fun = dnorm, xlim = c(-4,4))",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#classroom-example-rockpaperscissors",
    "href": "MA206-AY26-1/lesson-11.html#classroom-example-rockpaperscissors",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "Class Activity: With your eyes closed, on the count of three, everyone will make their first throw of Rock–Paper–Scissors.\n- 👊 = Rock\n- ✋ = Paper\n- ✌️ = Scissors\nHold your choice steady so we can tally the results. Closing your eyes helps reduce herding or copying from neighbors.\n\n\nDo students tend to choose Rock more or less often than random chance (\\(33\\%\\)) on their first throw?\n\n\n\nWith eyes closed, on the count of 3 each student chooses one option (Rock/Paper/Scissors) as if starting a game. We then tally the class counts:\n- \\(R\\) = number of Rock\n- \\(P\\) = number of Paper\n- \\(S\\) = number of Scissors\n- \\(n = R+P+S\\)\n\n# Enter the tallies you just collected:\nR &lt;- 2   # Rock count\nP &lt;- 10   # Paper count\nS &lt;- 7   # Scissors count\n\nn &lt;- R + P + S\nc(R = R, P = P, S = S, n = n)\n\n R  P  S  n \n 2 10  7 19 \n\n\n\n\n\nCompute the sample proportion choosing Rock and make a quick bar chart. Under complete randomness we’d expect each to be near \\(n/3\\).\n\npihat &lt;- R / n\npihat\n\n[1] 0.1052632\n\ndf &lt;- tibble(option = c(\"Rock\",\"Paper\",\"Scissors\"),\n             count  = c(R, P, S))\n\nggplot(df, aes(option, count)) +\n  geom_col() +\n  geom_hline(yintercept = n/3, linetype = 2) +\n  labs(title = \"Class First-Throw Choices\",\n       subtitle = \"Dashed line = expected count if choices were uniform (n/3)\",\n       x = NULL, y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\nWe’ll do two things:\n\nSimulate many samples of size \\(n\\) assuming \\(p_{Rock}=1/3\\), and estimate a two-sided \\(p\\)-value by comparing simulated proportions to the observed.\n\nDo the math version using the \\(z\\)-distribution.\n\n\n\n\npi0 &lt;- 1/3\nN  &lt;- 10000               # number of simulations\nR_obs &lt;- R                # keep observed Rock consistent with Step 2\npihat_obs &lt;- R_obs / n\n\n# Simulate many samples under H0\nsim_results &lt;- tibble(\n  trial = 1:N,\n  rocks = rbinom(N, size = n, prob = pi0)   # Rock counts in each simulated sample\n) |&gt;\n  mutate(pihat = rocks / n)                  # simulated sample proportions\n\nsim_results\n\n# A tibble: 10,000 × 3\n   trial rocks pihat\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     1     5 0.263\n 2     2     8 0.421\n 3     3     9 0.474\n 4     4     4 0.211\n 5     5     4 0.211\n 6     6     4 0.211\n 7     7     2 0.105\n 8     8     5 0.263\n 9     9     6 0.316\n10    10     4 0.211\n# ℹ 9,990 more rows\n\n\n\n# Plot the simulated sampling distribution with observed and null marked\nsim_results |&gt;\n  ggplot(aes(x = pihat)) +\n  geom_histogram(binwidth = 1/n, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = pihat_obs, color = \"firebrick\", linetype = 2, linewidth = 1.2) +\n  geom_vline(xintercept = pi0, color = \"gray40\", linewidth = 1.2) +\n  labs(title = paste0(\"Sampling Distribution of p̂i under H0 (n = \", n, \")\"),\n       subtitle = paste0(\"Observed p̂ = \", round(pihat_obs,3), \n                         \" | Null pi0 = \", round(pi0,3)),\n       x = \"p̂ (proportion Rock)\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Two-sided simulation p-value\nobs_abs_diff &lt;- abs(pihat_obs - pi0)\n\nsim_p_two_sided &lt;- sim_results |&gt; \n  mutate(pihat_different_than_null = pihat - pi0) |&gt;          # difference from null\n  mutate(abs_of_difference = abs(pihat_different_than_null)) |&gt; \n  mutate(is_extreme = abs_of_difference &gt;= obs_abs_diff) |&gt; # flag extremes\n  summarise(p_val = mean(is_extreme))                       # proportion of extremes = p-value\n\nsim_p_two_sided\n\n# A tibble: 1 × 1\n   p_val\n   &lt;dbl&gt;\n1 0.0503\n\n\n\n\n\n\n\\(H_0: p_{Rock} = 1/3\\)\n\n\\(H_A: p_{Rock} \\neq 1/3\\)\n\n\\(SE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\)\n\n# Standard Error under H0\nSE &lt;- sqrt(pi0 * (1 - pi0) / n)\nSE\n\n[1] 0.1081476\n\n\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n\\]\n\n# Test Statistic (z)\nz_stat &lt;- (pihat_obs - pi0) / SE\nz_stat\n\n[1] -2.108878\n\n\n\\[\np = 2 \\times P(Z \\geq |z|)\n\\]\n\n# Two-Sided p-value\np_val_two_sided &lt;- 2 * (1 - pnorm(abs(z_stat)))\np_val_two_sided\n\n[1] 0.03495507\n\n\n\n\n\n\nNow we bring the two approaches together:\n\nSimulation gave us an empirical \\(p\\)-value by resampling under \\(H_0\\).\n\nMath/analytic (\\(z\\)-test) gave us an approximate \\(p\\)-value using the standard normal.\n\n\n\n              n           R_obs       pihat_obs             pi0          z_stat \n        19.0000          2.0000          0.1050          0.3330         -2.1090 \nsim_p_two_sided   z_p_two_sided           alpha \n         0.0503          0.0350          0.0500 \n\n\nSimulation-based decision: Fail to reject H0 \n\n\nZ-approximation decision: Reject H0 (evidence of ≠ 1/3) \n\n\nInterpretation:\nWith \\(n\\) throws and observed \\(\\hat p\\), the simulation \\(p\\)-value tells us how unusual the result is if \\(p=1/3\\) were true. The \\(z\\) test gives a similar answer using a theoretical normal curve. At \\(\\alpha = 0.05\\), compare both to decide whether to reject \\(H_0\\).\n\n\n\n\n\nCode\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tibble)\n\nui &lt;- fluidPage(\n  titlePanel(\"One-Proportion Test: z-formulas + Live Simulation\"),\n  withMathJax(),\n  tags$hr(),\n  \n  fluidRow(\n    column(\n      width = 4,\n      h4(\"Inputs\"),\n      numericInput(\"x\", \"Observed successes (x)\", value = 2, min = 0, step = 1),\n      numericInput(\"n\", \"Sample size (n)\", value = 19, min = 1, step = 1),\n      sliderInput(\"pi0\", HTML(\"&pi;&lt;sub&gt;0&lt;/sub&gt; (null proportion)\"), min = 0, max = 1,\n                  value = 1/3, step = 0.01),\n      numericInput(\"B\", \"Simulations (B)\", value = 10000, min = 100, step = 100),\n      numericInput(\"seed\", \"Random seed\", value = 26, min = 1, step = 1),\n      helpText(\"Tip: This defaults to the R/P/S example: x = 2 (Rock), n = 19, π0 = 1/3.\")\n    ),\n    column(\n      width = 8,\n      h4(\"Formulas\"),\n      # Show the formulas via MathJax\n      div(style = \"font-size: 1.15em; margin-bottom: 8px;\",\n          \"$$ SE = \\\\sqrt{\\\\frac{\\\\pi_0(1 - \\\\pi_0)}{n}}, \\\\qquad z = \\\\frac{\\\\hat{p} - \\\\pi_0}{SE} $$\"\n      ),\n      h4(\"Computed Values\"),\n      tableOutput(\"value_table\"),\n      tags$br(),\n      h4(\"Sampling Distribution under H0 (Simulated)\"),\n      plotOutput(\"hist_plot\", height = \"330px\"),\n      helpText(\"Histogram shows simulated \\\\(\\\\hat p\\\\) under H0. Red line = observed \\\\(\\\\hat p\\\\). Gray line = \\\\(\\\\pi_0\\\\).\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  # Basic reactives\n  phat &lt;- reactive({\n    req(input$n &gt; 0)\n    input$x / input$n\n  })\n  \n  SE &lt;- reactive({\n    sqrt(input$pi0 * (1 - input$pi0) / input$n)\n  })\n  \n  z_stat &lt;- reactive({\n    (phat() - input$pi0) / SE()\n  })\n  \n  # p-values for three alternatives\n  p_right &lt;- reactive({ 1 - pnorm(z_stat()) })                 # H_A: p &gt; pi0\n  p_left  &lt;- reactive({ pnorm(z_stat()) })                     # H_A: p &lt; pi0\n  p_two   &lt;- reactive({ 2 * (1 - pnorm(abs(z_stat()))) })      # H_A: p != pi0\n  \n  # Simulation under H0\n  sim_df &lt;- reactive({\n    req(input$B &gt;= 100)\n    set.seed(input$seed)\n    rocks &lt;- rbinom(input$B, size = input$n, prob = input$pi0)\n    tibble(\n      phat = rocks / input$n\n    )\n  })\n  \n  # Output: table of computed values\n  output$value_table &lt;- renderTable({\n    tibble::tibble(\n      `x (successes)` = input$x,\n      `n (trials)`    = input$n,\n      `π0 (null)`     = round(input$pi0, 4),\n      `p̂ = x/n`      = round(phat(), 4),\n      `SE`            = round(SE(), 5),\n      `z`             = round(z_stat(), 4),\n      `p (right)`     = signif(p_right(), 4),\n      `p (left)`      = signif(p_left(), 4),\n      `p (two-sided)` = signif(p_two(), 4)\n    )\n  }, striped = TRUE, bordered = TRUE, spacing = \"s\", digits = 6)\n  \n  # Output: histogram with vertical lines at phat and pi0\n  output$hist_plot &lt;- renderPlot({\n    df &lt;- sim_df()\n    ggplot(df, aes(x = phat)) +\n      geom_histogram(binwidth = 1 / input$n, color = \"white\") +\n      geom_vline(xintercept = phat(), color = \"firebrick\", linetype = 2, linewidth = 1.2) +\n      geom_vline(xintercept = input$pi0, color = \"gray40\", linewidth = 1.2) +\n      labs(\n        x = expression(hat(p) ~ \"(proportion)\"),\n        y = \"Count\",\n        title = paste0(\"Simulated Sampling Distribution of \", expression(hat(p)), \" under H0\"),\n        subtitle = paste0(\"n = \", input$n, \", π0 = \", round(input$pi0, 3),\n                          \", observed p̂ = \", round(phat(), 3),\n                          \"; B = \", input$B)\n      ) +\n      theme_minimal(base_size = 12) +\n      xlim(c(0,1))\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#reference-table-z-tests-for-one-proportion",
    "href": "MA206-AY26-1/lesson-11.html#reference-table-z-tests-for-one-proportion",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "For all cases:\n\\[\nSE = \\sqrt{\\frac{\\pi_0(1 - \\pi_0)}{n}}, \\quad\nz = \\frac{\\hat{p} - \\pi_0}{SE}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#board-problem",
    "href": "MA206-AY26-1/lesson-11.html#board-problem",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "A hospital claims that 85% of discharge summaries are finalized within 24 hours.\nIn an audit of 60 summaries, 46 were finalized within 24 hours.\nResearch Question: Is the true proportion finalized within 24 hours less than 85%?\n\nState the hypotheses.\n\nExplain (in words) how you would simulate this test (do not actually simulate).\n\nThen, perform the mathematical one-proportion \\(z\\) test.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nDo fewer than 85% of discharge summaries get finalized within 24 hours?\n\n\n\n\n\\(H_0 : \\pi = 0.85\\)\n\n\\(H_A : \\pi &lt; 0.85\\)\n\n\n\n\n\n\\(n = 60\\) summaries\n\n\\(x = 46\\) finalized within 24h\n\n\\(\\hat{\\pi} = \\tfrac{x}{n} = \\tfrac{46}{60} = 0.767\\)\n\n\n\n\n\n\n\n\nUnder \\(H_0\\), simulate many samples of size \\(n=60\\) with \\(\\pi_0=0.85\\).\n\nFor each, compute \\(\\hat{\\pi}_{sim}\\).\n\nEstimate the left-tailed \\(p\\)-value as the fraction of simulations with \\(\\hat{\\pi}_{sim} \\le \\hat{\\pi}_{obs} = 0.767\\).\n\n\n\n\nStandard error under \\(H_0\\):\n\\[\nSE = \\sqrt{\\frac{\\pi_0 (1-\\pi_0)}{n}}\n   = \\sqrt{\\frac{0.85 \\cdot 0.15}{60}}\n   \\approx 0.046\n\\]\nTest statistic:\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n  = \\frac{0.767 - 0.85}{0.046}\n  \\approx -1.80\n\\]\n\\(p\\)-value (left-tailed):\n\\[\np = \\Phi(z) = \\Phi(-1.80) \\approx 0.036\n\\]\n\npnorm(-1.8)\n\n[1] 0.03593032\n\n\n\n\n\nAt \\(\\alpha = 0.05\\), since \\(p \\approx 0.036 &lt; 0.05\\), we reject \\(H_0\\).\n\n\n\nThere is statistical evidence that fewer than 85% of discharge summaries are completed within 24 hours.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html",
    "href": "MA206-AY26-1/lesson-12.html",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "⏰ Due 0700 ET on Lesson 13\n\nDay 1: Wednesday, 24 Sept 2025\n\nDay 2: Thursday, 25 Sept 2025\n\n\n📑 Worksheet: https://westpoint.instructure.com/courses/10295/assignments/216497 — don’t sleep on this!\nWill need applet for simulation\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12/120\n10:37\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\nFor all cases:\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.\n\n\n\n\nWe want to test whether Male cadets (Don’t worry ladies, we’ll get to you) are shorter than the average height of U.S. men aged 19–24, which is 72 inches.\n\n\n\\[\nH_0 : \\mu = 72\n\\]\n\\[\nH_A : \\mu &lt; 72\n\\]\n\n\n\n\nheights &lt;- c(70, 71, 69, 73, 68, 74, 71, 70, 72, 69, 70, 71, 68, 73)\n\n\n\n\n\n\n\nIs This a Good Sample?\n\n\n\nWe measured the heights of 14 cadets in this class.\nBut our research question is about all U.S. men aged 19–24.\n\nDo cadets in this class represent the broader population (what is that population)?\n\nWhat kinds of biases might be present?\n\nIf the sample isn’t random, how should that affect our conclusions?\n\n\n\n\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\qquad df = n - 1\n\\]\n\\[\n\\begin{aligned}\n\\bar{x} & = \\text{sample mean} \\\\[4pt]\n\\mu_0   & = \\text{hypothesized mean (72)} \\\\[4pt]\ns       & = \\text{sample standard deviation} \\\\[4pt]\nn       & = \\text{sample size} \\\\[4pt]\nt       & = \\text{test statistic}\n\\end{aligned}\n\\]\n\n\n\n\nn     &lt;- length(heights)\nxbar  &lt;- mean(heights)\ns     &lt;- sd(heights)\n\nc(n = n, mean = round(xbar, 2), sd = round(s, 2))\n\n    n  mean    sd \n14.00 70.64  1.86 \n\n\n\n\n\n\nmu0 &lt;- 72\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\n\n[1] -2.722848\n\n\n\n\n\n\ndf &lt;- n - 1\n\nggplot() +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  labs(title = \"t-Distribution with df = 13\")\n\n\n\n\n\n\n\n\nAdd the observed test statistic:\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, t_stat),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, color = \"firebrick\", linewidth = 1.2) +\n  labs(title = \"Left-tail Shaded: t-Distribution with Test Statistic\")\n\n\n\n\n\n\n\n\n\n\n\n\np_val &lt;- pt(t_stat, df = df)   \np_val\n\n[1] 0.008708986\n\n\n\n\n\nWith p ≈ .008, we do have evidence that male cadets are shorter than 72 inches.\n\n\n\n\nWe want to test whether female cadets are taller than the average height of U.S. women aged 19–24, which is 64 inches.\n\n\n\\[\nH_0 : \\mu = 64\n\\]\n\\[\nH_A : \\mu &gt; 64\n\\]\n\n\n\n\nheights &lt;- c(65, 67, 66, 64)\n\n\n\n\n\n\n\nIs This a Good Sample?\n\n\n\nWe measured the heights of only 4 female cadets in this class.\nBut our research question is about all U.S. women aged 19–24.\n\nDo these 4 cadets represent the broader population?\n\nWhat kinds of biases might be present?\n\nIf the sample isn’t random (or too small), how should that affect our conclusions?\n\n\n\n\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\qquad df = n - 1\n\\]\n\n\n\n\nn     &lt;- length(heights)\nxbar  &lt;- mean(heights)\ns     &lt;- sd(heights)\n\nc(n = n, mean = round(xbar, 2), sd = round(s, 2))\n\n    n  mean    sd \n 4.00 65.50  1.29 \n\n\n\n\n\n\nmu0 &lt;- 64\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\n\n[1] 2.32379\n\n\n\n\n\n\ndf &lt;- n - 1\n\nggplot() +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  labs(title = \"t-Distribution with df = 3\")\n\n\n\n\n\n\n\n\nAdd the observed test statistic and shade the right tail:\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(t_stat, 4),\n                geom = \"area\", fill = \"darkred\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, color = \"firebrick\", linewidth = 1.2) +\n  labs(title = \"Right-tail Shaded: t-Distribution with Test Statistic\")\n\n\n\n\n\n\n\n\n\n\n\n\np_val &lt;- 1 - pt(t_stat, df = df)   # one-tailed, greater than\np_val\n\n[1] 0.05136404\n\n\n\n\n\nWith p ≈ .051, we do not have strong evidence that female cadets are taller than 64 inches.\n\n\n\n\nSuppose the national average height of men aged 19–24 is 72 inches.\nWe want to test whether West Point Department of Math instructors are different (either taller or shorter).\n\n\n\\[\nH_0 : \\mu = 72\n\\]\n\\[\nH_A : \\mu \\neq 72\n\\]\n\n\n\nHere is a sample of heights (in inches) from 28 Math instructors:\n\nheights &lt;- c(71, 70, 73, 72, 74, 69, 71, 72, 70, 73, 72, 71, 75, 70, 72, 74, 71, 69, 73, 72, 70, 71, 74, 72, 70, 73, 71, 72)\n\n\n\n\n\n\n\nIs This a Good Sample?\n\n\n\nWe only used 28 instructors from the department.\nOur research question is about all West Point Math instructors compared to the national population.\n\nDo these 28 represent the full department fairly?\n\nWhat biases could be present?\n\nIf the sample is not random, how should that affect our conclusions?\n\n\n\n\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\qquad df = n - 1\n\\]\n\n\n\n\nn     &lt;- length(heights)\nxbar  &lt;- mean(heights)\ns     &lt;- sd(heights)\n\nc(n = n, mean = round(xbar, 2), sd = round(s, 2))\n\n    n  mean    sd \n28.00 71.68  1.56 \n\n\n\n\n\n\nmu0 &lt;- 72\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\n\n[1] -1.086979\n\n\n\n\n\n\ndf &lt;- n - 1\n\nggplot() +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  labs(title = paste(\"t-Distribution with df =\", df))\n\n\n\n\n\n\n\n\nAdd the observed test statistic and shade both tails:\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, -abs(t_stat)),\n                geom = \"area\", fill = \"steelblue\", alpha = 0.5) +\n  stat_function(fun = dt, args = list(df = df), xlim = c(abs(t_stat), 4),\n                geom = \"area\", fill = \"steelblue\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, color = \"firebrick\", linewidth = 1.2) +\n  geom_vline(xintercept = -t_stat, color = \"firebrick\", linewidth = 1.2) +\n  labs(title = \"Two-tailed Shaded: t-Distribution with Test Statistic\")\n\n\n\n\n\n\n\n\n\n\n\n\np_val &lt;- 2 * (1 - pt(abs(t_stat), df = df))   # two-tailed\np_val\n\n[1] 0.286655\n\n\n\n\n\nWith p ≈ .29, we do not have evidence that Math instructors’ heights differ from 72 inches.\n\n\n\n\nOverlay several \\(t\\) distributions with different degrees of freedom and the standard normal \\(N(0,1)\\) for comparison.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNationally, the average college student drinks about \\(3.2\\) cups of coffee per day.\nYou suspect cadets might consume a different amount (not necessarily more or less).\nYou collect a sample of \\(12\\) cadets with the following self-reported daily coffee consumption (in cups):\n\ncoffee &lt;- c(2.5, 3.0, 3.8, 4.1, 3.2, 2.9, 3.6, 4.0, 3.3, 2.7, 3.5, 3.9)\n\nTasks:\n\nState the null and alternative hypotheses.\n\nCompute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n\nWrite down the test statistic formula for a one-sample \\(t\\)-test.\n\nCalculate the test statistic.\n\nFind the \\(p\\)-value for the appropriate two-tailed test.\n\nState your conclusion in the context of the problem.\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses\n\\[\nH_0: \\mu = 3.2 \\qquad\\text{vs}\\qquad H_A: \\mu \\neq 3.2\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(coffee)\nxbar &lt;- mean(coffee)\ns    &lt;- sd(coffee)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n12.000  3.375  0.528 \n\n\nNumerically: \\(n = 12\\), \\(\\bar{x} \\approx 3.375\\), \\(s \\approx 0.528\\).\n3–4. Test statistic\n\nmu0   &lt;- 3.2\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n 1.149 11.000 \n\n\nNumerically: \\(t \\approx 1.149\\) with \\(df = 11\\).\n5. Two-tailed \\(p\\)-value\n\np_val &lt;- 2 * (1 - pt(abs(t_stat), df = df))\np_val\n\n[1] 0.2749615\n\n\nNumerically: \\(p \\approx 0.275\\).\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  # Left tail shading\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, -abs(t_stat)),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Right tail shading\n  stat_function(fun = dt, args = list(df = df), xlim = c(abs(t_stat), 4),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Overlay t density\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  # Vertical lines at ±t_stat\n  geom_vline(xintercept = c(-abs(t_stat), abs(t_stat)),\n             color = \"firebrick\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(title = \"Two-tailed Shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")\n\n\n\n\n\n\n\n\n6. Conclusion\nWith \\(p \\approx 0.275\\), we do not have evidence that cadets’ average coffee consumption differs from \\(3.2\\) cups per day.\n\n\n\n\n\n\n\n\nA recent campus wellness report suggests the average college student spends \\(3.0\\) hours per day on recreational screen time (not including coursework).\nYou suspect students in your section spend more than that.\nYou collect a sample of \\(12\\) students with the following daily screen-time values (in hours):\n\nscreen_time &lt;- c(2.5, 3.0, 3.1, 3.2, 3.3, 3.4, 3.8, 3.7, 3.8, 3.2, 3.5, 3.5)\n\nTasks: 1. State the null and alternative hypotheses.\n2. Compute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n3. Write down the test statistic formula for a one-sample \\(t\\)-test.\n4. Calculate the test statistic.\n5. Find the one-tailed \\(p\\)-value for the “greater than” test.\n6. State your conclusion in context.\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses \\[\nH_0: \\mu = 3.0\n\\qquad\\text{vs}\\qquad\nH_A: \\mu &gt; 3.0\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(screen_time)\nxbar &lt;- mean(screen_time)\ns    &lt;- sd(screen_time)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n12.000  3.333  0.373 \n\n\n3–4. Test statistic\n\nmu0    &lt;- 3.0\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n   3.1   11.0 \n\n\n5. One-tailed \\(p\\)-value (\\(H_A: \\mu &gt; \\mu_0\\))\n\np_val &lt;- 1 - pt(t_stat, df = df)\np_val\n\n[1] 0.005056453\n\n\n6. Conclusion\nWith the computed \\(t\\) and \\(p\\) above, interpret whether there is evidence that average daily recreational screen time in this section exceeds \\(3.0\\) hours.\n\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(t_stat, 4),\n                geom = \"area\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, linewidth = 1.2) +\n  labs(title = \"Right-tail shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\nPublic health guidelines recommend \\(7.0\\) hours of sleep on weeknights.\nYou suspect students in your section average less than that.\nYou collect a sample of \\(15\\) students’ self-reported weeknight sleep (in hours):\n\nsleep &lt;- c(6.6, 6.9, 7.1, 6.8, 7.0, 6.7, 6.5, 6.8, 6.9, 6.4, 6.6, 7.2, 6.7, 6.8, 6.5)\n\nTasks: 1. State the null and alternative hypotheses.\n2. Compute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n3. Write down the test statistic formula for a one-sample \\(t\\)-test.\n4. Calculate the test statistic.\n5. Find the one-tailed \\(p\\)-value for the “less than” test.\n6. State your conclusion in context.\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses \\[\nH_0: \\mu = 7.0\n\\qquad\\text{vs}\\qquad\nH_A: \\mu &lt; 7.0\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(sleep)\nxbar &lt;- mean(sleep)\ns    &lt;- sd(sleep)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n15.000  6.767  0.229 \n\n\n3–4. Test statistic\n\nmu0    &lt;- 7.0\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n-3.949 14.000 \n\n\n5. One-tailed \\(p\\)-value (\\(H_A: \\mu &lt; \\mu_0\\))\n\np_val &lt;- pt(t_stat, df = df)\np_val\n\n[1] 0.0007279758\n\n\n6. Conclusion\nReport the computed \\(t\\), \\(df\\), and \\(p\\), then conclude whether there is evidence that average weeknight sleep is less than \\(7.0\\) hours.\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  # Left tail shading up to t_stat\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, t_stat),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Overlay t density\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  # Vertical line at t_stat\n  geom_vline(xintercept = t_stat,\n             color = \"firebrick\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(title = \"Left-tail Shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nProject Milestone 3: Due Canvas 22 Sept\nExploration Exercise 1.5: Due at 0700 on Lesson 13\n24 September 2025 for Day 1\n\n25 September 2025 for Day 2)\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#milestone-3-lets-talk",
    "href": "MA206-AY26-1/lesson-12.html#milestone-3-lets-talk",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "📄 General Instructions\n\n\n\nAcademic Articles Worksheet\n\n📑 Use: Academic Articles Worksheet\n\n✅ Worth: 10 points\n\n⏰ Due: 0700 Friday, 19 Sept 2025\n\n🔗 Submit on Canvas: https://westpoint.instructure.com/courses/10295/assignments/223751\n\nIntroduction & Data Section\n\n📑 Use: Math Writing Template\n\n✅ Worth: 20 points\n\n⏰ Due: 0700 Sunday, 21 Sept 2025\n\n🔗 Submit on Canvas: https://westpoint.instructure.com/courses/10295/assignments/223738\n\n\n\n\n\n\n\n\nNote\n\n\n\nReminder: Also add both items to your binder with an updated Annex B (not graded yet).\n\n\n\n\n\n\n⏰ Due 0700 ET on Lesson 13\n\nDay 1: Wednesday, 24 Sept 2025\n\nDay 2: Thursday, 25 Sept 2025\n\n\n📑 Worksheet: https://westpoint.instructure.com/courses/10295/assignments/216497 — don’t sleep on this!"
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#one-proportion-z-test",
    "href": "MA206-AY26-1/lesson-12.html#one-proportion-z-test",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\nDuring my son and his team’s Little League World Series run, Cal got on base \\(18\\) out of \\(35\\) times. Across all of Texas East Little Leagues, the average on-base percentage was about \\(40\\%\\).\nQuestion: Is Cal really better than average, or is this just by chance?\n\n\nIf the average player gets on base at a \\(40\\%\\) rate, what is the probability that we would observe someone get on base \\(18/35 = 0.514\\) or higher?\nIn other words: if Cal were truly a 40% hitter, how often would we see a season this good (or better) just by chance?\n\n\n\nLet’s pretend we could replay Cal’s season many times under the assumption he is a \\(40\\%\\) hitter.\n\nlibrary(tidyverse)\n\n# simulate one season\none_season &lt;- rbinom(n = 35, size = 1, prob = 0.4)\none_season\n\n [1] 0 0 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1\n\n\n\n# total times on base\non_base_at_bats &lt;- sum(one_season)\non_base_at_bats\n\n[1] 15\n\n# as a proportion\non_base_at_bats / length(one_season)\n\n[1] 0.4285714\n\n\n\n\n\nNow let’s repeat this process \\(10{,}000\\) times.\n\nn &lt;- 10000\n\nsim_results &lt;- tibble(\n  trial = 1:n,\n  rocks = rbinom(n, size = 35, prob = 0.4)\n) |&gt; \n  mutate(proportion = rocks / 35)\n\nhead(sim_results)\n\n# A tibble: 6 × 3\n  trial rocks proportion\n  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1     1    14      0.4  \n2     2    14      0.4  \n3     3    12      0.343\n4     4    15      0.429\n5     5    11      0.314\n6     6    13      0.371\n\n\n\n\n\nHere’s the distribution of on-base proportions from the simulations.\n\non_base_hist &lt;- sim_results |&gt;\n  ggplot(aes(x = proportion)) +\n  geom_histogram(binwidth = 1/35, boundary = 0, fill = \"skyblue\", color = \"white\") \n\non_base_hist\n\n\n\n\n\n\n\n\n\n\n\nNow let’s add Cal’s observed rate (\\(0.514\\)):\n\non_base_hist +\n  geom_vline(xintercept = 0.514, color = \"firebrick\", linetype = 5, linewidth = 2)\n\n\n\n\n\n\n\n\n\n\n\nFinally, what proportion of simulated seasons were at least this extreme?\n\nsim_results |&gt; \n  summarise(prob_more_extreme = mean(proportion &gt;= 0.514))\n\n# A tibble: 1 × 1\n  prob_more_extreme\n              &lt;dbl&gt;\n1             0.115\n\n\n\n\n\nIn our simulation, only about X% of seasons produced an on-base percentage this high or higher if Cal were truly a 40% hitter.\n➡️ This suggests his observed \\(0.514\\) season is possibly to be due to chance alone — the evidence is not convincing that he might might be better than average."
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#lets-formalize-this-with-tintles-6-steps",
    "href": "MA206-AY26-1/lesson-12.html#lets-formalize-this-with-tintles-6-steps",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "Is Cal’s on-base percentage higher than the Texas East Little League average of 40%?\n\n\n\nWe have observational data from Cal’s 35 plate appearances during the Little League World Series run.\n- \\(n = 35\\) plate appearances\n- \\(x = 18\\) times on base\n- Observed proportion: \\(\\hat \\pi = \\tfrac{18}{35} \\approx 0.514\\)\nWe treat these 35 at-bats as a random sample from his true underlying ability.\n\n\n\nThe observed proportion of \\(0.514\\) is above the reference average of \\(0.40\\). This is about 11 percentage points higher. The key question is whether this difference is large enough to be unlikely by chance.\n\n\n\nWe set up hypotheses:\n\nNull hypothesis: \\(H_0 : \\pi = 0.40\\)\n\nAlternative hypothesis: \\(H_A : \\pi &gt; 0.40\\)\n\nCompute the standard error and test statistic:\n\\[\nSE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\n= \\sqrt{\\frac{0.40(0.60)}{35}}\n\\approx 0.083\n\\]\n\\[\nz = \\frac{\\hat \\pi - \\pi_0}{SE}\n= \\frac{0.514 - 0.40}{0.083}\n\\approx 1.37\n\\]\n\n\n\nThe one-tailed \\(p\\)-value is:\n\\[\np = P(Z \\geq 1.37) \\approx 0.085\n\\]\n\n1 - pnorm(q = 1.37, mean = 0, sd = 1)\n\n[1] 0.08534345\n\n\nAt \\(\\alpha = 0.05\\), this result is not statistically significant. We do not have strong enough evidence to conclude Cal’s on-base percentage is truly higher than 40%.\n\n\n\n\nThe observed rate of \\(0.514\\) is higher than average and suggestive (\\(p \\approx 0.085\\)).\n\nWith only 35 plate appearances, there is considerable variability — a larger sample would provide a clearer answer.\n\nContext matters: 35 at bats may not represent long-term ability. Future seasons with more data or more at-bats in a season could confirm or refute this pattern."
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#lets-further-define-a-few-things",
    "href": "MA206-AY26-1/lesson-12.html#lets-further-define-a-few-things",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "The null hypothesis (\\(H_0\\)) is the starting assumption — usually that there is “no difference” or “no effect.”\n\nIn our example: \\(H_0 : \\pi = 0.40\\)\n\nThis means we assume Cal’s true on-base probability is the same as the Texas East Little League average of 40%.\n\n\n\n\nThe alternative hypothesis (\\(H_A\\)) is what we want to investigate. Depending on the research question, there are three common forms:\n\nRight-tailed (greater than)\n\n\\(H_A : \\pi &gt; 0.40\\)\n\nIn context: Is Cal’s true on-base percentage higher than 40%?\n\nThis is the version we are using, because the natural question is whether he’s better than average.\n\nLeft-tailed (less than)\n\n\\(H_A : \\pi &lt; 0.40\\)\n\nIn context: Is Cal’s true on-base percentage lower than 40%?\n\nYou would ask this if you suspected Cal might actually be worse than average at getting on base.\n\nTwo-tailed (not equal)\n\n\\(H_A : \\pi \\neq 0.40\\)\n\nIn context: Is Cal’s true on-base percentage different from 40% (either higher or lower)?\n\nYou would use this if you want to know whether Cal performs differently than average, without assuming in advance which direction.\n\n\n\n\n\n\nIf you are asking, “Is Cal better than average?”, the right-tailed test is appropriate.\n\nIf the concern were that Cal struggles at the plate, you’d use a left-tailed test.\n\nIf you only care whether Cal is different from average in either direction, the two-tailed test is the right choice.\n\n\n\n\n\n\\(\\pi\\) (the Greek letter pi) represents the population proportion of average of getting on base.\n\nIn context: \\(\\pi\\) is Cal’s true long-run probability of getting on base each plate appearance.\n\nWe never observe \\(\\pi\\) directly — we estimate it with \\(\\hat{\\pi}\\) (the sample proportion).\n\n\n\n\n\n\\(\\alpha\\) (alpha) is the threshold for evidence against the null hypothesis.\n\nCommon choices: \\(\\alpha = 0.05\\) (5%) or \\(\\alpha = 0.01\\) (1%).\n\nInterpretation: If \\(p \\leq \\alpha\\), the result is considered statistically significant — unlikely to occur just by chance if \\(H_0\\) were true.\n\n\n\n\nThe \\(z\\) distribution (also called the standard normal distribution) is a bell-shaped curve with:\n- Mean = \\(0\\)\n- Standard deviation = \\(1\\)\nSo how do we get from our sample proportion \\(\\hat{\\pi}\\) to this special distribution?\n\n\nUnder the null hypothesis \\(H_0 : \\pi = 0.40\\), the sample proportion \\(\\hat{\\pi}\\) has a sampling distribution that is approximately normal (by the Central Limit Theorem) with:\n\nMean = \\(\\pi_0 = 0.40\\)\n\nStandard deviation = \\(SE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\)\n\nThis tells us what values of \\(\\hat{\\pi}\\) we would expect just by chance if the null were true.\n\n\n\nTo compare our observed \\(\\hat{\\pi}\\) to this distribution, we standardize it:\n\nSubtract the mean under \\(H_0\\)\n\nThis centers the distribution at \\(0\\) by measuring how far away our observed statistic is from the null.\n\n\\((\\hat{\\pi} - \\pi_0)\\)\n\nDivide by the standard deviation\n\nThis rescales differences into standard deviation units, so we can judge how unusual they are.\n\n\\(\\dfrac{\\hat{\\pi} - \\pi_0}{SE}\\)\n\n\n\n\n\nAfter centering and rescaling, the new standardized statistic follows (approximately) the standard normal distribution, \\(N(0,1)\\).\nThat’s why we call it the \\(z\\) statistic:\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n\\]\nNow we can use the \\(z\\) distribution to calculate probabilities (like \\(p\\)-values) for how extreme our observed result is relative to the null hypothesis.\n\nggplot() +\n  geom_function(fun = dnorm, xlim = c(-4,4))"
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#classroom-example-rockpaperscissors",
    "href": "MA206-AY26-1/lesson-12.html#classroom-example-rockpaperscissors",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "Class Activity: With your eyes closed, on the count of three, everyone will make their first throw of Rock–Paper–Scissors.\n- 👊 = Rock\n- ✋ = Paper\n- ✌️ = Scissors\nHold your choice steady so we can tally the results. Closing your eyes helps reduce herding or copying from neighbors.\n\n\nDo students tend to choose Rock more or less often than random chance (\\(33\\%\\)) on their first throw?\n\n\n\nWith eyes closed, on the count of 3 each student chooses one option (Rock/Paper/Scissors) as if starting a game. We then tally the class counts:\n- \\(R\\) = number of Rock\n- \\(P\\) = number of Paper\n- \\(S\\) = number of Scissors\n- \\(n = R+P+S\\)\n\n# Enter the tallies you just collected:\nR &lt;- 2   # Rock count\nP &lt;- 10   # Paper count\nS &lt;- 7   # Scissors count\n\nn &lt;- R + P + S\nc(R = R, P = P, S = S, n = n)\n\n R  P  S  n \n 2 10  7 19 \n\n\n\n\n\nCompute the sample proportion choosing Rock and make a quick bar chart. Under complete randomness we’d expect each to be near \\(n/3\\).\n\npihat &lt;- R / n\npihat\n\n[1] 0.1052632\n\ndf &lt;- tibble(option = c(\"Rock\",\"Paper\",\"Scissors\"),\n             count  = c(R, P, S))\n\nggplot(df, aes(option, count)) +\n  geom_col() +\n  geom_hline(yintercept = n/3, linetype = 2) +\n  labs(title = \"Class First-Throw Choices\",\n       subtitle = \"Dashed line = expected count if choices were uniform (n/3)\",\n       x = NULL, y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\nWe’ll do two things:\n\nSimulate many samples of size \\(n\\) assuming \\(p_{Rock}=1/3\\), and estimate a two-sided \\(p\\)-value by comparing simulated proportions to the observed.\n\nDo the math version using the \\(z\\)-distribution.\n\n\n\n\npi0 &lt;- 1/3\nN  &lt;- 10000               # number of simulations\nR_obs &lt;- R                # keep observed Rock consistent with Step 2\npihat_obs &lt;- R_obs / n\n\n# Simulate many samples under H0\nsim_results &lt;- tibble(\n  trial = 1:N,\n  rocks = rbinom(N, size = n, prob = pi0)   # Rock counts in each simulated sample\n) |&gt;\n  mutate(pihat = rocks / n)                  # simulated sample proportions\n\nsim_results\n\n# A tibble: 10,000 × 3\n   trial rocks pihat\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     1     4 0.211\n 2     2     5 0.263\n 3     3     4 0.211\n 4     4    10 0.526\n 5     5     6 0.316\n 6     6     5 0.263\n 7     7     3 0.158\n 8     8     9 0.474\n 9     9     5 0.263\n10    10     7 0.368\n# ℹ 9,990 more rows\n\n\n\n# Plot the simulated sampling distribution with observed and null marked\nsim_results |&gt;\n  ggplot(aes(x = pihat)) +\n  geom_histogram(binwidth = 1/n, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = pihat_obs, color = \"firebrick\", linetype = 2, linewidth = 1.2) +\n  geom_vline(xintercept = pi0, color = \"gray40\", linewidth = 1.2) +\n  labs(title = paste0(\"Sampling Distribution of p̂i under H0 (n = \", n, \")\"),\n       subtitle = paste0(\"Observed p̂ = \", round(pihat_obs,3), \n                         \" | Null pi0 = \", round(pi0,3)),\n       x = \"p̂ (proportion Rock)\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Two-sided simulation p-value\nobs_abs_diff &lt;- abs(pihat_obs - pi0)\n\nsim_p_two_sided &lt;- sim_results |&gt; \n  mutate(pihat_different_than_null = pihat - pi0) |&gt;          # difference from null\n  mutate(abs_of_difference = abs(pihat_different_than_null)) |&gt; \n  mutate(is_extreme = abs_of_difference &gt;= obs_abs_diff) |&gt; # flag extremes\n  summarise(p_val = mean(is_extreme))                       # proportion of extremes = p-value\n\nsim_p_two_sided\n\n# A tibble: 1 × 1\n   p_val\n   &lt;dbl&gt;\n1 0.0468\n\n\n\n\n\n\n\\(H_0: p_{Rock} = 1/3\\)\n\n\\(H_A: p_{Rock} \\neq 1/3\\)\n\n\\(SE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\)\n\n# Standard Error under H0\nSE &lt;- sqrt(pi0 * (1 - pi0) / n)\nSE\n\n[1] 0.1081476\n\n\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n\\]\n\n# Test Statistic (z)\nz_stat &lt;- (pihat_obs - pi0) / SE\nz_stat\n\n[1] -2.108878\n\n\n\\[\np = 2 \\times P(Z \\geq |z|)\n\\]\n\n# Two-Sided p-value\np_val_two_sided &lt;- 2 * (1 - pnorm(abs(z_stat)))\np_val_two_sided\n\n[1] 0.03495507\n\n\n\n\n\n\nNow we bring the two approaches together:\n\nSimulation gave us an empirical \\(p\\)-value by resampling under \\(H_0\\).\n\nMath/analytic (\\(z\\)-test) gave us an approximate \\(p\\)-value using the standard normal.\n\n\n\n              n           R_obs       pihat_obs             pi0          z_stat \n        19.0000          2.0000          0.1050          0.3330         -2.1090 \nsim_p_two_sided   z_p_two_sided           alpha \n         0.0468          0.0350          0.0500 \n\n\nSimulation-based decision: Reject H0 (evidence of ≠ 1/3) \n\n\nZ-approximation decision: Reject H0 (evidence of ≠ 1/3) \n\n\nInterpretation:\nWith \\(n\\) throws and observed \\(\\hat p\\), the simulation \\(p\\)-value tells us how unusual the result is if \\(p=1/3\\) were true. The \\(z\\) test gives a similar answer using a theoretical normal curve. At \\(\\alpha = 0.05\\), compare both to decide whether to reject \\(H_0\\).\n\n\n\n\n\nCode\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tibble)\n\nui &lt;- fluidPage(\n  titlePanel(\"One-Proportion Test: z-formulas + Live Simulation\"),\n  withMathJax(),\n  tags$hr(),\n  \n  fluidRow(\n    column(\n      width = 4,\n      h4(\"Inputs\"),\n      numericInput(\"x\", \"Observed successes (x)\", value = 2, min = 0, step = 1),\n      numericInput(\"n\", \"Sample size (n)\", value = 19, min = 1, step = 1),\n      sliderInput(\"pi0\", HTML(\"&pi;&lt;sub&gt;0&lt;/sub&gt; (null proportion)\"), min = 0, max = 1,\n                  value = 1/3, step = 0.01),\n      numericInput(\"B\", \"Simulations (B)\", value = 10000, min = 100, step = 100),\n      numericInput(\"seed\", \"Random seed\", value = 26, min = 1, step = 1),\n      helpText(\"Tip: This defaults to the R/P/S example: x = 2 (Rock), n = 19, π0 = 1/3.\")\n    ),\n    column(\n      width = 8,\n      h4(\"Formulas\"),\n      # Show the formulas via MathJax\n      div(style = \"font-size: 1.15em; margin-bottom: 8px;\",\n          \"$$ SE = \\\\sqrt{\\\\frac{\\\\pi_0(1 - \\\\pi_0)}{n}}, \\\\qquad z = \\\\frac{\\\\hat{p} - \\\\pi_0}{SE} $$\"\n      ),\n      h4(\"Computed Values\"),\n      tableOutput(\"value_table\"),\n      tags$br(),\n      h4(\"Sampling Distribution under H0 (Simulated)\"),\n      plotOutput(\"hist_plot\", height = \"330px\"),\n      helpText(\"Histogram shows simulated \\\\(\\\\hat p\\\\) under H0. Red line = observed \\\\(\\\\hat p\\\\). Gray line = \\\\(\\\\pi_0\\\\).\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  # Basic reactives\n  phat &lt;- reactive({\n    req(input$n &gt; 0)\n    input$x / input$n\n  })\n  \n  SE &lt;- reactive({\n    sqrt(input$pi0 * (1 - input$pi0) / input$n)\n  })\n  \n  z_stat &lt;- reactive({\n    (phat() - input$pi0) / SE()\n  })\n  \n  # p-values for three alternatives\n  p_right &lt;- reactive({ 1 - pnorm(z_stat()) })                 # H_A: p &gt; pi0\n  p_left  &lt;- reactive({ pnorm(z_stat()) })                     # H_A: p &lt; pi0\n  p_two   &lt;- reactive({ 2 * (1 - pnorm(abs(z_stat()))) })      # H_A: p != pi0\n  \n  # Simulation under H0\n  sim_df &lt;- reactive({\n    req(input$B &gt;= 100)\n    set.seed(input$seed)\n    rocks &lt;- rbinom(input$B, size = input$n, prob = input$pi0)\n    tibble(\n      phat = rocks / input$n\n    )\n  })\n  \n  # Output: table of computed values\n  output$value_table &lt;- renderTable({\n    tibble::tibble(\n      `x (successes)` = input$x,\n      `n (trials)`    = input$n,\n      `π0 (null)`     = round(input$pi0, 4),\n      `p̂ = x/n`      = round(phat(), 4),\n      `SE`            = round(SE(), 5),\n      `z`             = round(z_stat(), 4),\n      `p (right)`     = signif(p_right(), 4),\n      `p (left)`      = signif(p_left(), 4),\n      `p (two-sided)` = signif(p_two(), 4)\n    )\n  }, striped = TRUE, bordered = TRUE, spacing = \"s\", digits = 6)\n  \n  # Output: histogram with vertical lines at phat and pi0\n  output$hist_plot &lt;- renderPlot({\n    df &lt;- sim_df()\n    ggplot(df, aes(x = phat)) +\n      geom_histogram(binwidth = 1 / input$n, color = \"white\") +\n      geom_vline(xintercept = phat(), color = \"firebrick\", linetype = 2, linewidth = 1.2) +\n      geom_vline(xintercept = input$pi0, color = \"gray40\", linewidth = 1.2) +\n      labs(\n        x = expression(hat(p) ~ \"(proportion)\"),\n        y = \"Count\",\n        title = paste0(\"Simulated Sampling Distribution of \", expression(hat(p)), \" under H0\"),\n        subtitle = paste0(\"n = \", input$n, \", π0 = \", round(input$pi0, 3),\n                          \", observed p̂ = \", round(phat(), 3),\n                          \"; B = \", input$B)\n      ) +\n      theme_minimal(base_size = 12) +\n      xlim(c(0,1))\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#reference-table-z-tests-for-one-proportion",
    "href": "MA206-AY26-1/lesson-12.html#reference-table-z-tests-for-one-proportion",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "For all cases:\n\\[\nSE = \\sqrt{\\frac{\\pi_0(1 - \\pi_0)}{n}}, \\quad\nz = \\frac{\\hat{p} - \\pi_0}{SE}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution."
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#board-problem",
    "href": "MA206-AY26-1/lesson-12.html#board-problem",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "A hospital claims that 85% of discharge summaries are finalized within 24 hours.\nIn an audit of 60 summaries, 46 were finalized within 24 hours.\nResearch Question: Is the true proportion finalized within 24 hours less than 85%?\n\nState the hypotheses.\n\nExplain (in words) how you would simulate this test (do not actually simulate).\n\nThen, perform the mathematical one-proportion \\(z\\) test.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nDo fewer than 85% of discharge summaries get finalized within 24 hours?\n\n\n\n\n\\(H_0 : \\pi = 0.85\\)\n\n\\(H_A : \\pi &lt; 0.85\\)\n\n\n\n\n\n\\(n = 60\\) summaries\n\n\\(x = 46\\) finalized within 24h\n\n\\(\\hat{\\pi} = \\tfrac{x}{n} = \\tfrac{46}{60} = 0.767\\)\n\n\n\n\n\n\n\n\nUnder \\(H_0\\), simulate many samples of size \\(n=60\\) with \\(\\pi_0=0.85\\).\n\nFor each, compute \\(\\hat{\\pi}_{sim}\\).\n\nEstimate the left-tailed \\(p\\)-value as the fraction of simulations with \\(\\hat{\\pi}_{sim} \\le \\hat{\\pi}_{obs} = 0.767\\).\n\n\n\n\nStandard error under \\(H_0\\):\n\\[\nSE = \\sqrt{\\frac{\\pi_0 (1-\\pi_0)}{n}}\n   = \\sqrt{\\frac{0.85 \\cdot 0.15}{60}}\n   \\approx 0.046\n\\]\nTest statistic:\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n  = \\frac{0.767 - 0.85}{0.046}\n  \\approx -1.80\n\\]\n\\(p\\)-value (left-tailed):\n\\[\np = \\Phi(z) = \\Phi(-1.80) \\approx 0.036\n\\]\n\npnorm(-1.8)\n\n[1] 0.03593032\n\n\n\n\n\nAt \\(\\alpha = 0.05\\), since \\(p \\approx 0.036 &lt; 0.05\\), we reject \\(H_0\\).\n\n\n\nThere is statistical evidence that fewer than 85% of discharge summaries are completed within 24 hours."
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-12.html#before-you-leave",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nProject Milestone 3: Due Canvas 22 Sept\nExploration Exercise 1.5: Due at 0700 on Lesson 13\n24 September 2025 for Day 1\n\n25 September 2025 for Day 2)\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-12.html#lesson-administration",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "⏰ Due 0700 ET on Lesson 13\n\nDay 1: Wednesday, 24 Sept 2025\n\nDay 2: Thursday, 25 Sept 2025\n\n\n📑 Worksheet: https://westpoint.instructure.com/courses/10295/assignments/216497 — don’t sleep on this!\nWill need applet for simulation\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12/120\n10:37\n\n\n\n\n\n  Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#one-mean-t-test",
    "href": "MA206-AY26-1/lesson-12.html#one-mean-t-test",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "For all cases:\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.\n\n\n\n\nWe want to test whether Male cadets (Don’t worry ladies, we’ll get to you) are shorter than the average height of U.S. men aged 19–24, which is 72 inches.\n\n\n\\[\nH_0 : \\mu = 72\n\\]\n\\[\nH_A : \\mu &lt; 72\n\\]\n\n\n\n\nheights &lt;- c(70, 71, 69, 73, 68, 74, 71, 70, 72, 69, 70, 71, 68, 73)\n\n\n\n\n\n\n\nIs This a Good Sample?\n\n\n\nWe measured the heights of 14 cadets in this class.\nBut our research question is about all U.S. men aged 19–24.\n\nDo cadets in this class represent the broader population (what is that population)?\n\nWhat kinds of biases might be present?\n\nIf the sample isn’t random, how should that affect our conclusions?\n\n\n\n\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\qquad df = n - 1\n\\]\n\\[\n\\begin{aligned}\n\\bar{x} & = \\text{sample mean} \\\\[4pt]\n\\mu_0   & = \\text{hypothesized mean (72)} \\\\[4pt]\ns       & = \\text{sample standard deviation} \\\\[4pt]\nn       & = \\text{sample size} \\\\[4pt]\nt       & = \\text{test statistic}\n\\end{aligned}\n\\]\n\n\n\n\nn     &lt;- length(heights)\nxbar  &lt;- mean(heights)\ns     &lt;- sd(heights)\n\nc(n = n, mean = round(xbar, 2), sd = round(s, 2))\n\n    n  mean    sd \n14.00 70.64  1.86 \n\n\n\n\n\n\nmu0 &lt;- 72\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\n\n[1] -2.722848\n\n\n\n\n\n\ndf &lt;- n - 1\n\nggplot() +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  labs(title = \"t-Distribution with df = 13\")\n\n\n\n\n\n\n\n\nAdd the observed test statistic:\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, t_stat),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, color = \"firebrick\", linewidth = 1.2) +\n  labs(title = \"Left-tail Shaded: t-Distribution with Test Statistic\")\n\n\n\n\n\n\n\n\n\n\n\n\np_val &lt;- pt(t_stat, df = df)   \np_val\n\n[1] 0.008708986\n\n\n\n\n\nWith p ≈ .008, we do have evidence that male cadets are shorter than 72 inches.\n\n\n\n\nWe want to test whether female cadets are taller than the average height of U.S. women aged 19–24, which is 64 inches.\n\n\n\\[\nH_0 : \\mu = 64\n\\]\n\\[\nH_A : \\mu &gt; 64\n\\]\n\n\n\n\nheights &lt;- c(65, 67, 66, 64)\n\n\n\n\n\n\n\nIs This a Good Sample?\n\n\n\nWe measured the heights of only 4 female cadets in this class.\nBut our research question is about all U.S. women aged 19–24.\n\nDo these 4 cadets represent the broader population?\n\nWhat kinds of biases might be present?\n\nIf the sample isn’t random (or too small), how should that affect our conclusions?\n\n\n\n\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\qquad df = n - 1\n\\]\n\n\n\n\nn     &lt;- length(heights)\nxbar  &lt;- mean(heights)\ns     &lt;- sd(heights)\n\nc(n = n, mean = round(xbar, 2), sd = round(s, 2))\n\n    n  mean    sd \n 4.00 65.50  1.29 \n\n\n\n\n\n\nmu0 &lt;- 64\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\n\n[1] 2.32379\n\n\n\n\n\n\ndf &lt;- n - 1\n\nggplot() +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  labs(title = \"t-Distribution with df = 3\")\n\n\n\n\n\n\n\n\nAdd the observed test statistic and shade the right tail:\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(t_stat, 4),\n                geom = \"area\", fill = \"darkred\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, color = \"firebrick\", linewidth = 1.2) +\n  labs(title = \"Right-tail Shaded: t-Distribution with Test Statistic\")\n\n\n\n\n\n\n\n\n\n\n\n\np_val &lt;- 1 - pt(t_stat, df = df)   # one-tailed, greater than\np_val\n\n[1] 0.05136404\n\n\n\n\n\nWith p ≈ .051, we do not have strong evidence that female cadets are taller than 64 inches.\n\n\n\n\nSuppose the national average height of men aged 19–24 is 72 inches.\nWe want to test whether West Point Department of Math instructors are different (either taller or shorter).\n\n\n\\[\nH_0 : \\mu = 72\n\\]\n\\[\nH_A : \\mu \\neq 72\n\\]\n\n\n\nHere is a sample of heights (in inches) from 28 Math instructors:\n\nheights &lt;- c(71, 70, 73, 72, 74, 69, 71, 72, 70, 73, 72, 71, 75, 70, 72, 74, 71, 69, 73, 72, 70, 71, 74, 72, 70, 73, 71, 72)\n\n\n\n\n\n\n\nIs This a Good Sample?\n\n\n\nWe only used 28 instructors from the department.\nOur research question is about all West Point Math instructors compared to the national population.\n\nDo these 28 represent the full department fairly?\n\nWhat biases could be present?\n\nIf the sample is not random, how should that affect our conclusions?\n\n\n\n\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\qquad df = n - 1\n\\]\n\n\n\n\nn     &lt;- length(heights)\nxbar  &lt;- mean(heights)\ns     &lt;- sd(heights)\n\nc(n = n, mean = round(xbar, 2), sd = round(s, 2))\n\n    n  mean    sd \n28.00 71.68  1.56 \n\n\n\n\n\n\nmu0 &lt;- 72\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\n\n[1] -1.086979\n\n\n\n\n\n\ndf &lt;- n - 1\n\nggplot() +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  labs(title = paste(\"t-Distribution with df =\", df))\n\n\n\n\n\n\n\n\nAdd the observed test statistic and shade both tails:\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, -abs(t_stat)),\n                geom = \"area\", fill = \"steelblue\", alpha = 0.5) +\n  stat_function(fun = dt, args = list(df = df), xlim = c(abs(t_stat), 4),\n                geom = \"area\", fill = \"steelblue\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, color = \"firebrick\", linewidth = 1.2) +\n  geom_vline(xintercept = -t_stat, color = \"firebrick\", linewidth = 1.2) +\n  labs(title = \"Two-tailed Shaded: t-Distribution with Test Statistic\")\n\n\n\n\n\n\n\n\n\n\n\n\np_val &lt;- 2 * (1 - pt(abs(t_stat), df = df))   # two-tailed\np_val\n\n[1] 0.286655\n\n\n\n\n\nWith p ≈ .29, we do not have evidence that Math instructors’ heights differ from 72 inches.\n\n\n\n\nOverlay several \\(t\\) distributions with different degrees of freedom and the standard normal \\(N(0,1)\\) for comparison.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#review-z-tests-for-one-proportion",
    "href": "MA206-AY26-1/lesson-12.html#review-z-tests-for-one-proportion",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "For all cases:\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution."
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#t-distribution-exploration",
    "href": "MA206-AY26-1/lesson-12.html#t-distribution-exploration",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "Overlay several \\(t\\) distributions with different degrees of freedom and the standard normal \\(N(0,1)\\) for comparison.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#board-problem-coffee-consumption",
    "href": "MA206-AY26-1/lesson-12.html#board-problem-coffee-consumption",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "Nationally, the average college student drinks about \\(3.2\\) cups of coffee per day.\nYou suspect cadets might consume a different amount (not necessarily more or less).\nYou collect a sample of \\(12\\) cadets with the following self-reported daily coffee consumption (in cups):\n\ncoffee &lt;- c(2.5, 3.0, 3.8, 4.1, 3.2, 2.9, 3.6, 4.0, 3.3, 2.7, 3.5, 3.9)\n\nTasks:\n\nState the null and alternative hypotheses.\n\nCompute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n\nWrite down the test statistic formula for a one-sample \\(t\\)-test.\n\nCalculate the test statistic.\n\nFind the \\(p\\)-value for the appropriate two-tailed test.\n\nState your conclusion in the context of the problem.\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses\n\\[\nH_0: \\mu = 3.2 \\qquad\\text{vs}\\qquad H_A: \\mu \\neq 3.2\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(coffee)\nxbar &lt;- mean(coffee)\ns    &lt;- sd(coffee)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n12.000  3.375  0.528 \n\n\nNumerically: \\(n = 12\\), \\(\\bar{x} \\approx 3.375\\), \\(s \\approx 0.528\\).\n3–4. Test statistic\n\nmu0   &lt;- 3.2\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n 1.149 11.000 \n\n\nNumerically: \\(t \\approx 1.149\\) with \\(df = 11\\).\n5. Two-tailed \\(p\\)-value\n\np_val &lt;- 2 * (1 - pt(abs(t_stat), df = df))\np_val\n\n[1] 0.2749615\n\n\nNumerically: \\(p \\approx 0.275\\).\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  # Left tail shading\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, -abs(t_stat)),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Right tail shading\n  stat_function(fun = dt, args = list(df = df), xlim = c(abs(t_stat), 4),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Overlay t density\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  # Vertical lines at ±t_stat\n  geom_vline(xintercept = c(-abs(t_stat), abs(t_stat)),\n             color = \"firebrick\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(title = \"Two-tailed Shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")\n\n\n\n\n\n\n\n\n6. Conclusion\nWith \\(p \\approx 0.275\\), we do not have evidence that cadets’ average coffee consumption differs from \\(3.2\\) cups per day.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#board-problem-daily-screen-time",
    "href": "MA206-AY26-1/lesson-12.html#board-problem-daily-screen-time",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "A recent campus wellness report suggests the average college student spends \\(3.0\\) hours per day on recreational screen time (not including coursework).\nYou suspect students in your section spend more than that.\nYou collect a sample of \\(12\\) students with the following daily screen-time values (in hours):\n\nscreen_time &lt;- c(2.5, 3.0, 3.1, 3.2, 3.3, 3.4, 3.8, 3.7, 3.8, 3.2, 3.5, 3.5)\n\nTasks: 1. State the null and alternative hypotheses.\n2. Compute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n3. Write down the test statistic formula for a one-sample \\(t\\)-test.\n4. Calculate the test statistic.\n5. Find the one-tailed \\(p\\)-value for the “greater than” test.\n6. State your conclusion in context.\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses \\[\nH_0: \\mu = 3.0\n\\qquad\\text{vs}\\qquad\nH_A: \\mu &gt; 3.0\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(screen_time)\nxbar &lt;- mean(screen_time)\ns    &lt;- sd(screen_time)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n12.000  3.333  0.373 \n\n\n3–4. Test statistic\n\nmu0    &lt;- 3.0\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n   3.1   11.0 \n\n\n5. One-tailed \\(p\\)-value (\\(H_A: \\mu &gt; \\mu_0\\))\n\np_val &lt;- 1 - pt(t_stat, df = df)\np_val\n\n[1] 0.005056453\n\n\n6. Conclusion\nWith the computed \\(t\\) and \\(p\\) above, interpret whether there is evidence that average daily recreational screen time in this section exceeds \\(3.0\\) hours.\n\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(t_stat, 4),\n                geom = \"area\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, linewidth = 1.2) +\n  labs(title = \"Right-tail shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\nPublic health guidelines recommend \\(7.0\\) hours of sleep on weeknights.\nYou suspect students in your section average less than that.\nYou collect a sample of \\(15\\) students’ self-reported weeknight sleep (in hours):\n\nsleep &lt;- c(6.6, 6.9, 7.1, 6.8, 7.0, 6.7, 6.5, 6.8, 6.9, 6.4, 6.6, 7.2, 6.7, 6.8, 6.5)\n\nTasks: 1. State the null and alternative hypotheses.\n2. Compute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n3. Write down the test statistic formula for a one-sample \\(t\\)-test.\n4. Calculate the test statistic.\n5. Find the one-tailed \\(p\\)-value for the “less than” test.\n6. State your conclusion in context.\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses \\[\nH_0: \\mu = 7.0\n\\qquad\\text{vs}\\qquad\nH_A: \\mu &lt; 7.0\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(sleep)\nxbar &lt;- mean(sleep)\ns    &lt;- sd(sleep)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n15.000  6.767  0.229 \n\n\n3–4. Test statistic\n\nmu0    &lt;- 7.0\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n-3.949 14.000 \n\n\n5. One-tailed \\(p\\)-value (\\(H_A: \\mu &lt; \\mu_0\\))\n\np_val &lt;- pt(t_stat, df = df)\np_val\n\n[1] 0.0007279758\n\n\n6. Conclusion\nReport the computed \\(t\\), \\(df\\), and \\(p\\), then conclude whether there is evidence that average weeknight sleep is less than \\(7.0\\) hours.\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  # Left tail shading up to t_stat\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, t_stat),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Overlay t density\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  # Vertical line at t_stat\n  geom_vline(xintercept = t_stat,\n             color = \"firebrick\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(title = \"Left-tail Shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#board-problem-weeknight-sleep-less-than-one-sample-t-test",
    "href": "MA206-AY26-1/lesson-12.html#board-problem-weeknight-sleep-less-than-one-sample-t-test",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "Public health guidelines recommend \\(7.0\\) hours of sleep on weeknights.\nYou suspect students in your section average less than that.\nYou collect a sample of \\(15\\) students’ self-reported weeknight sleep (in hours):\n\nsleep &lt;- c(6.6, 6.9, 7.1, 6.8, 7.0, 6.7, 6.5, 6.8, 6.9, 6.4, 6.6, 7.2, 6.7, 6.8, 6.5)\n\nTasks: 1. State the null and alternative hypotheses.\n2. Compute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n3. Write down the test statistic formula for a one-sample \\(t\\)-test.\n4. Calculate the test statistic.\n5. Find the one-tailed \\(p\\)-value for the “less than” test.\n6. State your conclusion in context.\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses \\[\nH_0: \\mu = 7.0\n\\qquad\\text{vs}\\qquad\nH_A: \\mu &lt; 7.0\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(sleep)\nxbar &lt;- mean(sleep)\ns    &lt;- sd(sleep)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n15.000  6.767  0.229 \n\n\n3–4. Test statistic\n\nmu0    &lt;- 7.0\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n-3.949 14.000 \n\n\n5. One-tailed \\(p\\)-value (\\(H_A: \\mu &lt; \\mu_0\\))\n\np_val &lt;- pt(t_stat, df = df)\np_val\n\n[1] 0.0007279758\n\n\n6. Conclusion\nReport the computed \\(t\\), \\(df\\), and \\(p\\), then conclude whether there is evidence that average weeknight sleep is less than \\(7.0\\) hours.\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  # Left tail shading up to t_stat\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, t_stat),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Overlay t density\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  # Vertical line at t_stat\n  geom_vline(xintercept = t_stat,\n             color = \"firebrick\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(title = \"Left-tail Shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#board-problems",
    "href": "MA206-AY26-1/lesson-12.html#board-problems",
    "title": "Lesson 11: One Mean T-Test",
    "section": "",
    "text": "Nationally, the average college student drinks about \\(3.2\\) cups of coffee per day.\nYou suspect cadets might consume a different amount (not necessarily more or less).\nYou collect a sample of \\(12\\) cadets with the following self-reported daily coffee consumption (in cups):\n\ncoffee &lt;- c(2.5, 3.0, 3.8, 4.1, 3.2, 2.9, 3.6, 4.0, 3.3, 2.7, 3.5, 3.9)\n\nTasks:\n\nState the null and alternative hypotheses.\n\nCompute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n\nWrite down the test statistic formula for a one-sample \\(t\\)-test.\n\nCalculate the test statistic.\n\nFind the \\(p\\)-value for the appropriate two-tailed test.\n\nState your conclusion in the context of the problem.\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses\n\\[\nH_0: \\mu = 3.2 \\qquad\\text{vs}\\qquad H_A: \\mu \\neq 3.2\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(coffee)\nxbar &lt;- mean(coffee)\ns    &lt;- sd(coffee)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n12.000  3.375  0.528 \n\n\nNumerically: \\(n = 12\\), \\(\\bar{x} \\approx 3.375\\), \\(s \\approx 0.528\\).\n3–4. Test statistic\n\nmu0   &lt;- 3.2\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n 1.149 11.000 \n\n\nNumerically: \\(t \\approx 1.149\\) with \\(df = 11\\).\n5. Two-tailed \\(p\\)-value\n\np_val &lt;- 2 * (1 - pt(abs(t_stat), df = df))\np_val\n\n[1] 0.2749615\n\n\nNumerically: \\(p \\approx 0.275\\).\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  # Left tail shading\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, -abs(t_stat)),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Right tail shading\n  stat_function(fun = dt, args = list(df = df), xlim = c(abs(t_stat), 4),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Overlay t density\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  # Vertical lines at ±t_stat\n  geom_vline(xintercept = c(-abs(t_stat), abs(t_stat)),\n             color = \"firebrick\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(title = \"Two-tailed Shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")\n\n\n\n\n\n\n\n\n6. Conclusion\nWith \\(p \\approx 0.275\\), we do not have evidence that cadets’ average coffee consumption differs from \\(3.2\\) cups per day.\n\n\n\n\n\n\n\n\nA recent campus wellness report suggests the average college student spends \\(3.0\\) hours per day on recreational screen time (not including coursework).\nYou suspect students in your section spend more than that.\nYou collect a sample of \\(12\\) students with the following daily screen-time values (in hours):\n\nscreen_time &lt;- c(2.5, 3.0, 3.1, 3.2, 3.3, 3.4, 3.8, 3.7, 3.8, 3.2, 3.5, 3.5)\n\nTasks: 1. State the null and alternative hypotheses.\n2. Compute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n3. Write down the test statistic formula for a one-sample \\(t\\)-test.\n4. Calculate the test statistic.\n5. Find the one-tailed \\(p\\)-value for the “greater than” test.\n6. State your conclusion in context.\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses \\[\nH_0: \\mu = 3.0\n\\qquad\\text{vs}\\qquad\nH_A: \\mu &gt; 3.0\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(screen_time)\nxbar &lt;- mean(screen_time)\ns    &lt;- sd(screen_time)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n12.000  3.333  0.373 \n\n\n3–4. Test statistic\n\nmu0    &lt;- 3.0\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n   3.1   11.0 \n\n\n5. One-tailed \\(p\\)-value (\\(H_A: \\mu &gt; \\mu_0\\))\n\np_val &lt;- 1 - pt(t_stat, df = df)\np_val\n\n[1] 0.005056453\n\n\n6. Conclusion\nWith the computed \\(t\\) and \\(p\\) above, interpret whether there is evidence that average daily recreational screen time in this section exceeds \\(3.0\\) hours.\n\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(t_stat, 4),\n                geom = \"area\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, linewidth = 1.2) +\n  labs(title = \"Right-tail shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\nPublic health guidelines recommend \\(7.0\\) hours of sleep on weeknights.\nYou suspect students in your section average less than that.\nYou collect a sample of \\(15\\) students’ self-reported weeknight sleep (in hours):\n\nsleep &lt;- c(6.6, 6.9, 7.1, 6.8, 7.0, 6.7, 6.5, 6.8, 6.9, 6.4, 6.6, 7.2, 6.7, 6.8, 6.5)\n\nTasks: 1. State the null and alternative hypotheses.\n2. Compute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n3. Write down the test statistic formula for a one-sample \\(t\\)-test.\n4. Calculate the test statistic.\n5. Find the one-tailed \\(p\\)-value for the “less than” test.\n6. State your conclusion in context.\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses \\[\nH_0: \\mu = 7.0\n\\qquad\\text{vs}\\qquad\nH_A: \\mu &lt; 7.0\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(sleep)\nxbar &lt;- mean(sleep)\ns    &lt;- sd(sleep)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n15.000  6.767  0.229 \n\n\n3–4. Test statistic\n\nmu0    &lt;- 7.0\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n-3.949 14.000 \n\n\n5. One-tailed \\(p\\)-value (\\(H_A: \\mu &lt; \\mu_0\\))\n\np_val &lt;- pt(t_stat, df = df)\np_val\n\n[1] 0.0007279758\n\n\n6. Conclusion\nReport the computed \\(t\\), \\(df\\), and \\(p\\), then conclude whether there is evidence that average weeknight sleep is less than \\(7.0\\) hours.\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  # Left tail shading up to t_stat\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, t_stat),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Overlay t density\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  # Vertical line at t_stat\n  geom_vline(xintercept = t_stat,\n             color = \"firebrick\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(title = \"Left-tail Shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  }
]