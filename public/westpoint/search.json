[
  {
    "objectID": "MA206-AY26-1/lesson-4.html",
    "href": "MA206-AY26-1/lesson-4.html",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Project Milestone 2\n\nTidyverse Tutorial applied to your team’s data\nAnnex B (for addressing feedback)\n\nExploration 11.3B\n\n\n\n\n\n\nNot Cal, but….\n\n  Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\nPreviously 0-0\n\n\n\n\n\n\n1-0\n\n\n\n\n\n\n\n\n\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\n\n\n\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n   A B\n\n\n\n\n\nTwo events are mutually exclusive when it is impossible for both to happen at the same time.\nTherefore:\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n   A B\n\n\n\n\n\nTwo events \\(A\\) and \\(B\\) are independent if knowing that one occurs does not change the probability of the other.\n\\[\nP(A \\cap B) = P(A)P(B)\n\\]\nEquivalently (if \\(P(B) &gt; 0\\)):\n\\[\nP(A \\mid B) = P(A)\n\\]\nIntuition: The outcome of one event gives no information about the other.\nDoes Mutually Exclusive imply independence or no or undetermined?\n\n\n\n\nThe probability that event \\(A\\) occurs given that event \\(B\\) occurs.\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\n\\]\nIntuition: Restrict the sample space to \\(B\\); ask what fraction of those outcomes also fall in \\(A\\).\n\n\n      A B\n\n\n\n\n\n\nRelates intersections to conditional probabilities.\n\\[\\begin{align}\nP(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\n&& \\text{definition of conditional probability} \\\\[24pt]\n\nP(A \\mid B)\\,P(B) &= \\frac{P(A \\cap B)}{P(B)} \\cdot P(B)\n&& \\text{multiply both sides by $P(B)$} \\\\[24pt]\n\nP(A \\mid B)\\,P(B) &= P(A \\cap B)\n&& \\text{simplify} \\\\[24pt]\n\nP(B \\mid A)\\,P(A) &= P(A \\cap B)\n&& \\text{swap $A$ and $B$} \\\\[24pt]\n\nP(A \\cap B) &= P(A \\mid B)P(B) = P(B \\mid A)P(A)\n&& \\text{final multiplication rule}\n\\end{align}\\]\nIntuition: To find the chance that both happen, compute the chance that one happens, then multiply by the chance the other happens given that.\n\n\n\nRemember this?\nOut of 100 students:\n- 40 like pizza\n- 30 like burgers\n- 10 of those included above like both pizza and burgers\nExperiment: I select one student at random\nLet:\n- \\(A =\\) “student likes pizza”\n- \\(B =\\) “student likes burgers”\n\nWhat is \\(P(A \\mid B)\\), the probability that a student likes pizza given that they like burgers?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe start with the definition of conditional probability:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}.\n\\]\nNow substitute the known values:\n\\[\nP(A \\mid B) = \\frac{0.10}{0.30}.\n\\]\nSimplify:\n\\[\nP(A \\mid B) = \\frac{1}{3}\n\\]\n\n\n\n\nWhat is \\(P(B \\mid A)\\), the probability that a student likes burgers given that they like pizza?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe start with the definition of conditional probability:\n\\[\nP(B \\mid A) = \\frac{P(A \\cap B)}{P(A)}.\n\\]\nNow substitute the known values:\n\\[\nP(B \\mid A) = \\frac{0.10}{0.40}.\n\\]\nSimplify:\n\\[\nP(B \\mid A) = \\frac{1}{4}\n\\]\n\n\n\n\n\nAre \\(A\\) and \\(B\\) independent? (Check whether \\(P(A \\cap B) = P(A)P(B)\\) holds.)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo check independence, we compare \\(P(A \\cap B)\\) with \\(P(A)P(B)\\).\nFirst compute:\n\\[\nP(A)P(B) = 0.40 \\times 0.30 = 0.12.\n\\]\nNow compare with the actual value:\n\\[\nP(A \\cap B) = 0.10 \\neq 0.12.\n\\]\nTherefore, \\(A\\) and \\(B\\) are not independent.\nWe can also see this because:\n\\[\nP(A \\mid B) = \\tfrac{1}{3} \\neq P(A) = 0.40.\n\\]\n\n\n\n\nCompute \\(P(A \\cap B)\\) (the probability a student likes both pizza and burgers).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe can use the multiplication rule to compute \\(P(A \\cap B)\\) in two different ways:\n\nUsing \\(P(A \\cap B) = P(A \\mid B)P(B)\\)\n\nUsing \\(P(A \\cap B) = P(B \\mid A)P(A)\\)\n\n\nUsing \\(P(A \\mid B)P(B)\\):\n\\[\nP(A \\cap B) = \\left(\\tfrac{1}{3}\\right)(0.30) = 0.10.\n\\]\nUsing \\(P(B \\mid A)P(A)\\):\n\\[\nP(A \\cap B) = (0.25)(0.40) = 0.10.\n\\]\nBoth methods agree with the given value:\n\\[\nP(A \\cap B) = 0.10.\n\\]\n\n\n\n\nSuppose you learn that the student selected does not like burgers. What is \\(P(A \\mid B^c)\\)?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe start with the definition of conditional probability:\n\\[\nP(A \\mid B^c) = \\frac{P(A \\cap B^c)}{P(B^c)}.\n\\]\nFirst compute the numerator (draw a venn diagram if this does not make sense):\n\\[\nP(A \\cap B^c) = P(A) - P(A \\cap B) = 0.40 - 0.10 = 0.30.\n\\]\nNow compute the denominator:\n\\[\nP(B^c) = 1 - P(B) = 1 - 0.30 = 0.70.\n\\]\nSubstitute into the formula:\n\\[\nP(A \\mid B^c) = \\frac{0.30}{0.70}.\n\\]\nSimplify:\n\\[\nP(A \\mid B^c) = \\tfrac{3}{7} \\approx 0.429.\n\\]\n\n\n\n\n(Stretch) If I select two students at random with replacement, what is the probability that both like pizza?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSince the selections are with replacement, the draws are independent.\n\\[\nP(\\text{both like pizza}) = P(A) \\times P(A).\n\\]\nSubstitute values:\n\\[\nP(\\text{both like pizza}) = (0.40)(0.40).\n\\]\nSimplify:\n\\[\nP(\\text{both like pizza}) = 0.16.\n\\]\n\n\n\n\n\n(Stretch) If I select two students at random without replacement, what is the probability that both like pizza?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWithout replacement, the draws are dependent.\nFirst student:\n\\[\nP(\\text{first likes pizza}) = \\tfrac{40}{100}.\n\\]\nSecond student (after one pizza-lover is removed):\n\\[\nP(\\text{second likes pizza} \\mid \\text{first likes pizza}) = \\tfrac{39}{99}.\n\\]\nMultiply:\n\\[\nP(\\text{both like pizza}) = \\tfrac{40}{100} \\cdot \\tfrac{39}{99}.\n\\]\nSimplify:\n\\[\nP(\\text{both like pizza}) = \\tfrac{1560}{9900} = \\tfrac{26}{165} \\approx 0.158.\n\\]\nThis is slightly less than the probability with replacement.\n\n\n\n\n\n\n\nWe’ll model the sample space with a probability tree. Each edge is labeled by a (conditional) probability; each leaf is the product along its path and equals a joint probability.\nSetup (from Problem 1):\n- Out of 100 students: 40 like pizza, 30 like burgers, 10 like both.\n- Let \\(A\\) = “likes pizza”, \\(B\\) = “likes burgers”.\nFrom the counts:\n\\[\nP(A)=0.40,\\quad P(B)=0.30,\\quad P(A\\cap B)=0.10.\n\\] Useful conditionals:\n\\[\nP(B\\mid A)=\\frac{0.10}{0.40}=\\frac{1}{4}=0.25,\\qquad\nP(B\\mid A^c)=\\frac{P(B)-P(A\\cap B)}{P(A^c)}=\\frac{0.20}{0.60}=\\frac{1}{3}.\n\\] (And similarly \\(P(A\\mid B)=\\tfrac{1}{3}\\), \\(P(A\\mid B^c)=\\tfrac{3}{7}\\).)\n\n\n\nLeaves give \\(P(A\\cap B)\\), \\(P(A\\cap B^c)\\), \\(P(A^c\\cap B)\\), \\(P(A^c\\cap B^c)\\).\n\nStart\n├── A (0.40)\n│   ├── B       | P(B|A)=0.25  ⇒  P(A∩B)     = 0.40·0.25 = 0.10\n│   └── B^c     | P(B^c|A)=0.75          ⇒  P(A∩B^c)   = 0.40·0.75 = 0.30\n└── A^c (0.60)\n    ├── B       | P(B|A^c)=1/3 ⇒  P(A^c∩B)   = 0.60·(1/3) = 0.20\n    └── B^c     | P(B^c|A^c)=2/3          ⇒  P(A^c∩B^c) = 0.60·(2/3) = 0.40\nChecks: \\[\n\\begin{aligned}\n&\\text{Row sums: } 0.10+0.30=0.40=P(A),\\quad 0.20+0.40=0.60=P(A^c).\\\\\n&\\text{Column sums: } 0.10+0.20=0.30=P(B),\\quad 0.30+0.40=0.70=P(B^c).\n\\end{aligned}\n\\]\n\n\n\n\n\nSame joint probabilities appear; this is handy when a question conditions on \\(B\\).\n\nStart\n├── B (0.30)\n│   ├── A       | P(A|B)=1/3   ⇒  P(A∩B)     = 0.30·(1/3) = 0.10\n│   └── A^c     | 2/3          ⇒  P(A^c∩B)   = 0.30·(2/3) = 0.20\n└── B^c (0.70)\n    ├── A       | P(A|B^c)=3/7 ⇒  P(A∩B^c)   = 0.70·(3/7) = 0.30\n    └── A^c     | 4/7          ⇒  P(A^c∩B^c) = 0.70·(4/7) = 0.40\n\n\n\n\n\nJoint probability of a leaf = product along its path.\nExample: \\(P(A\\cap B)=0.40\\cdot 0.25=0.10\\).\nConditional probability = branch label.\nExample: From the first tree, \\(P(B\\mid A)=0.25\\).\n\nTotals: add leaves in the relevant region.\nExample: “Exactly one likes” \\(=P(A\\cap B^c)+P(A^c\\cap B)=0.30+0.20=0.50\\).\n\n\n\n\n\n\n\\(P(A\\mid B)=\\tfrac{1}{3}\\), \\(P(B\\mid A)=\\tfrac{1}{4}\\)\n\n\\(P(A\\cap B^c)=0.30\\), \\(P(A^c\\cap B)=0.20\\)\n\nExactly one likes pizza or burgers \\(=0.50\\)\n\n\\(P(A\\mid B^c)=\\tfrac{3}{7}\\)\n\n\nTip: Draw whichever tree matches the given condition first (branch on the given event); it makes reading the needed conditional straight from the edge label.\n\n\n\n\nProject Milestone 2 Annex B\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 5\n\n\n\n\n\nTidyverse Tutorial: In Class Lesson 5\nProject Milestone 2: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#welcome",
    "href": "MA206-AY26-1/lesson-4.html#welcome",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Project Milestone 2\n\nTidyverse Tutorial applied to your team’s data\nAnnex B (for addressing feedback)\n\nExploration 11.3B\n\n\n\n\n\n\nNot Cal, but….\n\n  Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\nPreviously 0-0\n\n\n\n\n\n\n1-0",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#more-probability",
    "href": "MA206-AY26-1/lesson-4.html#more-probability",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "For any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\n\n\n\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n   A B\n\n\n\n\n\nTwo events are mutually exclusive when it is impossible for both to happen at the same time.\nTherefore:\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n   A B\n\n\n\n\n\nTwo events \\(A\\) and \\(B\\) are independent if knowing that one occurs does not change the probability of the other.\n\\[\nP(A \\cap B) = P(A)P(B)\n\\]\nEquivalently (if \\(P(B) &gt; 0\\)):\n\\[\nP(A \\mid B) = P(A)\n\\]\nIntuition: The outcome of one event gives no information about the other.\nDoes Mutually Exclusive imply independence or no or undetermined?\n\n\n\n\nThe probability that event \\(A\\) occurs given that event \\(B\\) occurs.\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\n\\]\nIntuition: Restrict the sample space to \\(B\\); ask what fraction of those outcomes also fall in \\(A\\).\n\n\n      A B\n\n\n\n\n\n\nRelates intersections to conditional probabilities.\n\\[\\begin{align}\nP(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\n&& \\text{definition of conditional probability} \\\\[24pt]\n\nP(A \\mid B)\\,P(B) &= \\frac{P(A \\cap B)}{P(B)} \\cdot P(B)\n&& \\text{multiply both sides by $P(B)$} \\\\[24pt]\n\nP(A \\mid B)\\,P(B) &= P(A \\cap B)\n&& \\text{simplify} \\\\[24pt]\n\nP(B \\mid A)\\,P(A) &= P(A \\cap B)\n&& \\text{swap $A$ and $B$} \\\\[24pt]\n\nP(A \\cap B) &= P(A \\mid B)P(B) = P(B \\mid A)P(A)\n&& \\text{final multiplication rule}\n\\end{align}\\]\nIntuition: To find the chance that both happen, compute the chance that one happens, then multiply by the chance the other happens given that.\n\n\n\nRemember this?\nOut of 100 students:\n- 40 like pizza\n- 30 like burgers\n- 10 of those included above like both pizza and burgers\nExperiment: I select one student at random\nLet:\n- \\(A =\\) “student likes pizza”\n- \\(B =\\) “student likes burgers”\n\nWhat is \\(P(A \\mid B)\\), the probability that a student likes pizza given that they like burgers?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe start with the definition of conditional probability:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}.\n\\]\nNow substitute the known values:\n\\[\nP(A \\mid B) = \\frac{0.10}{0.30}.\n\\]\nSimplify:\n\\[\nP(A \\mid B) = \\frac{1}{3}\n\\]\n\n\n\n\nWhat is \\(P(B \\mid A)\\), the probability that a student likes burgers given that they like pizza?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe start with the definition of conditional probability:\n\\[\nP(B \\mid A) = \\frac{P(A \\cap B)}{P(A)}.\n\\]\nNow substitute the known values:\n\\[\nP(B \\mid A) = \\frac{0.10}{0.40}.\n\\]\nSimplify:\n\\[\nP(B \\mid A) = \\frac{1}{4}\n\\]\n\n\n\n\n\nAre \\(A\\) and \\(B\\) independent? (Check whether \\(P(A \\cap B) = P(A)P(B)\\) holds.)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo check independence, we compare \\(P(A \\cap B)\\) with \\(P(A)P(B)\\).\nFirst compute:\n\\[\nP(A)P(B) = 0.40 \\times 0.30 = 0.12.\n\\]\nNow compare with the actual value:\n\\[\nP(A \\cap B) = 0.10 \\neq 0.12.\n\\]\nTherefore, \\(A\\) and \\(B\\) are not independent.\nWe can also see this because:\n\\[\nP(A \\mid B) = \\tfrac{1}{3} \\neq P(A) = 0.40.\n\\]\n\n\n\n\nCompute \\(P(A \\cap B)\\) (the probability a student likes both pizza and burgers).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe can use the multiplication rule to compute \\(P(A \\cap B)\\) in two different ways:\n\nUsing \\(P(A \\cap B) = P(A \\mid B)P(B)\\)\n\nUsing \\(P(A \\cap B) = P(B \\mid A)P(A)\\)\n\n\nUsing \\(P(A \\mid B)P(B)\\):\n\\[\nP(A \\cap B) = \\left(\\tfrac{1}{3}\\right)(0.30) = 0.10.\n\\]\nUsing \\(P(B \\mid A)P(A)\\):\n\\[\nP(A \\cap B) = (0.25)(0.40) = 0.10.\n\\]\nBoth methods agree with the given value:\n\\[\nP(A \\cap B) = 0.10.\n\\]\n\n\n\n\nSuppose you learn that the student selected does not like burgers. What is \\(P(A \\mid B^c)\\)?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe start with the definition of conditional probability:\n\\[\nP(A \\mid B^c) = \\frac{P(A \\cap B^c)}{P(B^c)}.\n\\]\nFirst compute the numerator (draw a venn diagram if this does not make sense):\n\\[\nP(A \\cap B^c) = P(A) - P(A \\cap B) = 0.40 - 0.10 = 0.30.\n\\]\nNow compute the denominator:\n\\[\nP(B^c) = 1 - P(B) = 1 - 0.30 = 0.70.\n\\]\nSubstitute into the formula:\n\\[\nP(A \\mid B^c) = \\frac{0.30}{0.70}.\n\\]\nSimplify:\n\\[\nP(A \\mid B^c) = \\tfrac{3}{7} \\approx 0.429.\n\\]\n\n\n\n\n(Stretch) If I select two students at random with replacement, what is the probability that both like pizza?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSince the selections are with replacement, the draws are independent.\n\\[\nP(\\text{both like pizza}) = P(A) \\times P(A).\n\\]\nSubstitute values:\n\\[\nP(\\text{both like pizza}) = (0.40)(0.40).\n\\]\nSimplify:\n\\[\nP(\\text{both like pizza}) = 0.16.\n\\]\n\n\n\n\n\n(Stretch) If I select two students at random without replacement, what is the probability that both like pizza?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWithout replacement, the draws are dependent.\nFirst student:\n\\[\nP(\\text{first likes pizza}) = \\tfrac{40}{100}.\n\\]\nSecond student (after one pizza-lover is removed):\n\\[\nP(\\text{second likes pizza} \\mid \\text{first likes pizza}) = \\tfrac{39}{99}.\n\\]\nMultiply:\n\\[\nP(\\text{both like pizza}) = \\tfrac{40}{100} \\cdot \\tfrac{39}{99}.\n\\]\nSimplify:\n\\[\nP(\\text{both like pizza}) = \\tfrac{1560}{9900} = \\tfrac{26}{165} \\approx 0.158.\n\\]\nThis is slightly less than the probability with replacement.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#tree-diagrams",
    "href": "MA206-AY26-1/lesson-4.html#tree-diagrams",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "We’ll model the sample space with a probability tree. Each edge is labeled by a (conditional) probability; each leaf is the product along its path and equals a joint probability.\nSetup (from Problem 1):\n- Out of 100 students: 40 like pizza, 30 like burgers, 10 like both.\n- Let \\(A\\) = “likes pizza”, \\(B\\) = “likes burgers”.\nFrom the counts:\n\\[\nP(A)=0.40,\\quad P(B)=0.30,\\quad P(A\\cap B)=0.10.\n\\] Useful conditionals:\n\\[\nP(B\\mid A)=\\frac{0.10}{0.40}=\\frac{1}{4}=0.25,\\qquad\nP(B\\mid A^c)=\\frac{P(B)-P(A\\cap B)}{P(A^c)}=\\frac{0.20}{0.60}=\\frac{1}{3}.\n\\] (And similarly \\(P(A\\mid B)=\\tfrac{1}{3}\\), \\(P(A\\mid B^c)=\\tfrac{3}{7}\\).)\n\n\n\nLeaves give \\(P(A\\cap B)\\), \\(P(A\\cap B^c)\\), \\(P(A^c\\cap B)\\), \\(P(A^c\\cap B^c)\\).\n\nStart\n├── A (0.40)\n│   ├── B       | P(B|A)=0.25  ⇒  P(A∩B)     = 0.40·0.25 = 0.10\n│   └── B^c     | P(B^c|A)=0.75          ⇒  P(A∩B^c)   = 0.40·0.75 = 0.30\n└── A^c (0.60)\n    ├── B       | P(B|A^c)=1/3 ⇒  P(A^c∩B)   = 0.60·(1/3) = 0.20\n    └── B^c     | P(B^c|A^c)=2/3          ⇒  P(A^c∩B^c) = 0.60·(2/3) = 0.40\nChecks: \\[\n\\begin{aligned}\n&\\text{Row sums: } 0.10+0.30=0.40=P(A),\\quad 0.20+0.40=0.60=P(A^c).\\\\\n&\\text{Column sums: } 0.10+0.20=0.30=P(B),\\quad 0.30+0.40=0.70=P(B^c).\n\\end{aligned}\n\\]\n\n\n\n\n\nSame joint probabilities appear; this is handy when a question conditions on \\(B\\).\n\nStart\n├── B (0.30)\n│   ├── A       | P(A|B)=1/3   ⇒  P(A∩B)     = 0.30·(1/3) = 0.10\n│   └── A^c     | 2/3          ⇒  P(A^c∩B)   = 0.30·(2/3) = 0.20\n└── B^c (0.70)\n    ├── A       | P(A|B^c)=3/7 ⇒  P(A∩B^c)   = 0.70·(3/7) = 0.30\n    └── A^c     | 4/7          ⇒  P(A^c∩B^c) = 0.70·(4/7) = 0.40\n\n\n\n\n\nJoint probability of a leaf = product along its path.\nExample: \\(P(A\\cap B)=0.40\\cdot 0.25=0.10\\).\nConditional probability = branch label.\nExample: From the first tree, \\(P(B\\mid A)=0.25\\).\n\nTotals: add leaves in the relevant region.\nExample: “Exactly one likes” \\(=P(A\\cap B^c)+P(A^c\\cap B)=0.30+0.20=0.50\\).\n\n\n\n\n\n\n\\(P(A\\mid B)=\\tfrac{1}{3}\\), \\(P(B\\mid A)=\\tfrac{1}{4}\\)\n\n\\(P(A\\cap B^c)=0.30\\), \\(P(A^c\\cap B)=0.20\\)\n\nExactly one likes pizza or burgers \\(=0.50\\)\n\n\\(P(A\\mid B^c)=\\tfrac{3}{7}\\)\n\n\nTip: Draw whichever tree matches the given condition first (branch on the given event); it makes reading the needed conditional straight from the edge label.\n\n\n\n\nProject Milestone 2 Annex B",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-4.html#before-you-leave",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nLesson 5\n\n\n\n\n\nTidyverse Tutorial: In Class Lesson 5\nProject Milestone 2: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html",
    "href": "MA206-AY26-1/lesson-2.html",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\nDon’t forget\n\n\n\n\n\n\n\n\n\n\nGenAI Assignment\n\n\n\n\n\n\n\n\n\n\n\nLet’s go to Canvas\n\n\n\n\n\n\nYour work will result in a presentation and a technical report.\nBoth these files are found on Canvas.\nUltimately, you are going to conduct a linear regression where you determine how much one variable impacted by other variables. For example:\n\n\nCode\nlibrary(tidyverse)\n\nmtcars |&gt; \n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(\n    title = \"Fuel Efficiency vs. Vehicle Weight\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\"\n  )\n\n\n\n\n\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i = 1, 2, \\dots, n\\)\n\n\nCode\nmtcars |&gt; \n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Fuel Efficiency vs. Vehicle Weight\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\" \n  )\n\n\n\n\n\n\n\n\n\nBut there might be multiple things that can impact our dependent variable.\n\n\nCode\nmtcars |&gt; \n  group_by(cyl) |&gt; \n  ggplot(aes(x = wt, y = mpg, color = as.factor(cyl))) +\n  geom_point() +\n  labs(\n    title = \"Fuel Efficiency vs. Vehicle Weight and Cylinders\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\",\n    color = \"Cylinders\"\n  )\n\n\n\n\n\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i,\n\\quad i = 1, 2, \\dots, n\\)\n\n\nCode\nmtcars |&gt; \n  group_by(cyl) |&gt; \n  ggplot(aes(x = wt, y = mpg, color = as.factor(cyl))) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Fuel Efficiency vs. Vehicle Weight and Cylinders\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\",\n    color = \"Cylinders\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis milestone sets up your binder and makes sure you have acceptable data.\nLets navigate to it on Canvas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis are just ideas on where to get data\n\nWe’ll talk about the Dataset Worksheet in a moment\n\n\n\n\n\n\nWhat is synthetic data?\n\n\nCode\nlibrary(tidyverse)\nset.seed(206)\nn &lt;- 300\n\ndf &lt;- \n  tibble(id = 1:n) |&gt; \n  mutate(# Categorical variables\n    sport  = as.factor(sample(c(\"Handball\", \"Lacrosse\", \"Track\"), size = n, replace = TRUE, prob = c(0.33, 0.34, 0.33))),\n    gender = as.factor(sample(c(\"Female\", \"Male\"), size = n, replace = TRUE, prob = c(0.45, 0.55)))\n  ) |&gt; \n  mutate(# Quantitative variables\n    height_cm = round(rnorm(n, mean = ifelse(gender == \"Male\", 178, 166) + ifelse(sport == \"Track\", -1.0, ifelse(sport == \"Lacrosse\", 0.5, 0)), sd = 6.5), 1),\n    training_hours = pmax(0, round(rnorm(n, mean = ifelse(sport == \"Track\", 7.5, ifelse(sport == \"Lacrosse\", 6.0, 5.5)), sd = 2.0), 1)\n    )\n  ) |&gt; \n  mutate(# Response variable (run time in minutes)\n    run_time_min = 16.5 + \n      ifelse(sport == \"Handball\", 1.0, \n             ifelse(sport == \"Lacrosse\", 0.6, -1.4)) + \n      ifelse(gender == \"Female\", 0.7, 0) + (-0.22) * training_hours + (0.015) * height_cm + \n      ifelse(sport == \"Track\", -0.05 * training_hours, 0) + rnorm(n, 0, 0.8), run_time_min = round(pmin(pmax(run_time_min, 11), 24), 2)\n       )\n\n# Quick peek\ndf |&gt;  slice_head(n = 10)\n\n\n# A tibble: 10 × 6\n      id sport    gender height_cm training_hours run_time_min\n   &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1     1 Handball Female      171.            5.7         18.3\n 2     2 Track    Male        183.            7.9         14.5\n 3     3 Handball Female      152.            6.5         18.7\n 4     4 Lacrosse Male        187.            5.8         19.3\n 5     5 Lacrosse Male        179.            5.8         19.2\n 6     6 Lacrosse Female      170.            6.8         19.1\n 7     7 Track    Female      160.            6.4         17.2\n 8     8 Handball Male        172.            6.6         19.6\n 9     9 Track    Male        165.            8.7         14.6\n10    10 Handball Female      160.            3.9         19.3\n\n\n\n\n\n\n\nDid we satisfy this?\n\n\n\n\n\nHow about this?\n\n\n\n\n\nOkay, now this?\n\n\n\n\n\nAnd finally, this!\n\n\n\n\n\n\n\n\nThis is where you show me you’ve met the criteria.\n\n\n\n\n\n\n\nCode\nggplot2::diamonds\n\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\nCode\nggplot2::diamonds |&gt; summary()\n\n\n     carat               cut        color        clarity          depth      \n Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065   Min.   :43.00  \n 1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258   1st Qu.:61.00  \n Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194   Median :61.80  \n Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171   Mean   :61.75  \n 3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066   3rd Qu.:62.50  \n Max.   :5.0100                     I: 5422   VVS1   : 3655   Max.   :79.00  \n                                    J: 2808   (Other): 2531                  \n     table           price             x                y         \n Min.   :43.00   Min.   :  326   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710   1st Qu.: 4.720  \n Median :57.00   Median : 2401   Median : 5.700   Median : 5.710  \n Mean   :57.46   Mean   : 3933   Mean   : 5.731   Mean   : 5.735  \n 3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540   3rd Qu.: 6.540  \n Max.   :95.00   Max.   :18823   Max.   :10.740   Max.   :58.900  \n                                                                  \n       z         \n Min.   : 0.000  \n 1st Qu.: 2.910  \n Median : 3.530  \n Mean   : 3.539  \n 3rd Qu.: 4.040  \n Max.   :31.800  \n                 \n\n\n\n\n\n\nI’m glad you asked!\n\n\n\n\n\n\n\nAnnex B Milestone 2 has you do the Tidyverse Tutorial on your own data\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 3\n\n\n\n\n\nProject Milestone 1: Due 22 Aug (Friday) All Sections\nGenAI Certification: Due 25 August (Monday) All Sections",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#welcome",
    "href": "MA206-AY26-1/lesson-2.html#welcome",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\nDon’t forget",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#genai-assignment-due-25-aug",
    "href": "MA206-AY26-1/lesson-2.html#genai-assignment-due-25-aug",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "GenAI Assignment",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#project-the-long-view",
    "href": "MA206-AY26-1/lesson-2.html#project-the-long-view",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "Let’s go to Canvas\n\n\n\n\n\n\nYour work will result in a presentation and a technical report.\nBoth these files are found on Canvas.\nUltimately, you are going to conduct a linear regression where you determine how much one variable impacted by other variables. For example:\n\n\nCode\nlibrary(tidyverse)\n\nmtcars |&gt; \n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(\n    title = \"Fuel Efficiency vs. Vehicle Weight\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\"\n  )\n\n\n\n\n\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i = 1, 2, \\dots, n\\)\n\n\nCode\nmtcars |&gt; \n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Fuel Efficiency vs. Vehicle Weight\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\" \n  )\n\n\n\n\n\n\n\n\n\nBut there might be multiple things that can impact our dependent variable.\n\n\nCode\nmtcars |&gt; \n  group_by(cyl) |&gt; \n  ggplot(aes(x = wt, y = mpg, color = as.factor(cyl))) +\n  geom_point() +\n  labs(\n    title = \"Fuel Efficiency vs. Vehicle Weight and Cylinders\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\",\n    color = \"Cylinders\"\n  )\n\n\n\n\n\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i,\n\\quad i = 1, 2, \\dots, n\\)\n\n\nCode\nmtcars |&gt; \n  group_by(cyl) |&gt; \n  ggplot(aes(x = wt, y = mpg, color = as.factor(cyl))) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Fuel Efficiency vs. Vehicle Weight and Cylinders\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\",\n    color = \"Cylinders\"\n  )",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#project-milestone-1",
    "href": "MA206-AY26-1/lesson-2.html#project-milestone-1",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "This milestone sets up your binder and makes sure you have acceptable data.\nLets navigate to it on Canvas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis are just ideas on where to get data\n\nWe’ll talk about the Dataset Worksheet in a moment\n\n\n\n\n\n\nWhat is synthetic data?\n\n\nCode\nlibrary(tidyverse)\nset.seed(206)\nn &lt;- 300\n\ndf &lt;- \n  tibble(id = 1:n) |&gt; \n  mutate(# Categorical variables\n    sport  = as.factor(sample(c(\"Handball\", \"Lacrosse\", \"Track\"), size = n, replace = TRUE, prob = c(0.33, 0.34, 0.33))),\n    gender = as.factor(sample(c(\"Female\", \"Male\"), size = n, replace = TRUE, prob = c(0.45, 0.55)))\n  ) |&gt; \n  mutate(# Quantitative variables\n    height_cm = round(rnorm(n, mean = ifelse(gender == \"Male\", 178, 166) + ifelse(sport == \"Track\", -1.0, ifelse(sport == \"Lacrosse\", 0.5, 0)), sd = 6.5), 1),\n    training_hours = pmax(0, round(rnorm(n, mean = ifelse(sport == \"Track\", 7.5, ifelse(sport == \"Lacrosse\", 6.0, 5.5)), sd = 2.0), 1)\n    )\n  ) |&gt; \n  mutate(# Response variable (run time in minutes)\n    run_time_min = 16.5 + \n      ifelse(sport == \"Handball\", 1.0, \n             ifelse(sport == \"Lacrosse\", 0.6, -1.4)) + \n      ifelse(gender == \"Female\", 0.7, 0) + (-0.22) * training_hours + (0.015) * height_cm + \n      ifelse(sport == \"Track\", -0.05 * training_hours, 0) + rnorm(n, 0, 0.8), run_time_min = round(pmin(pmax(run_time_min, 11), 24), 2)\n       )\n\n# Quick peek\ndf |&gt;  slice_head(n = 10)\n\n\n# A tibble: 10 × 6\n      id sport    gender height_cm training_hours run_time_min\n   &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1     1 Handball Female      171.            5.7         18.3\n 2     2 Track    Male        183.            7.9         14.5\n 3     3 Handball Female      152.            6.5         18.7\n 4     4 Lacrosse Male        187.            5.8         19.3\n 5     5 Lacrosse Male        179.            5.8         19.2\n 6     6 Lacrosse Female      170.            6.8         19.1\n 7     7 Track    Female      160.            6.4         17.2\n 8     8 Handball Male        172.            6.6         19.6\n 9     9 Track    Male        165.            8.7         14.6\n10    10 Handball Female      160.            3.9         19.3\n\n\n\n\n\n\n\nDid we satisfy this?\n\n\n\n\n\nHow about this?\n\n\n\n\n\nOkay, now this?\n\n\n\n\n\nAnd finally, this!\n\n\n\n\n\n\n\n\nThis is where you show me you’ve met the criteria.\n\n\n\n\n\n\n\nCode\nggplot2::diamonds\n\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\nCode\nggplot2::diamonds |&gt; summary()\n\n\n     carat               cut        color        clarity          depth      \n Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065   Min.   :43.00  \n 1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258   1st Qu.:61.00  \n Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194   Median :61.80  \n Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171   Mean   :61.75  \n 3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066   3rd Qu.:62.50  \n Max.   :5.0100                     I: 5422   VVS1   : 3655   Max.   :79.00  \n                                    J: 2808   (Other): 2531                  \n     table           price             x                y         \n Min.   :43.00   Min.   :  326   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710   1st Qu.: 4.720  \n Median :57.00   Median : 2401   Median : 5.700   Median : 5.710  \n Mean   :57.46   Mean   : 3933   Mean   : 5.731   Mean   : 5.735  \n 3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540   3rd Qu.: 6.540  \n Max.   :95.00   Max.   :18823   Max.   :10.740   Max.   :58.900  \n                                                                  \n       z         \n Min.   : 0.000  \n 1st Qu.: 2.910  \n Median : 3.530  \n Mean   : 3.539  \n 3rd Qu.: 4.040  \n Max.   :31.800",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#so-where-can-i-get-data",
    "href": "MA206-AY26-1/lesson-2.html#so-where-can-i-get-data",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "I’m glad you asked!",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-2.html#before-you-leave",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "Annex B Milestone 2 has you do the Tidyverse Tutorial on your own data\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 3\n\n\n\n\n\nProject Milestone 1: Due 22 Aug (Friday) All Sections\nGenAI Certification: Due 25 August (Monday) All Sections",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/index.html",
    "href": "MA206-AY26-1/index.html",
    "title": "Course Administration",
    "section": "",
    "text": "This page contains my instructor notes and lesson resources for our class.\nFor complete details, schedules, and additional materials, please visit our Canvas course page here: Canvas Course.\nLTC Dusty Turner",
    "crumbs": [
      "MA206-AY26-1",
      "Course Information"
    ]
  },
  {
    "objectID": "MA206-AY26-1/index.html#instructor-notes-and-class-resources",
    "href": "MA206-AY26-1/index.html#instructor-notes-and-class-resources",
    "title": "Course Administration",
    "section": "",
    "text": "This page contains my instructor notes and lesson resources for our class.\nFor complete details, schedules, and additional materials, please visit our Canvas course page here: Canvas Course.\nLTC Dusty Turner",
    "crumbs": [
      "MA206-AY26-1",
      "Course Information"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html",
    "href": "MA206-AY26-1/lesson-1.html",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "https://xkcd.com/552/\n \n\n\n\n\n\n\n\n\n\n\nPlease Share:\n\nName\n\nHometown\n\nCompany\n\nBirthday\n\nDid you come directly from high school?\n\nAcademic Major\nWhat you do in the Corps (Sport, Club, etc)\n\nFavorite sports team\n\nPossible Branch\n\nWhy you picked your seat today\n\n\n\n\n\n\nSandhurst\nOCF\nF4\nDallas Cowboys, San Antonio Spurs, Texas Rangers\n\n\n\n12345\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2003-2007 BS, Operations Research: United States Military Academy (USMA)\n\n2007-2008 Engineer Basic Officer Course: Fort Leonard Wood, Missouri\n\n2008-2011 Platoon Leader / XO / AS3: Schofield Barracks, HI / Iraq\n\n2011-2012 Engineer Captain’s Career Course: Missouri S&T\n\n2012 MS, Engineering Management: Missouri S&T\n\n2012-2014 Company Commander: White Sands Missile Range, NM / Afghanistan\n\n2014-2016 MS, Integrated Systems Engineering: The Ohio State University\n\n2016-2019 Assistant Professor: United States Military Academy, West Point\n\n2019-2022 ORSA / Data Scientist: Center for Army Analysis, Ft. Belvoir\n\n2022-2025 PhD, Statistical Science: Baylor University (Waco, TX)\n\n2025-? Academy Professor: United States Military Academy, West Point\n\n\n\n\n\n\nCode\nlibrary(leaflet)\n\nplaces &lt;- data.frame(\n  place = c(\"USMA (West Point, NY)\",\n            \"Schofield Barracks, HI\",\n            \"Missouri S&T (Rolla, MO)\",\n            \"White Sands Missile Range, NM\",\n            \"The Ohio State University (Columbus, OH)\",\n            \"Baylor University (Waco, TX)\",\n            \"Center for Army Analysis (Ft. Belvoir, VA)\"),\n  lat = c(41.391, 21.483, 37.954, 32.389, 39.999, 31.549, 38.711),\n  lng = c(-73.959, -158.063, -91.774, -106.491, -83.018, -97.114, -77.147)\n)\n\nleaflet(places) |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(\n    ~lng, ~lat,\n    radius = 6,\n    color = \"black\",\n    fillColor = \"red\",\n    fillOpacity = 0.85\n  ) |&gt;\n  addLabelOnlyMarkers(\n    ~lng, ~lat,\n    # label = ~place,\n    labelOptions = labelOptions(\n      noHide = TRUE,\n      direction = \"top\",\n      textOnly = TRUE,\n      style = list(\"color\" = \"black\", \"font-size\" = \"12px\", \"font-weight\" = \"bold\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000–2004 BS, Economics: Michigan State University\n\n2004–2010 Project Manager: Epic Systems\n\n2011–2020 Consultant / Build Analyst (Epic Radiant & Cadence)\n\n2011–2013 Epic Radiant Build Consultant: Intellistar Consulting\n\n2014 Epic Radiant Build Analyst: Vonlay\n\n2016–2017 Epic Radiant Build Analyst: Huron\n\n2018–2020 Epic Cadence Build Analyst: Bluetree Network\n\n\n2020–Present Solutions & Application Architect / Principal Analyst: Mayo Clinic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n123456789101112131415\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1234567891011121314151617\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\nDevelop a Base of Knowledge\n\nLeverage Technology\n\nCommunicate Concepts and Results\n\nProblem Solving Techniques\n\nDevelop habits of mind\n\nDevelop an interdisciplinary perspective\n\n\n\n\n\n\n\n\nArrive prepared for each lesson\n\nEncourage independent thinking\nMaintain professionalism and respect at all times\n\nUphold the values of the Corps and our institution\n\nClear guidance and expectations for assignments\n\nBe a professional mentor\n\nMake Mistakes\n\n\n\n\n\n\n\n\n\n\nBe responsible for your learning\nArrive prepared for each lesson\n\nEngage actively in discussions and exercises\n\nMaintain professionalism and respect at all times\n\nUphold the values of the Corps and our institution - you are junior members of this profession\n\nCommunicate early if challenges arise\n\nMake mistakes\n\n\n\n\n\nComputers will only be used for course materials only\n\nNo food or gum allowed in the classroom\n\nOnly drinks in spill-proof containers are allowed\n\nLeave bags, backpacks, coats, and hats in the hallway\n\nStay awake in class. Stand up if you are tired\n\nArrive on time and do not start packing up before I dismiss the class\n\nBe respectful when others are speaking\n\nSupport the section marcher\n\n\n\n\n\n\n\n\nProbability\n\nFoundations: preliminaries, dataset exploration, tidyverse basics\n\nCore Probability: principles, conditional probability, rules of random variables\n\nRandom Variables: discrete, continuous, and named distributions\n\nStatistical Tests\n\nOne-sample tests: one proportion Z-test, one mean T-test\n\nConfidence Intervals: categorical and quantitative data\n\nComparative tests: two proportion Z-test, two mean T-test, paired data\n\nBroader Concepts: generalization, causation, and investigation labs\n\nRegression\n\nCorrelation & Simple Linear Regression\n\nMultiple Linear Regression (I–III)\n\nApplications: project work, presentations, writer’s workshop, course review\n\n\n\n\n\n\nCanvas\nCalendar (Day 1) (Day 2)\nBook: Introduction to Statistical Investigations (Digital or hard copy authorized)\n\nCourse Guide\n\nCourse Admin\n\nSpecific Help / Instructions\n\nR Code\n\n\nGraded Assignments\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nEvent\nPoints\n\n\n\n\nDay 0 Assignment\n15\n\n\nGenerative AI Certification\n25\n\n\nExploration Exercises (6 @ 10 points each)\n60\n\n\nStatistical Investigation Labs: - SIL 1 (25) - SIL 2 (25)\n50\n\n\nWPR 1\n125\n\n\nWPR 2\n140\n\n\nCourse Project: - Milestone 1 – Setup and Data (25) - Milestone 2 – EDA (25) - Milestone 3 – Intro & Academic Articles (30) - Milestone 4 – Literature Review (25) - Milestone 5 – Methodology (30) - Milestone 6 – Results, Discussion, Conclusion (60) - In-Progress Review (20) - Presentation (50) - Milestone 7 – Writer’s Workshop (25) - Final Turn In (30)\n300\n\n\nTEE\n285\n\n\nTOTAL\n1000\n\n\n\n\n\n\nProject\nA note on course grades\nWhere to get this presentation\n\n\n\n\n\n\n\nNote\n\n\n\nFor assignments worth less than 20 points that are turned in late:\n50% reduction if turned in before Lesson 30 For assignments turned in late worth 20 points or more:\n10% reduction per day until assignment is worth 0 points (10 days late)\n\n\n\n\n\n\n\nAcademic Integrity Brief\n\nAcademic Security\n\n\n\n\n\n\n\n\n\n\n\n\nGo to Lesson 1 on Canvas\nNote the objectives\nDo the reading\nWatch the videos\nNote the Course Guide\n\nDo the Homework (If applicable)\n\n\n\n\n\n\n\n\n\n\n\n\nAsk a question\nDo basketball cadets tend to be taller than cadets who aren’t on the basketball team?\nDesign a study and collect data\nMeasure the heights of some basketball cadets and some non-basketball cadets.\n\n\n\nBasketball cadets: 71, 72, 73, 74, 75, 75, 75, 75, 76, 76, 76, 77, 77, 77, 78 \n\n\nNon-basketball cadets: 68, 68, 70, 70, 70, 70, 70, 70, 72, 73\n\n\n\nExplore the data\nAverage basketball cadet height: 75.1 in\nAverage non-basketball cadet height: 70.1 in\n\n\n\n\n\n\n\n\n\n\n\nDraw inferences\nThe basketball group is taller by about 5 inches in this sample.\nFormulate conclusions\nLooks like basketball cadets are taller.\nLook back and ahead\nYou could improve this by measuring more cadets or using a statistical test.\n\n\n\n\n\nAsk a question\nIf the host opens a goat door, are you better off sticking with your first door or switching to the other unopened door?\nDesign a study and collect data\nWe simulate the game many times and record whether “stay” or “switch” wins the car.\nExplore the data\nDraw inferences\nFormulate conclusions\nLook back and ahead\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 2\n\n\n\n\n\nProject Milestone 1: Due 22 Aug (Friday) All Sections - Read through this and come to class with questions\nGenAI Certification: Due 25 August (Monday) All Sections",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#welcome",
    "href": "MA206-AY26-1/lesson-1.html#welcome",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "https://xkcd.com/552/",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#introductions",
    "href": "MA206-AY26-1/lesson-1.html#introductions",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Please Share:\n\nName\n\nHometown\n\nCompany\n\nBirthday\n\nDid you come directly from high school?\n\nAcademic Major\nWhat you do in the Corps (Sport, Club, etc)\n\nFavorite sports team\n\nPossible Branch\n\nWhy you picked your seat today\n\n\n\n\n\n\nSandhurst\nOCF\nF4\nDallas Cowboys, San Antonio Spurs, Texas Rangers\n\n\n\n12345\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2003-2007 BS, Operations Research: United States Military Academy (USMA)\n\n2007-2008 Engineer Basic Officer Course: Fort Leonard Wood, Missouri\n\n2008-2011 Platoon Leader / XO / AS3: Schofield Barracks, HI / Iraq\n\n2011-2012 Engineer Captain’s Career Course: Missouri S&T\n\n2012 MS, Engineering Management: Missouri S&T\n\n2012-2014 Company Commander: White Sands Missile Range, NM / Afghanistan\n\n2014-2016 MS, Integrated Systems Engineering: The Ohio State University\n\n2016-2019 Assistant Professor: United States Military Academy, West Point\n\n2019-2022 ORSA / Data Scientist: Center for Army Analysis, Ft. Belvoir\n\n2022-2025 PhD, Statistical Science: Baylor University (Waco, TX)\n\n2025-? Academy Professor: United States Military Academy, West Point\n\n\n\n\n\n\nCode\nlibrary(leaflet)\n\nplaces &lt;- data.frame(\n  place = c(\"USMA (West Point, NY)\",\n            \"Schofield Barracks, HI\",\n            \"Missouri S&T (Rolla, MO)\",\n            \"White Sands Missile Range, NM\",\n            \"The Ohio State University (Columbus, OH)\",\n            \"Baylor University (Waco, TX)\",\n            \"Center for Army Analysis (Ft. Belvoir, VA)\"),\n  lat = c(41.391, 21.483, 37.954, 32.389, 39.999, 31.549, 38.711),\n  lng = c(-73.959, -158.063, -91.774, -106.491, -83.018, -97.114, -77.147)\n)\n\nleaflet(places) |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(\n    ~lng, ~lat,\n    radius = 6,\n    color = \"black\",\n    fillColor = \"red\",\n    fillOpacity = 0.85\n  ) |&gt;\n  addLabelOnlyMarkers(\n    ~lng, ~lat,\n    # label = ~place,\n    labelOptions = labelOptions(\n      noHide = TRUE,\n      direction = \"top\",\n      textOnly = TRUE,\n      style = list(\"color\" = \"black\", \"font-size\" = \"12px\", \"font-weight\" = \"bold\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000–2004 BS, Economics: Michigan State University\n\n2004–2010 Project Manager: Epic Systems\n\n2011–2020 Consultant / Build Analyst (Epic Radiant & Cadence)\n\n2011–2013 Epic Radiant Build Consultant: Intellistar Consulting\n\n2014 Epic Radiant Build Analyst: Vonlay\n\n2016–2017 Epic Radiant Build Analyst: Huron\n\n2018–2020 Epic Cadence Build Analyst: Bluetree Network\n\n\n2020–Present Solutions & Application Architect / Principal Analyst: Mayo Clinic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n123456789101112131415\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1234567891011121314151617",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#expectations",
    "href": "MA206-AY26-1/lesson-1.html#expectations",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Develop a Base of Knowledge\n\nLeverage Technology\n\nCommunicate Concepts and Results\n\nProblem Solving Techniques\n\nDevelop habits of mind\n\nDevelop an interdisciplinary perspective\n\n\n\n\n\n\n\n\nArrive prepared for each lesson\n\nEncourage independent thinking\nMaintain professionalism and respect at all times\n\nUphold the values of the Corps and our institution\n\nClear guidance and expectations for assignments\n\nBe a professional mentor\n\nMake Mistakes\n\n\n\n\n\n\n\n\n\n\nBe responsible for your learning\nArrive prepared for each lesson\n\nEngage actively in discussions and exercises\n\nMaintain professionalism and respect at all times\n\nUphold the values of the Corps and our institution - you are junior members of this profession\n\nCommunicate early if challenges arise\n\nMake mistakes\n\n\n\n\n\nComputers will only be used for course materials only\n\nNo food or gum allowed in the classroom\n\nOnly drinks in spill-proof containers are allowed\n\nLeave bags, backpacks, coats, and hats in the hallway\n\nStay awake in class. Stand up if you are tired\n\nArrive on time and do not start packing up before I dismiss the class\n\nBe respectful when others are speaking\n\nSupport the section marcher",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#course-overview-admin",
    "href": "MA206-AY26-1/lesson-1.html#course-overview-admin",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Probability\n\nFoundations: preliminaries, dataset exploration, tidyverse basics\n\nCore Probability: principles, conditional probability, rules of random variables\n\nRandom Variables: discrete, continuous, and named distributions\n\nStatistical Tests\n\nOne-sample tests: one proportion Z-test, one mean T-test\n\nConfidence Intervals: categorical and quantitative data\n\nComparative tests: two proportion Z-test, two mean T-test, paired data\n\nBroader Concepts: generalization, causation, and investigation labs\n\nRegression\n\nCorrelation & Simple Linear Regression\n\nMultiple Linear Regression (I–III)\n\nApplications: project work, presentations, writer’s workshop, course review\n\n\n\n\n\n\nCanvas\nCalendar (Day 1) (Day 2)\nBook: Introduction to Statistical Investigations (Digital or hard copy authorized)\n\nCourse Guide\n\nCourse Admin\n\nSpecific Help / Instructions\n\nR Code\n\n\nGraded Assignments\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nEvent\nPoints\n\n\n\n\nDay 0 Assignment\n15\n\n\nGenerative AI Certification\n25\n\n\nExploration Exercises (6 @ 10 points each)\n60\n\n\nStatistical Investigation Labs: - SIL 1 (25) - SIL 2 (25)\n50\n\n\nWPR 1\n125\n\n\nWPR 2\n140\n\n\nCourse Project: - Milestone 1 – Setup and Data (25) - Milestone 2 – EDA (25) - Milestone 3 – Intro & Academic Articles (30) - Milestone 4 – Literature Review (25) - Milestone 5 – Methodology (30) - Milestone 6 – Results, Discussion, Conclusion (60) - In-Progress Review (20) - Presentation (50) - Milestone 7 – Writer’s Workshop (25) - Final Turn In (30)\n300\n\n\nTEE\n285\n\n\nTOTAL\n1000\n\n\n\n\n\n\nProject\nA note on course grades\nWhere to get this presentation\n\n\n\n\n\n\n\nNote\n\n\n\nFor assignments worth less than 20 points that are turned in late:\n50% reduction if turned in before Lesson 30 For assignments turned in late worth 20 points or more:\n10% reduction per day until assignment is worth 0 points (10 days late)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#mandatory-and-important-briefings",
    "href": "MA206-AY26-1/lesson-1.html#mandatory-and-important-briefings",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Academic Integrity Brief\n\nAcademic Security",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#lesson-1",
    "href": "MA206-AY26-1/lesson-1.html#lesson-1",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Go to Lesson 1 on Canvas\nNote the objectives\nDo the reading\nWatch the videos\nNote the Course Guide\n\nDo the Homework (If applicable)\n\n\n\n\n\n\n\n\n\n\n\n\nAsk a question\nDo basketball cadets tend to be taller than cadets who aren’t on the basketball team?\nDesign a study and collect data\nMeasure the heights of some basketball cadets and some non-basketball cadets.\n\n\n\nBasketball cadets: 71, 72, 73, 74, 75, 75, 75, 75, 76, 76, 76, 77, 77, 77, 78 \n\n\nNon-basketball cadets: 68, 68, 70, 70, 70, 70, 70, 70, 72, 73\n\n\n\nExplore the data\nAverage basketball cadet height: 75.1 in\nAverage non-basketball cadet height: 70.1 in\n\n\n\n\n\n\n\n\n\n\n\nDraw inferences\nThe basketball group is taller by about 5 inches in this sample.\nFormulate conclusions\nLooks like basketball cadets are taller.\nLook back and ahead\nYou could improve this by measuring more cadets or using a statistical test.\n\n\n\n\n\nAsk a question\nIf the host opens a goat door, are you better off sticking with your first door or switching to the other unopened door?\nDesign a study and collect data\nWe simulate the game many times and record whether “stay” or “switch” wins the car.\nExplore the data\nDraw inferences\nFormulate conclusions\nLook back and ahead",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-1.html#before-you-leave",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nLesson 2\n\n\n\n\n\nProject Milestone 1: Due 22 Aug (Friday) All Sections - Read through this and come to class with questions\nGenAI Certification: Due 25 August (Monday) All Sections",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html",
    "href": "MA206-AY26-1/lesson-3.html",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\nAn experiment is a process or action that produces observable outcomes.\n\nIt must have at least two possible outcomes.\n\nEach repetition of the experiment under the same conditions may produce a different outcome.\n\nExample 1: Flipping a coin\nExample 2: Rolling a die\n\n\n\n\nThe sample space is the set of all possible outcomes of a random experiment.\n\\(S = \\{ \\text{Outcome 1}, \\text{Outcome 2}, \\dots, \\text{Outcome n} \\}\\)\n\n\n\n\n\n\nSample space for coin flip\n\n\n\n\n\n\\(S = \\{ \\text{Heads}, \\text{Tails} \\}\\)\n\n\n\n\n\n\n\n\n\nSample space for die roll\n\n\n\n\n\n\\(S = \\{ 1,2,3,4,5,6 \\}\\)\n\n\n\n\n\n\n\nAn event is a subset of the sample space \\(S\\) of an experiment. So… the outcome of an experiment.\nNotation: If \\(A\\) is an event, then \\(A \\subseteq S\\)\nExample 1: Coin lands heads\n\n\n\n\n\n\nEvent heads in the coin flip sample space\n\n\n\n\n\nEvent \\(A =\\) “coin lands heads”\n\\[A = \\{ \\text{Heads} \\}\\]\n\n\n\nExample 2: Die lands 1 or 2\n\n\n\n\n\n\nEvent 1 or 2 in the die roll sample space\n\n\n\n\n\nEvent \\(B =\\) “die lands 1 or 2”\n\\[B = \\{ 1, 2 \\}\\]\n\n\n\n\n\n\n\nThe complement of an event \\(A\\), denoted \\(A^c\\), is the event that \\(A\\) does not occur.\nExample 1: Coin flip lands not heads\n\n\n\n\n\n\nComplement of \\(A =\\) ‘coin lands heads’\n\n\n\n\n\n\\(A^c =\\) “coin lands tails”\n\\[A^c = \\{ \\text{Tails} \\}\\]\n\n\n\nExample 2: Die lands not 1 or 2\n\n\n\n\n\n\nComplement of \\(B =\\) ‘die lands 1 or 2’\n\n\n\n\n\n\\(B^c =\\) “die lands 3,4,5,6”\n\\[B^c = \\{ 3,4,5,6 \\}\\]\n\n\n\n\n\n\n\nThe intersection of two events \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is the event that both \\(A\\) and \\(B\\) occur at the same time.\nExample: Coin flip + die roll\n- \\(A =\\) coin lands heads\n- \\(B =\\) die shows 1 or 2\n\n\n\n\n\n\nWhat is \\(A \\cap B\\)\n\n\n\n\n\n\\(A \\cap B =\\) “coin lands heads and die shows 1 or 2”\n\\[A \\cap B = \\{ (H,1), (H,2) \\}\\]\n\n\n\n\n\n\n\nThe union of two events \\(A\\) and \\(B\\), denoted \\(A \\cup B\\), is the event that either \\(A\\) occurs, or \\(B\\) occurs, or both occur.\nExample: Coin flip + die roll\n- \\(A =\\) coin lands heads\n- \\(B =\\) die shows 1 or 2\n\n\n\n\n\n\nWhat is \\(A \\cup B\\)\n\n\n\n\n\n\\(A \\cup B = \\{ (H,1), (H,2), (H,3), (H,4), (H,5), (H,6), (T,1), (T,2) \\}\\)\n\n\n\n\n\n\n\n\n\n\nThe probability of an event \\(A\\), written \\(P(A)\\), is a number between \\(0\\) and \\(1\\) that measures the likelihood that \\(A\\) occurs.\nFor equally likely outcomes:\n\\[\nP(A) = \\frac{|A|}{|S|}\n\\]\nwhere:\n- \\(|A| =\\) number of outcomes in event \\(A\\)\n- \\(|S| =\\) number of outcomes in the sample space\nAll probabilities are between 0 and 1, inclusive, so the probability of an event \\(A\\) is \\(0 \\leq P(A) \\leq 1\\).\nExample 1: Coin flip\n\n\n\n\n\n\nWhat is the probability of heads\n\n\n\n\n\n\nSample space: \\(S = \\{\\text{Heads}, \\text{Tails}\\}\\), so \\(|S| = 2\\)\n\nEvent \\(A =\\) “coin lands heads” → \\(A = \\{\\text{Heads}\\}\\), so \\(|A| = 1\\)\n\n\\[\nP(A) = \\tfrac{|A|}{|S|} = \\tfrac{1}{2}\n\\]\n\n\n\nExample 2: Die roll\n\n\n\n\n\n\nWhat is the probability of rolling 1 or 2\n\n\n\n\n\n\nSample space: \\(S = \\{1,2,3,4,5,6\\}\\), so \\(|S| = 6\\)\n\nEvent \\(B =\\) “die shows 1 or 2” → \\(B = \\{1,2\\}\\), so \\(|B| = 2\\)\n\n\\[\nP(B) = \\tfrac{|B|}{|S|} = \\tfrac{2}{6} = \\tfrac{1}{3}\n\\]\n\n\n\n\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\nExample 1 (coin):\n\n\n\n\n\n\nWhat is the probability of not heads\n\n\n\n\n\n\n\\(P(\\text{A}) = \\tfrac{1}{2}\\)\n\nSo \\(P(A^c) = 1 - P(A) = 1 - \\tfrac{1}{2} = \\tfrac{1}{2}\\)\n\n\n\n\nExample 2 (die):\n\n\n\n\n\n\nWhat is the probability of not rolling 1 or 2\n\n\n\n\n\n\n\\(P(B) = \\tfrac{1}{3}\\)\n\nSo \\(P(B^c) = 1 - P(B) = 1 - \\tfrac{1}{3} = \\tfrac{2}{3}\\)\n\n\n\n\n\n\n\n\n\nIf all outcomes are equally likely: \\[\nP(A \\cap B) = \\frac{|A \\cap B|}{|S|}\n\\]\nExample 1 (coin + die):\n\n\n\n\n\n\nWhat is the probability the coin is heads AND the die is 1 or 2\n\n\n\n\n\n\nSample space size: \\(|S| = 12\\) (2 coin outcomes × 6 die outcomes)\n\n\\(A =\\) coin lands heads\n\n\\(B =\\) die shows 1 or 2\n\n\\(A \\cap B = \\{(H,1),(H,2)\\}\\), so \\(|A \\cap B| = 2\\)\n\n\\[\nP(A \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\n\\]\n\n\n\n\n\n\n\nIf all outcomes are equally likely: \\[\nP(A \\cup B) = \\frac{|A \\cup B|}{|S|}\n\\]\nExample 1 (coin + die):\n\n\n\n\n\n\nWhat is the probability the coin is heads OR the die is 1 or 2\n\n\n\n\n\n\nSample space size: \\(|S| = 12\\)\n\n\\(A =\\) coin lands heads\n\n\\(B =\\) die shows 1 or 2\n\n\\(A \\cup B = \\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2)\\}\\), so \\(|A \\cup B| = 8\\)\n\n\\[\nP(A \\cup B) = \\tfrac{8}{12} = \\tfrac{2}{3}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\n\n\n\n\n\\(A^c\\) (T)\n\n\n\n\n\nCol Total\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\\(P(A \\cap B)\\)\n\\(P(A \\cap B^c)\\)\n\\(P(A)\\)\n\n\n\\(A^c\\) (T)\n\\(P(A^c \\cap B)\\)\n\\(P(A^c \\cap B^c)\\)\n\\(P(A^c)\\)\n\n\nCol Total\n\\(P(B)\\)\n\\(P(B^c)\\)\n\\(1\\)\n\n\n\nReminder:\n- \\(A =\\) “coin lands heads”\n- \\(B =\\) “die shows 1 or 2”\n- \\(A^c =\\) “coin lands tails”\n- \\(B^c =\\) “die shows 3–6”\nThere are \\(|S|=12\\) equally likely outcomes for (coin, die).\n\n\n\n\n\\(A \\cap B\\) = \\(\\{(H,1),(H,2)\\}\\)\n\\(P(A \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\\)\n\\(A \\cap B^c\\) = \\(\\{(H,3),(H,4),(H,5),(H,6)\\}\\)\n\\(P(A \\cap B^c) = \\tfrac{4}{12} = \\tfrac{1}{3}\\)\n\\(A^c \\cap B\\) = \\(\\{(T,1),(T,2)\\}\\)\n\\(P(A^c \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\\)\n\\(A^c \\cap B^c\\) = \\(\\{(T,3),(T,4),(T,5),(T,6)\\}\\)\n\\(P(A^c \\cap B^c) = \\tfrac{4}{12} = \\tfrac{1}{3}\\)\n\n\n\n\n\n\\(P(A) = P(A \\cap B) + P(A \\cap B^c) = \\tfrac{1}{6} + \\tfrac{1}{3} = \\tfrac{1}{2}\\)\n\\(P(A^c) = P(A^c \\cap B) + P(A^c \\cap B^c) = \\tfrac{1}{6} + \\tfrac{1}{3} = \\tfrac{1}{2}\\)\n\\(P(B) = P(A \\cap B) + P(A^c \\cap B) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{1}{3}\\)\n\\(P(B^c) = P(A \\cap B^c) + P(A^c \\cap B^c) = \\tfrac{1}{3} + \\tfrac{1}{3} = \\tfrac{2}{3}\\)\nTotal: \\(P(S)=1\\)\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\\(\\tfrac{1}{6}\\)\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{1}{2}\\)\n\n\n\\(A^c\\) (T)\n\\(\\tfrac{1}{6}\\)\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{1}{2}\\)\n\n\nCol Total\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{2}{3}\\)\n\\(1\\)\n\n\n\n\n\n\n\n\n\n\n   A: Heads B: Die is 1 or 2 A ∩ B^c (H,3) (H,4) (H,5) (H,6) P = 1/3 A ∩ B (H,1) (H,2) P = 1/6 A^c ∩ B (T,1) (T,2) P = 1/6 (A^c ∩ B^c) (T,3) (T,4) (T,5) (T,6) P = 1/3\n\n\n\n\n\n\n\n\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n\nTwo events are mutually exclusive when it is impossible for both to happen at the same time.\nTherefore:\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\n\nExample 1 (coin + die):\n\n\n\n\n\n\nCompute \\(P(A \\cup B)\\) using the addition rule\n\n\n\n\n\n\n\\(P(A)=\\tfrac{1}{2}\\)\n\n\\(P(B)=\\tfrac{1}{3}\\)\n\\(P(A\\cap B)=\\tfrac{1}{6}\\)\n\nApply the rule: \\[\nP(A\\cup B)=\\tfrac{1}{2}+\\tfrac{1}{3}-\\tfrac{1}{6}=\\tfrac{2}{3}.\n\\]\nSet view: \\[\nA\\cup B=\\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2)\\},\\quad |A\\cup B|=8.\n\\]\n\n\n\n\n\n\n\n\n\n\nOut of 100 students:\n- 40 like pizza\n- 30 like burgers\n- 10 of those included above like both pizza and burgers\nExperiment: I select one student at random\nLet:\n- \\(A =\\) “student likes pizza”\n- \\(B =\\) “student likes burgers”\n\nWhat is \\(P(A)\\)?\n\nWhat is \\(P(B)\\)?\n\nWhat is \\(P(A \\cap B)\\)?\n\nWhat is \\(P(A \\cup B)\\) using the addition rule?\n\nWhat is \\(P(A^c)\\), the probability a student does not like pizza?\n\nWhat is \\(P(B^c)\\), the probability a student does not like burgers?\n\nWhat is \\(P(A^c \\cap B^c)\\), the probability a student likes neither?\n\nVerify your results using the 2×2 probability table.\n\nRepresent the results with a Venn diagram.\n\n\n\n\n\n\n\nAnswers Problem 1\n\n\n\n\n\n\n\\(P(A) = \\tfrac{40}{100} = 0.40\\)\n\n\\(P(B) = \\tfrac{30}{100} = 0.30\\)\n\n\\(P(A \\cap B) = \\tfrac{10}{100} = 0.10\\)\n\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = 0.40 + 0.30 - 0.10 = 0.60\\)\n\n\\(P(A^c) = 1 - P(A) = 1 - 0.40 = 0.60\\)\n\n\\(P(B^c) = 1 - P(B) = 1 - 0.30 = 0.70\\)\n\n\\(P(A^c \\cap B^c) = 1 - (A \\cup \\ B) = 1 - \\tfrac{60}{100} = \\tfrac{40}{100} = 0.40\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (burger)\n\\(B^c\\) (not burger)\nRow Total\n\n\n\n\n\\(A\\) (pizza)\n\\(\\tfrac{10}{100} = 0.10\\)\n\\(\\tfrac{30}{100} = 0.30\\)\n\\(\\tfrac{40}{100} = 0.40\\)\n\n\n\\(A^c\\) (not pizza)\n\\(\\tfrac{20}{100} = 0.20\\)\n\\(\\tfrac{40}{100} = 0.40\\)\n\\(\\tfrac{60}{100} = 0.60\\)\n\n\nCol Total\n\\(\\tfrac{30}{100} = 0.30\\)\n\\(\\tfrac{70}{100} = 0.70\\)\n\\(1\\)\n\n\n\n\n\n\n\n\n     A: Pizza B: Burger  A ∩ Bᶜ 30 students P = 0.30  A ∩ B 10 students P = 0.10  Aᶜ ∩ B 20 students P = 0.20  Aᶜ ∩ Bᶜ 40 students P = 0.40\n\n\n\n\n\n\n\n\n\nShade \\(A \\cap B^c\\)\n\n\n\n\n\n\nAnswer Problem 2\n\n\n\n\n\n\n\n\n         A B\n\n\n\n\n\n\n\n\nShade \\((A \\cup B)^c\\)\n\n\n\n\n\n\nAnswer Problem 3\n\n\n\n\n\n\n\n\n          A B\n\n\n\n\n\n\n\n\nShade \\(\\big( (A \\cup B)^c \\big) \\cup (A \\cap B)\\)\n\n\n\n\n\n\nAnswer Problem 4\n\n\n\n\n\n\n\n\n\n\n\n\nShade \\((A \\cup B)^c \\cap C\\)\n\n\n\n\n\n\nAnswer Problem 5\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s Simulate\n\n\nCode\nlibrary(tidyverse)\n\npath &lt;- \"C:/Users/dusty.turner/OneDrive - West Point/Teaching/courses/lesson3.csv\"\n\ndat &lt;- read_csv(path)\n\ndat |&gt; \n  count(coin, dice)  |&gt; \n  mutate(prob = n / iter) |&gt; \n  mutate(A = coin == \"heads\", B = dice %in% 1:2) |&gt; \n  group_by(A,B) |&gt; \n  summarise(prob = sum(prob))\n\n\n\n\nCode\nlibrary(tidyverse)\n\nflip_coin_roll_dice &lt;- function(iterations){\n  \n    coin &lt;- sample(c(\"heads\", \"tails\"),size = iterations, replace = TRUE)\n    dice &lt;- sample(c(1:6),size = iterations, replace = TRUE)\n    \n    tibble(coin = coin, dice)\n}\n\niter &lt;- 1000\n\nflip_coin_roll_dice(iterations = iter) |&gt; \n  count(coin, dice)  |&gt; \n  mutate(prob = n / iter) |&gt; \n  mutate(A = coin == \"heads\", B = dice %in% 1:2) |&gt; \n  group_by(A,B) |&gt; \n  summarise(prob = sum(prob))",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#welcome",
    "href": "MA206-AY26-1/lesson-3.html#welcome",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#events",
    "href": "MA206-AY26-1/lesson-3.html#events",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "An experiment is a process or action that produces observable outcomes.\n\nIt must have at least two possible outcomes.\n\nEach repetition of the experiment under the same conditions may produce a different outcome.\n\nExample 1: Flipping a coin\nExample 2: Rolling a die\n\n\n\n\nThe sample space is the set of all possible outcomes of a random experiment.\n\\(S = \\{ \\text{Outcome 1}, \\text{Outcome 2}, \\dots, \\text{Outcome n} \\}\\)\n\n\n\n\n\n\nSample space for coin flip\n\n\n\n\n\n\\(S = \\{ \\text{Heads}, \\text{Tails} \\}\\)\n\n\n\n\n\n\n\n\n\nSample space for die roll\n\n\n\n\n\n\\(S = \\{ 1,2,3,4,5,6 \\}\\)\n\n\n\n\n\n\n\nAn event is a subset of the sample space \\(S\\) of an experiment. So… the outcome of an experiment.\nNotation: If \\(A\\) is an event, then \\(A \\subseteq S\\)\nExample 1: Coin lands heads\n\n\n\n\n\n\nEvent heads in the coin flip sample space\n\n\n\n\n\nEvent \\(A =\\) “coin lands heads”\n\\[A = \\{ \\text{Heads} \\}\\]\n\n\n\nExample 2: Die lands 1 or 2\n\n\n\n\n\n\nEvent 1 or 2 in the die roll sample space\n\n\n\n\n\nEvent \\(B =\\) “die lands 1 or 2”\n\\[B = \\{ 1, 2 \\}\\]\n\n\n\n\n\n\n\nThe complement of an event \\(A\\), denoted \\(A^c\\), is the event that \\(A\\) does not occur.\nExample 1: Coin flip lands not heads\n\n\n\n\n\n\nComplement of \\(A =\\) ‘coin lands heads’\n\n\n\n\n\n\\(A^c =\\) “coin lands tails”\n\\[A^c = \\{ \\text{Tails} \\}\\]\n\n\n\nExample 2: Die lands not 1 or 2\n\n\n\n\n\n\nComplement of \\(B =\\) ‘die lands 1 or 2’\n\n\n\n\n\n\\(B^c =\\) “die lands 3,4,5,6”\n\\[B^c = \\{ 3,4,5,6 \\}\\]\n\n\n\n\n\n\n\nThe intersection of two events \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is the event that both \\(A\\) and \\(B\\) occur at the same time.\nExample: Coin flip + die roll\n- \\(A =\\) coin lands heads\n- \\(B =\\) die shows 1 or 2\n\n\n\n\n\n\nWhat is \\(A \\cap B\\)\n\n\n\n\n\n\\(A \\cap B =\\) “coin lands heads and die shows 1 or 2”\n\\[A \\cap B = \\{ (H,1), (H,2) \\}\\]\n\n\n\n\n\n\n\nThe union of two events \\(A\\) and \\(B\\), denoted \\(A \\cup B\\), is the event that either \\(A\\) occurs, or \\(B\\) occurs, or both occur.\nExample: Coin flip + die roll\n- \\(A =\\) coin lands heads\n- \\(B =\\) die shows 1 or 2\n\n\n\n\n\n\nWhat is \\(A \\cup B\\)\n\n\n\n\n\n\\(A \\cup B = \\{ (H,1), (H,2), (H,3), (H,4), (H,5), (H,6), (T,1), (T,2) \\}\\)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#probability",
    "href": "MA206-AY26-1/lesson-3.html#probability",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "The probability of an event \\(A\\), written \\(P(A)\\), is a number between \\(0\\) and \\(1\\) that measures the likelihood that \\(A\\) occurs.\nFor equally likely outcomes:\n\\[\nP(A) = \\frac{|A|}{|S|}\n\\]\nwhere:\n- \\(|A| =\\) number of outcomes in event \\(A\\)\n- \\(|S| =\\) number of outcomes in the sample space\nAll probabilities are between 0 and 1, inclusive, so the probability of an event \\(A\\) is \\(0 \\leq P(A) \\leq 1\\).\nExample 1: Coin flip\n\n\n\n\n\n\nWhat is the probability of heads\n\n\n\n\n\n\nSample space: \\(S = \\{\\text{Heads}, \\text{Tails}\\}\\), so \\(|S| = 2\\)\n\nEvent \\(A =\\) “coin lands heads” → \\(A = \\{\\text{Heads}\\}\\), so \\(|A| = 1\\)\n\n\\[\nP(A) = \\tfrac{|A|}{|S|} = \\tfrac{1}{2}\n\\]\n\n\n\nExample 2: Die roll\n\n\n\n\n\n\nWhat is the probability of rolling 1 or 2\n\n\n\n\n\n\nSample space: \\(S = \\{1,2,3,4,5,6\\}\\), so \\(|S| = 6\\)\n\nEvent \\(B =\\) “die shows 1 or 2” → \\(B = \\{1,2\\}\\), so \\(|B| = 2\\)\n\n\\[\nP(B) = \\tfrac{|B|}{|S|} = \\tfrac{2}{6} = \\tfrac{1}{3}\n\\]\n\n\n\n\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\nExample 1 (coin):\n\n\n\n\n\n\nWhat is the probability of not heads\n\n\n\n\n\n\n\\(P(\\text{A}) = \\tfrac{1}{2}\\)\n\nSo \\(P(A^c) = 1 - P(A) = 1 - \\tfrac{1}{2} = \\tfrac{1}{2}\\)\n\n\n\n\nExample 2 (die):\n\n\n\n\n\n\nWhat is the probability of not rolling 1 or 2\n\n\n\n\n\n\n\\(P(B) = \\tfrac{1}{3}\\)\n\nSo \\(P(B^c) = 1 - P(B) = 1 - \\tfrac{1}{3} = \\tfrac{2}{3}\\)\n\n\n\n\n\n\n\n\n\nIf all outcomes are equally likely: \\[\nP(A \\cap B) = \\frac{|A \\cap B|}{|S|}\n\\]\nExample 1 (coin + die):\n\n\n\n\n\n\nWhat is the probability the coin is heads AND the die is 1 or 2\n\n\n\n\n\n\nSample space size: \\(|S| = 12\\) (2 coin outcomes × 6 die outcomes)\n\n\\(A =\\) coin lands heads\n\n\\(B =\\) die shows 1 or 2\n\n\\(A \\cap B = \\{(H,1),(H,2)\\}\\), so \\(|A \\cap B| = 2\\)\n\n\\[\nP(A \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\n\\]\n\n\n\n\n\n\n\nIf all outcomes are equally likely: \\[\nP(A \\cup B) = \\frac{|A \\cup B|}{|S|}\n\\]\nExample 1 (coin + die):\n\n\n\n\n\n\nWhat is the probability the coin is heads OR the die is 1 or 2\n\n\n\n\n\n\nSample space size: \\(|S| = 12\\)\n\n\\(A =\\) coin lands heads\n\n\\(B =\\) die shows 1 or 2\n\n\\(A \\cup B = \\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2)\\}\\), so \\(|A \\cup B| = 8\\)\n\n\\[\nP(A \\cup B) = \\tfrac{8}{12} = \\tfrac{2}{3}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\n\n\n\n\n\\(A^c\\) (T)\n\n\n\n\n\nCol Total\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\\(P(A \\cap B)\\)\n\\(P(A \\cap B^c)\\)\n\\(P(A)\\)\n\n\n\\(A^c\\) (T)\n\\(P(A^c \\cap B)\\)\n\\(P(A^c \\cap B^c)\\)\n\\(P(A^c)\\)\n\n\nCol Total\n\\(P(B)\\)\n\\(P(B^c)\\)\n\\(1\\)\n\n\n\nReminder:\n- \\(A =\\) “coin lands heads”\n- \\(B =\\) “die shows 1 or 2”\n- \\(A^c =\\) “coin lands tails”\n- \\(B^c =\\) “die shows 3–6”\nThere are \\(|S|=12\\) equally likely outcomes for (coin, die).\n\n\n\n\n\\(A \\cap B\\) = \\(\\{(H,1),(H,2)\\}\\)\n\\(P(A \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\\)\n\\(A \\cap B^c\\) = \\(\\{(H,3),(H,4),(H,5),(H,6)\\}\\)\n\\(P(A \\cap B^c) = \\tfrac{4}{12} = \\tfrac{1}{3}\\)\n\\(A^c \\cap B\\) = \\(\\{(T,1),(T,2)\\}\\)\n\\(P(A^c \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\\)\n\\(A^c \\cap B^c\\) = \\(\\{(T,3),(T,4),(T,5),(T,6)\\}\\)\n\\(P(A^c \\cap B^c) = \\tfrac{4}{12} = \\tfrac{1}{3}\\)\n\n\n\n\n\n\\(P(A) = P(A \\cap B) + P(A \\cap B^c) = \\tfrac{1}{6} + \\tfrac{1}{3} = \\tfrac{1}{2}\\)\n\\(P(A^c) = P(A^c \\cap B) + P(A^c \\cap B^c) = \\tfrac{1}{6} + \\tfrac{1}{3} = \\tfrac{1}{2}\\)\n\\(P(B) = P(A \\cap B) + P(A^c \\cap B) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{1}{3}\\)\n\\(P(B^c) = P(A \\cap B^c) + P(A^c \\cap B^c) = \\tfrac{1}{3} + \\tfrac{1}{3} = \\tfrac{2}{3}\\)\nTotal: \\(P(S)=1\\)\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\\(\\tfrac{1}{6}\\)\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{1}{2}\\)\n\n\n\\(A^c\\) (T)\n\\(\\tfrac{1}{6}\\)\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{1}{2}\\)\n\n\nCol Total\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{2}{3}\\)\n\\(1\\)\n\n\n\n\n\n\n\n\n\n\n   A: Heads B: Die is 1 or 2 A ∩ B^c (H,3) (H,4) (H,5) (H,6) P = 1/3 A ∩ B (H,1) (H,2) P = 1/6 A^c ∩ B (T,1) (T,2) P = 1/6 (A^c ∩ B^c) (T,3) (T,4) (T,5) (T,6) P = 1/3",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#probability-rules",
    "href": "MA206-AY26-1/lesson-3.html#probability-rules",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n\nTwo events are mutually exclusive when it is impossible for both to happen at the same time.\nTherefore:\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\n\nExample 1 (coin + die):\n\n\n\n\n\n\nCompute \\(P(A \\cup B)\\) using the addition rule\n\n\n\n\n\n\n\\(P(A)=\\tfrac{1}{2}\\)\n\n\\(P(B)=\\tfrac{1}{3}\\)\n\\(P(A\\cap B)=\\tfrac{1}{6}\\)\n\nApply the rule: \\[\nP(A\\cup B)=\\tfrac{1}{2}+\\tfrac{1}{3}-\\tfrac{1}{6}=\\tfrac{2}{3}.\n\\]\nSet view: \\[\nA\\cup B=\\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2)\\},\\quad |A\\cup B|=8.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#board-problems",
    "href": "MA206-AY26-1/lesson-3.html#board-problems",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "Out of 100 students:\n- 40 like pizza\n- 30 like burgers\n- 10 of those included above like both pizza and burgers\nExperiment: I select one student at random\nLet:\n- \\(A =\\) “student likes pizza”\n- \\(B =\\) “student likes burgers”\n\nWhat is \\(P(A)\\)?\n\nWhat is \\(P(B)\\)?\n\nWhat is \\(P(A \\cap B)\\)?\n\nWhat is \\(P(A \\cup B)\\) using the addition rule?\n\nWhat is \\(P(A^c)\\), the probability a student does not like pizza?\n\nWhat is \\(P(B^c)\\), the probability a student does not like burgers?\n\nWhat is \\(P(A^c \\cap B^c)\\), the probability a student likes neither?\n\nVerify your results using the 2×2 probability table.\n\nRepresent the results with a Venn diagram.\n\n\n\n\n\n\n\nAnswers Problem 1\n\n\n\n\n\n\n\\(P(A) = \\tfrac{40}{100} = 0.40\\)\n\n\\(P(B) = \\tfrac{30}{100} = 0.30\\)\n\n\\(P(A \\cap B) = \\tfrac{10}{100} = 0.10\\)\n\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = 0.40 + 0.30 - 0.10 = 0.60\\)\n\n\\(P(A^c) = 1 - P(A) = 1 - 0.40 = 0.60\\)\n\n\\(P(B^c) = 1 - P(B) = 1 - 0.30 = 0.70\\)\n\n\\(P(A^c \\cap B^c) = 1 - (A \\cup \\ B) = 1 - \\tfrac{60}{100} = \\tfrac{40}{100} = 0.40\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (burger)\n\\(B^c\\) (not burger)\nRow Total\n\n\n\n\n\\(A\\) (pizza)\n\\(\\tfrac{10}{100} = 0.10\\)\n\\(\\tfrac{30}{100} = 0.30\\)\n\\(\\tfrac{40}{100} = 0.40\\)\n\n\n\\(A^c\\) (not pizza)\n\\(\\tfrac{20}{100} = 0.20\\)\n\\(\\tfrac{40}{100} = 0.40\\)\n\\(\\tfrac{60}{100} = 0.60\\)\n\n\nCol Total\n\\(\\tfrac{30}{100} = 0.30\\)\n\\(\\tfrac{70}{100} = 0.70\\)\n\\(1\\)\n\n\n\n\n\n\n\n\n     A: Pizza B: Burger  A ∩ Bᶜ 30 students P = 0.30  A ∩ B 10 students P = 0.10  Aᶜ ∩ B 20 students P = 0.20  Aᶜ ∩ Bᶜ 40 students P = 0.40\n\n\n\n\n\n\n\n\n\nShade \\(A \\cap B^c\\)\n\n\n\n\n\n\nAnswer Problem 2\n\n\n\n\n\n\n\n\n         A B\n\n\n\n\n\n\n\n\nShade \\((A \\cup B)^c\\)\n\n\n\n\n\n\nAnswer Problem 3\n\n\n\n\n\n\n\n\n          A B\n\n\n\n\n\n\n\n\nShade \\(\\big( (A \\cup B)^c \\big) \\cup (A \\cap B)\\)\n\n\n\n\n\n\nAnswer Problem 4\n\n\n\n\n\n\n\n\n\n\n\n\nShade \\((A \\cup B)^c \\cap C\\)\n\n\n\n\n\n\nAnswer Problem 5",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#simulation",
    "href": "MA206-AY26-1/lesson-3.html#simulation",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "Let’s Simulate\n\n\nCode\nlibrary(tidyverse)\n\npath &lt;- \"C:/Users/dusty.turner/OneDrive - West Point/Teaching/courses/lesson3.csv\"\n\ndat &lt;- read_csv(path)\n\ndat |&gt; \n  count(coin, dice)  |&gt; \n  mutate(prob = n / iter) |&gt; \n  mutate(A = coin == \"heads\", B = dice %in% 1:2) |&gt; \n  group_by(A,B) |&gt; \n  summarise(prob = sum(prob))\n\n\n\n\nCode\nlibrary(tidyverse)\n\nflip_coin_roll_dice &lt;- function(iterations){\n  \n    coin &lt;- sample(c(\"heads\", \"tails\"),size = iterations, replace = TRUE)\n    dice &lt;- sample(c(1:6),size = iterations, replace = TRUE)\n    \n    tibble(coin = coin, dice)\n}\n\niter &lt;- 1000\n\nflip_coin_roll_dice(iterations = iter) |&gt; \n  count(coin, dice)  |&gt; \n  mutate(prob = n / iter) |&gt; \n  mutate(A = coin == \"heads\", B = dice %in% 1:2) |&gt; \n  group_by(A,B) |&gt; \n  summarise(prob = sum(prob))",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  }
]