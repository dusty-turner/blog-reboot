[
  {
    "objectID": "MA206x-AY26-2/lesson-5.html",
    "href": "MA206x-AY26-2/lesson-5.html",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "",
    "text": "Math vs TBD\n\n\n\n\n\n\nPreviously 8-1\n\n\n\n\n\n\nTBD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLast lesson we learned how to quantify spread:\n\n\n\n\n\n\n\n\n\nMeasure\nFormula\nResistant?\nBest For\n\n\n\n\nRange\nMax - Min\nNo\nQuick summary\n\n\nIQR\n\\(Q_3 - Q_1\\)\nYes\nSkewed data/outliers\n\n\nVariance\n\\(s^2 = \\frac{\\sum(x_i-\\bar{x})^2}{n-1}\\)\nNo\nStatistical inference\n\n\nStd Dev\n\\(s = \\sqrt{s^2}\\)\nNo\nGeneral purpose\n\n\n\n\n\n\n\n\n\nKey Insight from Lesson 4\n\n\n\nReport IQR with the median (for skewed data or outliers). Report standard deviation with the mean (for symmetric data).\n\n\n\n\n\n\n\n\n\n\nExecute EDA using appropriate graphs and summaries\nJustify choices of displays for variable types\nCommunicate findings with clear, concise annotations\n\n\n\n\n\n\n\n\n\n\n\n\nVantage\n\n\n\nLet’s go to Vantage and work on our Exploratory Data Analysis.\n\n\n\n\n\n\n\n\nBackup Option\n\n\n\nIf Vantage isn’t working for you, use Google Colab instead.\n\n\n\n\n\n\n\n\nYou have the rest of class to work on your Exploratory Data Analysis assignment.\n\n\n\n\n\n\nEDA Requirements\n\n\n\nYour EDA should include:\n\nIntroduction: What is your dataset? What questions are you exploring?\nVisual Displays: Choose appropriate plots for your variable types\n\nCategorical: bar charts, frequency tables\nQuantitative: histograms, boxplots, stem-and-leaf, dotplots\n\nNumerical Summaries: Report appropriate statistics\n\nCenter: mean and/or median\nSpread: standard deviation and/or IQR\n\nDescription: Use S-C-S-O to describe your distributions\nFindings: What did you learn from your data?\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember\n\n\n\n\nMatch your display to your variable type\n\nUse relative frequency histograms when comparing groups of different sizes\nUse boxplots to compare distributions side-by-side\n\nMatch your summary statistics to your distribution shape\n\nSymmetric: report mean and standard deviation\nSkewed or outliers: report median and IQR\n\nDescribe every plot using S-C-S-O: Shape, Center, Spread, Outliers\nAnnotate your graphs: titles, axis labels, clear legends\n\n\n\n\n\n\n\nIf you get stuck:\n\nCheck your notes from Lessons 1-4\nAsk your neighbor\nRaise your hand - I’m here to help!\n\n\n\n\n\n\n\n\n\nLab day to work on Exploratory Data Analysis\nApply everything from Lessons 1-4\n\nAny questions?\n\n\n\n\nLesson 6: Probability Basics\n\nSample spaces and events\nBasic probability rules\nComplement and addition rules\n\n\n\n\n\n\nWebAssign 1.4 - Due before Lesson 6\nExploratory Data Analysis - Due Lesson 9\nWPR I - Lesson 16",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-5.html#lesson-4-review",
    "href": "MA206x-AY26-2/lesson-5.html#lesson-4-review",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "",
    "text": "Last lesson we learned how to quantify spread:\n\n\n\n\n\n\n\n\n\nMeasure\nFormula\nResistant?\nBest For\n\n\n\n\nRange\nMax - Min\nNo\nQuick summary\n\n\nIQR\n\\(Q_3 - Q_1\\)\nYes\nSkewed data/outliers\n\n\nVariance\n\\(s^2 = \\frac{\\sum(x_i-\\bar{x})^2}{n-1}\\)\nNo\nStatistical inference\n\n\nStd Dev\n\\(s = \\sqrt{s^2}\\)\nNo\nGeneral purpose\n\n\n\n\n\n\n\n\n\nKey Insight from Lesson 4\n\n\n\nReport IQR with the median (for skewed data or outliers). Report standard deviation with the mean (for symmetric data).",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-5.html#today-eda-lab",
    "href": "MA206x-AY26-2/lesson-5.html#today-eda-lab",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "",
    "text": "Execute EDA using appropriate graphs and summaries\nJustify choices of displays for variable types\nCommunicate findings with clear, concise annotations",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-5.html#go-to-vantage",
    "href": "MA206x-AY26-2/lesson-5.html#go-to-vantage",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "",
    "text": "Vantage\n\n\n\nLet’s go to Vantage and work on our Exploratory Data Analysis.\n\n\n\n\n\n\n\n\nBackup Option\n\n\n\nIf Vantage isn’t working for you, use Google Colab instead.",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-5.html#eda-assignment",
    "href": "MA206x-AY26-2/lesson-5.html#eda-assignment",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "",
    "text": "You have the rest of class to work on your Exploratory Data Analysis assignment.\n\n\n\n\n\n\nEDA Requirements\n\n\n\nYour EDA should include:\n\nIntroduction: What is your dataset? What questions are you exploring?\nVisual Displays: Choose appropriate plots for your variable types\n\nCategorical: bar charts, frequency tables\nQuantitative: histograms, boxplots, stem-and-leaf, dotplots\n\nNumerical Summaries: Report appropriate statistics\n\nCenter: mean and/or median\nSpread: standard deviation and/or IQR\n\nDescription: Use S-C-S-O to describe your distributions\nFindings: What did you learn from your data?\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember\n\n\n\n\nMatch your display to your variable type\n\nUse relative frequency histograms when comparing groups of different sizes\nUse boxplots to compare distributions side-by-side\n\nMatch your summary statistics to your distribution shape\n\nSymmetric: report mean and standard deviation\nSkewed or outliers: report median and IQR\n\nDescribe every plot using S-C-S-O: Shape, Center, Spread, Outliers\nAnnotate your graphs: titles, axis labels, clear legends\n\n\n\n\n\n\n\nIf you get stuck:\n\nCheck your notes from Lessons 1-4\nAsk your neighbor\nRaise your hand - I’m here to help!",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-5.html#before-you-leave",
    "href": "MA206x-AY26-2/lesson-5.html#before-you-leave",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "",
    "text": "Lab day to work on Exploratory Data Analysis\nApply everything from Lessons 1-4\n\nAny questions?\n\n\n\n\nLesson 6: Probability Basics\n\nSample spaces and events\nBasic probability rules\nComplement and addition rules\n\n\n\n\n\n\nWebAssign 1.4 - Due before Lesson 6\nExploratory Data Analysis - Due Lesson 9\nWPR I - Lesson 16",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-3.html",
    "href": "MA206x-AY26-2/lesson-3.html",
    "title": "Lesson 3: Measures of Location",
    "section": "",
    "text": "1234\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1234567\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath vs CME\n\n\n\n\n\n\nPreviously 6-2\n\n\n\n\n\n\n7-2\n\n\n\n\n\n\n\n\n\n\n\n\nMath vs Systems\n\n\n\n\n\n\nPreviously 4-0\n\n\n\n\n\n\n5-0\n\n\n\n\n\n\n\n\n\nMath vs Systems\n\n\n\n\n\n\nPreviously 5-0\n\n\n\n\n\n\n6-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVantage\n\n\n\nLet’s go to Vantage and see if things are working.\n\n\n\n\n\n\n\n\n\n\n\n\nData Collection\n\n\n\nLet’s collect some real data!\n\nFind your pulse (wrist or neck)\nWhen instructed, count your heartbeats for 15 seconds\nReport your count to the instructor\n\nWe’ll use this data to practice everything we’ve learned in Lessons 1-3:\n\nLesson 1: What is our population? What is our sample? Is pulse rate categorical or quantitative?\nLesson 2: Create visual displays (stem-and-leaf, dotplot, histogram) and describe the distribution using S-C-S-O\nLesson 3: Calculate measures of center (mean, median, mode) and discuss which best represents a “typical” pulse rate\n\n\n\n\n\n\n\n\n\nLast lesson we learned about visual displays for data:\n\n\n\n\n\n\n\n\nDisplay\nBest For\nPreserves Exact Values?\n\n\n\n\nStem-and-leaf\nSmall datasets (n &lt; 50)\nYes\n\n\nDotplot\nSmall datasets with repeated values\nYes\n\n\nHistogram (Frequency)\nAny size, raw counts\nNo\n\n\nHistogram (Relative Freq)\nComparing groups of different sizes\nNo\n\n\n\n\n\n\n\n\nShape: Symmetric, skewed left, skewed right, unimodal, bimodal?\nCenter: Where is the “typical” value?\nSpread: How much variability is there?\nOutliers: Any unusual observations?\n\nToday we formalize center with numerical measures.\n\n\n\n\n\n\n\n\nCalculate and interpret mean, median, and mode\nDetermine percentiles and quartiles\nCompare measures of center for different distributions\n\n\n\n\nDevore, Section 1.3: Measures of Location\n\n\n\n\n\n\n\nVisual displays show us the shape of a distribution. But we often need a single number to summarize “where the data is.”\nThree common measures:\n\nMean (\\(\\bar{x}\\)): The arithmetic average\nMedian (\\(\\tilde{x}\\)): The middle value\nMode: The most frequent value\n\n\n\n\n\nThe sample mean is the arithmetic average of all observations:\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\]\nProperties:\n\nUses every data point\nSensitive to outliers (not resistant)\nThe “balance point” of the distribution\n\n\n\n\n\nSuppose 7 cadets reported their study hours last week:\n\n\nStudy hours: 8, 10, 12, 9, 11, 8, 45 \n\n\nSum: 103 \n\n\nn: 7 \n\n\nMean: 14.71 hours\n\n\nIs 14.7 hours a good representation of a “typical” cadet’s study time?\n\n\n\n\n\n\nOutlier Effect\n\n\n\nOne cadet reported 45 hours. This outlier pulls the mean up significantly. Most cadets studied 8-12 hours, but the mean suggests nearly 15 hours.\n\n\n\n\n\n\nThe sample median (\\(\\tilde{x}\\)) is the middle value when data is ordered from smallest to largest.\nFinding the median:\n\nOrder the data from smallest to largest\nIf \\(n\\) is odd: median = middle value (position \\(\\frac{n+1}{2}\\))\nIf \\(n\\) is even: median = average of two middle values\n\n\n\n\n\nSame study hours data:\n\n\nOrdered data: 8, 8, 9, 10, 11, 12, 45 \n\n\nn = 7 (odd)\n\n\nMiddle position: 4 \n\n\nMedian: 10 hours\n\n\nThe median (10 hours) is much more representative of typical study time than the mean (14.7 hours)!\n\n\n\n\nThe mode is the value that occurs most frequently.\n\n\nStudy hours: 8, 10, 12, 9, 11, 8, 45 \n\n\nMode: 8 hours (appears twice)\n\n\nNotes on the mode:\n\nA dataset can have no mode, one mode (unimodal), or multiple modes (bimodal, multimodal)\nLess commonly used than mean or median\nMost useful for categorical data or identifying clusters\n\n\n\n\n\n\n\n\nMeasure\nFormula/Method\nResistant to Outliers?\nUses All Data?\n\n\n\n\nMean\n\\(\\bar{x} = \\frac{\\sum x_i}{n}\\)\nNo\nYes\n\n\nMedian\nMiddle value\nYes\nNo\n\n\nMode\nMost frequent\nYes\nNo\n\n\n\n\n\n\n\n\n\n\nDistribution Shape\nRelationship\nBest Measure\n\n\n\n\nSymmetric\nMean ≈ Median\nMean (or either)\n\n\nSkewed Right\nMean &gt; Median\nMedian\n\n\nSkewed Left\nMean &lt; Median\nMedian\n\n\nOutliers Present\nMean pulled toward outliers\nMedian\n\n\n\n\n\n\n\n\n\nRule of Thumb\n\n\n\nWhen in doubt, report both the mean and median. If they differ substantially, investigate why!\n\n\n\n\n\n\n\n\n\nThe \\(p\\)th percentile is a value such that approximately \\(p\\%\\) of the data falls at or below it.\nCommon percentiles:\n\n25th percentile = First quartile (\\(Q_1\\))\n50th percentile = Median (\\(Q_2\\))\n75th percentile = Third quartile (\\(Q_3\\))\n\n\n\n\n\nQuartiles divide ordered data into four equal parts:\n\n\\(Q_1\\) (25th percentile): 25% of data below, 75% above\n\\(Q_2\\) (50th percentile): The median\n\\(Q_3\\) (75th percentile): 75% of data below, 25% above\n\n\n\nData: 61, 63, 65, 66, 66, 69, 69, 70, 70, 71, 71, 71, 71, 71, 72, 72, 75, 75, 75, 77 \n\n\nQ1 (25th percentile): 68.25 \n\n\nQ2 (Median): 71 \n\n\nQ3 (75th percentile): 72 \n\n\n\n\n\n\nMethod (Devore’s approach):\n\nOrder the data from smallest to largest\nFind the median (\\(Q_2\\)) - this divides data into lower and upper halves\n\\(Q_1\\) = median of the lower half\n\\(Q_3\\) = median of the upper half\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent software may use slightly different algorithms for quartiles. The differences are usually small for large datasets.\n\n\n\n\n\n\nAPFT push-up scores for 12 cadets:\n\n\nOrdered data: 42, 45, 48, 49, 51, 52, 55, 58, 60, 63, 67, 72 \n\n\nn = 12 \n\n\nLower half: 42, 45, 48, 49, 51, 52 \n\n\nUpper half: 55, 58, 60, 63, 67, 72 \n\n\nQ1 = median of lower half = 48.5 \n\n\nQ2 = median of all data = 53.5 \n\n\nQ3 = median of upper half = 61.5 \n\n\n\n\n\n\nThe five-number summary provides a quick snapshot of a distribution:\n\nMinimum\nQ1 (25th percentile)\nMedian (Q2, 50th percentile)\nQ3 (75th percentile)\nMaximum\n\n\n\nFive-Number Summary:\n\n\nMin: 42 \n\n\nQ1: 48.75 \n\n\nMedian: 53.5 \n\n\nQ3: 60.75 \n\n\nMax: 72 \n\n\n\n\n\n\nA boxplot (box-and-whisker plot) visualizes the five-number summary:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nWhat It Shows\n\n\n\n\nBox width\nInterquartile range (IQR = Q3 - Q1) - middle 50% of data\n\n\nLine in box\nMedian\n\n\nWhiskers\nExtend to min/max (or 1.5×IQR from box)\n\n\nPoints beyond whiskers\nPotential outliers\n\n\n\n\n\n\n\n\n\n\nThe following data represents daily high temperatures (°F) for a week in January at West Point:\n28, 32, 35, 31, 29, 33, 30\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCalculate the mean temperature\nCalculate the median temperature\nWhich measure better represents a “typical” January day? Why?\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nMean = \\(\\frac{28 + 32 + 35 + 31 + 29 + 33 + 30}{7} = \\frac{218}{7} = 31.14°F\\)\nOrdered: 28, 29, 30, 31, 32, 33, 35 Median = 31°F\nBoth are appropriate here! The data is roughly symmetric with no outliers, so mean ≈ median. Either measure works well.\n\n\n\n\n\n\n\n\nAnnual salaries (in thousands) for employees at a small company:\n45, 48, 52, 55, 58, 62, 68, 72, 85, 250\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCalculate the mean salary\nCalculate the median salary\nWhich measure would you report as the “typical” salary? Why?\nFind Q1 and Q3\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nMean = \\(\\frac{45 + 48 + ... + 250}{10} = \\frac{795}{10} = 79.5\\) thousand ($79,500)\nOrdered data (already sorted), n = 10 (even) Middle values are positions 5 and 6: 58 and 62 Median = \\(\\frac{58 + 62}{2} = 60\\) thousand ($60,000)\nThe median ($60,000) is more representative. The CEO’s salary of $250,000 is an outlier that pulls the mean up to $79,500, which is higher than 80% of the employees actually earn.\nLower half: 45, 48, 52, 55, 58 → Q1 = 52 thousand Upper half: 62, 68, 72, 85, 250 → Q3 = 72 thousand\n\n\n\n\n\n\n\n\nTwo sections took the same quiz. Here are their scores:\nSection A: 72, 75, 78, 80, 82, 85, 88 Section B: 60, 75, 78, 80, 82, 85, 100\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCalculate the mean and median for each section\nWhich section performed better on average?\nWhy do the means and medians tell different stories for Section B?\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nSection A:\n\n\nMean = 560/7 = 80\nMedian = 80\n\nSection B: - Mean = 560/7 = 80 - Median = 80\n\nBoth sections have the same mean (80) and same median (80)!\nThis is a trick question - the means and medians are the same for both sections. However, Section B has more variability (spread from 60 to 100) compared to Section A (spread from 72 to 88). This illustrates why we need measures of spread (next lesson!) in addition to measures of center.\n\n\n\n\n\n\n\n\n\n\n\n\nMean (\\(\\bar{x}\\)): Arithmetic average; uses all data but sensitive to outliers\nMedian (\\(\\tilde{x}\\)): Middle value; resistant to outliers\nMode: Most frequent value; useful for categorical data\nPercentiles: Values that divide data into parts (e.g., 25th, 50th, 75th)\nQuartiles: Q1, Q2 (median), Q3 divide data into four parts\nFive-number summary: Min, Q1, Median, Q3, Max\n\n\n\n\n\n\n\nRemember\n\n\n\n\nFor symmetric distributions: Mean ≈ Median\nFor skewed distributions: Median is usually preferred\nAlways consider outliers when choosing which measure to report\n\n\n\n\n\n\n\n\n\n\n\nMean, median, and mode as measures of center\nWhen to use each measure\nPercentiles, quartiles, and the five-number summary\nBoxplots as visual summaries\n\nAny questions?\n\n\n\n\nLesson 4: Measures of Variability\n\nRange and interquartile range (IQR)\nVariance and standard deviation\nComparing spread across distributions\n\n\n\n\n\n\nWebAssign 1.4 - Due before Lesson 4\nExploratory Data Analysis - Due Lesson 9\nWPR I - Lesson 16",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-3.html#go-to-vantage",
    "href": "MA206x-AY26-2/lesson-3.html#go-to-vantage",
    "title": "Lesson 3: Measures of Location",
    "section": "",
    "text": "Vantage\n\n\n\nLet’s go to Vantage and see if things are working.",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-3.html#practical-exercise-pulse-rate",
    "href": "MA206x-AY26-2/lesson-3.html#practical-exercise-pulse-rate",
    "title": "Lesson 3: Measures of Location",
    "section": "",
    "text": "Data Collection\n\n\n\nLet’s collect some real data!\n\nFind your pulse (wrist or neck)\nWhen instructed, count your heartbeats for 15 seconds\nReport your count to the instructor\n\nWe’ll use this data to practice everything we’ve learned in Lessons 1-3:\n\nLesson 1: What is our population? What is our sample? Is pulse rate categorical or quantitative?\nLesson 2: Create visual displays (stem-and-leaf, dotplot, histogram) and describe the distribution using S-C-S-O\nLesson 3: Calculate measures of center (mean, median, mode) and discuss which best represents a “typical” pulse rate",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-3.html#lesson-2-review",
    "href": "MA206x-AY26-2/lesson-3.html#lesson-2-review",
    "title": "Lesson 3: Measures of Location",
    "section": "",
    "text": "Last lesson we learned about visual displays for data:\n\n\n\n\n\n\n\n\nDisplay\nBest For\nPreserves Exact Values?\n\n\n\n\nStem-and-leaf\nSmall datasets (n &lt; 50)\nYes\n\n\nDotplot\nSmall datasets with repeated values\nYes\n\n\nHistogram (Frequency)\nAny size, raw counts\nNo\n\n\nHistogram (Relative Freq)\nComparing groups of different sizes\nNo\n\n\n\n\n\n\n\n\nShape: Symmetric, skewed left, skewed right, unimodal, bimodal?\nCenter: Where is the “typical” value?\nSpread: How much variability is there?\nOutliers: Any unusual observations?\n\nToday we formalize center with numerical measures.",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-3.html#lesson-3-content",
    "href": "MA206x-AY26-2/lesson-3.html#lesson-3-content",
    "title": "Lesson 3: Measures of Location",
    "section": "",
    "text": "Calculate and interpret mean, median, and mode\nDetermine percentiles and quartiles\nCompare measures of center for different distributions\n\n\n\n\nDevore, Section 1.3: Measures of Location",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-3.html#measures-of-center",
    "href": "MA206x-AY26-2/lesson-3.html#measures-of-center",
    "title": "Lesson 3: Measures of Location",
    "section": "",
    "text": "Visual displays show us the shape of a distribution. But we often need a single number to summarize “where the data is.”\nThree common measures:\n\nMean (\\(\\bar{x}\\)): The arithmetic average\nMedian (\\(\\tilde{x}\\)): The middle value\nMode: The most frequent value\n\n\n\n\n\nThe sample mean is the arithmetic average of all observations:\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\]\nProperties:\n\nUses every data point\nSensitive to outliers (not resistant)\nThe “balance point” of the distribution\n\n\n\n\n\nSuppose 7 cadets reported their study hours last week:\n\n\nStudy hours: 8, 10, 12, 9, 11, 8, 45 \n\n\nSum: 103 \n\n\nn: 7 \n\n\nMean: 14.71 hours\n\n\nIs 14.7 hours a good representation of a “typical” cadet’s study time?\n\n\n\n\n\n\nOutlier Effect\n\n\n\nOne cadet reported 45 hours. This outlier pulls the mean up significantly. Most cadets studied 8-12 hours, but the mean suggests nearly 15 hours.\n\n\n\n\n\n\nThe sample median (\\(\\tilde{x}\\)) is the middle value when data is ordered from smallest to largest.\nFinding the median:\n\nOrder the data from smallest to largest\nIf \\(n\\) is odd: median = middle value (position \\(\\frac{n+1}{2}\\))\nIf \\(n\\) is even: median = average of two middle values\n\n\n\n\n\nSame study hours data:\n\n\nOrdered data: 8, 8, 9, 10, 11, 12, 45 \n\n\nn = 7 (odd)\n\n\nMiddle position: 4 \n\n\nMedian: 10 hours\n\n\nThe median (10 hours) is much more representative of typical study time than the mean (14.7 hours)!\n\n\n\n\nThe mode is the value that occurs most frequently.\n\n\nStudy hours: 8, 10, 12, 9, 11, 8, 45 \n\n\nMode: 8 hours (appears twice)\n\n\nNotes on the mode:\n\nA dataset can have no mode, one mode (unimodal), or multiple modes (bimodal, multimodal)\nLess commonly used than mean or median\nMost useful for categorical data or identifying clusters\n\n\n\n\n\n\n\n\nMeasure\nFormula/Method\nResistant to Outliers?\nUses All Data?\n\n\n\n\nMean\n\\(\\bar{x} = \\frac{\\sum x_i}{n}\\)\nNo\nYes\n\n\nMedian\nMiddle value\nYes\nNo\n\n\nMode\nMost frequent\nYes\nNo\n\n\n\n\n\n\n\n\n\n\nDistribution Shape\nRelationship\nBest Measure\n\n\n\n\nSymmetric\nMean ≈ Median\nMean (or either)\n\n\nSkewed Right\nMean &gt; Median\nMedian\n\n\nSkewed Left\nMean &lt; Median\nMedian\n\n\nOutliers Present\nMean pulled toward outliers\nMedian\n\n\n\n\n\n\n\n\n\nRule of Thumb\n\n\n\nWhen in doubt, report both the mean and median. If they differ substantially, investigate why!",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-3.html#percentiles-and-quartiles",
    "href": "MA206x-AY26-2/lesson-3.html#percentiles-and-quartiles",
    "title": "Lesson 3: Measures of Location",
    "section": "",
    "text": "The \\(p\\)th percentile is a value such that approximately \\(p\\%\\) of the data falls at or below it.\nCommon percentiles:\n\n25th percentile = First quartile (\\(Q_1\\))\n50th percentile = Median (\\(Q_2\\))\n75th percentile = Third quartile (\\(Q_3\\))\n\n\n\n\n\nQuartiles divide ordered data into four equal parts:\n\n\\(Q_1\\) (25th percentile): 25% of data below, 75% above\n\\(Q_2\\) (50th percentile): The median\n\\(Q_3\\) (75th percentile): 75% of data below, 25% above\n\n\n\nData: 61, 63, 65, 66, 66, 69, 69, 70, 70, 71, 71, 71, 71, 71, 72, 72, 75, 75, 75, 77 \n\n\nQ1 (25th percentile): 68.25 \n\n\nQ2 (Median): 71 \n\n\nQ3 (75th percentile): 72 \n\n\n\n\n\n\nMethod (Devore’s approach):\n\nOrder the data from smallest to largest\nFind the median (\\(Q_2\\)) - this divides data into lower and upper halves\n\\(Q_1\\) = median of the lower half\n\\(Q_3\\) = median of the upper half\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent software may use slightly different algorithms for quartiles. The differences are usually small for large datasets.\n\n\n\n\n\n\nAPFT push-up scores for 12 cadets:\n\n\nOrdered data: 42, 45, 48, 49, 51, 52, 55, 58, 60, 63, 67, 72 \n\n\nn = 12 \n\n\nLower half: 42, 45, 48, 49, 51, 52 \n\n\nUpper half: 55, 58, 60, 63, 67, 72 \n\n\nQ1 = median of lower half = 48.5 \n\n\nQ2 = median of all data = 53.5 \n\n\nQ3 = median of upper half = 61.5 \n\n\n\n\n\n\nThe five-number summary provides a quick snapshot of a distribution:\n\nMinimum\nQ1 (25th percentile)\nMedian (Q2, 50th percentile)\nQ3 (75th percentile)\nMaximum\n\n\n\nFive-Number Summary:\n\n\nMin: 42 \n\n\nQ1: 48.75 \n\n\nMedian: 53.5 \n\n\nQ3: 60.75 \n\n\nMax: 72 \n\n\n\n\n\n\nA boxplot (box-and-whisker plot) visualizes the five-number summary:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nWhat It Shows\n\n\n\n\nBox width\nInterquartile range (IQR = Q3 - Q1) - middle 50% of data\n\n\nLine in box\nMedian\n\n\nWhiskers\nExtend to min/max (or 1.5×IQR from box)\n\n\nPoints beyond whiskers\nPotential outliers",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-3.html#practice-problems",
    "href": "MA206x-AY26-2/lesson-3.html#practice-problems",
    "title": "Lesson 3: Measures of Location",
    "section": "",
    "text": "The following data represents daily high temperatures (°F) for a week in January at West Point:\n28, 32, 35, 31, 29, 33, 30\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCalculate the mean temperature\nCalculate the median temperature\nWhich measure better represents a “typical” January day? Why?\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nMean = \\(\\frac{28 + 32 + 35 + 31 + 29 + 33 + 30}{7} = \\frac{218}{7} = 31.14°F\\)\nOrdered: 28, 29, 30, 31, 32, 33, 35 Median = 31°F\nBoth are appropriate here! The data is roughly symmetric with no outliers, so mean ≈ median. Either measure works well.\n\n\n\n\n\n\n\n\nAnnual salaries (in thousands) for employees at a small company:\n45, 48, 52, 55, 58, 62, 68, 72, 85, 250\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCalculate the mean salary\nCalculate the median salary\nWhich measure would you report as the “typical” salary? Why?\nFind Q1 and Q3\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nMean = \\(\\frac{45 + 48 + ... + 250}{10} = \\frac{795}{10} = 79.5\\) thousand ($79,500)\nOrdered data (already sorted), n = 10 (even) Middle values are positions 5 and 6: 58 and 62 Median = \\(\\frac{58 + 62}{2} = 60\\) thousand ($60,000)\nThe median ($60,000) is more representative. The CEO’s salary of $250,000 is an outlier that pulls the mean up to $79,500, which is higher than 80% of the employees actually earn.\nLower half: 45, 48, 52, 55, 58 → Q1 = 52 thousand Upper half: 62, 68, 72, 85, 250 → Q3 = 72 thousand\n\n\n\n\n\n\n\n\nTwo sections took the same quiz. Here are their scores:\nSection A: 72, 75, 78, 80, 82, 85, 88 Section B: 60, 75, 78, 80, 82, 85, 100\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCalculate the mean and median for each section\nWhich section performed better on average?\nWhy do the means and medians tell different stories for Section B?\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nSection A:\n\n\nMean = 560/7 = 80\nMedian = 80\n\nSection B: - Mean = 560/7 = 80 - Median = 80\n\nBoth sections have the same mean (80) and same median (80)!\nThis is a trick question - the means and medians are the same for both sections. However, Section B has more variability (spread from 60 to 100) compared to Section A (spread from 72 to 88). This illustrates why we need measures of spread (next lesson!) in addition to measures of center.",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-3.html#summary",
    "href": "MA206x-AY26-2/lesson-3.html#summary",
    "title": "Lesson 3: Measures of Location",
    "section": "",
    "text": "Mean (\\(\\bar{x}\\)): Arithmetic average; uses all data but sensitive to outliers\nMedian (\\(\\tilde{x}\\)): Middle value; resistant to outliers\nMode: Most frequent value; useful for categorical data\nPercentiles: Values that divide data into parts (e.g., 25th, 50th, 75th)\nQuartiles: Q1, Q2 (median), Q3 divide data into four parts\nFive-number summary: Min, Q1, Median, Q3, Max\n\n\n\n\n\n\n\nRemember\n\n\n\n\nFor symmetric distributions: Mean ≈ Median\nFor skewed distributions: Median is usually preferred\nAlways consider outliers when choosing which measure to report",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-3.html#before-you-leave",
    "href": "MA206x-AY26-2/lesson-3.html#before-you-leave",
    "title": "Lesson 3: Measures of Location",
    "section": "",
    "text": "Mean, median, and mode as measures of center\nWhen to use each measure\nPercentiles, quartiles, and the five-number summary\nBoxplots as visual summaries\n\nAny questions?\n\n\n\n\nLesson 4: Measures of Variability\n\nRange and interquartile range (IQR)\nVariance and standard deviation\nComparing spread across distributions\n\n\n\n\n\n\nWebAssign 1.4 - Due before Lesson 4\nExploratory Data Analysis - Due Lesson 9\nWPR I - Lesson 16",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-1.html",
    "href": "MA206x-AY26-2/lesson-1.html",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Section Marcher Duties:\n\nTake roll and report attendance\nDesignate a different inspector each day (find a system - rotate alphabetically?)\nEveryone will do this multiple times\n\nInspections:\n\nPolite check of uniforms - the idea is to help each other look right\nNo one is getting in trouble. We’re just helping each other avoid mistakes.\nYou can do the inspection before class starts\n\n\n\n\n\n\nThis is MA206x, not MA206. Some trepidation? Sure. Fear? None needed.\n\nTintle → Devore: Simulation-first approach → classical probability foundation\nFewer out-of-class graded events: AI is changing how we assess learning\nR in Vantage: Same language, new platform\n\nMore details later.\n\n\n\n\n\n\n\n\n\n\n\n\nPlease Share\n\n\n\n\n\n\nName\nHometown\nCompany\nBirthday\nAcademic Major\n\n\n\nDid you come directly from high school?\nWhat you do in the Corps (Sport, Club, etc)\nFavorite sports team\nPossible Branch\nWhy you picked your seat today\n\n\n\n\n\n\n\n\n\n\n\nSprint Football\nSandhurst\nOCF\nF4\nDallas Cowboys, San Antonio Spurs, Texas Rangers\n\n\n\n12345\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCareer Timeline\n\n\n\n\n\n\n\n2003-2007 BS, Operations Research: United States Military Academy (USMA)\n2007-2008 Engineer Basic Officer Course: Fort Leonard Wood, Missouri\n2008-2011 Platoon Leader / XO / AS3: Schofield Barracks, HI / Iraq\n2011-2012 Engineer Captain’s Career Course: Missouri S&T\n2012 MS, Engineering Management: Missouri S&T\n2012-2014 Company Commander: White Sands Missile Range, NM / Afghanistan\n2014-2016 MS, Integrated Systems Engineering: The Ohio State University\n2016-2019 Assistant Professor: United States Military Academy, West Point\n2019-2022 ORSA / Data Scientist: Center for Army Analysis, Ft. Belvoir\n2022-2025 PhD, Statistical Science: Baylor University (Waco, TX)\n2025-? Academy Professor: United States Military Academy, West Point\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nCareer Timeline\n\n\n\n\n\n\n\n2000–2004 BS, Economics: Michigan State University\n2004–2010 Project Manager: Epic Systems\n2011–2020 Consultant / Build Analyst (Epic Radiant & Cadence)\n\n2011–2013 Epic Radiant Build Consultant: Intellistar Consulting\n2014 Epic Radiant Build Analyst: Vonlay\n2016–2017 Epic Radiant Build Analyst: Huron\n2018–2020 Epic Cadence Build Analyst: Bluetree Network\n\n2020–Present Solutions & Application Architect / Principal Analyst: Mayo Clinic\n\n\n\n\n\n\n \n\n\n\n\n\n\n12345678910111213141516\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12345678910111213141516171819202122\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Goals\n\n\n\n\nDevelop a Base of Knowledge\nLeverage Technology and Army Platforms\nCommunicate Concepts and Results\nProblem Solving Techniques\nDevelop Habits of Mind\nDevelop an Interdisciplinary Perspective\nResponsible use of AI\n\n\n\n\n\n\n\n\n\n\n(Open discussion)\n\n\n\n\n\n\n\n\n\n\nMy Commitments\n\n\n\n\n\n\nArrive prepared for each lesson\nEncourage independent thinking\nMaintain professionalism and respect at all times\nUphold the values of the Corps and our institution\nClear guidance and expectations for assignments\nBe a professional mentor\nMake Mistakes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Responsibilities\n\n\n\n\n\n\nBe responsible for your learning\nArrive prepared for each lesson\nEngage actively in discussions and exercises\nMaintain professionalism and respect at all times\nUphold the values of the Corps and our institution - you are junior members of this profession \nMake mistakes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassroom Policies\n\n\n\n\n\n\nPolicy\nDetails\n\n\n\n\nComputers\nCourse materials only\n\n\nFood & Gum\nNot allowed in classroom\n\n\nDrinks\nSpill-proof containers only\n\n\nBags & Gear\nLeave in the hallway\n\n\nStaying Alert\nStand up if you’re tired\n\n\nPunctuality\nArrive on time; don’t pack up early\n\n\nLeadership\nSupport the section marcher\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlock 1: Foundations (WPR I)\n\nDescriptive Statistics - Types of data, sampling, measures of location & variability, EDA\nProbability - Basics, conditional probability, counting & independence\nDistributions - Discrete (binomial, Poisson) and continuous (normal, exponential)\n\nBlock 2: Inference (WPR II)\n\nInference Foundations - Central limit theorem, confidence intervals\nHypothesis Testing - One-sample t-test, one-proportion z-test, two-sample t-test, paired data, two-proportion z-test\n\nBlock 3: Modeling (Project & TEE)\n\nRegression - Simple linear regression, multiple linear regression, causality & confounding\nANOVA\n\n\n\n\n\n\n\n\n\n\n\nResources\n\n\n\n\n\n\nResource\nLink\n\n\n\n\nCanvas\nMA206x Canvas\n\n\nCengage WebAssign\nCengage WebAssign\n\n\nArmy Vantage\nArmy Vantage\n\n\nCalendar\nCourse Calendar\n\n\nTextbook\nDevore: Probability and Statistics for Engineering and the Sciences\n\n\nSyllabus\nCourse Syllabus\n\n\nDocumentation Brief\nDoc of Sources, Ack of Assistance, Cover Page\n\n\nAcademic Security Brief\nAcademic Security AY26-2\n\n\n\n\n\n\n\n\n\n\nAI is welcome and encouraged on WebAssign. Screenshot the question… maybe.\nBut when WPR time comes - no AI. So use it as a tool for learning, not a crutch.\nWebAssign is 15% of the course. You should get most/all those points.\nDon’t get those points at the peril of your WPR grades - that’s 60% of the course.\nWebAssign is forgiving: 5 tries per sub-question.\nDon’t forget documentation. Just tell me what you did.\n\n\n\n\n\n\n\n\nAssignment\nPoints\n\n\n\n\nWebAssign Homework\n150\n\n\nWPR I\n175\n\n\nWPR II\n175\n\n\nExploratory Data Analysis\n25\n\n\nTech Report\n125\n\n\nProject Presentation\n50\n\n\nTEE\n300\n\n\nTotal\n1000\n\n\n\n\n\n\n\n\n\n\nWhy do we collect data? Because we want to learn about something bigger than what we can directly observe.\n\nPopulation: The entire group we want to learn about\nSample: The subset we actually observe\nProcess: An ongoing mechanism that generates data over time\n\nThe whole course builds on this: we use samples to make claims about populations. That’s inference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation\nSample\n\n\n\n\nWhat we have\nUsually unknown\nObservable data\n\n\nNotation\nGreek letters (\\(\\mu\\), \\(\\sigma\\), \\(p\\))\nLatin letters (\\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\))\n\n\n\n\nA statistic estimates a parameter. This is the foundation of Blocks 2 and 3.\n\n\n\n\n\nCategorical: Labels or categories (e.g., branch, major, yes/no)\nQuantitative: Numbers with meaningful arithmetic (e.g., height, GPA, time)\n\nWhy does this matter? The type of variable determines:\n\nHow you summarize it (proportions vs means)\nHow you visualize it (bar charts vs histograms)\nWhich inference method you use (z-test for proportions vs t-test for means)\n\n\n\n\n\n\n\n\n\nPopulation vs Sample vs Process\nParameters vs Statistics\nCategorical vs Quantitative variables\n\nAny questions?\n\n\n\n\nLesson 2: Sampling & Study Design\n\nObservational studies vs designed experiments\nCommon sampling methods and biases\nWhy randomization matters\n\n\n\n\n\n\nWebAssign 1.2 - Due before Lesson 2\nExploratory Data Analysis - Due Lesson 9\nWPR I - Lesson 16",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-1.html#welcome",
    "href": "MA206x-AY26-2/lesson-1.html#welcome",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Section Marcher Duties:\n\nTake roll and report attendance\nDesignate a different inspector each day (find a system - rotate alphabetically?)\nEveryone will do this multiple times\n\nInspections:\n\nPolite check of uniforms - the idea is to help each other look right\nNo one is getting in trouble. We’re just helping each other avoid mistakes.\nYou can do the inspection before class starts",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-1.html#whats-different-about-ma206x",
    "href": "MA206x-AY26-2/lesson-1.html#whats-different-about-ma206x",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "This is MA206x, not MA206. Some trepidation? Sure. Fear? None needed.\n\nTintle → Devore: Simulation-first approach → classical probability foundation\nFewer out-of-class graded events: AI is changing how we assess learning\nR in Vantage: Same language, new platform\n\nMore details later.",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-1.html#introductions",
    "href": "MA206x-AY26-2/lesson-1.html#introductions",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Please Share\n\n\n\n\n\n\nName\nHometown\nCompany\nBirthday\nAcademic Major\n\n\n\nDid you come directly from high school?\nWhat you do in the Corps (Sport, Club, etc)\nFavorite sports team\nPossible Branch\nWhy you picked your seat today\n\n\n\n\n\n\n\n\n\n\n\nSprint Football\nSandhurst\nOCF\nF4\nDallas Cowboys, San Antonio Spurs, Texas Rangers\n\n\n\n12345\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCareer Timeline\n\n\n\n\n\n\n\n2003-2007 BS, Operations Research: United States Military Academy (USMA)\n2007-2008 Engineer Basic Officer Course: Fort Leonard Wood, Missouri\n2008-2011 Platoon Leader / XO / AS3: Schofield Barracks, HI / Iraq\n2011-2012 Engineer Captain’s Career Course: Missouri S&T\n2012 MS, Engineering Management: Missouri S&T\n2012-2014 Company Commander: White Sands Missile Range, NM / Afghanistan\n2014-2016 MS, Integrated Systems Engineering: The Ohio State University\n2016-2019 Assistant Professor: United States Military Academy, West Point\n2019-2022 ORSA / Data Scientist: Center for Army Analysis, Ft. Belvoir\n2022-2025 PhD, Statistical Science: Baylor University (Waco, TX)\n2025-? Academy Professor: United States Military Academy, West Point\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nCareer Timeline\n\n\n\n\n\n\n\n2000–2004 BS, Economics: Michigan State University\n2004–2010 Project Manager: Epic Systems\n2011–2020 Consultant / Build Analyst (Epic Radiant & Cadence)\n\n2011–2013 Epic Radiant Build Consultant: Intellistar Consulting\n2014 Epic Radiant Build Analyst: Vonlay\n2016–2017 Epic Radiant Build Analyst: Huron\n2018–2020 Epic Cadence Build Analyst: Bluetree Network\n\n2020–Present Solutions & Application Architect / Principal Analyst: Mayo Clinic\n\n\n\n\n\n\n \n\n\n\n\n\n\n12345678910111213141516\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12345678910111213141516171819202122",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-1.html#expectations",
    "href": "MA206x-AY26-2/lesson-1.html#expectations",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Course Goals\n\n\n\n\nDevelop a Base of Knowledge\nLeverage Technology and Army Platforms\nCommunicate Concepts and Results\nProblem Solving Techniques\nDevelop Habits of Mind\nDevelop an Interdisciplinary Perspective\nResponsible use of AI\n\n\n\n\n\n\n\n\n\n\n(Open discussion)\n\n\n\n\n\n\n\n\n\n\nMy Commitments\n\n\n\n\n\n\nArrive prepared for each lesson\nEncourage independent thinking\nMaintain professionalism and respect at all times\nUphold the values of the Corps and our institution\nClear guidance and expectations for assignments\nBe a professional mentor\nMake Mistakes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Responsibilities\n\n\n\n\n\n\nBe responsible for your learning\nArrive prepared for each lesson\nEngage actively in discussions and exercises\nMaintain professionalism and respect at all times\nUphold the values of the Corps and our institution - you are junior members of this profession \nMake mistakes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassroom Policies\n\n\n\n\n\n\nPolicy\nDetails\n\n\n\n\nComputers\nCourse materials only\n\n\nFood & Gum\nNot allowed in classroom\n\n\nDrinks\nSpill-proof containers only\n\n\nBags & Gear\nLeave in the hallway\n\n\nStaying Alert\nStand up if you’re tired\n\n\nPunctuality\nArrive on time; don’t pack up early\n\n\nLeadership\nSupport the section marcher",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-1.html#course-overview",
    "href": "MA206x-AY26-2/lesson-1.html#course-overview",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Block 1: Foundations (WPR I)\n\nDescriptive Statistics - Types of data, sampling, measures of location & variability, EDA\nProbability - Basics, conditional probability, counting & independence\nDistributions - Discrete (binomial, Poisson) and continuous (normal, exponential)\n\nBlock 2: Inference (WPR II)\n\nInference Foundations - Central limit theorem, confidence intervals\nHypothesis Testing - One-sample t-test, one-proportion z-test, two-sample t-test, paired data, two-proportion z-test\n\nBlock 3: Modeling (Project & TEE)\n\nRegression - Simple linear regression, multiple linear regression, causality & confounding\nANOVA\n\n\n\n\n\n\n\n\n\n\n\nResources\n\n\n\n\n\n\nResource\nLink\n\n\n\n\nCanvas\nMA206x Canvas\n\n\nCengage WebAssign\nCengage WebAssign\n\n\nArmy Vantage\nArmy Vantage\n\n\nCalendar\nCourse Calendar\n\n\nTextbook\nDevore: Probability and Statistics for Engineering and the Sciences\n\n\nSyllabus\nCourse Syllabus\n\n\nDocumentation Brief\nDoc of Sources, Ack of Assistance, Cover Page\n\n\nAcademic Security Brief\nAcademic Security AY26-2\n\n\n\n\n\n\n\n\n\n\nAI is welcome and encouraged on WebAssign. Screenshot the question… maybe.\nBut when WPR time comes - no AI. So use it as a tool for learning, not a crutch.\nWebAssign is 15% of the course. You should get most/all those points.\nDon’t get those points at the peril of your WPR grades - that’s 60% of the course.\nWebAssign is forgiving: 5 tries per sub-question.\nDon’t forget documentation. Just tell me what you did.\n\n\n\n\n\n\n\n\nAssignment\nPoints\n\n\n\n\nWebAssign Homework\n150\n\n\nWPR I\n175\n\n\nWPR II\n175\n\n\nExploratory Data Analysis\n25\n\n\nTech Report\n125\n\n\nProject Presentation\n50\n\n\nTEE\n300\n\n\nTotal\n1000",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-1.html#todays-lesson",
    "href": "MA206x-AY26-2/lesson-1.html#todays-lesson",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Why do we collect data? Because we want to learn about something bigger than what we can directly observe.\n\nPopulation: The entire group we want to learn about\nSample: The subset we actually observe\nProcess: An ongoing mechanism that generates data over time\n\nThe whole course builds on this: we use samples to make claims about populations. That’s inference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation\nSample\n\n\n\n\nWhat we have\nUsually unknown\nObservable data\n\n\nNotation\nGreek letters (\\(\\mu\\), \\(\\sigma\\), \\(p\\))\nLatin letters (\\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\))\n\n\n\n\nA statistic estimates a parameter. This is the foundation of Blocks 2 and 3.\n\n\n\n\n\nCategorical: Labels or categories (e.g., branch, major, yes/no)\nQuantitative: Numbers with meaningful arithmetic (e.g., height, GPA, time)\n\nWhy does this matter? The type of variable determines:\n\nHow you summarize it (proportions vs means)\nHow you visualize it (bar charts vs histograms)\nWhich inference method you use (z-test for proportions vs t-test for means)",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-1.html#before-you-leave",
    "href": "MA206x-AY26-2/lesson-1.html#before-you-leave",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Population vs Sample vs Process\nParameters vs Statistics\nCategorical vs Quantitative variables\n\nAny questions?\n\n\n\n\nLesson 2: Sampling & Study Design\n\nObservational studies vs designed experiments\nCommon sampling methods and biases\nWhy randomization matters\n\n\n\n\n\n\nWebAssign 1.2 - Due before Lesson 2\nExploratory Data Analysis - Due Lesson 9\nWPR I - Lesson 16",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html",
    "href": "MA206-AY26-1/lesson-9.html",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\\[\nF_X(x)=P(X\\le x)=\n\\begin{cases}\n0, & x&lt;1, \\\\[6pt]\n\\dfrac{1}{6}, & 1\\le x&lt;2, \\\\[6pt]\n\\dfrac{2}{6}, & 2\\le x&lt;3, \\\\[6pt]\n\\dfrac{3}{6}, & 3\\le x&lt;4, \\\\[6pt]\n\\dfrac{4}{6}, & 4\\le x&lt;5, \\\\[6pt]\n\\dfrac{5}{6}, & 5\\le x&lt;6, \\\\[6pt]\n1, & x\\ge 6~.\n\\end{cases}\n\\]\n\n\n\n\n\\(P(X=2)\\)\n\\[\nP(X=2) = \\dfrac{1}{6}\n\\]\n\\(P(X \\le 4)\\)\n\\[\nP(X \\le 4) = P(X=1)+P(X=2)+P(X=3)+P(X=4)\n= \\dfrac{1}{6}+\\dfrac{1}{6}+\\dfrac{1}{6}+\\dfrac{1}{6}\n= \\dfrac{4}{6} = \\dfrac{2}{3}\n\\] This is an unnamed distribution.\nA named distribution is a probability distribution with a standard formula and notation that’s been given a name because it occurs so often in applications.\n\n\n\nSuppose a basketball player makes a free throw with probability \\(p=0.7\\) (and misses with probability \\(0.3\\)).\nIf they shoot 4 times, what is the probability of exactly 2 made shots?\n\nOne sequence: Make, Make, Miss, Miss.\nProbability \\(= 0.7 \\times 0.7 \\times 0.3 \\times 0.3\\).\nBut there are many possible sequences with 2 makes and 2 misses.\nThe number of such sequences is\n\\[\n\\binom{4}{2} = 6.\n\\]\n\nSo\n\\[\nP(\\text{2 makes in 4 shots}) = \\binom{4}{2}(0.7^2)(0.3^2).\n\\]\n\n\n\nIf \\(X\\) is the number of successes in \\(n\\) independent trials, each with success probability \\(p\\), then\n\\[\nP(X = x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x=0,1,\\dots,n.\n\\]\nWe write this as\n\\[\nX \\sim \\text{Binomial}(n,p).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Binomial(4, 0.7): distribution of made shots in 4 attempts\n\n\n\n\n\n\n\n\nFor \\(X \\sim \\mathrm{Binomial}(n,p)\\):\n- \\(\\mathbb{E}[X] = np\\)\n- \\(\\mathrm{Var}(X) = np(1-p)\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{np(1-p)}\\)\nFor \\(n=4\\), \\(p=0.7\\):\n- \\(\\mathbb{E}[X] = 4(0.7) = 2.8\\)\n- \\(\\mathrm{Var}(X) = 4(0.7)(0.3) = 0.84\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{0.84} \\approx 0.917\\)\n\\[\nP(X = x) = \\binom{n}{x} p^x (1-p)^{\\,n-x}, \\quad x=0,1,\\dots,n.\n\\]\n\n\n\n\n\\(P(X=2)\\)\n\\[\n\\binom{4}{2}(0.7)^2(0.3)^2\n\\]\n\n\ndbinom(2, size = 4, prob = .7)\n\n[1] 0.2646\n\n\n\n\\(P(X \\le 2)\\)\n\\[\n\\sum_{x=0}^{2} \\binom{4}{x}(0.7)^x(0.3)^{4-x}\n\\]\n\n\npbinom(2, size = 4, prob = .7)\n\n[1] 0.3483\n\n\n\n\\(P(X &gt; 2)\\) \\[\n1 - P(X \\le 2)\n\\]\n\n\n1 - pbinom(2, size = 4, prob = .7)\n\n[1] 0.6517\n\n\n\n\n\n\nSuppose a basketball player makes a free throw with probability \\(p=0.7\\) (and misses with probability \\(0.3\\)).\nLet \\(X\\) be the number of failures before the first success. Then:\n- \\(X=0\\): success on the first shot.\n- \\(X=2\\): two misses, then a make on the 3rd shot.\nFor example, the probability of \\(X=2\\) is \\[\nP(X=2) = (1-p)^2\\,p = (0.3)^2(0.7).\n\\]\n\n\n\nIf \\(X\\) is the number of failures before the first success in independent trials, then \\[\nP(X = x) = (1-p)^x\\,p, \\quad x=0,1,2,\\dots\n\\]\nWe write this as \\[\nX \\sim \\mathrm{Geometric}(p).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Geometric(\\(p=0.7\\)): probability of \\(x\\) failures before first success\n\n\n\n\n\n\n\n\n\nFor \\(X \\sim \\mathrm{Geometric}(p)\\):\n- \\(\\mathbb{E}[X] = \\dfrac{1-p}{p}\\)\n- \\(\\mathrm{Var}(X) = \\dfrac{1-p}{p^2}\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{\\dfrac{1-p}{p^2}}\\)\nFor \\(p=0.7\\):\n- \\(\\mathbb{E}[X] = \\dfrac{0.3}{0.7} \\approx 0.429\\)\n- \\(\\mathrm{Var}(X) = \\dfrac{0.3}{0.49} \\approx 0.612\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{0.612} \\approx 0.782\\)\n\n\n\n\n\n\\(P(X=2)\\) \\[\n(1-p)^2\\,p\n\\]\n\n\ndgeom(2, prob = 0.7)\n\n[1] 0.063\n\n\n\n\\(P(X \\le 2)\\) \\[\n\\sum_{x=0}^{2} (1-p)^x\\,p\n\\]\n\n\npgeom(2, prob = 0.7)\n\n[1] 0.973\n\n\n\n\\(P(X &gt; 2)\\) \\[\n1 - P(X \\le 2) = (1-p)^3\n\\]\n\n\n1 - pgeom(2, prob = 0.7)\n\n[1] 0.027\n\n\n\n\n\n\n\n\n\n\n\\[\nf(x) =\n\\begin{cases}\n2(1-x), & 0 \\le x \\le 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\\[\nF_X(x) =\n\\begin{cases}\n0, & x &lt; 0, \\\\[6pt]\n2x - x^2, & 0 \\le x \\le 1, \\\\[6pt]\n1, & x &gt; 1~.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\\(P(X \\le 0.5)\\)\n\\[\nF_X(0.5) = 2(0.5) - (0.5)^2 = 1 - 0.25 = 0.75\n\\]\n\\(P(0.25 \\le X \\le 0.75)\\)\n\\[\nF_X(0.75) - F_X(0.25) = (2(0.75) - 0.75^2) - (2(0.25) - 0.25^2) = 0.9375 - 0.4375 = 0.5\n\\]\n\n\n\n\nSuppose the heights of adult men in a population are approximately Normal with mean \\(\\mu = 68\\) inches and standard deviation \\(\\sigma = 3\\) inches.\nWhat is the probability that a randomly chosen man is taller than 72 inches?\nWe compute this directly from the Normal distribution:\n\\[\nP(X &gt; 72) = \\int_{72}^{\\infty} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx.\n\\]\n\n\n\nIf \\(X\\) is a continuous random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right), \\quad -\\infty &lt; x &lt; \\infty.\n\\]\nWe write this as\n\\[\nX \\sim \\text{Normal}(\\mu,\\sigma^2).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Normal(68, 3): density with shading for P(X &gt; 72)\n\n\n\n\n\n\n\n\n\nFor \\(X \\sim \\mathrm{Normal}(\\mu,\\sigma^2)\\):\n- \\(\\mathbb{E}[X] = \\mu\\)\n- \\(\\mathrm{Var}(X) = \\sigma^2\\)\n- \\(\\mathrm{SD}(X) = \\sigma\\)\nFor \\(\\mu=68\\), \\(\\sigma=3\\):\n- \\(\\mathbb{E}[X] = 68\\)\n- \\(\\mathrm{Var}(X) = 9\\)\n- \\(\\mathrm{SD}(X) = 3\\)\n\n\n\n\n\n\\(P(X=70)\\)\n\\[\nP(X=70) = 0 \\quad \\text{(point probability in a continuous distribution is zero)}\n\\]\n\n\n\n\\(P(X \\le 70)\\)\n\\[\nP(X \\le 70) = \\int_{-\\infty}^{70} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx\n\\]\n\n\npnorm(70, mean = 68, sd = 3)\n\n[1] 0.7475075\n\n\n\n\n\\(P(X &gt; 72)\\)\n\\[\nP(X &gt; 72) = \\int_{72}^{\\infty} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx\n\\]\n\n\n1 - pnorm(72, mean = 68, sd = 3)\n\n[1] 0.09121122\n\n\n\n\n\\(P(65 \\le X \\le 70)\\)\n\\[\nP(65 \\le X \\le 70) = \\int_{65}^{70} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx\n\\]\n\n\npnorm(70, mean = 68, sd = 3) - pnorm(65, mean = 68, sd = 3)\n\n[1] 0.5888522\n\n\n\n\n\n\n\n\nA soldier hits a target with probability \\(p=0.6\\) on each shot, independently of other shots. Suppose they fire \\(n=5\\) times. Let \\(X\\) be the number of hits.\n\nWrite the distribution of \\(X\\).\n\nFind \\(\\mathbb{E}[X]\\), \\(\\mathrm{Var}(X)\\), and \\(\\mathrm{SD}(X)\\).\n\nCompute:\n\n\\(P(X=3)\\)\n\n\\(P(X \\le 2)\\)\n\n\\(P(X &lt; 2)\\)\n\n\\(P(X &gt; 4)\\)\n\n\\(P(X \\ge 4)\\)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(X \\sim \\mathrm{Binomial}(n=5, p=0.6)\\)\n\\(\\mathbb{E}[X] = np = 5(0.6) = 3\\)\n\\(\\mathrm{Var}(X) = np(1-p) = 5(0.6)(0.4) = 1.2\\)\n\\(\\mathrm{SD}(X) = \\sqrt{1.2} \\approx 1.095\\)\nProbabilities:\n\n\ndbinom(3, size=5, prob=0.6)        # P(X=3)\n\n[1] 0.3456\n\npbinom(2, size=5, prob=0.6)        # P(X ≤ 2)\n\n[1] 0.31744\n\npbinom(1, size=5, prob=0.6)        # P(X ≤ 2)\n\n[1] 0.08704\n\n1 - pbinom(4, size=5, prob=0.6)    # P(X &gt; 4)\n\n[1] 0.07776\n\n1 - pbinom(3, size=5, prob=0.6)    # P(X &gt; 4)\n\n[1] 0.33696\n\n\n\n\n\n\n\n\n\nA cadet passes a marksmanship test with probability \\(p=0.4\\) on each attempt, independently of other attempts. Let \\(Y\\) be the number of failures before the first pass.\n\nWrite the distribution of \\(Y\\).\n\nFind \\(\\mathbb{E}[Y]\\), \\(\\mathrm{Var}(Y)\\), and \\(\\mathrm{SD}(Y)\\).\n\nCompute:\n\n\\(P(Y=2)\\)\n\n\\(P(Y \\le 3)\\)\n\n\\(P(Y &gt; 4)\\)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(Y \\sim \\mathrm{Geometric}(p=0.4)\\), where \\(Y =\\) failures before first success\n\\(\\mathbb{E}[Y] = \\dfrac{1-p}{p} = \\dfrac{0.6}{0.4} = 1.5\\)\n\\(\\mathrm{Var}(Y) = \\dfrac{1-p}{p^2} = \\dfrac{0.6}{0.16} = 3.75\\)\n\\(\\mathrm{SD}(Y) = \\sqrt{3.75} \\approx 1.936\\)\nProbabilities:\n\n\ndgeom(2, prob=0.4)        # P(Y=2)\n\n[1] 0.144\n\npgeom(3, prob=0.4)        # P(Y ≤ 3)\n\n[1] 0.8704\n\n1 - pgeom(4, prob=0.4)    # P(Y &gt; 4)\n\n[1] 0.07776\n\n\n\n\n\n\n\n\n\nSuppose the times (in minutes) to complete a training run are approximately Normal with mean \\(\\mu=50\\) and standard deviation \\(\\sigma=8.\\) Let \\(Z\\) be the time for a randomly chosen runner.\n\nWrite the distribution of \\(Z\\).\n\nFind \\(\\mathbb{E}[Z]\\), \\(\\mathrm{Var}(Z)\\), and \\(\\mathrm{SD}(Z)\\).\n\nCompute:\n\n\\(P(Z \\le 45)\\)\n\n\\(P(Z &gt; 60)\\)\n\n\\(P(40 \\le Z \\le 55)\\)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(Z \\sim \\mathrm{Normal}(\\mu=50, \\sigma^2=64)\\)\n\\(\\mathbb{E}[Z] = \\mu = 50\\)\n\\(\\mathrm{Var}(Z) = \\sigma^2 = 64\\)\n\\(\\mathrm{SD}(Z) = \\sigma = 8\\)\nProbabilities:\n\n\npnorm(45, mean=50, sd=8)                         # P(Z ≤ 45)\n\n[1] 0.2659855\n\n1 - pnorm(60, mean=50, sd=8)                     # P(Z &gt; 60)\n\n[1] 0.1056498\n\npnorm(55, mean=50, sd=8) - pnorm(40, mean=50, sd=8)   # P(40 ≤ Z ≤ 55)\n\n[1] 0.6283647\n\n\n\n\n\n\n\n\nLet the random variable \\(X\\) denote the time (in minutes) a patient waits at a doctor’s office before being called.\n\\[\nf_X(x)=\n\\begin{cases}\n\\dfrac{1}{2}, & 2 \\le x &lt; 3, \\\\[6pt]\n\\dfrac{x}{16}, & 3 \\le x \\le 5, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nIs \\(X\\) a discrete or continuous random variable? Sketch the PDF and label the axes.\n\nFind the cumulative distribution function (CDF), \\(F_X(x)\\), in curly-brace notation.\n\nCompute \\(P(X \\le 2.6)\\).\n\nCompute \\(P(2.5 \\le X \\le 4)\\).\n\nVerify that this is a valid probability distribution.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nVerify Valid PDF \\[\n\\int_{2}^{3}\\frac{1}{2}\\,dx + \\int_{3}^{5}\\frac{x}{16}\\,dx\n= \\frac{1}{2} + \\frac{1}{16}\\cdot \\frac{25-9}{2}\n= \\frac{1}{2} + \\frac{16}{32}\n= \\frac{1}{2} + \\frac{1}{2}\n= 1~.\n\\]\n\n\n\n\\(X\\) is continuous. The PDF is flat at height \\(1/2\\) on \\([2,3)\\), then jumps down to \\(f(3)=3/16\\) and increases linearly to \\(f(5)=5/16\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFrom PDF to CDF\n\nWe want the cumulative distribution function (CDF):\n\\[\nF_X(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f_X(t)\\,dt\n\\]\nwhere the PDF is\n\\[\nf_X(t) =\n\\begin{cases}\n\\dfrac{1}{2}, & 2 \\le t &lt; 3, \\\\[6pt]\n\\dfrac{t}{16}, & 3 \\le t \\le 5, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\nNo probability has accumulated yet:\n\\[\nF_X(x) = 0.\n\\]\n\n\n\nWe integrate from 2 up to \\(x\\):\n\\[\nF_X(x) = \\int_{2}^{x} \\frac{1}{2}\\,dt\n= \\left[\\frac{t}{2}\\right]_{t=2}^{t=x}\n= \\frac{x}{2} - \\frac{2}{2}\n= \\frac{x}{2} - 1.\n\\]\n\n\n\nWe add all mass up to 3 plus the integral from 3 to \\(x\\):\n\\[\nF_X(x) = \\int_{2}^{3} \\frac{1}{2}\\,dt \\;+\\; \\int_{3}^{x} \\frac{t}{16}\\,dt.\n\\]\nFirst part:\n\\[\n\\int_{2}^{3} \\frac{1}{2}\\,dt\n= \\left[\\frac{t}{2}\\right]_{2}^{3}\n= \\frac{3}{2} - \\frac{2}{2}\n= 0.5.\n\\]\nSecond part (leaving the constant inside):\n\\[\n\\int_{3}^{x} \\frac{t}{16}\\,dt\n= \\left[\\frac{t^2}{32}\\right]_{t=3}^{t=x}\n= \\frac{x^2}{32} - \\frac{9}{32}\n= \\frac{x^2 - 9}{32}.\n\\]\nSo together:\n\\[\nF_X(x) = 0.5 + \\frac{x^2 - 9}{32}\n= \\frac{16}{32} + \\frac{x^2 - 9}{32}\n= \\frac{x^2 + 7}{32}.\n\\]\n\n\n\nAll probability mass is included:\n\\[\nF_X(x) = 1.\n\\]\n\n\n\n\\[\nF_X(x) =\n\\begin{cases}\n0, & x &lt; 2, \\\\[6pt]\n\\dfrac{x}{2} - 1, & 2 \\le x &lt; 3, \\\\[6pt]\n\\dfrac{x^2 + 7}{32}, & 3 \\le x \\le 5, \\\\[6pt]\n1, & x &gt; 5~.\n\\end{cases}\n\\]\n\n\nSince \\(2.6 \\in [2,3)\\), \\[\nP(X \\le 2.6)=F_X(2.6)=\\frac{2.6-2}{2}=0.3.\n\\]\n\n\n\nSplit at \\(3\\): \\[\n\\begin{aligned}\nP(2.5 \\le X \\le 4)\n&= \\int_{2.5}^{3} \\frac{1}{2}\\,dx + \\int_{3}^{4} \\frac{x}{16}\\,dx \\\\[6pt]\n&= \\frac{1}{2}(0.5) + \\frac{1}{16}\\cdot \\frac{4^2-3^2}{2} \\\\[6pt]\n&= 0.25 + \\frac{1}{16}\\cdot \\frac{16-9}{2}\n= 0.25 + \\frac{1}{16}\\cdot \\frac{7}{2}\n= 0.25 + \\frac{7}{32}\n= \\frac{8}{32} + \\frac{7}{32}\n= \\frac{15}{32}\n\\approx 0.46875~.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\nResources:\n\nComputer with blank script in RStudio\nCourse Guide\nTidyverse Tutorial\nOne Page (Front and Back) of personally handwritten notes\nIssued Calculator\n\n\n\n\n\n\nVenn Diagrams\n\nDefinitions of Union/Intersection\n\nAddition Rule\n\nComplement Rule\n\nConditional Probability\n\nMultiplication Rule\n\nLaw of Total Probability\n\nBayes’ Theorem\n\nTree Diagrams\n\nRules of Independence and Mutually Exclusive Events\n\nTake Scenarios and Answer Questions Related to Probability\n\n\n\n\n\nIdentify appropriate distribution (named or unnamed)\n\nGo from PDF to CDF for unnamed distributions\n\nFind probabilities\n\nFind expected values\n\nFind variances\n\nLinear Transformation Rules (Adding/Subtracting Expected Values and Variances)\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#discrete-distribution-reminders",
    "href": "MA206-AY26-1/lesson-9.html#discrete-distribution-reminders",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\\[\nF_X(x)=P(X\\le x)=\n\\begin{cases}\n0, & x&lt;1, \\\\[6pt]\n\\dfrac{1}{6}, & 1\\le x&lt;2, \\\\[6pt]\n\\dfrac{2}{6}, & 2\\le x&lt;3, \\\\[6pt]\n\\dfrac{3}{6}, & 3\\le x&lt;4, \\\\[6pt]\n\\dfrac{4}{6}, & 4\\le x&lt;5, \\\\[6pt]\n\\dfrac{5}{6}, & 5\\le x&lt;6, \\\\[6pt]\n1, & x\\ge 6~.\n\\end{cases}\n\\]\n\n\n\n\n\\(P(X=2)\\)\n\\[\nP(X=2) = \\dfrac{1}{6}\n\\]\n\\(P(X \\le 4)\\)\n\\[\nP(X \\le 4) = P(X=1)+P(X=2)+P(X=3)+P(X=4)\n= \\dfrac{1}{6}+\\dfrac{1}{6}+\\dfrac{1}{6}+\\dfrac{1}{6}\n= \\dfrac{4}{6} = \\dfrac{2}{3}\n\\] This is an unnamed distribution.\nA named distribution is a probability distribution with a standard formula and notation that’s been given a name because it occurs so often in applications.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#the-binomial-distribution",
    "href": "MA206-AY26-1/lesson-9.html#the-binomial-distribution",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "Suppose a basketball player makes a free throw with probability \\(p=0.7\\) (and misses with probability \\(0.3\\)).\nIf they shoot 4 times, what is the probability of exactly 2 made shots?\n\nOne sequence: Make, Make, Miss, Miss.\nProbability \\(= 0.7 \\times 0.7 \\times 0.3 \\times 0.3\\).\nBut there are many possible sequences with 2 makes and 2 misses.\nThe number of such sequences is\n\\[\n\\binom{4}{2} = 6.\n\\]\n\nSo\n\\[\nP(\\text{2 makes in 4 shots}) = \\binom{4}{2}(0.7^2)(0.3^2).\n\\]\n\n\n\nIf \\(X\\) is the number of successes in \\(n\\) independent trials, each with success probability \\(p\\), then\n\\[\nP(X = x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x=0,1,\\dots,n.\n\\]\nWe write this as\n\\[\nX \\sim \\text{Binomial}(n,p).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Binomial(4, 0.7): distribution of made shots in 4 attempts\n\n\n\n\n\n\n\n\nFor \\(X \\sim \\mathrm{Binomial}(n,p)\\):\n- \\(\\mathbb{E}[X] = np\\)\n- \\(\\mathrm{Var}(X) = np(1-p)\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{np(1-p)}\\)\nFor \\(n=4\\), \\(p=0.7\\):\n- \\(\\mathbb{E}[X] = 4(0.7) = 2.8\\)\n- \\(\\mathrm{Var}(X) = 4(0.7)(0.3) = 0.84\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{0.84} \\approx 0.917\\)\n\\[\nP(X = x) = \\binom{n}{x} p^x (1-p)^{\\,n-x}, \\quad x=0,1,\\dots,n.\n\\]\n\n\n\n\n\\(P(X=2)\\)\n\\[\n\\binom{4}{2}(0.7)^2(0.3)^2\n\\]\n\n\ndbinom(2, size = 4, prob = .7)\n\n[1] 0.2646\n\n\n\n\\(P(X \\le 2)\\)\n\\[\n\\sum_{x=0}^{2} \\binom{4}{x}(0.7)^x(0.3)^{4-x}\n\\]\n\n\npbinom(2, size = 4, prob = .7)\n\n[1] 0.3483\n\n\n\n\\(P(X &gt; 2)\\) \\[\n1 - P(X \\le 2)\n\\]\n\n\n1 - pbinom(2, size = 4, prob = .7)\n\n[1] 0.6517",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#the-geometric-distribution",
    "href": "MA206-AY26-1/lesson-9.html#the-geometric-distribution",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "Suppose a basketball player makes a free throw with probability \\(p=0.7\\) (and misses with probability \\(0.3\\)).\nLet \\(X\\) be the number of failures before the first success. Then:\n- \\(X=0\\): success on the first shot.\n- \\(X=2\\): two misses, then a make on the 3rd shot.\nFor example, the probability of \\(X=2\\) is \\[\nP(X=2) = (1-p)^2\\,p = (0.3)^2(0.7).\n\\]\n\n\n\nIf \\(X\\) is the number of failures before the first success in independent trials, then \\[\nP(X = x) = (1-p)^x\\,p, \\quad x=0,1,2,\\dots\n\\]\nWe write this as \\[\nX \\sim \\mathrm{Geometric}(p).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Geometric(\\(p=0.7\\)): probability of \\(x\\) failures before first success\n\n\n\n\n\n\n\n\n\nFor \\(X \\sim \\mathrm{Geometric}(p)\\):\n- \\(\\mathbb{E}[X] = \\dfrac{1-p}{p}\\)\n- \\(\\mathrm{Var}(X) = \\dfrac{1-p}{p^2}\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{\\dfrac{1-p}{p^2}}\\)\nFor \\(p=0.7\\):\n- \\(\\mathbb{E}[X] = \\dfrac{0.3}{0.7} \\approx 0.429\\)\n- \\(\\mathrm{Var}(X) = \\dfrac{0.3}{0.49} \\approx 0.612\\)\n- \\(\\mathrm{SD}(X) = \\sqrt{0.612} \\approx 0.782\\)\n\n\n\n\n\n\\(P(X=2)\\) \\[\n(1-p)^2\\,p\n\\]\n\n\ndgeom(2, prob = 0.7)\n\n[1] 0.063\n\n\n\n\\(P(X \\le 2)\\) \\[\n\\sum_{x=0}^{2} (1-p)^x\\,p\n\\]\n\n\npgeom(2, prob = 0.7)\n\n[1] 0.973\n\n\n\n\\(P(X &gt; 2)\\) \\[\n1 - P(X \\le 2) = (1-p)^3\n\\]\n\n\n1 - pgeom(2, prob = 0.7)\n\n[1] 0.027",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#continuous-distribution-reminder",
    "href": "MA206-AY26-1/lesson-9.html#continuous-distribution-reminder",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "\\[\nf(x) =\n\\begin{cases}\n2(1-x), & 0 \\le x \\le 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\\[\nF_X(x) =\n\\begin{cases}\n0, & x &lt; 0, \\\\[6pt]\n2x - x^2, & 0 \\le x \\le 1, \\\\[6pt]\n1, & x &gt; 1~.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\\(P(X \\le 0.5)\\)\n\\[\nF_X(0.5) = 2(0.5) - (0.5)^2 = 1 - 0.25 = 0.75\n\\]\n\\(P(0.25 \\le X \\le 0.75)\\)\n\\[\nF_X(0.75) - F_X(0.25) = (2(0.75) - 0.75^2) - (2(0.25) - 0.25^2) = 0.9375 - 0.4375 = 0.5\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#the-normal-distribution",
    "href": "MA206-AY26-1/lesson-9.html#the-normal-distribution",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "Suppose the heights of adult men in a population are approximately Normal with mean \\(\\mu = 68\\) inches and standard deviation \\(\\sigma = 3\\) inches.\nWhat is the probability that a randomly chosen man is taller than 72 inches?\nWe compute this directly from the Normal distribution:\n\\[\nP(X &gt; 72) = \\int_{72}^{\\infty} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx.\n\\]\n\n\n\nIf \\(X\\) is a continuous random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right), \\quad -\\infty &lt; x &lt; \\infty.\n\\]\nWe write this as\n\\[\nX \\sim \\text{Normal}(\\mu,\\sigma^2).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Normal(68, 3): density with shading for P(X &gt; 72)\n\n\n\n\n\n\n\n\n\nFor \\(X \\sim \\mathrm{Normal}(\\mu,\\sigma^2)\\):\n- \\(\\mathbb{E}[X] = \\mu\\)\n- \\(\\mathrm{Var}(X) = \\sigma^2\\)\n- \\(\\mathrm{SD}(X) = \\sigma\\)\nFor \\(\\mu=68\\), \\(\\sigma=3\\):\n- \\(\\mathbb{E}[X] = 68\\)\n- \\(\\mathrm{Var}(X) = 9\\)\n- \\(\\mathrm{SD}(X) = 3\\)\n\n\n\n\n\n\\(P(X=70)\\)\n\\[\nP(X=70) = 0 \\quad \\text{(point probability in a continuous distribution is zero)}\n\\]\n\n\n\n\\(P(X \\le 70)\\)\n\\[\nP(X \\le 70) = \\int_{-\\infty}^{70} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx\n\\]\n\n\npnorm(70, mean = 68, sd = 3)\n\n[1] 0.7475075\n\n\n\n\n\\(P(X &gt; 72)\\)\n\\[\nP(X &gt; 72) = \\int_{72}^{\\infty} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx\n\\]\n\n\n1 - pnorm(72, mean = 68, sd = 3)\n\n[1] 0.09121122\n\n\n\n\n\\(P(65 \\le X \\le 70)\\)\n\\[\nP(65 \\le X \\le 70) = \\int_{65}^{70} \\frac{1}{3\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-68)^2}{18}\\right)\\,dx\n\\]\n\n\npnorm(70, mean = 68, sd = 3) - pnorm(65, mean = 68, sd = 3)\n\n[1] 0.5888522",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#practice-problems",
    "href": "MA206-AY26-1/lesson-9.html#practice-problems",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "A soldier hits a target with probability \\(p=0.6\\) on each shot, independently of other shots. Suppose they fire \\(n=5\\) times. Let \\(X\\) be the number of hits.\n\nWrite the distribution of \\(X\\).\n\nFind \\(\\mathbb{E}[X]\\), \\(\\mathrm{Var}(X)\\), and \\(\\mathrm{SD}(X)\\).\n\nCompute:\n\n\\(P(X=3)\\)\n\n\\(P(X \\le 2)\\)\n\n\\(P(X &lt; 2)\\)\n\n\\(P(X &gt; 4)\\)\n\n\\(P(X \\ge 4)\\)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(X \\sim \\mathrm{Binomial}(n=5, p=0.6)\\)\n\\(\\mathbb{E}[X] = np = 5(0.6) = 3\\)\n\\(\\mathrm{Var}(X) = np(1-p) = 5(0.6)(0.4) = 1.2\\)\n\\(\\mathrm{SD}(X) = \\sqrt{1.2} \\approx 1.095\\)\nProbabilities:\n\n\ndbinom(3, size=5, prob=0.6)        # P(X=3)\n\n[1] 0.3456\n\npbinom(2, size=5, prob=0.6)        # P(X ≤ 2)\n\n[1] 0.31744\n\npbinom(1, size=5, prob=0.6)        # P(X ≤ 2)\n\n[1] 0.08704\n\n1 - pbinom(4, size=5, prob=0.6)    # P(X &gt; 4)\n\n[1] 0.07776\n\n1 - pbinom(3, size=5, prob=0.6)    # P(X &gt; 4)\n\n[1] 0.33696\n\n\n\n\n\n\n\n\n\nA cadet passes a marksmanship test with probability \\(p=0.4\\) on each attempt, independently of other attempts. Let \\(Y\\) be the number of failures before the first pass.\n\nWrite the distribution of \\(Y\\).\n\nFind \\(\\mathbb{E}[Y]\\), \\(\\mathrm{Var}(Y)\\), and \\(\\mathrm{SD}(Y)\\).\n\nCompute:\n\n\\(P(Y=2)\\)\n\n\\(P(Y \\le 3)\\)\n\n\\(P(Y &gt; 4)\\)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(Y \\sim \\mathrm{Geometric}(p=0.4)\\), where \\(Y =\\) failures before first success\n\\(\\mathbb{E}[Y] = \\dfrac{1-p}{p} = \\dfrac{0.6}{0.4} = 1.5\\)\n\\(\\mathrm{Var}(Y) = \\dfrac{1-p}{p^2} = \\dfrac{0.6}{0.16} = 3.75\\)\n\\(\\mathrm{SD}(Y) = \\sqrt{3.75} \\approx 1.936\\)\nProbabilities:\n\n\ndgeom(2, prob=0.4)        # P(Y=2)\n\n[1] 0.144\n\npgeom(3, prob=0.4)        # P(Y ≤ 3)\n\n[1] 0.8704\n\n1 - pgeom(4, prob=0.4)    # P(Y &gt; 4)\n\n[1] 0.07776\n\n\n\n\n\n\n\n\n\nSuppose the times (in minutes) to complete a training run are approximately Normal with mean \\(\\mu=50\\) and standard deviation \\(\\sigma=8.\\) Let \\(Z\\) be the time for a randomly chosen runner.\n\nWrite the distribution of \\(Z\\).\n\nFind \\(\\mathbb{E}[Z]\\), \\(\\mathrm{Var}(Z)\\), and \\(\\mathrm{SD}(Z)\\).\n\nCompute:\n\n\\(P(Z \\le 45)\\)\n\n\\(P(Z &gt; 60)\\)\n\n\\(P(40 \\le Z \\le 55)\\)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(Z \\sim \\mathrm{Normal}(\\mu=50, \\sigma^2=64)\\)\n\\(\\mathbb{E}[Z] = \\mu = 50\\)\n\\(\\mathrm{Var}(Z) = \\sigma^2 = 64\\)\n\\(\\mathrm{SD}(Z) = \\sigma = 8\\)\nProbabilities:\n\n\npnorm(45, mean=50, sd=8)                         # P(Z ≤ 45)\n\n[1] 0.2659855\n\n1 - pnorm(60, mean=50, sd=8)                     # P(Z &gt; 60)\n\n[1] 0.1056498\n\npnorm(55, mean=50, sd=8) - pnorm(40, mean=50, sd=8)   # P(40 ≤ Z ≤ 55)\n\n[1] 0.6283647\n\n\n\n\n\n\n\n\nLet the random variable \\(X\\) denote the time (in minutes) a patient waits at a doctor’s office before being called.\n\\[\nf_X(x)=\n\\begin{cases}\n\\dfrac{1}{2}, & 2 \\le x &lt; 3, \\\\[6pt]\n\\dfrac{x}{16}, & 3 \\le x \\le 5, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nIs \\(X\\) a discrete or continuous random variable? Sketch the PDF and label the axes.\n\nFind the cumulative distribution function (CDF), \\(F_X(x)\\), in curly-brace notation.\n\nCompute \\(P(X \\le 2.6)\\).\n\nCompute \\(P(2.5 \\le X \\le 4)\\).\n\nVerify that this is a valid probability distribution.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nVerify Valid PDF \\[\n\\int_{2}^{3}\\frac{1}{2}\\,dx + \\int_{3}^{5}\\frac{x}{16}\\,dx\n= \\frac{1}{2} + \\frac{1}{16}\\cdot \\frac{25-9}{2}\n= \\frac{1}{2} + \\frac{16}{32}\n= \\frac{1}{2} + \\frac{1}{2}\n= 1~.\n\\]\n\n\n\n\\(X\\) is continuous. The PDF is flat at height \\(1/2\\) on \\([2,3)\\), then jumps down to \\(f(3)=3/16\\) and increases linearly to \\(f(5)=5/16\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFrom PDF to CDF\n\nWe want the cumulative distribution function (CDF):\n\\[\nF_X(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f_X(t)\\,dt\n\\]\nwhere the PDF is\n\\[\nf_X(t) =\n\\begin{cases}\n\\dfrac{1}{2}, & 2 \\le t &lt; 3, \\\\[6pt]\n\\dfrac{t}{16}, & 3 \\le t \\le 5, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\nNo probability has accumulated yet:\n\\[\nF_X(x) = 0.\n\\]\n\n\n\nWe integrate from 2 up to \\(x\\):\n\\[\nF_X(x) = \\int_{2}^{x} \\frac{1}{2}\\,dt\n= \\left[\\frac{t}{2}\\right]_{t=2}^{t=x}\n= \\frac{x}{2} - \\frac{2}{2}\n= \\frac{x}{2} - 1.\n\\]\n\n\n\nWe add all mass up to 3 plus the integral from 3 to \\(x\\):\n\\[\nF_X(x) = \\int_{2}^{3} \\frac{1}{2}\\,dt \\;+\\; \\int_{3}^{x} \\frac{t}{16}\\,dt.\n\\]\nFirst part:\n\\[\n\\int_{2}^{3} \\frac{1}{2}\\,dt\n= \\left[\\frac{t}{2}\\right]_{2}^{3}\n= \\frac{3}{2} - \\frac{2}{2}\n= 0.5.\n\\]\nSecond part (leaving the constant inside):\n\\[\n\\int_{3}^{x} \\frac{t}{16}\\,dt\n= \\left[\\frac{t^2}{32}\\right]_{t=3}^{t=x}\n= \\frac{x^2}{32} - \\frac{9}{32}\n= \\frac{x^2 - 9}{32}.\n\\]\nSo together:\n\\[\nF_X(x) = 0.5 + \\frac{x^2 - 9}{32}\n= \\frac{16}{32} + \\frac{x^2 - 9}{32}\n= \\frac{x^2 + 7}{32}.\n\\]\n\n\n\nAll probability mass is included:\n\\[\nF_X(x) = 1.\n\\]\n\n\n\n\\[\nF_X(x) =\n\\begin{cases}\n0, & x &lt; 2, \\\\[6pt]\n\\dfrac{x}{2} - 1, & 2 \\le x &lt; 3, \\\\[6pt]\n\\dfrac{x^2 + 7}{32}, & 3 \\le x \\le 5, \\\\[6pt]\n1, & x &gt; 5~.\n\\end{cases}\n\\]\n\n\nSince \\(2.6 \\in [2,3)\\), \\[\nP(X \\le 2.6)=F_X(2.6)=\\frac{2.6-2}{2}=0.3.\n\\]\n\n\n\nSplit at \\(3\\): \\[\n\\begin{aligned}\nP(2.5 \\le X \\le 4)\n&= \\int_{2.5}^{3} \\frac{1}{2}\\,dx + \\int_{3}^{4} \\frac{x}{16}\\,dx \\\\[6pt]\n&= \\frac{1}{2}(0.5) + \\frac{1}{16}\\cdot \\frac{4^2-3^2}{2} \\\\[6pt]\n&= 0.25 + \\frac{1}{16}\\cdot \\frac{16-9}{2}\n= 0.25 + \\frac{1}{16}\\cdot \\frac{7}{2}\n= 0.25 + \\frac{7}{32}\n= \\frac{8}{32} + \\frac{7}{32}\n= \\frac{15}{32}\n\\approx 0.46875~.\n\\end{aligned}\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#lesson-10",
    "href": "MA206-AY26-1/lesson-9.html#lesson-10",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "Resources:\n\nComputer with blank script in RStudio\nCourse Guide\nTidyverse Tutorial\nOne Page (Front and Back) of personally handwritten notes\nIssued Calculator\n\n\n\n\n\n\nVenn Diagrams\n\nDefinitions of Union/Intersection\n\nAddition Rule\n\nComplement Rule\n\nConditional Probability\n\nMultiplication Rule\n\nLaw of Total Probability\n\nBayes’ Theorem\n\nTree Diagrams\n\nRules of Independence and Mutually Exclusive Events\n\nTake Scenarios and Answer Questions Related to Probability\n\n\n\n\n\nIdentify appropriate distribution (named or unnamed)\n\nGo from PDF to CDF for unnamed distributions\n\nFind probabilities\n\nFind expected values\n\nFind variances\n\nLinear Transformation Rules (Adding/Subtracting Expected Values and Variances)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-9.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-9.html#before-you-leave",
    "title": "Lesson 9: Named Distributions",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 9"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html",
    "href": "MA206-AY26-1/lesson-7.html",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "Notation is hard\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Distribution Function (PDF)\nLet \\(X\\in\\{1,2,3,4,5,6\\}\\) be the points shown on one fair die roll.\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P(X=x)\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\n\n\n\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nCalculate the expected value of a discrete random variable\n\\[\n\\mu_X = E[X] = \\sum_x x \\cdot P(X=x).\n\\]\n\\[\nE[X] = 1\\cdot\\tfrac16 + 2\\cdot\\tfrac16 + 3\\cdot\\tfrac16 + 4\\cdot\\tfrac16 + 5\\cdot\\tfrac16 + 6\\cdot\\tfrac16\n= \\tfrac{21}{6} = 3.5.\n\\]\nCalculate the variance and standard deviation of a discrete random variable\n\\[\n\\mathrm{Var}(X)=\\sum_x (x-\\mu_X)^2\\,P(X=x)\n\\]\n\\[\n\\mathrm{Var}(X)=\\left(1-3.5\\right)^2\\cdot\\tfrac16\n+\\left(2-3.5\\right)^2\\cdot\\tfrac16\n+\\left(3-3.5\\right)^2\\cdot\\tfrac16\n+\\left(4-3.5\\right)^2\\cdot\\tfrac16\n+\\left(5-3.5\\right)^2\\cdot\\tfrac16\n+\\left(6-3.5\\right)^2\\cdot\\tfrac16\n=\\tfrac{35}{12}\n\\]\n\\[\n\\mathrm{SD}(X)=\\sqrt{\\mathrm{Var}(X)}.\n\\]\n\\[\n\\mathrm{SD}(X)=\\sqrt{\\tfrac{35}{12}}\\approx1.7078.\n\\]\n\n\n\n\n\nFor a discrete random variable \\(X\\), the cumulative distribution function (CDF) is defined as\n\\[\nF(x) = P(X \\leq x) = \\sum_{t \\leq x} P(X=t).\n\\]\nFor the fair die,\nCumulative Distribution Function (CDF)\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(F(x) = P(X \\leq x)\\)\n\\(1/6\\)\n\\(2/6\\)\n\\(3/6\\)\n\\(4/6\\)\n\\(5/6\\)\n\\(6/6\\)\n\n\n\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 1, \\\\[6pt]\n\\dfrac{1}{6}, & 1 \\leq x &lt; 2, \\\\[6pt]\n\\dfrac{2}{6}, & 2 \\leq x &lt; 3, \\\\[6pt]\n\\dfrac{3}{6}, & 3 \\leq x &lt; 4, \\\\[6pt]\n\\dfrac{4}{6}, & 4 \\leq x &lt; 5, \\\\[6pt]\n\\dfrac{5}{6}, & 5 \\leq x &lt; 6, \\\\[6pt]\n1, & x \\geq 6.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is \\(P(X \\leq 2)\\, ?\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe sum probabilities for \\(X=1\\) and \\(X=2\\):\n\\[\nP(X \\leq 2) = P(1) + P(2) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]\n\n\n\n\nWhat is \\(P\\!\\big(X &lt; \\mu_X + 1\\,\\mathrm{SD}\\big), \\quad \\mu_X=3.5,\\ \\mathrm{SD}\\approx1.7078 \\, ?\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCompute the cutoff:\n\\[\n\\mu_X + 1\\,\\mathrm{SD} \\approx 3.5 + 1.7078 = 5.2078.\n\\]\nSo we want \\(P(X &lt; 5.2078)\\). Since \\(X\\) is discrete, this means \\(X \\leq 5\\).\n\\[\nP(X \\leq 5) = \\tfrac{5}{6}.\n\\]\n\n\n\n\nWhat is \\(P(X &gt; 4)\\, ?\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPossible outcomes are \\(X=5\\) or \\(X=6\\):\n\\[\nP(X &gt; 4) = P(5) + P(6) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]\n\n\n\n\n\n\n\n\n\nDraw one ball uniformly at random from a hat with three balls: - Red earns $3, Blue earns $6, Green earns $12.\nOutcomes and probabilities (uniform over three balls):\n\n\n\nOutcome \\(x\\)\n$3\n$6\n$12\n\n\n\n\n\\(P(X_A = x)\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1/3\\)\n\n\n\nExpected value (from the PDF): \\[\nE[X_A] \\;=\\; 3\\cdot\\tfrac13 + 6\\cdot\\tfrac13 + 12\\cdot\\tfrac13\n= \\tfrac{3+6+12}{3} \\;=\\; 7.\n\\]\nVariance (from the PDF): \\[\n\\operatorname{Var}(X_A)\n= \\sum_x (x - E[X_A])^2 P(X_A=x)\n= \\tfrac13(3-7)^2 + \\tfrac13(6-7)^2 + \\tfrac13(12-7)^2.\n\\]\nCompute: \\[\n(3-7)^2=16,\\quad (6-7)^2=1,\\quad (12-7)^2=25 \\;\\;\\Rightarrow\\;\\;\n\\operatorname{Var}(X_A) = \\tfrac{16+1+25}{3} = \\tfrac{42}{3} = 14.\n\\]\nThus \\(\\mathrm{SD}(X_A)=\\sqrt{14}\\approx 3.7417\\).\n\n\n\nFlip a fair coin:\n- Tails earns $0, Heads earns $5. - Heads/Tails with probability \\(1/2\\) each.\n\n\n\nOutcome \\(x\\)\n$0\n$5\n\n\n\n\n\\(P(X_B = x)\\)\n\\(1/2\\)\n\\(1/2\\)\n\n\n\nExpected value (from the PDF): \\[\nE[X_B] \\;=\\; 0\\cdot\\tfrac12 + 5\\cdot\\tfrac12 \\;=\\; 2.5.\n\\]\nVariance (from the PDF): \\[\n\\operatorname{Var}(X_B)\n= \\sum_x (x - E[X_B])^2 P(X_B=x)\n= \\tfrac12(0-2.5)^2 + \\tfrac12(5-2.5)^2\n= \\tfrac12(6.25) + \\tfrac12(6.25) = 6.25.\n\\]\nThus \\(\\mathrm{SD}(X_B)=\\sqrt{6.25}=2.5\\).\n\n\n\nNow imagine a scenario where one draws a ball and flips a coin and earns the value of the draw and the flip.\nNew PDF. Each pair \\((X_A, X_B)\\) has probability \\((1/3)(1/2)=1/6\\). The possible sums:\n\n\\(3+0=3\\)\n\n\\(3+5=8\\)\n\n\\(6+0=6\\)\n\n\\(6+5=11\\)\n\n\\(12+0=12\\)\n\n\\(12+5=17\\)\n\nSo:\n\n\n\n\\(y\\)\n3\n6\n8\n11\n12\n17\n\n\n\n\n\\(P(Y=y)\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\n\n\nExpected Value (from the PDF) $$ \\[\\begin{align*}\nE[Y]\n&= \\sum_y y \\, P(Y=y) \\\\\n&= \\tfrac{1}{6}(3+6+8+11+12+17) \\\\\n&= \\tfrac{57}{6} \\\\\n&= 9.5.\n\\end{align*}\\]\n$$\nVariance From the PDF \\[\n\\begin{align*}\n\\operatorname{Var}(Y)\n&= \\tfrac{1}{6}\\bigl((3-9.5)^2 + (6-9.5)^2 + (8-9.5)^2 + (11-9.5)^2 + (12-9.5)^2 + (17-9.5)^2\\bigr) \\\\\n&= \\tfrac{1}{6}(42.25 + 12.25 + 2.25 + 2.25 + 6.25 + 56.25) \\\\\n&= \\tfrac{121.5}{6} \\\\\n&= 20.25.\n\\end{align*}\n\\]\n\\[\n\\mathrm{SD}(Y) = \\sqrt{20.25} = 4.5.\n\\]\n\n\n\n\n\nExpectation is linear (no conditions on indepenence): \\[\nE[aX + bY + c] \\;=\\; a\\,E[X] + b\\,E[Y] + c\n\\]\nVariance (independence required): \\[\n\\operatorname{Var}(aX + bY + c) \\;=\\; a^2 \\operatorname{Var}(X) + b^2 \\operatorname{Var}(Y)\n\\]\nStandard deviation:\n\\[\n\\mathrm{SD}(X) = \\sqrt{\\operatorname{Var}(X)}.\n\\]\n\n\n\nWe could grind through the PMF, but this is faster—everything collapses into two lines:\n\nLinearity of expectation (no conditions on independence): \\[\nE[aX + bY + c] = a\\,E[X] + b\\,E[Y] + c\n\\]\nVariance (Independence Required): \\[\n\\operatorname{Var}(aX + bY + c) = a^2 \\operatorname{Var}(X) + b^2 \\operatorname{Var}(Y)\n\\quad\\text{if } X \\perp Y.\n\\] Apply to \\(Y=X_A+X_B\\) with \\(X_A \\perp X_B\\):\n\nWe wanted to know \\(E[Y] = E[X_A + X_B]\\) and \\(\\operatorname{Var}(Y) = \\operatorname{Var}(X_A + X_B)\\)\n\\[\nE[Y]=E[X_A]+E[X_B]=7+2.5=9.5,\\qquad\n\\operatorname{Var}(Y)=\\operatorname{Var}(X_A)+\\operatorname{Var}(X_B)=14+6.25=20.25,\n\\] \\[\n\\mathrm{SD}(Y)=\\sqrt{20.25}=4.5.\n\\]\n\n\n\nNow suppose the player pays $10 up front before playing the ball game (\\(X_A\\)) and coin game (\\(X_B\\)).\nThe net winnings are \\[\nW = X_A + X_B - 10.\n\\]\nUsing the linear rules from the start:\n\nExpectation: \\[\nE[W] = E[X_A] + E[X_B] - 10 = 7 + 2.5 - 10 = -0.5.\n\\]\nVariance (independence of \\(X_A\\) and \\(X_B\\)): \\[\n\\operatorname{Var}(W) = \\operatorname{Var}(X_A + X_B - 10)\n= \\operatorname{Var}(X_A) + \\operatorname{Var}(X_B)\n= 14 + 6.25 = 20.25.\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(W) = \\sqrt{20.25} = 4.5.\n\\]\n\n\n\n\n\nSpinner A:\n- $2 with probability \\(0.5\\)\n- $5 with probability \\(0.3\\)\n- $10 with probability \\(0.2\\)\nSo \\(X_A\\) is a discrete random variable.\nSpinner B:\n- $1 with probability \\(0.4\\)\n- $4 with probability \\(0.6\\)\nSo \\(X_B\\) is a discrete random variable.\n\n\nWrite out the probability distribution for each spinner.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSpinner A: \\[\nf_{X_A}(x) =\n\\begin{cases}\n0.5, & x = 2 \\\\\n0.3, & x = 5 \\\\\n0.2, & x = 10 \\\\\n0,   & \\text{otherwise.}\n\\end{cases}\n\\]\nSpinner B: \\[\nf_{X_B}(x) =\n\\begin{cases}\n0.4, & x = 1 \\\\\n0.6, & x = 4 \\\\\n0,   & \\text{otherwise.}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSpinner A: \\[\nE[X_A] = 2(0.5) + 5(0.3) + 10(0.2) = 4.5\n\\]\n\\[\n\\operatorname{Var}(X_A)\n= 0.5(2-4.5)^2 + 0.3(5-4.5)^2 + 0.2(10-4.5)^2\n= 9.25\n\\]\nSpinner B: \\[\nE[X_B] = 1(0.4) + 4(0.6) = 2.8\n\\]\n\\[\n\\operatorname{Var}(X_B)\n= 0.4(1-2.8)^2 + 0.6(4-2.8)^2\n= 2.16\n\\]\n\n\n\n\n\n\nIn this game show, you play both games concurrently.\nLet \\(Y = X_A + X_B\\), assuming independence.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nE[Y] = E[X_A] + E[X_B] = 4.5 + 2.8 = 7.3\n\\]\n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(X_A) + \\operatorname{Var}(X_B) = 9.25 + 2.16 = 11.41\n\\]\n\\[\n\\mathrm{SD}(Y) = \\sqrt{11.41} \\approx 3.38\n\\]\n\n\n\n\n\n\nNow suppose in a bonus round the winnings are doubled for Spinner A and tripled for Spinner B:\n\\[\nW = 2X_A + 3X_B.\n\\]\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nExpectation: \\[\nE[W] = 2E[X_A] + 3E[X_B] = 17.4\n\\]\nVariance: \\[\n\\operatorname{Var}(W) = 2^2 \\operatorname{Var}(X_A) + 3^2 \\operatorname{Var}(X_B) = 56.44\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(W) = \\sqrt{56.44} \\approx 7.52\n\\]\n\n\n\n\n\n\nNow suppose it costs $15 to enter the bonus round.\nThe net winnings are \\[\nZ = 2X_A + 3X_B - 15.\n\\]\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nExpectation: \\[\nE[Z] = 2E[X_A] + 3E[X_B] - 15 = 2.4\n\\]\nVariance (Assuming Independence): \\[\n\\operatorname{Var}(Z) = 56.44\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(Z) = \\sqrt{56.44} \\approx 7.52\n\\]\n\n\n\n\n\n\n\nYou draw one ball from each bag, independently.\nBag A (5 balls total):\n- 2 red balls worth $3 each,\n- 2 blue balls worth $7 each,\n- 1 gold ball worth $15.\nLet \\(X_A\\) be the payout from Bag A.\nBag B (4 balls total):\n- 1 black ball worth $0,\n- 1 green ball worth $4,\n- 2 purple balls worth $8.\nLet \\(X_B\\) be the payout from Bag B.\n\nFind \\(E[X_A]\\) and \\(\\operatorname{Var}(X_A)\\) from the PDF definition.\n\nFind \\(E[X_B]\\) and \\(\\operatorname{Var}(X_B)\\) from the PDF definition.\n\nCompute the expected value of both games played together assuming independence.\n\nBonus round with scaling and a fee. In a special round, the payout is multiplied: you get three times the Bag A value but half the Bag B value, then pay a flat fee of $20.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n1) Bag A. Probabilities: \\(P(X_A=3)=\\tfrac{2}{5}\\), \\(P(X_A=7)=\\tfrac{2}{5}\\), \\(P(X_A=15)=\\tfrac{1}{5}\\).\n\\[\nE[X_A] = 3\\cdot\\tfrac{2}{5} + 7\\cdot\\tfrac{2}{5} + 15\\cdot\\tfrac{1}{5}\n= \\tfrac{6+14+15}{5} = 7.\n\\]\n\\[\n\\operatorname{Var}(X_A) = 0.4(3-7)^2 + 0.4(7-7)^2 + 0.2(15-7)^2\n= 0.4(16) + 0 + 0.2(64) = 6.4 + 12.8 = 19.2.\n\\]\n2) Bag B. Probabilities: \\(P(X_B=0)=\\tfrac{1}{4}\\), \\(P(X_B=4)=\\tfrac{1}{4}\\), \\(P(X_B=8)=\\tfrac{1}{2}\\).\n\\[\nE[X_B] = 0\\cdot\\tfrac{1}{4} + 4\\cdot\\tfrac{1}{4} + 8\\cdot\\tfrac{1}{2}\n= 1 + 4 = 5.\n\\]\n\\[\n\\operatorname{Var}(X_B) = 0.25(0-5)^2 + 0.25(4-5)^2 + 0.5(8-5)^2\n= 0.25(25) + 0.25(1) + 0.5(9)\n= 6.25 + 0.25 + 4.5 = 11.\n\\]\n3) Both bags together. Let \\(Y = X_A + X_B\\).\n\\[\nE[Y] = E[X_A] + E[X_B] = 7 + 5 = 12.\n\\]\n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(X_A) + \\operatorname{Var}(X_B)\n= 19.2 + 11 = 30.2.\n\\]\n\\[\n\\mathrm{SD}(Y) = \\sqrt{30.2} \\approx 5.50.\n\\]\n4) Bonus round with fee.\n\\[\nZ = 3X_A + \\tfrac{1}{2}X_B - 20.\n\\]\nExpectation: \\[\nE[Z] = 3E[X_A] + \\tfrac{1}{2}E[X_B] - 20\n= 3(7) + 0.5(5) - 20\n= 21 + 2.5 - 20 = 3.5.\n\\]\nVariance: \\[\n\\operatorname{Var}(Z) = 3^2 \\operatorname{Var}(X_A) + \\left(\\tfrac{1}{2}\\right)^2 \\operatorname{Var}(X_B)\n= 9(19.2) + 0.25(11) = 172.8 + 2.75 = 175.55.\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(Z) = \\sqrt{175.55} \\approx 13.25.\n\\]\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\n\n\n\nWPR 1: Lesson 10\nExploration Exercise 11.3B\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#review",
    "href": "MA206-AY26-1/lesson-7.html#review",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "Probability Distribution Function (PDF)\nLet \\(X\\in\\{1,2,3,4,5,6\\}\\) be the points shown on one fair die roll.\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P(X=x)\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\n\n\n\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nCalculate the expected value of a discrete random variable\n\\[\n\\mu_X = E[X] = \\sum_x x \\cdot P(X=x).\n\\]\n\\[\nE[X] = 1\\cdot\\tfrac16 + 2\\cdot\\tfrac16 + 3\\cdot\\tfrac16 + 4\\cdot\\tfrac16 + 5\\cdot\\tfrac16 + 6\\cdot\\tfrac16\n= \\tfrac{21}{6} = 3.5.\n\\]\nCalculate the variance and standard deviation of a discrete random variable\n\\[\n\\mathrm{Var}(X)=\\sum_x (x-\\mu_X)^2\\,P(X=x)\n\\]\n\\[\n\\mathrm{Var}(X)=\\left(1-3.5\\right)^2\\cdot\\tfrac16\n+\\left(2-3.5\\right)^2\\cdot\\tfrac16\n+\\left(3-3.5\\right)^2\\cdot\\tfrac16\n+\\left(4-3.5\\right)^2\\cdot\\tfrac16\n+\\left(5-3.5\\right)^2\\cdot\\tfrac16\n+\\left(6-3.5\\right)^2\\cdot\\tfrac16\n=\\tfrac{35}{12}\n\\]\n\\[\n\\mathrm{SD}(X)=\\sqrt{\\mathrm{Var}(X)}.\n\\]\n\\[\n\\mathrm{SD}(X)=\\sqrt{\\tfrac{35}{12}}\\approx1.7078.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#from-pdf-to-cdf-cumulative-distribution-function",
    "href": "MA206-AY26-1/lesson-7.html#from-pdf-to-cdf-cumulative-distribution-function",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "For a discrete random variable \\(X\\), the cumulative distribution function (CDF) is defined as\n\\[\nF(x) = P(X \\leq x) = \\sum_{t \\leq x} P(X=t).\n\\]\nFor the fair die,\nCumulative Distribution Function (CDF)\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(F(x) = P(X \\leq x)\\)\n\\(1/6\\)\n\\(2/6\\)\n\\(3/6\\)\n\\(4/6\\)\n\\(5/6\\)\n\\(6/6\\)\n\n\n\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 1, \\\\[6pt]\n\\dfrac{1}{6}, & 1 \\leq x &lt; 2, \\\\[6pt]\n\\dfrac{2}{6}, & 2 \\leq x &lt; 3, \\\\[6pt]\n\\dfrac{3}{6}, & 3 \\leq x &lt; 4, \\\\[6pt]\n\\dfrac{4}{6}, & 4 \\leq x &lt; 5, \\\\[6pt]\n\\dfrac{5}{6}, & 5 \\leq x &lt; 6, \\\\[6pt]\n1, & x \\geq 6.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is \\(P(X \\leq 2)\\, ?\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe sum probabilities for \\(X=1\\) and \\(X=2\\):\n\\[\nP(X \\leq 2) = P(1) + P(2) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]\n\n\n\n\nWhat is \\(P\\!\\big(X &lt; \\mu_X + 1\\,\\mathrm{SD}\\big), \\quad \\mu_X=3.5,\\ \\mathrm{SD}\\approx1.7078 \\, ?\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCompute the cutoff:\n\\[\n\\mu_X + 1\\,\\mathrm{SD} \\approx 3.5 + 1.7078 = 5.2078.\n\\]\nSo we want \\(P(X &lt; 5.2078)\\). Since \\(X\\) is discrete, this means \\(X \\leq 5\\).\n\\[\nP(X \\leq 5) = \\tfrac{5}{6}.\n\\]\n\n\n\n\nWhat is \\(P(X &gt; 4)\\, ?\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPossible outcomes are \\(X=5\\) or \\(X=6\\):\n\\[\nP(X &gt; 4) = P(5) + P(6) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#calculate-expected-values-of-linear-transformations-of-random-variables",
    "href": "MA206-AY26-1/lesson-7.html#calculate-expected-values-of-linear-transformations-of-random-variables",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "Draw one ball uniformly at random from a hat with three balls: - Red earns $3, Blue earns $6, Green earns $12.\nOutcomes and probabilities (uniform over three balls):\n\n\n\nOutcome \\(x\\)\n$3\n$6\n$12\n\n\n\n\n\\(P(X_A = x)\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1/3\\)\n\n\n\nExpected value (from the PDF): \\[\nE[X_A] \\;=\\; 3\\cdot\\tfrac13 + 6\\cdot\\tfrac13 + 12\\cdot\\tfrac13\n= \\tfrac{3+6+12}{3} \\;=\\; 7.\n\\]\nVariance (from the PDF): \\[\n\\operatorname{Var}(X_A)\n= \\sum_x (x - E[X_A])^2 P(X_A=x)\n= \\tfrac13(3-7)^2 + \\tfrac13(6-7)^2 + \\tfrac13(12-7)^2.\n\\]\nCompute: \\[\n(3-7)^2=16,\\quad (6-7)^2=1,\\quad (12-7)^2=25 \\;\\;\\Rightarrow\\;\\;\n\\operatorname{Var}(X_A) = \\tfrac{16+1+25}{3} = \\tfrac{42}{3} = 14.\n\\]\nThus \\(\\mathrm{SD}(X_A)=\\sqrt{14}\\approx 3.7417\\).\n\n\n\nFlip a fair coin:\n- Tails earns $0, Heads earns $5. - Heads/Tails with probability \\(1/2\\) each.\n\n\n\nOutcome \\(x\\)\n$0\n$5\n\n\n\n\n\\(P(X_B = x)\\)\n\\(1/2\\)\n\\(1/2\\)\n\n\n\nExpected value (from the PDF): \\[\nE[X_B] \\;=\\; 0\\cdot\\tfrac12 + 5\\cdot\\tfrac12 \\;=\\; 2.5.\n\\]\nVariance (from the PDF): \\[\n\\operatorname{Var}(X_B)\n= \\sum_x (x - E[X_B])^2 P(X_B=x)\n= \\tfrac12(0-2.5)^2 + \\tfrac12(5-2.5)^2\n= \\tfrac12(6.25) + \\tfrac12(6.25) = 6.25.\n\\]\nThus \\(\\mathrm{SD}(X_B)=\\sqrt{6.25}=2.5\\).\n\n\n\nNow imagine a scenario where one draws a ball and flips a coin and earns the value of the draw and the flip.\nNew PDF. Each pair \\((X_A, X_B)\\) has probability \\((1/3)(1/2)=1/6\\). The possible sums:\n\n\\(3+0=3\\)\n\n\\(3+5=8\\)\n\n\\(6+0=6\\)\n\n\\(6+5=11\\)\n\n\\(12+0=12\\)\n\n\\(12+5=17\\)\n\nSo:\n\n\n\n\\(y\\)\n3\n6\n8\n11\n12\n17\n\n\n\n\n\\(P(Y=y)\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\n\n\nExpected Value (from the PDF) $$ \\[\\begin{align*}\nE[Y]\n&= \\sum_y y \\, P(Y=y) \\\\\n&= \\tfrac{1}{6}(3+6+8+11+12+17) \\\\\n&= \\tfrac{57}{6} \\\\\n&= 9.5.\n\\end{align*}\\]\n$$\nVariance From the PDF \\[\n\\begin{align*}\n\\operatorname{Var}(Y)\n&= \\tfrac{1}{6}\\bigl((3-9.5)^2 + (6-9.5)^2 + (8-9.5)^2 + (11-9.5)^2 + (12-9.5)^2 + (17-9.5)^2\\bigr) \\\\\n&= \\tfrac{1}{6}(42.25 + 12.25 + 2.25 + 2.25 + 6.25 + 56.25) \\\\\n&= \\tfrac{121.5}{6} \\\\\n&= 20.25.\n\\end{align*}\n\\]\n\\[\n\\mathrm{SD}(Y) = \\sqrt{20.25} = 4.5.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#expected-value-and-variance-rules",
    "href": "MA206-AY26-1/lesson-7.html#expected-value-and-variance-rules",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "Expectation is linear (no conditions on indepenence): \\[\nE[aX + bY + c] \\;=\\; a\\,E[X] + b\\,E[Y] + c\n\\]\nVariance (independence required): \\[\n\\operatorname{Var}(aX + bY + c) \\;=\\; a^2 \\operatorname{Var}(X) + b^2 \\operatorname{Var}(Y)\n\\]\nStandard deviation:\n\\[\n\\mathrm{SD}(X) = \\sqrt{\\operatorname{Var}(X)}.\n\\]\n\n\n\nWe could grind through the PMF, but this is faster—everything collapses into two lines:\n\nLinearity of expectation (no conditions on independence): \\[\nE[aX + bY + c] = a\\,E[X] + b\\,E[Y] + c\n\\]\nVariance (Independence Required): \\[\n\\operatorname{Var}(aX + bY + c) = a^2 \\operatorname{Var}(X) + b^2 \\operatorname{Var}(Y)\n\\quad\\text{if } X \\perp Y.\n\\] Apply to \\(Y=X_A+X_B\\) with \\(X_A \\perp X_B\\):\n\nWe wanted to know \\(E[Y] = E[X_A + X_B]\\) and \\(\\operatorname{Var}(Y) = \\operatorname{Var}(X_A + X_B)\\)\n\\[\nE[Y]=E[X_A]+E[X_B]=7+2.5=9.5,\\qquad\n\\operatorname{Var}(Y)=\\operatorname{Var}(X_A)+\\operatorname{Var}(X_B)=14+6.25=20.25,\n\\] \\[\n\\mathrm{SD}(Y)=\\sqrt{20.25}=4.5.\n\\]\n\n\n\nNow suppose the player pays $10 up front before playing the ball game (\\(X_A\\)) and coin game (\\(X_B\\)).\nThe net winnings are \\[\nW = X_A + X_B - 10.\n\\]\nUsing the linear rules from the start:\n\nExpectation: \\[\nE[W] = E[X_A] + E[X_B] - 10 = 7 + 2.5 - 10 = -0.5.\n\\]\nVariance (independence of \\(X_A\\) and \\(X_B\\)): \\[\n\\operatorname{Var}(W) = \\operatorname{Var}(X_A + X_B - 10)\n= \\operatorname{Var}(X_A) + \\operatorname{Var}(X_B)\n= 14 + 6.25 = 20.25.\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(W) = \\sqrt{20.25} = 4.5.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#spinner-game",
    "href": "MA206-AY26-1/lesson-7.html#spinner-game",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "Spinner A:\n- $2 with probability \\(0.5\\)\n- $5 with probability \\(0.3\\)\n- $10 with probability \\(0.2\\)\nSo \\(X_A\\) is a discrete random variable.\nSpinner B:\n- $1 with probability \\(0.4\\)\n- $4 with probability \\(0.6\\)\nSo \\(X_B\\) is a discrete random variable.\n\n\nWrite out the probability distribution for each spinner.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSpinner A: \\[\nf_{X_A}(x) =\n\\begin{cases}\n0.5, & x = 2 \\\\\n0.3, & x = 5 \\\\\n0.2, & x = 10 \\\\\n0,   & \\text{otherwise.}\n\\end{cases}\n\\]\nSpinner B: \\[\nf_{X_B}(x) =\n\\begin{cases}\n0.4, & x = 1 \\\\\n0.6, & x = 4 \\\\\n0,   & \\text{otherwise.}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSpinner A: \\[\nE[X_A] = 2(0.5) + 5(0.3) + 10(0.2) = 4.5\n\\]\n\\[\n\\operatorname{Var}(X_A)\n= 0.5(2-4.5)^2 + 0.3(5-4.5)^2 + 0.2(10-4.5)^2\n= 9.25\n\\]\nSpinner B: \\[\nE[X_B] = 1(0.4) + 4(0.6) = 2.8\n\\]\n\\[\n\\operatorname{Var}(X_B)\n= 0.4(1-2.8)^2 + 0.6(4-2.8)^2\n= 2.16\n\\]\n\n\n\n\n\n\nIn this game show, you play both games concurrently.\nLet \\(Y = X_A + X_B\\), assuming independence.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nE[Y] = E[X_A] + E[X_B] = 4.5 + 2.8 = 7.3\n\\]\n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(X_A) + \\operatorname{Var}(X_B) = 9.25 + 2.16 = 11.41\n\\]\n\\[\n\\mathrm{SD}(Y) = \\sqrt{11.41} \\approx 3.38\n\\]\n\n\n\n\n\n\nNow suppose in a bonus round the winnings are doubled for Spinner A and tripled for Spinner B:\n\\[\nW = 2X_A + 3X_B.\n\\]\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nExpectation: \\[\nE[W] = 2E[X_A] + 3E[X_B] = 17.4\n\\]\nVariance: \\[\n\\operatorname{Var}(W) = 2^2 \\operatorname{Var}(X_A) + 3^2 \\operatorname{Var}(X_B) = 56.44\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(W) = \\sqrt{56.44} \\approx 7.52\n\\]\n\n\n\n\n\n\nNow suppose it costs $15 to enter the bonus round.\nThe net winnings are \\[\nZ = 2X_A + 3X_B - 15.\n\\]\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nExpectation: \\[\nE[Z] = 2E[X_A] + 3E[X_B] - 15 = 2.4\n\\]\nVariance (Assuming Independence): \\[\n\\operatorname{Var}(Z) = 56.44\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(Z) = \\sqrt{56.44} \\approx 7.52\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#board-problem-two-bags-of-balls",
    "href": "MA206-AY26-1/lesson-7.html#board-problem-two-bags-of-balls",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "You draw one ball from each bag, independently.\nBag A (5 balls total):\n- 2 red balls worth $3 each,\n- 2 blue balls worth $7 each,\n- 1 gold ball worth $15.\nLet \\(X_A\\) be the payout from Bag A.\nBag B (4 balls total):\n- 1 black ball worth $0,\n- 1 green ball worth $4,\n- 2 purple balls worth $8.\nLet \\(X_B\\) be the payout from Bag B.\n\nFind \\(E[X_A]\\) and \\(\\operatorname{Var}(X_A)\\) from the PDF definition.\n\nFind \\(E[X_B]\\) and \\(\\operatorname{Var}(X_B)\\) from the PDF definition.\n\nCompute the expected value of both games played together assuming independence.\n\nBonus round with scaling and a fee. In a special round, the payout is multiplied: you get three times the Bag A value but half the Bag B value, then pay a flat fee of $20.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n1) Bag A. Probabilities: \\(P(X_A=3)=\\tfrac{2}{5}\\), \\(P(X_A=7)=\\tfrac{2}{5}\\), \\(P(X_A=15)=\\tfrac{1}{5}\\).\n\\[\nE[X_A] = 3\\cdot\\tfrac{2}{5} + 7\\cdot\\tfrac{2}{5} + 15\\cdot\\tfrac{1}{5}\n= \\tfrac{6+14+15}{5} = 7.\n\\]\n\\[\n\\operatorname{Var}(X_A) = 0.4(3-7)^2 + 0.4(7-7)^2 + 0.2(15-7)^2\n= 0.4(16) + 0 + 0.2(64) = 6.4 + 12.8 = 19.2.\n\\]\n2) Bag B. Probabilities: \\(P(X_B=0)=\\tfrac{1}{4}\\), \\(P(X_B=4)=\\tfrac{1}{4}\\), \\(P(X_B=8)=\\tfrac{1}{2}\\).\n\\[\nE[X_B] = 0\\cdot\\tfrac{1}{4} + 4\\cdot\\tfrac{1}{4} + 8\\cdot\\tfrac{1}{2}\n= 1 + 4 = 5.\n\\]\n\\[\n\\operatorname{Var}(X_B) = 0.25(0-5)^2 + 0.25(4-5)^2 + 0.5(8-5)^2\n= 0.25(25) + 0.25(1) + 0.5(9)\n= 6.25 + 0.25 + 4.5 = 11.\n\\]\n3) Both bags together. Let \\(Y = X_A + X_B\\).\n\\[\nE[Y] = E[X_A] + E[X_B] = 7 + 5 = 12.\n\\]\n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(X_A) + \\operatorname{Var}(X_B)\n= 19.2 + 11 = 30.2.\n\\]\n\\[\n\\mathrm{SD}(Y) = \\sqrt{30.2} \\approx 5.50.\n\\]\n4) Bonus round with fee.\n\\[\nZ = 3X_A + \\tfrac{1}{2}X_B - 20.\n\\]\nExpectation: \\[\nE[Z] = 3E[X_A] + \\tfrac{1}{2}E[X_B] - 20\n= 3(7) + 0.5(5) - 20\n= 21 + 2.5 - 20 = 3.5.\n\\]\nVariance: \\[\n\\operatorname{Var}(Z) = 3^2 \\operatorname{Var}(X_A) + \\left(\\tfrac{1}{2}\\right)^2 \\operatorname{Var}(X_B)\n= 9(19.2) + 0.25(11) = 172.8 + 2.75 = 175.55.\n\\]\nStandard deviation: \\[\n\\mathrm{SD}(Z) = \\sqrt{175.55} \\approx 13.25.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-7.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-7.html#before-you-leave",
    "title": "Lesson 7: Random Variable Rules",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\n\n\n\nWPR 1: Lesson 10\nExploration Exercise 11.3B\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 7"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-5.html",
    "href": "MA206-AY26-1/lesson-5.html",
    "title": "Lesson 5: Tidyverse Tutorial",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ✏️ Edit this workbook (opens in Excel for the web) \n\n\n\n\n\nThis is an individual assignment. You SHOULD sit next to your partner and help each other. Collaboration in class does NOT need to be documented. Collaboration outside of this class (even with your partner) DOES need to be documented.\nLets go to the Course Project Page for specific instructions. You should refer to this as you do this milestone.\nWe are going to execute AY26_1Tidyverse-Lab.Rmd on wage data set\nReference the Tidyverse Tutorial as needed (a lot!)\nComplete the Tidyverse Lab and submit on Canvas NLT 0700 on 5 September 2025 for Day 1 cadets and 0700 on 4 September 2025 for Day 2 cadets\nMust include an Annex B (for addressing feedback) in this Canvas submission\nAdd the Tidyverse-Lab pdf and Annex B to your binder.\n\n\n\n\nSuite of packages\n\n\n\n\n\nTons of extensions\nggvfields\n\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 6\n\n\n\n\n\nMilestone 2 / Tidyverse Tutorial: Due 0700 Lesson 7\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-5.html#welcome",
    "href": "MA206-AY26-1/lesson-5.html#welcome",
    "title": "Lesson 5: Tidyverse Tutorial",
    "section": "",
    "text": "Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-5.html#tidyverse-tutorial",
    "href": "MA206-AY26-1/lesson-5.html#tidyverse-tutorial",
    "title": "Lesson 5: Tidyverse Tutorial",
    "section": "",
    "text": "✏️ Edit this workbook (opens in Excel for the web) \n\n\n\n\n\nThis is an individual assignment. You SHOULD sit next to your partner and help each other. Collaboration in class does NOT need to be documented. Collaboration outside of this class (even with your partner) DOES need to be documented.\nLets go to the Course Project Page for specific instructions. You should refer to this as you do this milestone.\nWe are going to execute AY26_1Tidyverse-Lab.Rmd on wage data set\nReference the Tidyverse Tutorial as needed (a lot!)\nComplete the Tidyverse Lab and submit on Canvas NLT 0700 on 5 September 2025 for Day 1 cadets and 0700 on 4 September 2025 for Day 2 cadets\nMust include an Annex B (for addressing feedback) in this Canvas submission\nAdd the Tidyverse-Lab pdf and Annex B to your binder.\n\n\n\n\nSuite of packages\n\n\n\n\n\nTons of extensions\nggvfields",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-5.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-5.html#before-you-leave",
    "title": "Lesson 5: Tidyverse Tutorial",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nLesson 6\n\n\n\n\n\nMilestone 2 / Tidyverse Tutorial: Due 0700 Lesson 7\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 5"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-30.html",
    "href": "MA206-AY26-1/lesson-30.html",
    "title": "Lesson 30 - Course Review",
    "section": "",
    "text": "Math vs USMAPS\n\n\n\n\n\n\nPreviously 3-1\n\n\n\n\n\n\n4-1\n\n\n\n\n\n\n\n\n\nMath vs USMAPS\n\n\n\n\n\n\nPreviously 4-1\n\n\n\n\n\n\n5-1\n\n\n\n\n\n\n\n\n\nMath vs Garrison\n\n\n\n\n\n\nPreviously 5-1\n\n\n\n\n\n\n6-1\n\n\n\n\n\n\n\n\n\nMath vs Garrison\n\n\n\n\n\n\nPreviously 6-1\n\n\n\n\n\n\n7-1\n\n\n\n\n\n\n\n\n\n\n\n\nMath vs EECS\n\n\n\n\n\n\nPreviously 2-0\n\n\n\n\n\n\n3-0\n\n\n\n\n\n\n\n\n\nMath vs EECS\n\n\n\n\n\n\nPreviously 3-0\n\n\n\n\n\n\n4-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n0730\n1100\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\n\n\n\nG1\n\n\n\n\n\nWed, 17 Dec (1300-1630): TH324\n\nAfari-Aikins, John\nColdren, Nathan\nConti, Annabella\nFreeman, Brandon\nGeorge, Joshua\nGoetz, Charles\n\nWed, 17 Dec (1300-1630): TH322\n\nLavery, Harrison\nMcDaniel, Jack\nMcDonnell, Hunter\nMcKillop, John\n\nWed, 17 Dec (1300-1630): TH323\n\nMeers, Tehya\nMidberry, James\n\nWed, 17 Dec (1300-1630): TH321\n\nMinicozzi, John\nNoack, Macoy\n\nThu, 18 Dec (0730-1100): TH321\n\nDin, Jenna\nGupta, Aarav\nLawrence, Karina\n\n\n\n\n\n\n\n\n\n\nH1\n\n\n\n\n\nWed, 17 Dec (0730-1100): BH 171A\n\nZagame, Samuel\n\nWed, 17 Dec (1300-1630): TH322\n\nArengo, Mary\nChambers, Cherokee\nDohl, Chad\nHudson-Odoi, Vanessa\nKinkead, Lucas\nRecords, Benjamin\nShelton, Sawyer\nStockbower, Tatiana\nThanepohn, Trevor\nWalter, Benjamin\nWills, Liam\nWint, Logan\n\nThu, 18 Dec (0730-1100): TH321\n\nChau, Paul\nJohnson, Joseph\nRubio, Andrew\nSager, Campbell\nVann, Nehemiah\n\n\n\n\n\n\n\n\n\n\nI1\n\n\n\n\n\nWed, 17 Dec (1300-1630): TH323\n\nAguilar Winchell, Benjamin\nAhn, David\nAndrade, Elena\nBettencourt, Jacob\nBhutani, Dillon\nCampbell, Evan\nForgues, Barbara\nJo, Alex\nLanham, Logan\nMaan, Bahawal\nSchwartz, Joseph\nSindler, Allan\nTahmazian, Isabela\nWamre, Gabrielle\n\nThu, 18 Dec (0730-1100): TH321\n\nHelmkamp, Braeden\nOgordi, Daniel\nPark, Sangwoo\nSmith, Gennaro\n\n\n\n\n\n\n\n\n\n\nI2\n\n\n\n\n\nWed, 17 Dec (1300-1630): TH321\n\nArdisana, James\nBachmann, Christian\nBarksdale, Jordon\nBarvitskie, Mason\nCorbett, Chas\nDavidson, Justin\nGroebner, Samuel\nHarris, Parker\nKim, Danny\nMantell, Jack\nMcKane, Angelina\nNguyen, Ta\nOxendine, Jake\nPatterson, Alyssa\nSpeaks, Brennan\n\nThu, 18 Dec (0730-1100): TH321\n\nArterberry, Myles\nMcPherson, Paige\nWilliams, Caleb\n\n\n\n\n\n\n\n\nTEE Cover Sheet\n\n\nAuthorized Resources:\n\nYour computer with access to a blank RStudio document (.R, .rmd, or .qmd tab)\nCourse Guide\nTidyverse Tutorial\nTwo pages (front and back) of personally handwritten notes\nIssued calculator\n\nUnauthorized Resources:\n\nInternet\nGenerative AI\nTextbook\nCourse applets\nEmail or other electronic communications\nMusic devices\nFriends / Instructor\n\nImportant Reminders:\n\nShow enough work to logically present your thought process (R code or equations) - demonstrate mastery of course material, not simply a correct answer\nPrepare RStudio by clicking the broom button in “History,” “Environment,” and “Plots” panes; use CTRL+L to clear the Console\nAcademic security: Do not discuss content until released on 19 December @ 1630\n\nAdvice:\n\nDon’t just put down an answer - show how you got to that answer even if you used R/calculator\nYou’ve been taught everything on this exam!\n\n\n\n\n\n\n\n\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\nMultiplication Rule: \\(P(A \\cap B) = P(A) \\cdot P(B|A)\\)\nComplement Rule: \\(P(A^c) = 1 - P(A)\\)\nConditional Probability: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\nBayes’ Theorem: \\(P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\)\nMutually Exclusive: \\(P(A \\cap B) = 0\\), so \\(P(A \\cup B) = P(A) + P(B)\\)\nIndependence: \\(P(A|B) = P(A)\\), so \\(P(A \\cap B) = P(A) \\cdot P(B)\\)\n\n\n\n\n\nIdentify observational units in a study\nRecognize variable types (categorical vs quantitative)\nDistinguish between observational study and experimental study\nIdentify confounding variables\nKnow sampling methods (convenience sample, random sample, etc.)\nChoose appropriate plot types based on variable type:\n\nCategorical: bar graph\nQuantitative: histogram, dot plot, box plot\n\n\n\n\n\n\nKnow the difference between parameter (population) and statistic (sample)\nKnow when we can claim causality (random assignment)\nKnow when we can claim generalization (random sampling)\n\n\n\n\n\nKnow when to use PMF, PDF, and CDF for discrete and continuous variables\nCalculate probabilities using PMF (discrete) and PDF (continuous)\nGo from PDF to CDF for continuous random variables to include piecewise PDFs (integration)\nCalculate expected value \\(E(X)\\), variance \\(Var(X)\\), and standard deviation \\(SD(X)\\)\nApply linear transformation rules: \\(E(aX + b) = aE(X) + b\\) and \\(Var(aX + b) = a^2 Var(X)\\)\n\n\n\n\nKnow when to use each test, validity conditions, and how to execute:\n\n\n\n\n\n\n\nTest\nWhen to Use\n\n\n\n\nOne-proportion z-test\nOne categorical variable, testing proportion against a value\n\n\nOne-sample t-test\nOne quantitative variable, testing mean against a value\n\n\nTwo-proportion z-test\nComparing proportions between two groups\n\n\nTwo-sample t-test\nComparing means between two independent groups\n\n\nPaired t-test\nComparing means for paired/matched data\n\n\n\nFor each test, know how to:\n\nWrite null and alternative hypotheses (symbols and words)\nCheck validity conditions\nCalculate statistic, standardized statistic, and p-value\nInterpret p-value (probability of observing data at least as extreme, given null is true)\nDraw conclusions comparing p-value to significance level\nKnow when to use z-distribution vs t-distribution:\n\nz: proportions (known population standard deviation)\nt: means (unknown population standard deviation, using sample SD)\n\n\n\n\n\n\nInterpret confidence intervals correctly\nCalculate margin of error (half the width of CI)\nDetermine if a value is plausible based on CI\n\n\n\n\n\nDescribe scatterplot characteristics: direction, form, strength of association\nExecute simple and multiple linear regression with data\nInterpret coefficients for continuous variables (“for each 1-unit increase…”)\nInterpret coefficients for categorical variables (“compared to the reference group…”)\nInterpret coefficients “after controlling for” or “after accounting for” other variables\nInterpret interaction terms\nCompare models using p-values and \\(R^2\\)\nAssess statistical significance of coefficients\n\n\n\n\n\n\n\nResearchers studied where people stop when approaching a stop sign. Do people have a preference for stopping position, or do they choose randomly between stopping as a single car, following another car, or leading a group?\n\nStop &lt;- ma206data::chap8_Stop\nhead(Stop)\n\n# A tibble: 6 × 1\n  position_stop\n  &lt;chr&gt;        \n1 single       \n2 single       \n3 single       \n4 single       \n5 single       \n6 single       \n\ntable(Stop)\n\nposition_stop\nfollow   lead single \n    98     42    176 \n\n\nStep 1: Set up hypotheses\nIf people had no preference, they would choose each position with equal probability (1/3).\n\\(H_0: \\pi = 1/3\\) (no preference - people choose “single” at the same rate as random chance)\n\\(H_a: \\pi &gt; 1/3\\) (people prefer to stop as a single car)\nStep 2: Calculate the statistic\n\n# Count successes (single) and total\nn &lt;- nrow(Stop)\nx &lt;- sum(Stop$position_stop == \"single\")\np_hat &lt;- x / n\n\ncat(\"n =\", n, \"\\n\")\n\nn = 316 \n\ncat(\"x (single) =\", x, \"\\n\")\n\nx (single) = 176 \n\ncat(\"p-hat =\", round(p_hat, 3), \"\\n\")\n\np-hat = 0.557 \n\n\nStep 3: Conduct the test\n\\[z = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}}\\]\n\n# Null hypothesis value\npi_0 &lt;- 1/3\n\n# Standard error under the null\nSE &lt;- sqrt(pi_0 * (1 - pi_0) / n)\n\n# Standardized statistic (z-score)\nz &lt;- (p_hat - pi_0) / SE\n\ncat(\"Standard Error =\", round(SE, 4), \"\\n\")\n\nStandard Error = 0.0265 \n\ncat(\"z =\", round(z, 3), \"\\n\")\n\nz = 8.433 \n\n# p-value (one-sided, greater than)\np_value &lt;- 1 - pnorm(z)\ncat(\"p-value =\", round(p_value, 4), \"\\n\")\n\np-value = 0 \n\n\nStep 4: Confidence Interval\n\\[\\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\n# For CI, use standard error based on p-hat (not pi_0)\nSE_ci &lt;- sqrt(p_hat * (1 - p_hat) / n)\n\n# 90% CI (for alpha = 0.10)\nz_star &lt;- qnorm(0.95)  # one-sided, so 0.95\nCI_lower &lt;- p_hat - z_star * SE_ci\nCI_upper &lt;- p_hat + z_star * SE_ci\n\ncat(\"90% CI: (\", round(CI_lower, 3), \",\", round(CI_upper, 3), \")\\n\")\n\n90% CI: ( 0.511 , 0.603 )\n\n\nStep 5: Draw conclusion\nCompare p-value to significance level and state conclusion in context.\n\n\n\nResearchers want to understand factors that affect lung capacity (FEV - Forced Expiratory Volume). They collected data on age, height, gender, and smoking status.\n\nFEV &lt;- ma206data::fev\nhead(FEV)\n\n# A tibble: 6 × 5\n    Age   FEV Height Gender Smoker\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n1    11  3.90   67   Female no    \n2    11  3.98   68.5 Male   no    \n3     8  2.17   57   Male   no    \n4    11  3.74   68   Male   no    \n5    11  2.94   63   Female no    \n6    15  2.73   63   Female no    \n\n\n\n\n\nggplot(FEV, aes(x = Height, y = FEV)) +\n  geom_point() +\n  labs(x = \"Height (inches)\", y = \"FEV (liters)\",\n       title = \"Height vs. Lung Capacity\")\n\n\n\n\n\n\n\n\nLets talk association direction, form, and strength:\nDirection:\nForm:\nStrength:\n\n\n\n\nmodel1 &lt;- lm(FEV ~ Height + Age + Smoker, data = FEV)\nsummary(model1)\n\n\nCall:\nlm(formula = FEV ~ Height + Age + Smoker, data = FEV)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5349 -0.2903 -0.0146  0.2812  1.9197 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.77287    0.24718 -19.309  &lt; 2e-16 ***\nHeight       0.11288    0.00520  21.707  &lt; 2e-16 ***\nAge          0.05142    0.01049   4.900 1.24e-06 ***\nSmokeryes   -0.13031    0.06685  -1.949   0.0517 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4422 on 589 degrees of freedom\nMultiple R-squared:  0.7466,    Adjusted R-squared:  0.7454 \nF-statistic: 578.6 on 3 and 589 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation of Height coefficient:\nAfter controlling for Age and Smoker status, for each additional inch of height, FEV increases by approximately ___ liters, on average.\nInterpretation of Smoker coefficient:\nAfter controlling for Height and Age, smokers have an FEV that is approximately ___ liters [higher/lower] than non-smokers, on average.\nIs Height statistically significant at \\(\\alpha = 0.10\\)?\nLook at p-value for Height coefficient.\n\n\n\n\nmodel2 &lt;- lm(FEV ~ Height * Smoker + Age, data = FEV)\nsummary(model2)\n\n\nCall:\nlm(formula = FEV ~ Height * Smoker + Age, data = FEV)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.51896 -0.28187 -0.01598  0.27434  1.92389 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -4.603951   0.252466 -18.236  &lt; 2e-16 ***\nHeight            0.109355   0.005309  20.599  &lt; 2e-16 ***\nSmokeryes        -3.781015   1.261569  -2.997  0.00284 ** \nAge               0.056087   0.010552   5.315 1.51e-07 ***\nHeight:Smokeryes  0.055393   0.019115   2.898  0.00390 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4395 on 588 degrees of freedom\nMultiple R-squared:  0.7502,    Adjusted R-squared:  0.7485 \nF-statistic: 441.5 on 4 and 588 DF,  p-value: &lt; 2.2e-16\n\n\nDoes smoking status change the association between Height and FEV?\nLook at the p-value for the interaction term (Height:Smokeryes). If p-value &lt; \\(\\alpha\\), then yes, the relationship between Height and FEV differs for smokers vs non-smokers.\n\n\n\nWhich model would you recommend?\n\n\n\nModel\nR-squared\nKey p-values\n\n\n\n\nModel 1 (no interaction)\n___\n\n\n\nModel 2 (with interaction)\n___\nInteraction p-value: ___\n\n\n\nRecommendation: If the interaction term is not significant, prefer the simpler model (Model 1). If significant, the interaction model (Model 2) better captures the relationship.\n\n\n\n\n\nReview Course Related Reviews with Solutions for WPR1 and WPR2\nTEE Review\nReview WPR 1 and WPR 2\n\nReview all Exploration Exercises\n\nReview Board Problems\n\nWork these problems as if you’ve never seen them before - don’t just skip to the answer and assume you know it.\n\n\n\n\nThank you for a great semester in MA206! Good luck on your TEE and future endeavors.\n\n\n\n\n\n\n\n\nAny questions for me?\nCourse evaluations",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 30"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-30.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-30.html#lesson-administration",
    "title": "Lesson 30 - Course Review",
    "section": "",
    "text": "Math vs USMAPS\n\n\n\n\n\n\nPreviously 3-1\n\n\n\n\n\n\n4-1\n\n\n\n\n\n\n\n\n\nMath vs USMAPS\n\n\n\n\n\n\nPreviously 4-1\n\n\n\n\n\n\n5-1\n\n\n\n\n\n\n\n\n\nMath vs Garrison\n\n\n\n\n\n\nPreviously 5-1\n\n\n\n\n\n\n6-1\n\n\n\n\n\n\n\n\n\nMath vs Garrison\n\n\n\n\n\n\nPreviously 6-1\n\n\n\n\n\n\n7-1\n\n\n\n\n\n\n\n\n\n\n\n\nMath vs EECS\n\n\n\n\n\n\nPreviously 2-0\n\n\n\n\n\n\n3-0\n\n\n\n\n\n\n\n\n\nMath vs EECS\n\n\n\n\n\n\nPreviously 3-0\n\n\n\n\n\n\n4-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n0730\n1100\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\n\n\n\nG1\n\n\n\n\n\nWed, 17 Dec (1300-1630): TH324\n\nAfari-Aikins, John\nColdren, Nathan\nConti, Annabella\nFreeman, Brandon\nGeorge, Joshua\nGoetz, Charles\n\nWed, 17 Dec (1300-1630): TH322\n\nLavery, Harrison\nMcDaniel, Jack\nMcDonnell, Hunter\nMcKillop, John\n\nWed, 17 Dec (1300-1630): TH323\n\nMeers, Tehya\nMidberry, James\n\nWed, 17 Dec (1300-1630): TH321\n\nMinicozzi, John\nNoack, Macoy\n\nThu, 18 Dec (0730-1100): TH321\n\nDin, Jenna\nGupta, Aarav\nLawrence, Karina\n\n\n\n\n\n\n\n\n\n\nH1\n\n\n\n\n\nWed, 17 Dec (0730-1100): BH 171A\n\nZagame, Samuel\n\nWed, 17 Dec (1300-1630): TH322\n\nArengo, Mary\nChambers, Cherokee\nDohl, Chad\nHudson-Odoi, Vanessa\nKinkead, Lucas\nRecords, Benjamin\nShelton, Sawyer\nStockbower, Tatiana\nThanepohn, Trevor\nWalter, Benjamin\nWills, Liam\nWint, Logan\n\nThu, 18 Dec (0730-1100): TH321\n\nChau, Paul\nJohnson, Joseph\nRubio, Andrew\nSager, Campbell\nVann, Nehemiah\n\n\n\n\n\n\n\n\n\n\nI1\n\n\n\n\n\nWed, 17 Dec (1300-1630): TH323\n\nAguilar Winchell, Benjamin\nAhn, David\nAndrade, Elena\nBettencourt, Jacob\nBhutani, Dillon\nCampbell, Evan\nForgues, Barbara\nJo, Alex\nLanham, Logan\nMaan, Bahawal\nSchwartz, Joseph\nSindler, Allan\nTahmazian, Isabela\nWamre, Gabrielle\n\nThu, 18 Dec (0730-1100): TH321\n\nHelmkamp, Braeden\nOgordi, Daniel\nPark, Sangwoo\nSmith, Gennaro\n\n\n\n\n\n\n\n\n\n\nI2\n\n\n\n\n\nWed, 17 Dec (1300-1630): TH321\n\nArdisana, James\nBachmann, Christian\nBarksdale, Jordon\nBarvitskie, Mason\nCorbett, Chas\nDavidson, Justin\nGroebner, Samuel\nHarris, Parker\nKim, Danny\nMantell, Jack\nMcKane, Angelina\nNguyen, Ta\nOxendine, Jake\nPatterson, Alyssa\nSpeaks, Brennan\n\nThu, 18 Dec (0730-1100): TH321\n\nArterberry, Myles\nMcPherson, Paige\nWilliams, Caleb",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 30"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-30.html#tee-overview",
    "href": "MA206-AY26-1/lesson-30.html#tee-overview",
    "title": "Lesson 30 - Course Review",
    "section": "",
    "text": "TEE Cover Sheet\n\n\nAuthorized Resources:\n\nYour computer with access to a blank RStudio document (.R, .rmd, or .qmd tab)\nCourse Guide\nTidyverse Tutorial\nTwo pages (front and back) of personally handwritten notes\nIssued calculator\n\nUnauthorized Resources:\n\nInternet\nGenerative AI\nTextbook\nCourse applets\nEmail or other electronic communications\nMusic devices\nFriends / Instructor\n\nImportant Reminders:\n\nShow enough work to logically present your thought process (R code or equations) - demonstrate mastery of course material, not simply a correct answer\nPrepare RStudio by clicking the broom button in “History,” “Environment,” and “Plots” panes; use CTRL+L to clear the Console\nAcademic security: Do not discuss content until released on 19 December @ 1630\n\nAdvice:\n\nDon’t just put down an answer - show how you got to that answer even if you used R/calculator\nYou’ve been taught everything on this exam!",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 30"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-30.html#course-review---non-exhaustive-list-of-topics",
    "href": "MA206-AY26-1/lesson-30.html#course-review---non-exhaustive-list-of-topics",
    "title": "Lesson 30 - Course Review",
    "section": "",
    "text": "Addition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\nMultiplication Rule: \\(P(A \\cap B) = P(A) \\cdot P(B|A)\\)\nComplement Rule: \\(P(A^c) = 1 - P(A)\\)\nConditional Probability: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\nBayes’ Theorem: \\(P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\)\nMutually Exclusive: \\(P(A \\cap B) = 0\\), so \\(P(A \\cup B) = P(A) + P(B)\\)\nIndependence: \\(P(A|B) = P(A)\\), so \\(P(A \\cap B) = P(A) \\cdot P(B)\\)\n\n\n\n\n\nIdentify observational units in a study\nRecognize variable types (categorical vs quantitative)\nDistinguish between observational study and experimental study\nIdentify confounding variables\nKnow sampling methods (convenience sample, random sample, etc.)\nChoose appropriate plot types based on variable type:\n\nCategorical: bar graph\nQuantitative: histogram, dot plot, box plot\n\n\n\n\n\n\nKnow the difference between parameter (population) and statistic (sample)\nKnow when we can claim causality (random assignment)\nKnow when we can claim generalization (random sampling)\n\n\n\n\n\nKnow when to use PMF, PDF, and CDF for discrete and continuous variables\nCalculate probabilities using PMF (discrete) and PDF (continuous)\nGo from PDF to CDF for continuous random variables to include piecewise PDFs (integration)\nCalculate expected value \\(E(X)\\), variance \\(Var(X)\\), and standard deviation \\(SD(X)\\)\nApply linear transformation rules: \\(E(aX + b) = aE(X) + b\\) and \\(Var(aX + b) = a^2 Var(X)\\)\n\n\n\n\nKnow when to use each test, validity conditions, and how to execute:\n\n\n\n\n\n\n\nTest\nWhen to Use\n\n\n\n\nOne-proportion z-test\nOne categorical variable, testing proportion against a value\n\n\nOne-sample t-test\nOne quantitative variable, testing mean against a value\n\n\nTwo-proportion z-test\nComparing proportions between two groups\n\n\nTwo-sample t-test\nComparing means between two independent groups\n\n\nPaired t-test\nComparing means for paired/matched data\n\n\n\nFor each test, know how to:\n\nWrite null and alternative hypotheses (symbols and words)\nCheck validity conditions\nCalculate statistic, standardized statistic, and p-value\nInterpret p-value (probability of observing data at least as extreme, given null is true)\nDraw conclusions comparing p-value to significance level\nKnow when to use z-distribution vs t-distribution:\n\nz: proportions (known population standard deviation)\nt: means (unknown population standard deviation, using sample SD)\n\n\n\n\n\n\nInterpret confidence intervals correctly\nCalculate margin of error (half the width of CI)\nDetermine if a value is plausible based on CI\n\n\n\n\n\nDescribe scatterplot characteristics: direction, form, strength of association\nExecute simple and multiple linear regression with data\nInterpret coefficients for continuous variables (“for each 1-unit increase…”)\nInterpret coefficients for categorical variables (“compared to the reference group…”)\nInterpret coefficients “after controlling for” or “after accounting for” other variables\nInterpret interaction terms\nCompare models using p-values and \\(R^2\\)\nAssess statistical significance of coefficients",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 30"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-30.html#in-class-examples",
    "href": "MA206-AY26-1/lesson-30.html#in-class-examples",
    "title": "Lesson 30 - Course Review",
    "section": "",
    "text": "Researchers studied where people stop when approaching a stop sign. Do people have a preference for stopping position, or do they choose randomly between stopping as a single car, following another car, or leading a group?\n\nStop &lt;- ma206data::chap8_Stop\nhead(Stop)\n\n# A tibble: 6 × 1\n  position_stop\n  &lt;chr&gt;        \n1 single       \n2 single       \n3 single       \n4 single       \n5 single       \n6 single       \n\ntable(Stop)\n\nposition_stop\nfollow   lead single \n    98     42    176 \n\n\nStep 1: Set up hypotheses\nIf people had no preference, they would choose each position with equal probability (1/3).\n\\(H_0: \\pi = 1/3\\) (no preference - people choose “single” at the same rate as random chance)\n\\(H_a: \\pi &gt; 1/3\\) (people prefer to stop as a single car)\nStep 2: Calculate the statistic\n\n# Count successes (single) and total\nn &lt;- nrow(Stop)\nx &lt;- sum(Stop$position_stop == \"single\")\np_hat &lt;- x / n\n\ncat(\"n =\", n, \"\\n\")\n\nn = 316 \n\ncat(\"x (single) =\", x, \"\\n\")\n\nx (single) = 176 \n\ncat(\"p-hat =\", round(p_hat, 3), \"\\n\")\n\np-hat = 0.557 \n\n\nStep 3: Conduct the test\n\\[z = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}}\\]\n\n# Null hypothesis value\npi_0 &lt;- 1/3\n\n# Standard error under the null\nSE &lt;- sqrt(pi_0 * (1 - pi_0) / n)\n\n# Standardized statistic (z-score)\nz &lt;- (p_hat - pi_0) / SE\n\ncat(\"Standard Error =\", round(SE, 4), \"\\n\")\n\nStandard Error = 0.0265 \n\ncat(\"z =\", round(z, 3), \"\\n\")\n\nz = 8.433 \n\n# p-value (one-sided, greater than)\np_value &lt;- 1 - pnorm(z)\ncat(\"p-value =\", round(p_value, 4), \"\\n\")\n\np-value = 0 \n\n\nStep 4: Confidence Interval\n\\[\\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\n# For CI, use standard error based on p-hat (not pi_0)\nSE_ci &lt;- sqrt(p_hat * (1 - p_hat) / n)\n\n# 90% CI (for alpha = 0.10)\nz_star &lt;- qnorm(0.95)  # one-sided, so 0.95\nCI_lower &lt;- p_hat - z_star * SE_ci\nCI_upper &lt;- p_hat + z_star * SE_ci\n\ncat(\"90% CI: (\", round(CI_lower, 3), \",\", round(CI_upper, 3), \")\\n\")\n\n90% CI: ( 0.511 , 0.603 )\n\n\nStep 5: Draw conclusion\nCompare p-value to significance level and state conclusion in context.\n\n\n\nResearchers want to understand factors that affect lung capacity (FEV - Forced Expiratory Volume). They collected data on age, height, gender, and smoking status.\n\nFEV &lt;- ma206data::fev\nhead(FEV)\n\n# A tibble: 6 × 5\n    Age   FEV Height Gender Smoker\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n1    11  3.90   67   Female no    \n2    11  3.98   68.5 Male   no    \n3     8  2.17   57   Male   no    \n4    11  3.74   68   Male   no    \n5    11  2.94   63   Female no    \n6    15  2.73   63   Female no    \n\n\n\n\n\nggplot(FEV, aes(x = Height, y = FEV)) +\n  geom_point() +\n  labs(x = \"Height (inches)\", y = \"FEV (liters)\",\n       title = \"Height vs. Lung Capacity\")\n\n\n\n\n\n\n\n\nLets talk association direction, form, and strength:\nDirection:\nForm:\nStrength:\n\n\n\n\nmodel1 &lt;- lm(FEV ~ Height + Age + Smoker, data = FEV)\nsummary(model1)\n\n\nCall:\nlm(formula = FEV ~ Height + Age + Smoker, data = FEV)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5349 -0.2903 -0.0146  0.2812  1.9197 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.77287    0.24718 -19.309  &lt; 2e-16 ***\nHeight       0.11288    0.00520  21.707  &lt; 2e-16 ***\nAge          0.05142    0.01049   4.900 1.24e-06 ***\nSmokeryes   -0.13031    0.06685  -1.949   0.0517 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4422 on 589 degrees of freedom\nMultiple R-squared:  0.7466,    Adjusted R-squared:  0.7454 \nF-statistic: 578.6 on 3 and 589 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation of Height coefficient:\nAfter controlling for Age and Smoker status, for each additional inch of height, FEV increases by approximately ___ liters, on average.\nInterpretation of Smoker coefficient:\nAfter controlling for Height and Age, smokers have an FEV that is approximately ___ liters [higher/lower] than non-smokers, on average.\nIs Height statistically significant at \\(\\alpha = 0.10\\)?\nLook at p-value for Height coefficient.\n\n\n\n\nmodel2 &lt;- lm(FEV ~ Height * Smoker + Age, data = FEV)\nsummary(model2)\n\n\nCall:\nlm(formula = FEV ~ Height * Smoker + Age, data = FEV)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.51896 -0.28187 -0.01598  0.27434  1.92389 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -4.603951   0.252466 -18.236  &lt; 2e-16 ***\nHeight            0.109355   0.005309  20.599  &lt; 2e-16 ***\nSmokeryes        -3.781015   1.261569  -2.997  0.00284 ** \nAge               0.056087   0.010552   5.315 1.51e-07 ***\nHeight:Smokeryes  0.055393   0.019115   2.898  0.00390 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4395 on 588 degrees of freedom\nMultiple R-squared:  0.7502,    Adjusted R-squared:  0.7485 \nF-statistic: 441.5 on 4 and 588 DF,  p-value: &lt; 2.2e-16\n\n\nDoes smoking status change the association between Height and FEV?\nLook at the p-value for the interaction term (Height:Smokeryes). If p-value &lt; \\(\\alpha\\), then yes, the relationship between Height and FEV differs for smokers vs non-smokers.\n\n\n\nWhich model would you recommend?\n\n\n\nModel\nR-squared\nKey p-values\n\n\n\n\nModel 1 (no interaction)\n___\n\n\n\nModel 2 (with interaction)\n___\nInteraction p-value: ___\n\n\n\nRecommendation: If the interaction term is not significant, prefer the simpler model (Model 1). If significant, the interaction model (Model 2) better captures the relationship.\n\n\n\n\n\nReview Course Related Reviews with Solutions for WPR1 and WPR2\nTEE Review\nReview WPR 1 and WPR 2\n\nReview all Exploration Exercises\n\nReview Board Problems\n\nWork these problems as if you’ve never seen them before - don’t just skip to the answer and assume you know it.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 30"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-30.html#thank-you",
    "href": "MA206-AY26-1/lesson-30.html#thank-you",
    "title": "Lesson 30 - Course Review",
    "section": "",
    "text": "Thank you for a great semester in MA206! Good luck on your TEE and future endeavors.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 30"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-30.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-30.html#before-you-leave",
    "title": "Lesson 30 - Course Review",
    "section": "",
    "text": "Any questions for me?\nCourse evaluations",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 30"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-29.html",
    "href": "MA206-AY26-1/lesson-29.html",
    "title": "Lesson 29 - Writers Workshop",
    "section": "",
    "text": "Today: Peer Review of Draft Technical Reports\nBring: 3 copies of your draft technical report\nBring: Pen or pencil to provide feedback\nInstructions\n\n\n\n\n\nLesson 30\nFinal Technical Report\nCourse Review\nDue: Submit final version incorporating feedback\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\nG1\n\n\n\n\n\nWed, 17 Dec (1300-1630)\n\nAfari-Aikins, John\nColdren, Nathan\nConti, Annabella\nFreeman, Brandon\nGeorge, Joshua\nGoetz, Charles\nLavery, Harrison\nMcDaniel, Jack\nMcDonnell, Hunter\nMcKillop, John\nMeers, Tehya\nMidberry, James\nMinicozzi, John\nNoack, Macoy\n\nThu, 18 Dec (0730-1100)\n\nDin, Jenna\nGupta, Aarav\nLawrence, Karina\n\n\n\n\n\n\n\n\n\n\nH1\n\n\n\n\n\nWed, 17 Dec (1300-1630)\n\nArengo, Mary\nChambers, Cherokee\nDohl, Chad\nHudson-Odoi, Vanessa\nKinkead, Lucas\nRecords, Benjamin\nShelton, Sawyer\nStockbower, Tatiana\nThanepohn, Trevor\nWalter, Benjamin\nWills, Liam\nWint, Logan\nZagame, Samuel\n\nThu, 18 Dec (0730-1100)\n\nChau, Paul\nJohnson, Joseph\nRubio, Andrew\nSager, Campbell\nVann, Nehemiah\n\n\n\n\n\n\n\n\n\n\nI1\n\n\n\n\n\nWed, 17 Dec (1300-1630)\n\nAguilar Winchell, Benjamin\nAhn, David\nAndrade, Elena\nBettencourt, Jacob\nBhutani, Dillon\nCampbell, Evan\nForgues, Barbara\nHelmkamp, Braeden\nJo, Alex\nLanham, Logan\nMaan, Bahawal\nSchwartz, Joseph\nSindler, Allan\nTahmazian, Isabela\nWamre, Gabrielle\n\nThu, 18 Dec (0730-1100)\n\nOgordi, Daniel\nPark, Sangwoo\nSmith, Gennaro\n\n\n\n\n\n\n\n\n\n\nI2\n\n\n\n\n\nWed, 17 Dec (1300-1630)\n\nArdisana, James\nBachmann, Christian\nBarksdale, Jordon\nBarvitskie, Mason\nCorbett, Chas\nDavidson, Justin\nGroebner, Samuel\nHarris, Parker\nKim, Danny\nMantell, Jack\nMcKane, Angelina\nNguyen, Ta\nOxendine, Jake\nPatterson, Alyssa\nSpeaks, Brennan\n\nThu, 18 Dec (0730-1100)\n\nArterberry, Myles\nMcPherson, Paige\nWilliams, Caleb\n\n\n\n\n\n\nMath vs …\n\n\n\n\n\n\nPreviously 0-1\n\n\n\n\n\n\n1-1\n\n\n\n\n\n\n\n\n\nMath vs Garrison\n\n\n\n\n\n\nPreviously 1-1\n\n\n\n\n\n\n2-1\n\n\n\n\n\n\n\n\n\nMath vs Garrison\n\n\n\n\n\n\nPreviously 2-1\n\n\n\n\n\n\n3-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEach person should have 3 copies\nGive them to me - I will hand them back out one at a time\nReading (15 minutes) - Read each report and make notes directly on the paper\n\nFeedback should be specific to the rubric I handed out\n\nProvide verbal feedback (5 minutes total)\nRepeat for each report\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nIncorporate today’s feedback into your final technical report\nSubmit Milestone 8 next lesson",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 29"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-29.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-29.html#lesson-administration",
    "title": "Lesson 29 - Writers Workshop",
    "section": "",
    "text": "Today: Peer Review of Draft Technical Reports\nBring: 3 copies of your draft technical report\nBring: Pen or pencil to provide feedback\nInstructions\n\n\n\n\n\nLesson 30\nFinal Technical Report\nCourse Review\nDue: Submit final version incorporating feedback\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\nG1\n\n\n\n\n\nWed, 17 Dec (1300-1630)\n\nAfari-Aikins, John\nColdren, Nathan\nConti, Annabella\nFreeman, Brandon\nGeorge, Joshua\nGoetz, Charles\nLavery, Harrison\nMcDaniel, Jack\nMcDonnell, Hunter\nMcKillop, John\nMeers, Tehya\nMidberry, James\nMinicozzi, John\nNoack, Macoy\n\nThu, 18 Dec (0730-1100)\n\nDin, Jenna\nGupta, Aarav\nLawrence, Karina\n\n\n\n\n\n\n\n\n\n\nH1\n\n\n\n\n\nWed, 17 Dec (1300-1630)\n\nArengo, Mary\nChambers, Cherokee\nDohl, Chad\nHudson-Odoi, Vanessa\nKinkead, Lucas\nRecords, Benjamin\nShelton, Sawyer\nStockbower, Tatiana\nThanepohn, Trevor\nWalter, Benjamin\nWills, Liam\nWint, Logan\nZagame, Samuel\n\nThu, 18 Dec (0730-1100)\n\nChau, Paul\nJohnson, Joseph\nRubio, Andrew\nSager, Campbell\nVann, Nehemiah\n\n\n\n\n\n\n\n\n\n\nI1\n\n\n\n\n\nWed, 17 Dec (1300-1630)\n\nAguilar Winchell, Benjamin\nAhn, David\nAndrade, Elena\nBettencourt, Jacob\nBhutani, Dillon\nCampbell, Evan\nForgues, Barbara\nHelmkamp, Braeden\nJo, Alex\nLanham, Logan\nMaan, Bahawal\nSchwartz, Joseph\nSindler, Allan\nTahmazian, Isabela\nWamre, Gabrielle\n\nThu, 18 Dec (0730-1100)\n\nOgordi, Daniel\nPark, Sangwoo\nSmith, Gennaro\n\n\n\n\n\n\n\n\n\n\nI2\n\n\n\n\n\nWed, 17 Dec (1300-1630)\n\nArdisana, James\nBachmann, Christian\nBarksdale, Jordon\nBarvitskie, Mason\nCorbett, Chas\nDavidson, Justin\nGroebner, Samuel\nHarris, Parker\nKim, Danny\nMantell, Jack\nMcKane, Angelina\nNguyen, Ta\nOxendine, Jake\nPatterson, Alyssa\nSpeaks, Brennan\n\nThu, 18 Dec (0730-1100)\n\nArterberry, Myles\nMcPherson, Paige\nWilliams, Caleb\n\n\n\n\n\n\nMath vs …\n\n\n\n\n\n\nPreviously 0-1\n\n\n\n\n\n\n1-1\n\n\n\n\n\n\n\n\n\nMath vs Garrison\n\n\n\n\n\n\nPreviously 1-1\n\n\n\n\n\n\n2-1\n\n\n\n\n\n\n\n\n\nMath vs Garrison\n\n\n\n\n\n\nPreviously 2-1\n\n\n\n\n\n\n3-1",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 29"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-29.html#writers-workshop-process",
    "href": "MA206-AY26-1/lesson-29.html#writers-workshop-process",
    "title": "Lesson 29 - Writers Workshop",
    "section": "",
    "text": "Each person should have 3 copies\nGive them to me - I will hand them back out one at a time\nReading (15 minutes) - Read each report and make notes directly on the paper\n\nFeedback should be specific to the rubric I handed out\n\nProvide verbal feedback (5 minutes total)\nRepeat for each report",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 29"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-29.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-29.html#before-you-leave",
    "title": "Lesson 29 - Writers Workshop",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nIncorporate today’s feedback into your final technical report\nSubmit Milestone 8 next lesson",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 29"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-27.html",
    "href": "MA206-AY26-1/lesson-27.html",
    "title": "Lesson 27 - Course Project In-Progress Review",
    "section": "",
    "text": "Download your feedback from GradeScope.\n\n\n\n\nLesson 27 - Today!!\nDraft Presentation\nPresentation Template\n\n\n\n\n\nNeed to know who will not be here on presentation day because of CPRC\n\n2/3 December - Primary\n\n24/25 Nov Alternate\n\nMy current expectations\n\n\n\n\n\n\nG1\n\n\n\n\n\n2 Dec\n\n1: Charles Goetz, John McKillop\n\n2: Jack McDaniel, Hunter McDonnell\n\n4: Jenna Din, Karina Lawrence\n\n5: John Afari-Aikins, Joshua George\n\n6: Harrison Lavery, James Midberry, Macoy Noack\n\n7: Nathan Coldren, Aarav Gupta\n\n8: Brandon Freeman, John Minicozzi\n\n24 Nov\n- 3: Annabella Conti, Tehya Meers\n\n\n\n\n\n\n\n\n\nH1\n\n\n\n\n\n2 Dec\n\n1: Joseph Johnson, Nehemiah Vann\n\n2: Lucas Kinkead, Andrew Rubio\n\n3: Cherokee Chambers, Trevor Thanepohn\n\n4: Mary Arengo, Chad Dohl\n\n5: Vanessa Hudson-Odoi, Tatiana Stockbower\n\n6: Sawyer Shelton, Benjamin Walter\n\n9: Campbell Sager, Samuel Zagame\n\n24 Nov\n\n7: Liam Wills, Logan Wint\n\n8: Paul Chau, Benjamin Records\n\n\n\n\n\n\n\n\n\n\nI1\n\n\n\n\n\n2 Dec\n\n1: Elena Andrade, Gabrielle Warnre\n\n2: Evan Campbell, Allan Sindler\n\n3: Benjamin Aguilar Winchell, Dillon Bhutani\n\n4: Bahawal Maan, Joseph Schwartz\n\n5: Jacob Bettencourt, Daniel Ogordi\n\n6: Braeden Helmkamp, Isabela Tahmazian\n\n7: Barbara Forgues, Alex Jo\n\n8: David Ahn, Sangwoo Park\n\n9: Gennaro Smith\n\n24 Nov\n\n9: Logan Lanham\n\n\n\n\n\n\n\n\n\n\nI2\n\n\n\n\n\n3 Dec\n25 Nov\n\nPaige McPherson, Justin Davidson\n\nChristian Bachmann, Parker Harris\n\n\n\n\n\n\n\n\nDraft Technical Report\nLesson 29: Bring 3 Copies\nBring a pen to mark edits\nInstructions\n\n\n\n\n\nLesson 30\nFinal Technical Report\nCourse Review\n\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\nMath 1 vs Systems and Friends\n\n\n\n\n\n\nPreviously 7-0\n\n\n\n\n\n\n8-0\n\n\n\n\n\n\n\n\n\n\n\n\nMath 1 vs…\n\n\n\n\n\n\nPreviously 0-0\n\n\n\n\n\n\n0-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 27"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-27.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-27.html#lesson-administration",
    "title": "Lesson 27 - Course Project In-Progress Review",
    "section": "",
    "text": "Download your feedback from GradeScope.\n\n\n\n\nLesson 27 - Today!!\nDraft Presentation\nPresentation Template\n\n\n\n\n\nNeed to know who will not be here on presentation day because of CPRC\n\n2/3 December - Primary\n\n24/25 Nov Alternate\n\nMy current expectations\n\n\n\n\n\n\nG1\n\n\n\n\n\n2 Dec\n\n1: Charles Goetz, John McKillop\n\n2: Jack McDaniel, Hunter McDonnell\n\n4: Jenna Din, Karina Lawrence\n\n5: John Afari-Aikins, Joshua George\n\n6: Harrison Lavery, James Midberry, Macoy Noack\n\n7: Nathan Coldren, Aarav Gupta\n\n8: Brandon Freeman, John Minicozzi\n\n24 Nov\n- 3: Annabella Conti, Tehya Meers\n\n\n\n\n\n\n\n\n\nH1\n\n\n\n\n\n2 Dec\n\n1: Joseph Johnson, Nehemiah Vann\n\n2: Lucas Kinkead, Andrew Rubio\n\n3: Cherokee Chambers, Trevor Thanepohn\n\n4: Mary Arengo, Chad Dohl\n\n5: Vanessa Hudson-Odoi, Tatiana Stockbower\n\n6: Sawyer Shelton, Benjamin Walter\n\n9: Campbell Sager, Samuel Zagame\n\n24 Nov\n\n7: Liam Wills, Logan Wint\n\n8: Paul Chau, Benjamin Records\n\n\n\n\n\n\n\n\n\n\nI1\n\n\n\n\n\n2 Dec\n\n1: Elena Andrade, Gabrielle Warnre\n\n2: Evan Campbell, Allan Sindler\n\n3: Benjamin Aguilar Winchell, Dillon Bhutani\n\n4: Bahawal Maan, Joseph Schwartz\n\n5: Jacob Bettencourt, Daniel Ogordi\n\n6: Braeden Helmkamp, Isabela Tahmazian\n\n7: Barbara Forgues, Alex Jo\n\n8: David Ahn, Sangwoo Park\n\n9: Gennaro Smith\n\n24 Nov\n\n9: Logan Lanham\n\n\n\n\n\n\n\n\n\n\nI2\n\n\n\n\n\n3 Dec\n25 Nov\n\nPaige McPherson, Justin Davidson\n\nChristian Bachmann, Parker Harris\n\n\n\n\n\n\n\n\nDraft Technical Report\nLesson 29: Bring 3 Copies\nBring a pen to mark edits\nInstructions\n\n\n\n\n\nLesson 30\nFinal Technical Report\nCourse Review\n\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\nMath 1 vs Systems and Friends\n\n\n\n\n\n\nPreviously 7-0\n\n\n\n\n\n\n8-0\n\n\n\n\n\n\n\n\n\n\n\n\nMath 1 vs…\n\n\n\n\n\n\nPreviously 0-0\n\n\n\n\n\n\n0-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 27"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-27.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-27.html#before-you-leave",
    "title": "Lesson 27 - Course Project In-Progress Review",
    "section": "",
    "text": "Any questions for me?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 27"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-25.html",
    "href": "MA206-AY26-1/lesson-25.html",
    "title": "Lesson 25 - Multiple Linear Regression II",
    "section": "",
    "text": "Lesson 25.5\nChange: 9 November 2359\nMilestone 5 Instructions\n\n\n\n\n\nLesson 26\n12-13 November\nLink: TBD\n\n\n\n\n\nLesson 26.5\n17 November\nMilestone 6 Instructions\n\n\n\n\n\nNeed to know who will not be here on presentation day because of CPRC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath 1 vs…\n\n\n\n\n\n\nPreviously 0-0\n\n\n\n\n\n\n1-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI got nothing… So enjoy these\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\nRemember this from last class?\n\nmod1 &lt;- lm(formula = mpg ~ wt + as.factor(cyl), data = mtcars)\nsummary(mod1)\n\n\nCall:\nlm(formula = mpg ~ wt + as.factor(cyl), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5890 -1.2357 -0.5159  1.3845  5.7915 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      33.9908     1.8878  18.006  &lt; 2e-16 ***\nwt               -3.2056     0.7539  -4.252 0.000213 ***\nas.factor(cyl)6  -4.2556     1.3861  -3.070 0.004718 ** \nas.factor(cyl)8  -6.0709     1.6523  -3.674 0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.557 on 28 degrees of freedom\nMultiple R-squared:  0.8374,    Adjusted R-squared:   0.82 \nF-statistic: 48.08 on 3 and 28 DF,  p-value: 3.594e-11\n\n\nThe fitted regression equation is:\n\\[\n\\widehat{mpg} = 33.99 - 3.21(\\text{wt}) - 4.26(\\text{cyl6}) - 6.07(\\text{cyl8})\n\\]\nTo predict fuel efficiency for a 3000-lb, 6-cylinder car:\n\\[\n\\begin{aligned}\n\\widehat{mpg}\n&= 33.99 - 3.21(3.0) - 4.26(1) - 6.07(0) \\\\[6pt]\n&= 33.99 - 9.63 - 4.26 \\\\[6pt]\n&= 20.10\n\\end{aligned}\n\\]\nSo the model predicts:\n\\[\n\\boxed{\\widehat{mpg} = 20.1}\n\\]\n\n\n\n\n\nmod2 &lt;- lm(formula = mpg ~ wt + as.factor(cyl) + hp + hp:as.factor(cyl), data = mtcars)\nsummary(mod2)\n\n\nCall:\nlm(formula = mpg ~ wt + as.factor(cyl) + hp + hp:as.factor(cyl), \n    data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1864 -1.4098 -0.4022  1.0186  4.3920 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         41.87732    3.23293  12.953 1.37e-12 ***\nwt                  -3.05994    0.68275  -4.482 0.000143 ***\nas.factor(cyl)6     -9.98213    5.76950  -1.730 0.095931 .  \nas.factor(cyl)8    -11.72793    4.22507  -2.776 0.010276 *  \nhp                  -0.09947    0.03487  -2.853 0.008576 ** \nas.factor(cyl)6:hp   0.07809    0.05236   1.492 0.148335    \nas.factor(cyl)8:hp   0.08602    0.03703   2.323 0.028601 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.3 on 25 degrees of freedom\nMultiple R-squared:  0.8826,    Adjusted R-squared:  0.8544 \nF-statistic: 31.32 on 6 and 25 DF,  p-value: 1.831e-10\n\n\nWe can write the fitted model as\n\\[\n\\begin{aligned}\n\\widehat{mpg}\n&= 41.877\n- 3.060\\,(\\text{wt})\n- 9.982\\,(\\text{cyl6})\n- 11.728\\,(\\text{cyl8}) - 0.09947\\,(\\text{hp})\n+ 0.07809\\,(\\text{hp}\\times \\text{cyl6})\n+ 0.08602\\,(\\text{hp}\\times \\text{cyl8})\n\\end{aligned}\n\\]\n\n\nWhich one is better?\nLets turn to \\(R^2\\)\nModel 1: .8374\nModel 2: .8826\n\n\n\n\nLinearity — the relationship between predictors and the response is roughly linear\nIndependence — observations are independent of each other\nNormal Distribution — residuals are approximately normal\nEqual Variance — variability of residuals is consistent across fitted values\n\n\nCheck whether the relationship between predictors and the response is roughly linear.\nUse a residuals vs fitted plot — you want to see a random scatter (no pattern).\n\nplot(mod1, which = 1)\n\n\n\n\n\n\n\n\n\n\n\nWe can’t test this from the model alone — it depends on how the data were collected.\nYou must verify that each observation is independent (e.g., different cars, people, or trials).\n\n\n\nCheck whether residuals are approximately normally distributed using a Q-Q plot.\n\nplot(mod1, which = 2)\n\n\n\n\n\n\n\n\n\n\n\nCheck for constant variance (homoscedasticity) — residuals should have similar spread across fitted values.\n\nplot(mod1, which = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\ndata(\"diamonds\")\n\ndiamonds |&gt; \n  sample_n(size = 1000) |&gt; \n  pivot_longer(cols = -c(cut, clarity, price, color)) |&gt; \n  ggplot(aes(x = value, y = price, colour = color)) +\n  geom_point() +\n  facet_wrap(~name, scales = \"free\")\n\n\n\n\n\n\n\nmod1 &lt;- lm(formula = price ~ carat + depth + table + x + y + z + cut + color + clarity, data = diamonds)\nsummary(mod1)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + x + y + z + cut + \n    color + clarity, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21376.0   -592.4   -183.5    376.4  10694.2 \n\nCoefficients:\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)  5753.762    396.630   14.507  &lt; 2e-16 ***\ncarat       11256.978     48.628  231.494  &lt; 2e-16 ***\ndepth         -63.806      4.535  -14.071  &lt; 2e-16 ***\ntable         -26.474      2.912   -9.092  &lt; 2e-16 ***\nx           -1008.261     32.898  -30.648  &lt; 2e-16 ***\ny               9.609     19.333    0.497  0.61918    \nz             -50.119     33.486   -1.497  0.13448    \ncut.L         584.457     22.478   26.001  &lt; 2e-16 ***\ncut.Q        -301.908     17.994  -16.778  &lt; 2e-16 ***\ncut.C         148.035     15.483    9.561  &lt; 2e-16 ***\ncut^4         -20.794     12.377   -1.680  0.09294 .  \ncolor.L     -1952.160     17.342 -112.570  &lt; 2e-16 ***\ncolor.Q      -672.054     15.777  -42.597  &lt; 2e-16 ***\ncolor.C      -165.283     14.725  -11.225  &lt; 2e-16 ***\ncolor^4        38.195     13.527    2.824  0.00475 ** \ncolor^5       -95.793     12.776   -7.498 6.59e-14 ***\ncolor^6       -48.466     11.614   -4.173 3.01e-05 ***\nclarity.L    4097.431     30.259  135.414  &lt; 2e-16 ***\nclarity.Q   -1925.004     28.227  -68.197  &lt; 2e-16 ***\nclarity.C     982.205     24.152   40.668  &lt; 2e-16 ***\nclarity^4    -364.918     19.285  -18.922  &lt; 2e-16 ***\nclarity^5     233.563     15.752   14.828  &lt; 2e-16 ***\nclarity^6       6.883     13.715    0.502  0.61575    \nclarity^7      90.640     12.103    7.489 7.06e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1130 on 53916 degrees of freedom\nMultiple R-squared:  0.9198,    Adjusted R-squared:  0.9198 \nF-statistic: 2.688e+04 on 23 and 53916 DF,  p-value: &lt; 2.2e-16\n\nmod2 &lt;- lm(formula = price ~ carat + depth + table + x + z + cut + color + clarity, data = diamonds)\nsummary(mod2)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + x + z + cut + color + \n    clarity, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21378.8   -592.5   -183.5    376.3  10694.1 \n\nCoefficients:\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)  5768.782    395.474   14.587  &lt; 2e-16 ***\ncarat       11257.752     48.602  231.630  &lt; 2e-16 ***\ndepth         -64.003      4.517  -14.168  &lt; 2e-16 ***\ntable         -26.501      2.911   -9.103  &lt; 2e-16 ***\nx           -1000.354     28.795  -34.740  &lt; 2e-16 ***\nz             -47.925     33.194   -1.444  0.14880    \ncut.L         584.600     22.476   26.010  &lt; 2e-16 ***\ncut.Q        -302.211     17.983  -16.805  &lt; 2e-16 ***\ncut.C         148.446     15.461    9.601  &lt; 2e-16 ***\ncut^4         -20.619     12.371   -1.667  0.09559 .  \ncolor.L     -1952.179     17.342 -112.572  &lt; 2e-16 ***\ncolor.Q      -672.075     15.777  -42.599  &lt; 2e-16 ***\ncolor.C      -165.277     14.725  -11.224  &lt; 2e-16 ***\ncolor^4        38.193     13.526    2.824  0.00475 ** \ncolor^5       -95.780     12.776   -7.497 6.64e-14 ***\ncolor^6       -48.452     11.614   -4.172 3.02e-05 ***\nclarity.L    4097.613     30.256  135.431  &lt; 2e-16 ***\nclarity.Q   -1925.133     28.226  -68.205  &lt; 2e-16 ***\nclarity.C     982.322     24.150   40.676  &lt; 2e-16 ***\nclarity^4    -364.976     19.285  -18.926  &lt; 2e-16 ***\nclarity^5     233.635     15.751   14.833  &lt; 2e-16 ***\nclarity^6       6.871     13.715    0.501  0.61640    \nclarity^7      90.622     12.103    7.487 7.13e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1130 on 53917 degrees of freedom\nMultiple R-squared:  0.9198,    Adjusted R-squared:  0.9198 \nF-statistic: 2.81e+04 on 22 and 53917 DF,  p-value: &lt; 2.2e-16\n\n## Candidate model\nmod3 &lt;- lm(formula = price ~ carat + depth + table + x + cut + color + clarity, data = diamonds)\nsummary(mod3)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + x + cut + color + \n    clarity, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21385.0   -592.4   -183.7    376.5  10694.6 \n\nCoefficients:\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)  5935.107    378.328   15.688  &lt; 2e-16 ***\ncarat       11256.968     48.600  231.626  &lt; 2e-16 ***\ndepth         -66.769      4.091  -16.322  &lt; 2e-16 ***\ntable         -26.457      2.911   -9.089  &lt; 2e-16 ***\nx           -1029.478     20.549  -50.098  &lt; 2e-16 ***\ncut.L         584.717     22.476   26.015  &lt; 2e-16 ***\ncut.Q        -302.037     17.983  -16.795  &lt; 2e-16 ***\ncut.C         148.065     15.459    9.578  &lt; 2e-16 ***\ncut^4         -21.253     12.364   -1.719  0.08562 .  \ncolor.L     -1952.128     17.342 -112.568  &lt; 2e-16 ***\ncolor.Q      -672.207     15.777  -42.608  &lt; 2e-16 ***\ncolor.C      -165.451     14.724  -11.236  &lt; 2e-16 ***\ncolor^4        38.261     13.526    2.829  0.00468 ** \ncolor^5       -95.816     12.776   -7.500 6.50e-14 ***\ncolor^6       -48.441     11.614   -4.171 3.04e-05 ***\nclarity.L    4096.912     30.253  135.423  &lt; 2e-16 ***\nclarity.Q   -1924.681     28.224  -68.192  &lt; 2e-16 ***\nclarity.C     982.004     24.149   40.664  &lt; 2e-16 ***\nclarity^4    -364.870     19.285  -18.920  &lt; 2e-16 ***\nclarity^5     233.449     15.751   14.822  &lt; 2e-16 ***\nclarity^6       6.973     13.715    0.508  0.61114    \nclarity^7      90.738     12.103    7.497 6.63e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1130 on 53918 degrees of freedom\nMultiple R-squared:  0.9198,    Adjusted R-squared:  0.9198 \nF-statistic: 2.944e+04 on 21 and 53918 DF,  p-value: &lt; 2.2e-16\n\ndiamonds |&gt; \n  select(-c(cut, color, clarity)) |&gt; \n  sample_n(1000) |&gt; \n  pairs()\n\n\n\n\n\n\n\nmod4 &lt;- lm(formula = price ~ carat + depth + table + x + cut + color + clarity + carat:color, data = diamonds)\nsummary(mod4)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + x + cut + color + \n    clarity + carat:color, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16732.9   -513.8   -136.6    358.5  10550.5 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   10393.339    359.347  28.923  &lt; 2e-16 ***\ncarat         12646.011     49.034 257.900  &lt; 2e-16 ***\ndepth           -97.355      3.860 -25.223  &lt; 2e-16 ***\ntable           -28.297      2.735 -10.348  &lt; 2e-16 ***\nx             -1624.864     20.623 -78.791  &lt; 2e-16 ***\ncut.L           594.594     21.114  28.161  &lt; 2e-16 ***\ncut.Q          -274.247     16.896 -16.232  &lt; 2e-16 ***\ncut.C           132.434     14.523   9.119  &lt; 2e-16 ***\ncut^4           -12.347     11.613  -1.063  0.28770    \ncolor.L         329.781     34.252   9.628  &lt; 2e-16 ***\ncolor.Q         513.842     31.643  16.239  &lt; 2e-16 ***\ncolor.C        -119.817     29.087  -4.119 3.81e-05 ***\ncolor^4          69.759     26.171   2.665  0.00769 ** \ncolor^5         250.440     24.647  10.161  &lt; 2e-16 ***\ncolor^6         125.580     22.195   5.658 1.54e-08 ***\nclarity.L      4113.880     28.459 144.556  &lt; 2e-16 ***\nclarity.Q     -1981.152     26.554 -74.607  &lt; 2e-16 ***\nclarity.C       991.093     22.698  43.665  &lt; 2e-16 ***\nclarity^4      -353.021     18.117 -19.485  &lt; 2e-16 ***\nclarity^5       208.673     14.801  14.099  &lt; 2e-16 ***\nclarity^6        26.858     12.884   2.085  0.03711 *  \nclarity^7       107.303     11.372   9.436  &lt; 2e-16 ***\ncarat:color.L -2454.649     34.150 -71.878  &lt; 2e-16 ***\ncarat:color.Q -1020.521     31.031 -32.887  &lt; 2e-16 ***\ncarat:color.C   212.081     29.261   7.248 4.29e-13 ***\ncarat:color^4    19.164     27.152   0.706  0.48031    \ncarat:color^5  -396.193     26.071 -15.196  &lt; 2e-16 ***\ncarat:color^6  -192.062     24.204  -7.935 2.14e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1061 on 53912 degrees of freedom\nMultiple R-squared:  0.9293,    Adjusted R-squared:  0.9292 \nF-statistic: 2.623e+04 on 27 and 53912 DF,  p-value: &lt; 2.2e-16\n\n## Candidate model\nmod5 &lt;- lm(formula = price ~ carat + depth + table + x + cut + color + clarity + carat:color + clarity:carat, data = diamonds)\nsummary(mod5)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + x + cut + color + \n    clarity + carat:color + clarity:carat, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22707.3   -301.6     -0.9    271.9   8784.4 \n\nCoefficients:\n                 Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)     21244.877    290.112   73.230  &lt; 2e-16 ***\ncarat           16453.723     45.710  359.960  &lt; 2e-16 ***\ndepth            -163.331      3.068  -53.232  &lt; 2e-16 ***\ntable             -39.541      2.158  -18.324  &lt; 2e-16 ***\nx               -3095.798     18.360 -168.620  &lt; 2e-16 ***\ncut.L             514.155     16.659   30.863  &lt; 2e-16 ***\ncut.Q            -163.316     13.343  -12.239  &lt; 2e-16 ***\ncut.C              71.287     11.459    6.221 4.98e-10 ***\ncut^4               7.700      9.160    0.841  0.40059    \ncolor.L           667.031     27.200   24.523  &lt; 2e-16 ***\ncolor.Q           367.411     25.005   14.694  &lt; 2e-16 ***\ncolor.C            15.497     22.987    0.674  0.50022    \ncolor^4           104.494     20.654    5.059 4.22e-07 ***\ncolor^5           200.370     19.445   10.305  &lt; 2e-16 ***\ncolor^6            24.333     17.523    1.389  0.16495    \nclarity.L       -3011.134     48.745  -61.774  &lt; 2e-16 ***\nclarity.Q        1767.264     44.991   39.280  &lt; 2e-16 ***\nclarity.C       -1411.691     37.984  -37.166  &lt; 2e-16 ***\nclarity^4         863.630     30.089   28.703  &lt; 2e-16 ***\nclarity^5        -454.165     24.029  -18.901  &lt; 2e-16 ***\nclarity^6         112.514     20.512    5.485 4.15e-08 ***\nclarity^7         -91.019     17.879   -5.091 3.58e-07 ***\ncarat:color.L   -2936.145     27.224 -107.853  &lt; 2e-16 ***\ncarat:color.Q    -861.382     24.531  -35.114  &lt; 2e-16 ***\ncarat:color.C      33.212     23.141    1.435  0.15124    \ncarat:color^4     -64.953     21.442   -3.029  0.00245 ** \ncarat:color^5    -293.108     20.575  -14.246  &lt; 2e-16 ***\ncarat:color^6     -24.459     19.136   -1.278  0.20119    \ncarat:clarity.L  7881.781     51.240  153.820  &lt; 2e-16 ***\ncarat:clarity.Q -2165.582     45.783  -47.301  &lt; 2e-16 ***\ncarat:clarity.C  1714.611     41.112   41.706  &lt; 2e-16 ***\ncarat:clarity^4  -945.245     36.053  -26.218  &lt; 2e-16 ***\ncarat:clarity^5   488.264     31.404   15.548  &lt; 2e-16 ***\ncarat:clarity^6    28.838     27.156    1.062  0.28827    \ncarat:clarity^7   279.667     21.976   12.726  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 837 on 53905 degrees of freedom\nMultiple R-squared:  0.956, Adjusted R-squared:  0.956 \nF-statistic: 3.445e+04 on 34 and 53905 DF,  p-value: &lt; 2.2e-16\n\n#Linear\nplot(mod5, which = 1)\n\n\n\n\n\n\n\n#Inedpendence\n\n# N \nplot(mod5, which = 2)\n\n\n\n\n\n\n\n#E \nplot(mod5, which = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n\nIn addition to carat size, what other variables might be associated with the price of a diamond?\nCreate a comparative box plot of price (response variable on the y-axis) by cut (explanatory variable on the x-axis).\nWhich cut category tends to have higher prices? Is this what you would expect?\nFit a simple linear model for price using carat as the explanatory variable and price as the response variable.\n\n\n\nWrite out the regression equation with intercept, coefficients, and variable names.\n\nInterpret the coefficient of carat in the context of this model.\n\nWhat is the strength of evidence that carat is related to price?\n\n\nCreate a scatterplot of price versus carat, colored by cut.\nDo higher-quality cuts tend to cluster at different price or carat ranges?\nHow might this influence your interpretation of the relationship between carat and price?\nFit a multiple regression model for price using both carat and cut as explanatory variables.\n\n\n\nWrite out the regression equation with intercept, coefficients, and variable names.\n\nInterpret the coefficient for cut while controlling for carat.\n\n\nHow much total variation in diamond price is explained by this model (i.e., (R^2))?\nThe model in #5 assumes that the effect of carat on price is the same across all cuts.\nHow can we check whether this assumption is valid?\nFit a multiple regression model for price using both carat and cut, including an interaction between them.\nWrite out the regression equation with intercept, coefficients, and variable names.\nAmong the Ideal cut diamonds, how much does price increase for a one-unit increase in carat?\nAmong the Fair cut diamonds, how much does price increase for a one-unit increase in carat?\nIs the interaction between carat and cut statistically significant?\nState your null and alternative hypotheses for the interaction term and report the test statistic and p-value.\nTo what population are you willing to generalize your results?\nCan you draw a cause-and-effect conclusion about carat size and diamond price? Why or why not?\n\n\n\n\nAsk a Research Question\n\nIn addition to carat size, what other variables might be associated with the price of a diamond?\n\nDesign a Study and Explore the Data\nThe diamonds dataset in R contains information about 53,940 diamonds, including their price (in U.S. dollars), carat, cut, color, clarity, and several physical measurements (x, y, z, depth, table).\nWe’ll examine how carat size and quality characteristics relate to diamond price.\n\nUse R to create a comparative box plot of price (Response Variable on the y-axis) by cut (Explanatory Variable on the x-axis).\n\nWhich cut category tends to have higher prices?\n\nIs this what you would expect?\n\n\n\nggplot(diamonds, aes(x = cut, y = price)) +\n  geom_boxplot() +\n  labs(title = \"Diamond Price by Cut Quality\")\n\n\n\n\n\n\n\n\n\nCreate a simple linear model for price using carat as the Explanatory Variable and price as the Response Variable.\n\n\nmod1 &lt;- lm(price ~ carat, data = diamonds)\nsummary(mod1)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2256.36      13.06  -172.8   &lt;2e-16 ***\ncarat        7756.43      14.07   551.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: &lt; 2.2e-16\n\n\n\nWrite out the simple linear regression equation with intercept, coefficients, and variable names.\n\n\\[\n\\widehat{price} = b_0 + b_1(\\text{carat})\n\\]\n\nInterpret the coefficient of carat in the context of this model.\n\nBased on the p-value for the slope, what is the strength of evidence that carat is related to price?\n\n\nGenerate a scatterplot of price (y-axis) versus carat (x-axis), colored by cut.\n\n\nggplot(diamonds, aes(x = carat, y = price, color = cut)) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"Diamond Price vs. Carat, by Cut\")\n\n\n\n\n\n\n\n\n\nDo higher-quality cuts tend to cluster at different price or carat ranges?\n\nHow might this influence your interpretation of the relationship between carat and price?\n\n\nFit a multiple regression model for price using both carat and cut as Explanatory Variables.\n\n\nmod2 &lt;- lm(price ~ carat + cut, data = diamonds)\nsummary(mod2)\n\n\nCall:\nlm(formula = price ~ carat + cut, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17540.7   -791.6    -37.6    522.1  12721.4 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) -2701.38      15.43 -175.061  &lt; 2e-16 ***\ncarat        7871.08      13.98  563.040  &lt; 2e-16 ***\ncut.L        1239.80      26.10   47.502  &lt; 2e-16 ***\ncut.Q        -528.60      23.13  -22.851  &lt; 2e-16 ***\ncut.C         367.91      20.21   18.201  &lt; 2e-16 ***\ncut^4          74.59      16.24    4.593 4.37e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1511 on 53934 degrees of freedom\nMultiple R-squared:  0.8565,    Adjusted R-squared:  0.8565 \nF-statistic: 6.437e+04 on 5 and 53934 DF,  p-value: &lt; 2.2e-16\n\n\n\nWrite out the multiple regression equation with intercept, coefficients, and variable names.\n\nInterpret the coefficient for cut while controlling for carat.\n\n\nHow much total variation in diamond price is explained by this model (i.e., (R^2))?\nThis model assumes that the effect of carat on price is the same across all cuts.\n\nHow can we check whether this assumption is valid?\n\nCreate a multiple regression model for price using both carat and cut, including an interaction between them.\n\n\nmod3 &lt;- lm(price ~ carat * cut, data = diamonds)\nsummary(mod3)\n\n\nCall:\nlm(formula = price ~ carat * cut, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14878.3   -793.0    -23.0    546.3  12706.2 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) -2271.95      20.94 -108.513  &lt; 2e-16 ***\ncarat        7468.05      19.49  383.200  &lt; 2e-16 ***\ncut.L        -278.21      57.17   -4.866 1.14e-06 ***\ncut.Q         363.22      50.51    7.191 6.50e-13 ***\ncut.C        -172.96      42.81   -4.041 5.34e-05 ***\ncut^4          67.55      33.40    2.022   0.0431 *  \ncarat:cut.L  1538.10      50.96   30.183  &lt; 2e-16 ***\ncarat:cut.Q  -781.89      45.89  -17.037  &lt; 2e-16 ***\ncarat:cut.C   509.65      41.36   12.321  &lt; 2e-16 ***\ncarat:cut^4    69.70      34.38    2.027   0.0426 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1498 on 53930 degrees of freedom\nMultiple R-squared:  0.8591,    Adjusted R-squared:  0.859 \nF-statistic: 3.653e+04 on 9 and 53930 DF,  p-value: &lt; 2.2e-16\n\n\nWrite out the regression equation with intercept, coefficients, and variable names.\n\\[\n\\widehat{price} = b_0 + b_1(\\text{carat}) + b_2(\\text{cut}) + b_3(\\text{carat} \\times \\text{cut})\n\\]\n\nAmong the Ideal cut diamonds, how much does price increase for a one-unit increase in carat?\nAmong the Fair cut diamonds, how much does price increase for a one-unit increase in carat?\nIs the interaction between carat and cut statistically significant?\n\nState your null and alternative hypotheses for the interaction term.\n\nReport the test statistic and p-value.\n\nTo what population are you willing to generalize your results?\n\nCan you draw a cause-and-effect conclusion about carat size and diamond price?\n\nWhy or why not?\n\nCheck each of the four Validity Conditions for the multiple regression you ran in #8.\n\nInclude all three validity plots for your regression model.\n\nJustify each of the four conditions: Linearity, Independence, Normality, and Equal Variance.\n\n\n\n\n\nplot(mod3, which = 1)\n\n\n\n\n\n\n\n\n\n\n\nMust be justified based on data collection (not tested statistically).\n\n\n\n\nplot(mod3, which = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mod3, which = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 25"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-25.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-25.html#lesson-administration",
    "title": "Lesson 25 - Multiple Linear Regression II",
    "section": "",
    "text": "Lesson 25.5\nChange: 9 November 2359\nMilestone 5 Instructions\n\n\n\n\n\nLesson 26\n12-13 November\nLink: TBD\n\n\n\n\n\nLesson 26.5\n17 November\nMilestone 6 Instructions\n\n\n\n\n\nNeed to know who will not be here on presentation day because of CPRC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath 1 vs…\n\n\n\n\n\n\nPreviously 0-0\n\n\n\n\n\n\n1-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI got nothing… So enjoy these\n\n\n\n\n\n  Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 25"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-25.html#multiple-linear-regression",
    "href": "MA206-AY26-1/lesson-25.html#multiple-linear-regression",
    "title": "Lesson 25 - Multiple Linear Regression II",
    "section": "",
    "text": "Remember this from last class?\n\nmod1 &lt;- lm(formula = mpg ~ wt + as.factor(cyl), data = mtcars)\nsummary(mod1)\n\n\nCall:\nlm(formula = mpg ~ wt + as.factor(cyl), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5890 -1.2357 -0.5159  1.3845  5.7915 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      33.9908     1.8878  18.006  &lt; 2e-16 ***\nwt               -3.2056     0.7539  -4.252 0.000213 ***\nas.factor(cyl)6  -4.2556     1.3861  -3.070 0.004718 ** \nas.factor(cyl)8  -6.0709     1.6523  -3.674 0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.557 on 28 degrees of freedom\nMultiple R-squared:  0.8374,    Adjusted R-squared:   0.82 \nF-statistic: 48.08 on 3 and 28 DF,  p-value: 3.594e-11\n\n\nThe fitted regression equation is:\n\\[\n\\widehat{mpg} = 33.99 - 3.21(\\text{wt}) - 4.26(\\text{cyl6}) - 6.07(\\text{cyl8})\n\\]\nTo predict fuel efficiency for a 3000-lb, 6-cylinder car:\n\\[\n\\begin{aligned}\n\\widehat{mpg}\n&= 33.99 - 3.21(3.0) - 4.26(1) - 6.07(0) \\\\[6pt]\n&= 33.99 - 9.63 - 4.26 \\\\[6pt]\n&= 20.10\n\\end{aligned}\n\\]\nSo the model predicts:\n\\[\n\\boxed{\\widehat{mpg} = 20.1}\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 25"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-25.html#interactions",
    "href": "MA206-AY26-1/lesson-25.html#interactions",
    "title": "Lesson 25 - Multiple Linear Regression II",
    "section": "",
    "text": "mod2 &lt;- lm(formula = mpg ~ wt + as.factor(cyl) + hp + hp:as.factor(cyl), data = mtcars)\nsummary(mod2)\n\n\nCall:\nlm(formula = mpg ~ wt + as.factor(cyl) + hp + hp:as.factor(cyl), \n    data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1864 -1.4098 -0.4022  1.0186  4.3920 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         41.87732    3.23293  12.953 1.37e-12 ***\nwt                  -3.05994    0.68275  -4.482 0.000143 ***\nas.factor(cyl)6     -9.98213    5.76950  -1.730 0.095931 .  \nas.factor(cyl)8    -11.72793    4.22507  -2.776 0.010276 *  \nhp                  -0.09947    0.03487  -2.853 0.008576 ** \nas.factor(cyl)6:hp   0.07809    0.05236   1.492 0.148335    \nas.factor(cyl)8:hp   0.08602    0.03703   2.323 0.028601 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.3 on 25 degrees of freedom\nMultiple R-squared:  0.8826,    Adjusted R-squared:  0.8544 \nF-statistic: 31.32 on 6 and 25 DF,  p-value: 1.831e-10\n\n\nWe can write the fitted model as\n\\[\n\\begin{aligned}\n\\widehat{mpg}\n&= 41.877\n- 3.060\\,(\\text{wt})\n- 9.982\\,(\\text{cyl6})\n- 11.728\\,(\\text{cyl8}) - 0.09947\\,(\\text{hp})\n+ 0.07809\\,(\\text{hp}\\times \\text{cyl6})\n+ 0.08602\\,(\\text{hp}\\times \\text{cyl8})\n\\end{aligned}\n\\]\n\n\nWhich one is better?\nLets turn to \\(R^2\\)\nModel 1: .8374\nModel 2: .8826",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 25"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-25.html#validity-conditions",
    "href": "MA206-AY26-1/lesson-25.html#validity-conditions",
    "title": "Lesson 25 - Multiple Linear Regression II",
    "section": "",
    "text": "Linearity — the relationship between predictors and the response is roughly linear\nIndependence — observations are independent of each other\nNormal Distribution — residuals are approximately normal\nEqual Variance — variability of residuals is consistent across fitted values\n\n\nCheck whether the relationship between predictors and the response is roughly linear.\nUse a residuals vs fitted plot — you want to see a random scatter (no pattern).\n\nplot(mod1, which = 1)\n\n\n\n\n\n\n\n\n\n\n\nWe can’t test this from the model alone — it depends on how the data were collected.\nYou must verify that each observation is independent (e.g., different cars, people, or trials).\n\n\n\nCheck whether residuals are approximately normally distributed using a Q-Q plot.\n\nplot(mod1, which = 2)\n\n\n\n\n\n\n\n\n\n\n\nCheck for constant variance (homoscedasticity) — residuals should have similar spread across fitted values.\n\nplot(mod1, which = 1)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 25"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-25.html#adventure-one---example-of-what-to-do-for-milestone-5",
    "href": "MA206-AY26-1/lesson-25.html#adventure-one---example-of-what-to-do-for-milestone-5",
    "title": "Lesson 25 - Multiple Linear Regression II",
    "section": "",
    "text": "library(tidyverse)\n\ndata(\"diamonds\")\n\ndiamonds |&gt; \n  sample_n(size = 1000) |&gt; \n  pivot_longer(cols = -c(cut, clarity, price, color)) |&gt; \n  ggplot(aes(x = value, y = price, colour = color)) +\n  geom_point() +\n  facet_wrap(~name, scales = \"free\")\n\n\n\n\n\n\n\nmod1 &lt;- lm(formula = price ~ carat + depth + table + x + y + z + cut + color + clarity, data = diamonds)\nsummary(mod1)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + x + y + z + cut + \n    color + clarity, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21376.0   -592.4   -183.5    376.4  10694.2 \n\nCoefficients:\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)  5753.762    396.630   14.507  &lt; 2e-16 ***\ncarat       11256.978     48.628  231.494  &lt; 2e-16 ***\ndepth         -63.806      4.535  -14.071  &lt; 2e-16 ***\ntable         -26.474      2.912   -9.092  &lt; 2e-16 ***\nx           -1008.261     32.898  -30.648  &lt; 2e-16 ***\ny               9.609     19.333    0.497  0.61918    \nz             -50.119     33.486   -1.497  0.13448    \ncut.L         584.457     22.478   26.001  &lt; 2e-16 ***\ncut.Q        -301.908     17.994  -16.778  &lt; 2e-16 ***\ncut.C         148.035     15.483    9.561  &lt; 2e-16 ***\ncut^4         -20.794     12.377   -1.680  0.09294 .  \ncolor.L     -1952.160     17.342 -112.570  &lt; 2e-16 ***\ncolor.Q      -672.054     15.777  -42.597  &lt; 2e-16 ***\ncolor.C      -165.283     14.725  -11.225  &lt; 2e-16 ***\ncolor^4        38.195     13.527    2.824  0.00475 ** \ncolor^5       -95.793     12.776   -7.498 6.59e-14 ***\ncolor^6       -48.466     11.614   -4.173 3.01e-05 ***\nclarity.L    4097.431     30.259  135.414  &lt; 2e-16 ***\nclarity.Q   -1925.004     28.227  -68.197  &lt; 2e-16 ***\nclarity.C     982.205     24.152   40.668  &lt; 2e-16 ***\nclarity^4    -364.918     19.285  -18.922  &lt; 2e-16 ***\nclarity^5     233.563     15.752   14.828  &lt; 2e-16 ***\nclarity^6       6.883     13.715    0.502  0.61575    \nclarity^7      90.640     12.103    7.489 7.06e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1130 on 53916 degrees of freedom\nMultiple R-squared:  0.9198,    Adjusted R-squared:  0.9198 \nF-statistic: 2.688e+04 on 23 and 53916 DF,  p-value: &lt; 2.2e-16\n\nmod2 &lt;- lm(formula = price ~ carat + depth + table + x + z + cut + color + clarity, data = diamonds)\nsummary(mod2)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + x + z + cut + color + \n    clarity, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21378.8   -592.5   -183.5    376.3  10694.1 \n\nCoefficients:\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)  5768.782    395.474   14.587  &lt; 2e-16 ***\ncarat       11257.752     48.602  231.630  &lt; 2e-16 ***\ndepth         -64.003      4.517  -14.168  &lt; 2e-16 ***\ntable         -26.501      2.911   -9.103  &lt; 2e-16 ***\nx           -1000.354     28.795  -34.740  &lt; 2e-16 ***\nz             -47.925     33.194   -1.444  0.14880    \ncut.L         584.600     22.476   26.010  &lt; 2e-16 ***\ncut.Q        -302.211     17.983  -16.805  &lt; 2e-16 ***\ncut.C         148.446     15.461    9.601  &lt; 2e-16 ***\ncut^4         -20.619     12.371   -1.667  0.09559 .  \ncolor.L     -1952.179     17.342 -112.572  &lt; 2e-16 ***\ncolor.Q      -672.075     15.777  -42.599  &lt; 2e-16 ***\ncolor.C      -165.277     14.725  -11.224  &lt; 2e-16 ***\ncolor^4        38.193     13.526    2.824  0.00475 ** \ncolor^5       -95.780     12.776   -7.497 6.64e-14 ***\ncolor^6       -48.452     11.614   -4.172 3.02e-05 ***\nclarity.L    4097.613     30.256  135.431  &lt; 2e-16 ***\nclarity.Q   -1925.133     28.226  -68.205  &lt; 2e-16 ***\nclarity.C     982.322     24.150   40.676  &lt; 2e-16 ***\nclarity^4    -364.976     19.285  -18.926  &lt; 2e-16 ***\nclarity^5     233.635     15.751   14.833  &lt; 2e-16 ***\nclarity^6       6.871     13.715    0.501  0.61640    \nclarity^7      90.622     12.103    7.487 7.13e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1130 on 53917 degrees of freedom\nMultiple R-squared:  0.9198,    Adjusted R-squared:  0.9198 \nF-statistic: 2.81e+04 on 22 and 53917 DF,  p-value: &lt; 2.2e-16\n\n## Candidate model\nmod3 &lt;- lm(formula = price ~ carat + depth + table + x + cut + color + clarity, data = diamonds)\nsummary(mod3)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + x + cut + color + \n    clarity, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21385.0   -592.4   -183.7    376.5  10694.6 \n\nCoefficients:\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)  5935.107    378.328   15.688  &lt; 2e-16 ***\ncarat       11256.968     48.600  231.626  &lt; 2e-16 ***\ndepth         -66.769      4.091  -16.322  &lt; 2e-16 ***\ntable         -26.457      2.911   -9.089  &lt; 2e-16 ***\nx           -1029.478     20.549  -50.098  &lt; 2e-16 ***\ncut.L         584.717     22.476   26.015  &lt; 2e-16 ***\ncut.Q        -302.037     17.983  -16.795  &lt; 2e-16 ***\ncut.C         148.065     15.459    9.578  &lt; 2e-16 ***\ncut^4         -21.253     12.364   -1.719  0.08562 .  \ncolor.L     -1952.128     17.342 -112.568  &lt; 2e-16 ***\ncolor.Q      -672.207     15.777  -42.608  &lt; 2e-16 ***\ncolor.C      -165.451     14.724  -11.236  &lt; 2e-16 ***\ncolor^4        38.261     13.526    2.829  0.00468 ** \ncolor^5       -95.816     12.776   -7.500 6.50e-14 ***\ncolor^6       -48.441     11.614   -4.171 3.04e-05 ***\nclarity.L    4096.912     30.253  135.423  &lt; 2e-16 ***\nclarity.Q   -1924.681     28.224  -68.192  &lt; 2e-16 ***\nclarity.C     982.004     24.149   40.664  &lt; 2e-16 ***\nclarity^4    -364.870     19.285  -18.920  &lt; 2e-16 ***\nclarity^5     233.449     15.751   14.822  &lt; 2e-16 ***\nclarity^6       6.973     13.715    0.508  0.61114    \nclarity^7      90.738     12.103    7.497 6.63e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1130 on 53918 degrees of freedom\nMultiple R-squared:  0.9198,    Adjusted R-squared:  0.9198 \nF-statistic: 2.944e+04 on 21 and 53918 DF,  p-value: &lt; 2.2e-16\n\ndiamonds |&gt; \n  select(-c(cut, color, clarity)) |&gt; \n  sample_n(1000) |&gt; \n  pairs()\n\n\n\n\n\n\n\nmod4 &lt;- lm(formula = price ~ carat + depth + table + x + cut + color + clarity + carat:color, data = diamonds)\nsummary(mod4)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + x + cut + color + \n    clarity + carat:color, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16732.9   -513.8   -136.6    358.5  10550.5 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   10393.339    359.347  28.923  &lt; 2e-16 ***\ncarat         12646.011     49.034 257.900  &lt; 2e-16 ***\ndepth           -97.355      3.860 -25.223  &lt; 2e-16 ***\ntable           -28.297      2.735 -10.348  &lt; 2e-16 ***\nx             -1624.864     20.623 -78.791  &lt; 2e-16 ***\ncut.L           594.594     21.114  28.161  &lt; 2e-16 ***\ncut.Q          -274.247     16.896 -16.232  &lt; 2e-16 ***\ncut.C           132.434     14.523   9.119  &lt; 2e-16 ***\ncut^4           -12.347     11.613  -1.063  0.28770    \ncolor.L         329.781     34.252   9.628  &lt; 2e-16 ***\ncolor.Q         513.842     31.643  16.239  &lt; 2e-16 ***\ncolor.C        -119.817     29.087  -4.119 3.81e-05 ***\ncolor^4          69.759     26.171   2.665  0.00769 ** \ncolor^5         250.440     24.647  10.161  &lt; 2e-16 ***\ncolor^6         125.580     22.195   5.658 1.54e-08 ***\nclarity.L      4113.880     28.459 144.556  &lt; 2e-16 ***\nclarity.Q     -1981.152     26.554 -74.607  &lt; 2e-16 ***\nclarity.C       991.093     22.698  43.665  &lt; 2e-16 ***\nclarity^4      -353.021     18.117 -19.485  &lt; 2e-16 ***\nclarity^5       208.673     14.801  14.099  &lt; 2e-16 ***\nclarity^6        26.858     12.884   2.085  0.03711 *  \nclarity^7       107.303     11.372   9.436  &lt; 2e-16 ***\ncarat:color.L -2454.649     34.150 -71.878  &lt; 2e-16 ***\ncarat:color.Q -1020.521     31.031 -32.887  &lt; 2e-16 ***\ncarat:color.C   212.081     29.261   7.248 4.29e-13 ***\ncarat:color^4    19.164     27.152   0.706  0.48031    \ncarat:color^5  -396.193     26.071 -15.196  &lt; 2e-16 ***\ncarat:color^6  -192.062     24.204  -7.935 2.14e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1061 on 53912 degrees of freedom\nMultiple R-squared:  0.9293,    Adjusted R-squared:  0.9292 \nF-statistic: 2.623e+04 on 27 and 53912 DF,  p-value: &lt; 2.2e-16\n\n## Candidate model\nmod5 &lt;- lm(formula = price ~ carat + depth + table + x + cut + color + clarity + carat:color + clarity:carat, data = diamonds)\nsummary(mod5)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + x + cut + color + \n    clarity + carat:color + clarity:carat, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22707.3   -301.6     -0.9    271.9   8784.4 \n\nCoefficients:\n                 Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)     21244.877    290.112   73.230  &lt; 2e-16 ***\ncarat           16453.723     45.710  359.960  &lt; 2e-16 ***\ndepth            -163.331      3.068  -53.232  &lt; 2e-16 ***\ntable             -39.541      2.158  -18.324  &lt; 2e-16 ***\nx               -3095.798     18.360 -168.620  &lt; 2e-16 ***\ncut.L             514.155     16.659   30.863  &lt; 2e-16 ***\ncut.Q            -163.316     13.343  -12.239  &lt; 2e-16 ***\ncut.C              71.287     11.459    6.221 4.98e-10 ***\ncut^4               7.700      9.160    0.841  0.40059    \ncolor.L           667.031     27.200   24.523  &lt; 2e-16 ***\ncolor.Q           367.411     25.005   14.694  &lt; 2e-16 ***\ncolor.C            15.497     22.987    0.674  0.50022    \ncolor^4           104.494     20.654    5.059 4.22e-07 ***\ncolor^5           200.370     19.445   10.305  &lt; 2e-16 ***\ncolor^6            24.333     17.523    1.389  0.16495    \nclarity.L       -3011.134     48.745  -61.774  &lt; 2e-16 ***\nclarity.Q        1767.264     44.991   39.280  &lt; 2e-16 ***\nclarity.C       -1411.691     37.984  -37.166  &lt; 2e-16 ***\nclarity^4         863.630     30.089   28.703  &lt; 2e-16 ***\nclarity^5        -454.165     24.029  -18.901  &lt; 2e-16 ***\nclarity^6         112.514     20.512    5.485 4.15e-08 ***\nclarity^7         -91.019     17.879   -5.091 3.58e-07 ***\ncarat:color.L   -2936.145     27.224 -107.853  &lt; 2e-16 ***\ncarat:color.Q    -861.382     24.531  -35.114  &lt; 2e-16 ***\ncarat:color.C      33.212     23.141    1.435  0.15124    \ncarat:color^4     -64.953     21.442   -3.029  0.00245 ** \ncarat:color^5    -293.108     20.575  -14.246  &lt; 2e-16 ***\ncarat:color^6     -24.459     19.136   -1.278  0.20119    \ncarat:clarity.L  7881.781     51.240  153.820  &lt; 2e-16 ***\ncarat:clarity.Q -2165.582     45.783  -47.301  &lt; 2e-16 ***\ncarat:clarity.C  1714.611     41.112   41.706  &lt; 2e-16 ***\ncarat:clarity^4  -945.245     36.053  -26.218  &lt; 2e-16 ***\ncarat:clarity^5   488.264     31.404   15.548  &lt; 2e-16 ***\ncarat:clarity^6    28.838     27.156    1.062  0.28827    \ncarat:clarity^7   279.667     21.976   12.726  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 837 on 53905 degrees of freedom\nMultiple R-squared:  0.956, Adjusted R-squared:  0.956 \nF-statistic: 3.445e+04 on 34 and 53905 DF,  p-value: &lt; 2.2e-16\n\n#Linear\nplot(mod5, which = 1)\n\n\n\n\n\n\n\n#Inedpendence\n\n# N \nplot(mod5, which = 2)\n\n\n\n\n\n\n\n#E \nplot(mod5, which = 1)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 25"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-25.html#in-class-exercise---adventure-two---will-revisit-lesson-26",
    "href": "MA206-AY26-1/lesson-25.html#in-class-exercise---adventure-two---will-revisit-lesson-26",
    "title": "Lesson 25 - Multiple Linear Regression II",
    "section": "",
    "text": "library(tidyverse)\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n\nIn addition to carat size, what other variables might be associated with the price of a diamond?\nCreate a comparative box plot of price (response variable on the y-axis) by cut (explanatory variable on the x-axis).\nWhich cut category tends to have higher prices? Is this what you would expect?\nFit a simple linear model for price using carat as the explanatory variable and price as the response variable.\n\n\n\nWrite out the regression equation with intercept, coefficients, and variable names.\n\nInterpret the coefficient of carat in the context of this model.\n\nWhat is the strength of evidence that carat is related to price?\n\n\nCreate a scatterplot of price versus carat, colored by cut.\nDo higher-quality cuts tend to cluster at different price or carat ranges?\nHow might this influence your interpretation of the relationship between carat and price?\nFit a multiple regression model for price using both carat and cut as explanatory variables.\n\n\n\nWrite out the regression equation with intercept, coefficients, and variable names.\n\nInterpret the coefficient for cut while controlling for carat.\n\n\nHow much total variation in diamond price is explained by this model (i.e., (R^2))?\nThe model in #5 assumes that the effect of carat on price is the same across all cuts.\nHow can we check whether this assumption is valid?\nFit a multiple regression model for price using both carat and cut, including an interaction between them.\nWrite out the regression equation with intercept, coefficients, and variable names.\nAmong the Ideal cut diamonds, how much does price increase for a one-unit increase in carat?\nAmong the Fair cut diamonds, how much does price increase for a one-unit increase in carat?\nIs the interaction between carat and cut statistically significant?\nState your null and alternative hypotheses for the interaction term and report the test statistic and p-value.\nTo what population are you willing to generalize your results?\nCan you draw a cause-and-effect conclusion about carat size and diamond price? Why or why not?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 25"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-25.html#partial-solution",
    "href": "MA206-AY26-1/lesson-25.html#partial-solution",
    "title": "Lesson 25 - Multiple Linear Regression II",
    "section": "",
    "text": "Ask a Research Question\n\nIn addition to carat size, what other variables might be associated with the price of a diamond?\n\nDesign a Study and Explore the Data\nThe diamonds dataset in R contains information about 53,940 diamonds, including their price (in U.S. dollars), carat, cut, color, clarity, and several physical measurements (x, y, z, depth, table).\nWe’ll examine how carat size and quality characteristics relate to diamond price.\n\nUse R to create a comparative box plot of price (Response Variable on the y-axis) by cut (Explanatory Variable on the x-axis).\n\nWhich cut category tends to have higher prices?\n\nIs this what you would expect?\n\n\n\nggplot(diamonds, aes(x = cut, y = price)) +\n  geom_boxplot() +\n  labs(title = \"Diamond Price by Cut Quality\")\n\n\n\n\n\n\n\n\n\nCreate a simple linear model for price using carat as the Explanatory Variable and price as the Response Variable.\n\n\nmod1 &lt;- lm(price ~ carat, data = diamonds)\nsummary(mod1)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2256.36      13.06  -172.8   &lt;2e-16 ***\ncarat        7756.43      14.07   551.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: &lt; 2.2e-16\n\n\n\nWrite out the simple linear regression equation with intercept, coefficients, and variable names.\n\n\\[\n\\widehat{price} = b_0 + b_1(\\text{carat})\n\\]\n\nInterpret the coefficient of carat in the context of this model.\n\nBased on the p-value for the slope, what is the strength of evidence that carat is related to price?\n\n\nGenerate a scatterplot of price (y-axis) versus carat (x-axis), colored by cut.\n\n\nggplot(diamonds, aes(x = carat, y = price, color = cut)) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"Diamond Price vs. Carat, by Cut\")\n\n\n\n\n\n\n\n\n\nDo higher-quality cuts tend to cluster at different price or carat ranges?\n\nHow might this influence your interpretation of the relationship between carat and price?\n\n\nFit a multiple regression model for price using both carat and cut as Explanatory Variables.\n\n\nmod2 &lt;- lm(price ~ carat + cut, data = diamonds)\nsummary(mod2)\n\n\nCall:\nlm(formula = price ~ carat + cut, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17540.7   -791.6    -37.6    522.1  12721.4 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) -2701.38      15.43 -175.061  &lt; 2e-16 ***\ncarat        7871.08      13.98  563.040  &lt; 2e-16 ***\ncut.L        1239.80      26.10   47.502  &lt; 2e-16 ***\ncut.Q        -528.60      23.13  -22.851  &lt; 2e-16 ***\ncut.C         367.91      20.21   18.201  &lt; 2e-16 ***\ncut^4          74.59      16.24    4.593 4.37e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1511 on 53934 degrees of freedom\nMultiple R-squared:  0.8565,    Adjusted R-squared:  0.8565 \nF-statistic: 6.437e+04 on 5 and 53934 DF,  p-value: &lt; 2.2e-16\n\n\n\nWrite out the multiple regression equation with intercept, coefficients, and variable names.\n\nInterpret the coefficient for cut while controlling for carat.\n\n\nHow much total variation in diamond price is explained by this model (i.e., (R^2))?\nThis model assumes that the effect of carat on price is the same across all cuts.\n\nHow can we check whether this assumption is valid?\n\nCreate a multiple regression model for price using both carat and cut, including an interaction between them.\n\n\nmod3 &lt;- lm(price ~ carat * cut, data = diamonds)\nsummary(mod3)\n\n\nCall:\nlm(formula = price ~ carat * cut, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14878.3   -793.0    -23.0    546.3  12706.2 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) -2271.95      20.94 -108.513  &lt; 2e-16 ***\ncarat        7468.05      19.49  383.200  &lt; 2e-16 ***\ncut.L        -278.21      57.17   -4.866 1.14e-06 ***\ncut.Q         363.22      50.51    7.191 6.50e-13 ***\ncut.C        -172.96      42.81   -4.041 5.34e-05 ***\ncut^4          67.55      33.40    2.022   0.0431 *  \ncarat:cut.L  1538.10      50.96   30.183  &lt; 2e-16 ***\ncarat:cut.Q  -781.89      45.89  -17.037  &lt; 2e-16 ***\ncarat:cut.C   509.65      41.36   12.321  &lt; 2e-16 ***\ncarat:cut^4    69.70      34.38    2.027   0.0426 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1498 on 53930 degrees of freedom\nMultiple R-squared:  0.8591,    Adjusted R-squared:  0.859 \nF-statistic: 3.653e+04 on 9 and 53930 DF,  p-value: &lt; 2.2e-16\n\n\nWrite out the regression equation with intercept, coefficients, and variable names.\n\\[\n\\widehat{price} = b_0 + b_1(\\text{carat}) + b_2(\\text{cut}) + b_3(\\text{carat} \\times \\text{cut})\n\\]\n\nAmong the Ideal cut diamonds, how much does price increase for a one-unit increase in carat?\nAmong the Fair cut diamonds, how much does price increase for a one-unit increase in carat?\nIs the interaction between carat and cut statistically significant?\n\nState your null and alternative hypotheses for the interaction term.\n\nReport the test statistic and p-value.\n\nTo what population are you willing to generalize your results?\n\nCan you draw a cause-and-effect conclusion about carat size and diamond price?\n\nWhy or why not?\n\nCheck each of the four Validity Conditions for the multiple regression you ran in #8.\n\nInclude all three validity plots for your regression model.\n\nJustify each of the four conditions: Linearity, Independence, Normality, and Equal Variance.\n\n\n\n\n\nplot(mod3, which = 1)\n\n\n\n\n\n\n\n\n\n\n\nMust be justified based on data collection (not tested statistically).\n\n\n\n\nplot(mod3, which = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mod3, which = 1)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 25"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-25.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-25.html#before-you-leave",
    "title": "Lesson 25 - Multiple Linear Regression II",
    "section": "",
    "text": "Any questions for me?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 25"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-23.html",
    "href": "MA206-AY26-1/lesson-23.html",
    "title": "Lesson 23: Correlation & Simple Linear Regression",
    "section": "",
    "text": "Lesson 25.5\n7 November\nMilestone 5 Instructions\n\n\n\n\n\nLesson 26\n12-13 November\nLink: TBD\n\n\n\n\n\nLesson 26.5\n17 November\nMilestone 6 Instructions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see a clear trend here from weight to miles per gallon:\n\n\n\n\n\n\n\n\n\nThere should be some type of y = mx + b formula that could help us model this relationship or predict a new point in here somewhere.\nWhat about this one?\n\n\n\n\n\n\n\n\n\nOr this one?\n\n\n\n\n\n\n\n\n\nOr this one?\n\n\n\n\n\n\n\n\n\nAre these lines good? Are these lines good enough? Are these lines the best?\nWouldn’t it be nice if we could mathematically decide what the best line is for this?\nHow would we do that? The idea is that a best line would minimize the red lines in this plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe just saw that some lines fit the data better than others. But we need a mathematical way to decide what “best” means.\n\n\nFor each point \\((x_i, y_i)\\), the line makes a prediction:\n\\[\n\\hat{y}_i = \\beta_0 + \\beta_1 x_i\n\\]\nThe error (residual) is the difference between the actual \\(y_i\\) and the predicted \\(\\hat{y}_i\\):\n\\[\n\\begin{align*}\ne_i &= y_i - \\hat{y}_i \\\\\n    &= y_i - (\\beta_0 + \\beta_1 x_i) \\\\\n    &= y_i - \\beta_0 - \\beta_1 x_i\n\\end{align*}\n\\]\n\n\n\nWe want all the errors to be as small as possible. But errors can be positive or negative, so we square them and add them up:\n\\[\nS(\\beta_0, \\beta_1) = \\sum_{i=1}^n e_i^2\n= \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2\n\\]\nThis function \\(S(\\beta_0, \\beta_1)\\) is our loss function. The “best” line is the one that makes this sum as small as possible.\n\n\n\nWe take partial derivatives of \\(S\\) with respect to \\(\\beta_0\\) and \\(\\beta_1\\), set them to zero, and solve.\n\nDerivative with respect to \\(\\beta_0\\):\n\n\\[\n\\begin{align*}\n\\frac{\\partial S}{\\partial \\beta_0} &= -2\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\beta_1 x_i\\right) = 0 \\\\[6pt]\n&= \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\beta_1 x_i\\right) = 0 \\\\[6pt]\n&= \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\beta_0 - \\sum_{i=1}^n \\beta_1x_i \\\\[6pt]\n&= \\sum_{i=1}^n y_i - n\\beta_0 - \\beta_1 \\sum_{i=1}^n x_i \\\\[6pt]\nn\\beta_0 &= \\sum_{i=1}^n y_i - \\beta_1 \\sum_{i=1}^n x_i \\\\[6pt]\n\\beta_0 &= \\frac{1}{n}\\sum_{i=1}^n y_i - \\beta_1 \\frac{1}{n}\\sum_{i=1}^n x_i \\\\[6pt]\n\\beta_0 &= \\bar{y} - \\beta_1 \\bar{x}.\n\\end{align*}\n\\]\n\nDerivative with respect to \\(\\beta_1\\):\n\n\\[\n\\begin{align*}\n\\frac{\\partial S}{\\partial \\beta_1}\n&= -2\\sum_{i=1}^n x_i\\left(y_i - \\beta_0 - \\beta_1 x_i\\right) = 0 \\\\[6pt]\n&= \\sum_{i=1}^n \\left(x_i y_i - \\beta_0 x_i - \\beta_1 x_i^2\\right) = 0 \\\\[6pt]\n&= \\sum_{i=1}^n x_i y_i \\;-\\; \\beta_0 \\sum_{i=1}^n x_i \\;-\\; \\beta_1 \\sum_{i=1}^n x_i^2 = 0 \\\\[6pt]\n&= \\sum_{i=1}^n x_i y_i \\;-\\; (\\bar y - \\beta_1 \\bar x)\\sum_{i=1}^n x_i \\;-\\; \\beta_1 \\sum_{i=1}^n x_i^2 = 0 \\\\[6pt]\n&= \\sum_{i=1}^n x_i y_i \\;-\\; (\\bar y - \\beta_1 \\bar x)n \\bar x \\;-\\; \\beta_1 \\sum_{i=1}^n x_i^2 = 0 \\\\[6pt]\n&= \\sum_{i=1}^n x_i y_i \\;-\\; n\\bar x \\bar y \\;+\\; \\beta_1 n \\bar x^2 \\;-\\; \\beta_1 \\sum_{i=1}^n x_i^2 = 0 \\\\[6pt]\n\\beta_1\\!\\left(\\sum_{i=1}^n x_i^2 - n\\bar x^2\\right) &= \\sum_{i=1}^n x_i y_i - n\\bar x \\bar y \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n x_i y_i - n\\bar x \\bar y}{\\sum_{i=1}^n x_i^2 - n\\bar x^2} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n x_i y_i - n\\bar x \\bar y - n\\bar x \\bar y + n\\bar x \\bar y}{\\sum_{i=1}^n x_i^2 - n\\bar x^2 - 2 n \\bar x \\bar x + 2 n \\bar x \\bar x} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n x_i y_i - \\bar x (n \\bar y) - \\bar y (n\\bar x)  + n\\bar x \\bar y}{\\sum_{i=1}^n x_i^2 - n\\bar x^2 - 2 \\bar x (n \\bar x) + 2 n \\bar x^2} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n x_i y_i - \\bar x \\sum_{i=1}^n y_i - \\bar y \\sum_{i=1}^n x_i  + n\\bar x \\bar y}{\\sum_{i=1}^n x_i^2 - n\\bar x^2 - 2\\bar x \\sum_{i=1}^n x_i + 2n\\bar x^2} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n (x_i y_i - \\bar x y_i - \\bar y x_i  + \\bar x \\bar y)}{\\sum_{i=1}^n x_i^2 - 2\\bar x \\sum_{i=1}^n x_i + n\\bar x^2} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)}{\\sum_{i=1}^n x_i^2 - 2\\bar x \\sum_{i=1}^n x_i + \\sum_{i=1}^n \\bar x^2} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)}{\\sum_{i=1}^n x_i^2 - 2\\bar x x_i + \\bar x^2} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)}{\\sum_{i=1}^n (x_i-\\bar x)^2}.\n\\end{align*}\n\\]\nSo to sum it up:\n\\[\n\\begin{aligned}\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i=1}^n (x_i - \\bar x)^2} \\\\[6pt]\n\\beta_0 &= \\bar y - \\beta_1 \\bar x\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\hat{y}_i = \\beta_0 + \\beta_1 x_i\n\\]\n\n\\(\\hat{\\beta}_1\\) is the slope — how much \\(y\\) changes on average when \\(x\\) increases by 1.\n\n\\(\\hat{\\beta}_0\\) is the intercept — the value of \\(y\\) when \\(x = 0\\).\n\nSo remember this plot?\n\n\n\n\n\n\n\n\n\nThese formulas guarantee that our line is the one that minimizes the red “error bars”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm(formula = mpg~wt, data = mtcars)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\n\nSo this means\n\\[\n\\widehat{mpg} \\;=\\; 37.39 \\;-\\; 5.34 \\, wt\n\\]\n\n\n\n\n\n\nInterpretation\n\n\n\n\nIntercept (37.39): If a car had zero weight (not realistic, but useful mathematically), the predicted mpg would be 37.39.\n\nSlope (-5.34): For each additional 1000 lbs of weight, the predicted fuel efficiency decreases on average by about 5.3 mpg.\n\n\n\n\n\n\n\nHow do we know if we can “trust” our slope coefficient, \\(\\beta_1\\)?\nWe frame this as a hypothesis test:\n\\[\nH_0: \\; \\beta_1 = 0 \\quad \\text{(no linear relationship)}\n\\]\n\\[\nH_A: \\; \\beta_1 \\neq 0 \\quad \\text{(some linear relationship exists)}\n\\]\n\nmod &lt;- lm(mpg ~ wt, data = mtcars)\nsummary(mod)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\n\n\n\n\n\nFor a little more depth\n\n\n\nThe test statistic here is calculated as\n\\[\nt = \\frac{\\beta_1 - \\text{null}}{SE} = \\frac{37.2851}{1.8776} = 19.858\n\\]\nAnd if you remember…\n\n2 * (1 - pt(abs(19.858), 32))\n\n[1] 0\n\n\n\n\n\n\n\nHow good is this model? One way to measure it is with the coefficient of determination:\n\\[\nR^2 = 0.7528\n\\]\nThis means that about 75% of the variation in mpg can be explained by the linear relationship with car weight.\nThe remaining 25% of the variation is due to other factors not captured by this model (such as horsepower, number of cylinders, transmission type, etc.).\n\n\n\n\n\n\nKey Point\n\n\n\n\nA high \\(R^2\\) (close to 1) means the model explains a lot of the variation.\n\nA low \\(R^2\\) (close to 0) means the model doesn’t explain much variation.\n\nHere, \\(R^2 = 0.75\\) suggests the model is quite strong for such a simple one-variable regression.\n\n\n\n\n\n\n\n\nDo this regression for mpg against hp, drat, and disp.\n\nReport the regression equation.\n\nState the hypothesis test and conclusion.\n\nDecide if each variable has a linear relationship with mpg.\n\nFinally, which model is best when it comes to explaining mpg? Why?\n\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStep 1: Fit the regression models\n\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n\n\nModel 1 (hp):\n\\[\\widehat{mpg}=30.10-0.07\\,hp\\]\n\n\\(H_0:\\;\\beta_1=0 \\quad \\text{(no linear relationship)}\\)\n\\(H_A:\\;\\beta_1\\neq 0 \\quad \\text{(linear relationship exists)}\\)\n\n\\(p&lt;0.001\\) → reject \\(H_0\\), horsepower is significantly related to mpg.\n\n\\(R^2=0.602\\) → explains ~60% of variation.\n\n\n\n\nCall:\nlm(formula = mpg ~ drat, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0775 -2.6803 -0.2095  2.2976  9.0225 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -7.525      5.477  -1.374     0.18    \ndrat           7.678      1.507   5.096 1.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.485 on 30 degrees of freedom\nMultiple R-squared:  0.464, Adjusted R-squared:  0.4461 \nF-statistic: 25.97 on 1 and 30 DF,  p-value: 1.776e-05\n\n\n\nModel 2 (drat):\n\\[\\widehat{mpg}=-7.52+7.68\\,drat\\]\n\n\\(H_0:\\;\\beta_1=0 \\quad \\text{(no linear relationship)}\\)\n\\(H_A:\\;\\beta_1\\neq 0 \\quad \\text{(linear relationship exists)}\\)\n\n\\(p&lt;0.001\\) → reject \\(H_0\\), drat is significantly related to mpg.\n\n\\(R^2=0.464\\) → explains ~46% of variation.\n\n\n\n\nCall:\nlm(formula = mpg ~ disp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8922 -2.2022 -0.9631  1.6272  7.2305 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.599855   1.229720  24.070  &lt; 2e-16 ***\ndisp        -0.041215   0.004712  -8.747 9.38e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.251 on 30 degrees of freedom\nMultiple R-squared:  0.7183,    Adjusted R-squared:  0.709 \nF-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10\n\n\n\nModel 3 (disp):\n\\[\\widehat{mpg}=29.60-0.04\\,disp\\]\n\n\\(H_0:\\;\\beta_1=0 \\quad \\text{(no linear relationship)}\\)\n\\(H_A:\\;\\beta_1\\neq 0 \\quad \\text{(linear relationship exists)}\\)\n\n\\(p&lt;0.001\\) → reject \\(H_0\\), displacement is significantly related to mpg.\n\n\\(R^2=0.718\\) → explains ~72% of variation.\n\n\nStep 2: Compare models\n\nAll three predictors (hp, drat, disp) show significant evidence of a linear relationship with mpg.\n\nAmong them, displacement (disp) has the highest \\(R^2\\) (0.718), meaning it explains the most variation in mpg.\n\nTherefore, the simple regression with displacement is the best single-variable model for predicting mpg in this dataset.\n\nAnswer: Displacement (\\(disp\\)) gives the best simple linear regression model for mpg because it explains the largest proportion of variation (\\(R^2=0.72\\)).\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 23"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-23.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-23.html#lesson-administration",
    "title": "Lesson 23: Correlation & Simple Linear Regression",
    "section": "",
    "text": "Lesson 25.5\n7 November\nMilestone 5 Instructions\n\n\n\n\n\nLesson 26\n12-13 November\nLink: TBD\n\n\n\n\n\nLesson 26.5\n17 November\nMilestone 6 Instructions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 23"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-23.html#linear-regression",
    "href": "MA206-AY26-1/lesson-23.html#linear-regression",
    "title": "Lesson 23: Correlation & Simple Linear Regression",
    "section": "",
    "text": "We can see a clear trend here from weight to miles per gallon:\n\n\n\n\n\n\n\n\n\nThere should be some type of y = mx + b formula that could help us model this relationship or predict a new point in here somewhere.\nWhat about this one?\n\n\n\n\n\n\n\n\n\nOr this one?\n\n\n\n\n\n\n\n\n\nOr this one?\n\n\n\n\n\n\n\n\n\nAre these lines good? Are these lines good enough? Are these lines the best?\nWouldn’t it be nice if we could mathematically decide what the best line is for this?\nHow would we do that? The idea is that a best line would minimize the red lines in this plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe just saw that some lines fit the data better than others. But we need a mathematical way to decide what “best” means.\n\n\nFor each point \\((x_i, y_i)\\), the line makes a prediction:\n\\[\n\\hat{y}_i = \\beta_0 + \\beta_1 x_i\n\\]\nThe error (residual) is the difference between the actual \\(y_i\\) and the predicted \\(\\hat{y}_i\\):\n\\[\n\\begin{align*}\ne_i &= y_i - \\hat{y}_i \\\\\n    &= y_i - (\\beta_0 + \\beta_1 x_i) \\\\\n    &= y_i - \\beta_0 - \\beta_1 x_i\n\\end{align*}\n\\]\n\n\n\nWe want all the errors to be as small as possible. But errors can be positive or negative, so we square them and add them up:\n\\[\nS(\\beta_0, \\beta_1) = \\sum_{i=1}^n e_i^2\n= \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2\n\\]\nThis function \\(S(\\beta_0, \\beta_1)\\) is our loss function. The “best” line is the one that makes this sum as small as possible.\n\n\n\nWe take partial derivatives of \\(S\\) with respect to \\(\\beta_0\\) and \\(\\beta_1\\), set them to zero, and solve.\n\nDerivative with respect to \\(\\beta_0\\):\n\n\\[\n\\begin{align*}\n\\frac{\\partial S}{\\partial \\beta_0} &= -2\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\beta_1 x_i\\right) = 0 \\\\[6pt]\n&= \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\beta_1 x_i\\right) = 0 \\\\[6pt]\n&= \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\beta_0 - \\sum_{i=1}^n \\beta_1x_i \\\\[6pt]\n&= \\sum_{i=1}^n y_i - n\\beta_0 - \\beta_1 \\sum_{i=1}^n x_i \\\\[6pt]\nn\\beta_0 &= \\sum_{i=1}^n y_i - \\beta_1 \\sum_{i=1}^n x_i \\\\[6pt]\n\\beta_0 &= \\frac{1}{n}\\sum_{i=1}^n y_i - \\beta_1 \\frac{1}{n}\\sum_{i=1}^n x_i \\\\[6pt]\n\\beta_0 &= \\bar{y} - \\beta_1 \\bar{x}.\n\\end{align*}\n\\]\n\nDerivative with respect to \\(\\beta_1\\):\n\n\\[\n\\begin{align*}\n\\frac{\\partial S}{\\partial \\beta_1}\n&= -2\\sum_{i=1}^n x_i\\left(y_i - \\beta_0 - \\beta_1 x_i\\right) = 0 \\\\[6pt]\n&= \\sum_{i=1}^n \\left(x_i y_i - \\beta_0 x_i - \\beta_1 x_i^2\\right) = 0 \\\\[6pt]\n&= \\sum_{i=1}^n x_i y_i \\;-\\; \\beta_0 \\sum_{i=1}^n x_i \\;-\\; \\beta_1 \\sum_{i=1}^n x_i^2 = 0 \\\\[6pt]\n&= \\sum_{i=1}^n x_i y_i \\;-\\; (\\bar y - \\beta_1 \\bar x)\\sum_{i=1}^n x_i \\;-\\; \\beta_1 \\sum_{i=1}^n x_i^2 = 0 \\\\[6pt]\n&= \\sum_{i=1}^n x_i y_i \\;-\\; (\\bar y - \\beta_1 \\bar x)n \\bar x \\;-\\; \\beta_1 \\sum_{i=1}^n x_i^2 = 0 \\\\[6pt]\n&= \\sum_{i=1}^n x_i y_i \\;-\\; n\\bar x \\bar y \\;+\\; \\beta_1 n \\bar x^2 \\;-\\; \\beta_1 \\sum_{i=1}^n x_i^2 = 0 \\\\[6pt]\n\\beta_1\\!\\left(\\sum_{i=1}^n x_i^2 - n\\bar x^2\\right) &= \\sum_{i=1}^n x_i y_i - n\\bar x \\bar y \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n x_i y_i - n\\bar x \\bar y}{\\sum_{i=1}^n x_i^2 - n\\bar x^2} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n x_i y_i - n\\bar x \\bar y - n\\bar x \\bar y + n\\bar x \\bar y}{\\sum_{i=1}^n x_i^2 - n\\bar x^2 - 2 n \\bar x \\bar x + 2 n \\bar x \\bar x} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n x_i y_i - \\bar x (n \\bar y) - \\bar y (n\\bar x)  + n\\bar x \\bar y}{\\sum_{i=1}^n x_i^2 - n\\bar x^2 - 2 \\bar x (n \\bar x) + 2 n \\bar x^2} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n x_i y_i - \\bar x \\sum_{i=1}^n y_i - \\bar y \\sum_{i=1}^n x_i  + n\\bar x \\bar y}{\\sum_{i=1}^n x_i^2 - n\\bar x^2 - 2\\bar x \\sum_{i=1}^n x_i + 2n\\bar x^2} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n (x_i y_i - \\bar x y_i - \\bar y x_i  + \\bar x \\bar y)}{\\sum_{i=1}^n x_i^2 - 2\\bar x \\sum_{i=1}^n x_i + n\\bar x^2} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)}{\\sum_{i=1}^n x_i^2 - 2\\bar x \\sum_{i=1}^n x_i + \\sum_{i=1}^n \\bar x^2} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)}{\\sum_{i=1}^n x_i^2 - 2\\bar x x_i + \\bar x^2} \\\\[6pt]\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)}{\\sum_{i=1}^n (x_i-\\bar x)^2}.\n\\end{align*}\n\\]\nSo to sum it up:\n\\[\n\\begin{aligned}\n\\beta_1 &= \\dfrac{\\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i=1}^n (x_i - \\bar x)^2} \\\\[6pt]\n\\beta_0 &= \\bar y - \\beta_1 \\bar x\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\hat{y}_i = \\beta_0 + \\beta_1 x_i\n\\]\n\n\\(\\hat{\\beta}_1\\) is the slope — how much \\(y\\) changes on average when \\(x\\) increases by 1.\n\n\\(\\hat{\\beta}_0\\) is the intercept — the value of \\(y\\) when \\(x = 0\\).\n\nSo remember this plot?\n\n\n\n\n\n\n\n\n\nThese formulas guarantee that our line is the one that minimizes the red “error bars”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm(formula = mpg~wt, data = mtcars)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\n\nSo this means\n\\[\n\\widehat{mpg} \\;=\\; 37.39 \\;-\\; 5.34 \\, wt\n\\]\n\n\n\n\n\n\nInterpretation\n\n\n\n\nIntercept (37.39): If a car had zero weight (not realistic, but useful mathematically), the predicted mpg would be 37.39.\n\nSlope (-5.34): For each additional 1000 lbs of weight, the predicted fuel efficiency decreases on average by about 5.3 mpg.\n\n\n\n\n\n\n\nHow do we know if we can “trust” our slope coefficient, \\(\\beta_1\\)?\nWe frame this as a hypothesis test:\n\\[\nH_0: \\; \\beta_1 = 0 \\quad \\text{(no linear relationship)}\n\\]\n\\[\nH_A: \\; \\beta_1 \\neq 0 \\quad \\text{(some linear relationship exists)}\n\\]\n\nmod &lt;- lm(mpg ~ wt, data = mtcars)\nsummary(mod)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\n\n\n\n\n\nFor a little more depth\n\n\n\nThe test statistic here is calculated as\n\\[\nt = \\frac{\\beta_1 - \\text{null}}{SE} = \\frac{37.2851}{1.8776} = 19.858\n\\]\nAnd if you remember…\n\n2 * (1 - pt(abs(19.858), 32))\n\n[1] 0\n\n\n\n\n\n\n\nHow good is this model? One way to measure it is with the coefficient of determination:\n\\[\nR^2 = 0.7528\n\\]\nThis means that about 75% of the variation in mpg can be explained by the linear relationship with car weight.\nThe remaining 25% of the variation is due to other factors not captured by this model (such as horsepower, number of cylinders, transmission type, etc.).\n\n\n\n\n\n\nKey Point\n\n\n\n\nA high \\(R^2\\) (close to 1) means the model explains a lot of the variation.\n\nA low \\(R^2\\) (close to 0) means the model doesn’t explain much variation.\n\nHere, \\(R^2 = 0.75\\) suggests the model is quite strong for such a simple one-variable regression.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 23"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-23.html#board-problem",
    "href": "MA206-AY26-1/lesson-23.html#board-problem",
    "title": "Lesson 23: Correlation & Simple Linear Regression",
    "section": "",
    "text": "Do this regression for mpg against hp, drat, and disp.\n\nReport the regression equation.\n\nState the hypothesis test and conclusion.\n\nDecide if each variable has a linear relationship with mpg.\n\nFinally, which model is best when it comes to explaining mpg? Why?\n\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStep 1: Fit the regression models\n\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n\n\nModel 1 (hp):\n\\[\\widehat{mpg}=30.10-0.07\\,hp\\]\n\n\\(H_0:\\;\\beta_1=0 \\quad \\text{(no linear relationship)}\\)\n\\(H_A:\\;\\beta_1\\neq 0 \\quad \\text{(linear relationship exists)}\\)\n\n\\(p&lt;0.001\\) → reject \\(H_0\\), horsepower is significantly related to mpg.\n\n\\(R^2=0.602\\) → explains ~60% of variation.\n\n\n\n\nCall:\nlm(formula = mpg ~ drat, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0775 -2.6803 -0.2095  2.2976  9.0225 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -7.525      5.477  -1.374     0.18    \ndrat           7.678      1.507   5.096 1.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.485 on 30 degrees of freedom\nMultiple R-squared:  0.464, Adjusted R-squared:  0.4461 \nF-statistic: 25.97 on 1 and 30 DF,  p-value: 1.776e-05\n\n\n\nModel 2 (drat):\n\\[\\widehat{mpg}=-7.52+7.68\\,drat\\]\n\n\\(H_0:\\;\\beta_1=0 \\quad \\text{(no linear relationship)}\\)\n\\(H_A:\\;\\beta_1\\neq 0 \\quad \\text{(linear relationship exists)}\\)\n\n\\(p&lt;0.001\\) → reject \\(H_0\\), drat is significantly related to mpg.\n\n\\(R^2=0.464\\) → explains ~46% of variation.\n\n\n\n\nCall:\nlm(formula = mpg ~ disp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8922 -2.2022 -0.9631  1.6272  7.2305 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.599855   1.229720  24.070  &lt; 2e-16 ***\ndisp        -0.041215   0.004712  -8.747 9.38e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.251 on 30 degrees of freedom\nMultiple R-squared:  0.7183,    Adjusted R-squared:  0.709 \nF-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10\n\n\n\nModel 3 (disp):\n\\[\\widehat{mpg}=29.60-0.04\\,disp\\]\n\n\\(H_0:\\;\\beta_1=0 \\quad \\text{(no linear relationship)}\\)\n\\(H_A:\\;\\beta_1\\neq 0 \\quad \\text{(linear relationship exists)}\\)\n\n\\(p&lt;0.001\\) → reject \\(H_0\\), displacement is significantly related to mpg.\n\n\\(R^2=0.718\\) → explains ~72% of variation.\n\n\nStep 2: Compare models\n\nAll three predictors (hp, drat, disp) show significant evidence of a linear relationship with mpg.\n\nAmong them, displacement (disp) has the highest \\(R^2\\) (0.718), meaning it explains the most variation in mpg.\n\nTherefore, the simple regression with displacement is the best single-variable model for predicting mpg in this dataset.\n\nAnswer: Displacement (\\(disp\\)) gives the best simple linear regression model for mpg because it explains the largest proportion of variation (\\(R^2=0.72\\)).",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 23"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-23.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-23.html#before-you-leave",
    "title": "Lesson 23: Correlation & Simple Linear Regression",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 23"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-21.html",
    "href": "MA206-AY26-1/lesson-21.html",
    "title": "Lesson 21: Statistical Investigation Lab",
    "section": "",
    "text": "In honor of Thayer Time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEE7.2 due 0700 23 Oct\n\n\n\n\nToday is SIL 2\n\n\n\n\n\nNext Lesson\n\n\n\n\n\n\n\nTo Help Prepare\nExploration Exercise 2.3\nExploration Exercise 3.2\nBoard Problems on each lesson, 11-20\nCoursewide review to be published in the coming days\nFor WPR\n\n8x11 Note Sheet written by you front and back\n\nCourse Guide\n\nCalculator\n\nR/RStudio\n\nPen/Pencil\n\nPositive Can Do Attitude\n\nWater / Caffeine\n\nNo AI, no outside internet, no buddies, no website\n\n\n\n\n\n\n\nMath 1 vs EECS\n\n\n\n\n\n\nPreviously 6-0\n\n\n\n\n\n\n7-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\pi = \\pi_0\\)\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nConfidence Interval for \\(\\pi\\) (one proportion)\n\\[\n\\hat{p} \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true population proportion \\(\\pi\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu = \\mu_0\\)\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu &lt; \\mu_0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) (degrees of freedom)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nConfidence Interval for \\(\\mu\\) (one mean)\n\\[\n\\bar{x} \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\\frac{s}{\\sqrt{n}},\n\\qquad df = n-1\n\\] I am \\((1 - \\alpha)\\%\\) confident that the true population mean \\((\\mu)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\pi_1 - \\pi_2 = 0\\)\n\\[\nz \\;=\\; \\frac{(\\hat{p}_1 - \\hat{p}_2) - (\\pi_1 - \\pi_2)}{\\sqrt{\\hat{p}(1-\\hat{p})\\left(\\tfrac{1}{n_1} + \\tfrac{1}{n_2}\\right)}}\n\\]\nWhere the pooled proportion is\n\\[\n\\hat{p} \\;=\\; \\frac{x_1 + x_2}{n_1 + n_2}.\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 &gt; 0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 &lt; 0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 \\neq 0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p}_1 = x_1/n_1\\) (sample proportion in group 1)\n\n\\(\\hat{p}_2 = x_2/n_2\\) (sample proportion in group 2)\n\n\\(\\pi_1, \\pi_2\\) = hypothesized proportions under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nConfidence Interval for \\(\\pi_1 - \\pi_2\\) (unpooled SE)\n\\[\n(\\hat{p}_1 - \\hat{p}_2) \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\n\\sqrt{\\tfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\tfrac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true difference in population proportions \\((\\pi_1 - \\pi_2)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu_1 - \\mu_2 = 0\\)\n\\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - 0}{\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu_1 - \\mu_2 &gt; 0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu_1 - \\mu_2 &lt; 0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu_1 - \\mu_2 \\neq 0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}_1,\\ \\bar{x}_2\\) = sample means in groups 1 and 2\n\n\\(s_1,\\ s_2\\) = sample standard deviations\n\n\\(n_1,\\ n_2\\) = sample sizes\n\n\\(df = n_1 + n_2 - 2\\)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\\[\n(\\bar{x}_1 - \\bar{x}_2) \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\n\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}},\n\\qquad df = n_1+n_2-2\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true difference in population means \\((\\mu_1 - \\mu_2)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu_d = 0\\)\n(where \\(d = \\text{Before – After}\\) is the difference within each pair)\n\\[\nt = \\frac{\\bar{d} - 0}{s_d / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu_d &gt; 0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu_d &lt; 0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu_d \\neq 0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{d}\\) = mean of the paired differences\n\n\\(s_d\\) = standard deviation of the paired differences\n\n\\(n\\) = number of pairs\n\n\\(df = n - 1\\) (degrees of freedom)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nConfidence Interval for \\(\\mu_d\\) (paired mean difference)\n\\[\n\\bar{d} \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\\frac{s_d}{\\sqrt{n}},\n\\qquad df = n-1\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true mean difference in the population \\((\\mu_d)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\nStrength of evidence: Smaller \\(p\\) means stronger evidence against \\(H_0\\).\n\n\n\n\n\n\nGeneralization: We can generalize results to a larger population if the data come from a random and representative sample of that population.\n\nCausation: We can claim causation if participants are randomly assigned to treatments in an experiment. Without random assignment, we can only conclude association, not causation.\n\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n& \\text{Randomly Sampled} & \\text{Not Randomly Sampled} \\\\\n\\hline\n\\textbf{Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: Yes}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: Yes}\n\\end{array} \\\\\n\\hline\n\\textbf{Not Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: No}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: No}\n\\end{array} \\\\\n\\hline\n\\end{array}\n\\]\n\n\n\n\n\nParameters vs. Statistics: A parameter is a fixed (but usually unknown) numerical value describing a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\pi\\)). A statistic is a numerical value computed from a sample (e.g., \\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\)).\n\nParameters = target (what we want to know).\n\nStatistics = evidence (what we can actually measure).\n\nWe use statistics to estimate parameters, and because different samples give different statistics, we capture this variability with confidence intervals.\n\n\n\n\n\n\nProportions\n\nIndependence / Randomness: data come from a random sample (or are representative of the population). For two-sample tests, groups are independent.\n\nSuccess–Failure Condition: each proportion must have at least\n\n\\(n\\hat{p} \\geq 10\\) (expected successes)\n\n\\(n(1 - \\hat{p}) \\geq 10\\) (expected failures)\n\n\nMeans\n\nIndependence / Randomness: data come from a random sample (or are representative of the population). For two-sample tests, groups (or paired differences) are independent.\n\nSample Size / Shape:\n\nData should be approximately normally distributed when \\(n &gt; 20\\).\n\nIf \\(n \\leq 20\\): distribution should be roughly symmetric with no extreme skew or outliers.\n\nFor paired tests: check the distribution of the differences.\n\n\n\n\n\n\n\nIt is the half-width of the confidence interval — the distance from the sample statistic to either endpoint of the interval.\nSo a confidence interval can always be written as:\n\\[ \\text{Sample Statistic} \\; \\pm \\; \\text{Margin of Error}. \\]\nFor example, for a population mean the confidence interval is:\n\\[ \\bar{x} \\; \\pm \\; t_{\\alpha/2, \\; df} \\times \\frac{s}{\\sqrt{n}}. \\]\nThe margin of error is:\n\\[ t_{\\alpha/2, \\; df} \\times \\frac{s}{\\sqrt{n}} \\]\nFor a population proportion the confidence interval is:\n\\[ \\hat{p} \\; \\pm \\; z_{\\alpha/2} \\times \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}. \\]\nThe margin of error is:\n\\[ z_{\\alpha/2} \\times \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} \\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponent\n1-Sample Mean\n2-Sample Mean\nPaired Mean\n1-Sample Prop\n2-Sample Prop\n\n\n\n\nNull (H0)\n\n\n\n\n\n\n\nAlt &gt; Alt &lt; Alt =\n\n\n\n\n\n\n\nTest Stat\n\n\n\n\n\n\n\np-value &gt; p-value &gt; p-value =\n\n\n\n\n\n\n\nCI Formula\n\n\n\n\n\n\n\n\n\n\n\n\nA nutritionist wonders if teenagers consume more sodium than the recommended 2,300 mg per day.\nShe collects data from 40 teens across several high schools in her state. The teens self-reported their daily intake, averaging 2,450 mg per day with a standard deviation of 400 mg. Many of the students were also athletes, which could influence dietary choices.\n\n\n\nNationwide surveys suggest about 62% of adults own a smartphone.\nIn a phone survey of 150 adults from one midsized city, 105 reported owning a smartphone. The city has a relatively young population compared to the national average, which could affect the results.\n\n\n\nA school district wants to know whether male and female students spend different amounts of time on homework each week.\n\n\n\nIn a sample of 35 male students, the average reported time was 12.4 hours with a standard deviation of 4.2 hours.\n\nIn a sample of 40 female students, the average was 14.1 hours with a standard deviation of 3.9 hours.\nAll students were enrolled in honors-level courses, which may influence homework expectations.\n\n\n\nA company is comparing two training programs to see if one produces higher certification pass rates.\n\n\n\nOf the 60 employees enrolled in Program A, 48 passed the exam.\n\nOf the 55 employees in Program B, 38 passed the exam.\nSupervisors assigned employees to the training programs based on their work schedules, and employees in Program A tended to have more prior experience with the material.\n\n\n\nA researcher measures the resting heart rates of 25 participants before and after an 8-week aerobic training program.\nOn average, heart rates were 5.2 beats per minute lower after training, with the differences across participants having a standard deviation of 7.5 bpm. Some participants also reported starting new diets at the same time as the training program.\n\n\n\n\n\nWhat is the research question?\nAre the variables categorical or quantitative? What type of test are you completing?\nWhat is the response variable and what is the explanatory variable in this study?\nIs there a potential confounding variable? If so, what might it be?\nDescribe the parameter of interest in context of the question.\nState the null and alternative hypotheses in both symbols and words.\nList the appropriate summary statistic(s) by name, symbol, and value.\nState the appropriate validity conditions and whether they are met.\nReport the standardized statistic, the p-value, and a confidence interval.\nBased on an α = 0.05 significance level, do you reject or fail to reject the null? Provide your evidence.\nCan the results of this study be generalized to a larger population? Why or why not?\nCan we assume a causal relationship from these results? Why or why not?\n\n\n\n\n\n\n\n\n\n\nScenario 1\n\n\n\n\n\nA nutritionist wonders if teenagers consume more sodium than the recommended 2,300 mg per day.\nShe collects data from 40 teens across several high schools in her state. The teens self-reported their daily intake, averaging 2,450 mg per day with a standard deviation of 400 mg. Many of the students were also athletes, which could influence dietary choices.\n\nResearch Question\nDo teenagers consume more sodium on average than the recommended 2,300 mg per day?\nVariables & Test\nQuantitative (sodium intake). One-sample t-test for a mean.\nResponse & Explanatory\nResponse = sodium intake (mg/day).\nExplanatory = comparison to recommended guideline.\nConfounding Variable\nAthletic status (athletes may consume more sodium).\nParameter of Interest\nµ = true mean sodium intake of all teenagers.\nHypotheses\nH₀: µ = 2300\nHₐ: µ &gt; 2300\nSummary Statistics\nn = 40, x̄ = 2450, s = 400.\nValidity Conditions\nSample size \\(n = 40\\) is larger than 20, so the mean intake can be reasonably modeled with a t-test.\nTest Statistic & CI\n\nTest statistic:\n\\[ t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}} \\]\nxbar &lt;- 2450\nmu0 &lt;- 2300\ns &lt;- 400\nn &lt;- 40\n\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\np-value (right-tailed):\ndf &lt;- n - 1\np_value &lt;- 1 - pt(t_stat, df)\np_value\n95% CI:\nalpha &lt;- 0.05\nt_star &lt;- qt(1 - alpha/2, df)\n\nME &lt;- t_star * s / sqrt(n)\nlower &lt;- xbar - ME\nupper &lt;- xbar + ME\nc(lower, upper)\nResult: \\(t \\approx 2.37\\), \\(p \\approx 0.011\\).\nI am 95% confident the true mean sodium intake is between 2346 and 2554 mg/day.\n\nDecision at α = 0.05\nReject H₀. We conclude it is possible that the true mean sodium intake of teenagers is greater than 2300 mg per day. In context, this suggests teens in the study tend to consume more sodium than recommended.\nGeneralization\nSample from one state → cannot generalize to all U.S. teens.\nCausality\nObservational design → cannot infer causality.\n\n\n\n\n\n\n\n\n\n\n\n\nScenario 2\n\n\n\n\n\nNationwide surveys suggest about 62% of adults own a smartphone.\nIn a phone survey of 150 adults from one midsized city, 105 reported owning a smartphone. The city has a relatively young population compared to the national average, which could affect the results.\n\nResearch Question\nIs the proportion of smartphone ownership in this city different from 62%?\nVariables & Test\nCategorical (own smartphone or not). One-sample z-test for a proportion.\nResponse & Explanatory\nResponse = smartphone ownership (yes/no).\nExplanatory = national benchmark proportion (\\(p_0 = 0.62\\)).\nConfounding Variable\nAge distribution of the city (younger population may have higher ownership).\nParameter of Interest\n\\(p =\\) true proportion of adults in this city who own a smartphone.\nHypotheses\n\\(H_0: p = 0.62\\)\n\\(H_a: p \\ne 0.62\\)\nSummary Statistics\n\\(n = 150,\\; x = 105,\\; \\hat{p} = 105/150 = 0.70\\)\nValidity Conditions\nSuccess–failure condition holds: \\(n\\hat{p} = 105 \\geq 10\\) and \\(n(1-\\hat{p}) = 45 \\geq 10\\). Random/representative sample assumed.\nTest Statistic, p-value, and 95% CI\n\nFormulas\n- Test statistic: \\(z = \\dfrac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}\\)\n- Two-sided p-value: \\(2 \\,[1-\\Phi(|z|)]\\)\n- 95% CI: \\(\\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}}\\) with \\(z_{0.025} \\approx 1.96\\)\nR code (template)\n# Given data\nn &lt;- 150\nx &lt;- 105\np.hat &lt;- x / n\np0 &lt;- 0.62\n\n# Test statistic & p-value (two-sided)\nse0 &lt;- sqrt(p0*(1 - p0)/n)\nz &lt;- (p.hat - p0) / se0\np.value &lt;- 2 * (1 - pnorm(abs(z)))\n\nz; p.value\n# 95% confidence interval for p\nalpha &lt;- 0.05\nz.star &lt;- qnorm(1 - alpha/2)\nse.hat &lt;- sqrt(p.hat*(1 - p.hat)/n)\nME &lt;- z.star * se.hat\nlower &lt;- p.hat - ME\nupper &lt;- p.hat + ME\nc(lower, upper)\nNumerical results\n\\(z \\approx 2.02\\), two-sided \\(p \\approx 0.044\\).\n95% CI: I am 95% confident that the true proportion of adults in this city who own a smartphone is between 0.627 and 0.773.\n\nDecision at \\(\\alpha = 0.05\\)\nReject \\(H_0\\). We conclude it is possible that the true proportion is not equal to 0.62. In context, smartphone ownership in this city likely differs from the national rate.\nGeneralization\nThis is one midsized city; generalization to all U.S. adults is limited without broader sampling.\nCausality\nThis is a survey (no random assignment), so we cannot infer a causal explanation for differences in ownership.\n\n\n\n\n\n\n\n\n\n\n\nScenario 3\n\n\n\n\n\nA school district wants to know whether male and female students spend different amounts of time on homework each week.\n- 35 male students: \\(\\bar{x}_1 = 12.4\\) hours, \\(s_1 = 4.2\\)\n- 40 female students: \\(\\bar{x}_2 = 14.1\\) hours, \\(s_2 = 3.9\\)\nAll students were in honors-level courses, which may influence homework expectations.\n\nResearch Question\nDo male and female students differ in average weekly homework hours?\nVariables & Test\nQuantitative response (hours), categorical explanatory (gender). Two-sample t-test for means.\nResponse & Explanatory\nResponse = hours of homework.\nExplanatory = gender (male vs female).\nConfounding Variable\nHonors course enrollment.\nParameter of Interest\n\\(\\mu_2 - \\mu_1 =\\) difference in true mean hours (female − male).\nHypotheses\n\\(H_0: \\mu_2 - \\mu_1 = 0\\)\n\\(H_a: \\mu_2 - \\mu_1 \\ne 0\\)\nSummary Statistics\n\\(n_1 = 35, \\; \\bar{x}_1 = 12.4, \\; s_1 = 4.2\\)\n\\(n_2 = 40, \\; \\bar{x}_2 = 14.1, \\; s_2 = 3.9\\)\nValidity Conditions\nBoth groups have \\(n &gt; 20\\), so approximate normality is reasonable.\nTest Statistic, p-value, and 95% CI\n\nFormulas\n- Pooled variance:\n\\[ s_p^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2} \\]\n\nStandard error:\n\\[ SE = \\sqrt{s_p^2 \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)} \\]\nTest statistic:\n\\[ t = \\frac{(\\bar{x}_2 - \\bar{x}_1) - 0}{SE} \\]\nDegrees of freedom:\n\\[ df = n_1 + n_2 - 2 \\]\nConfidence interval:\n\\[ (\\bar{x}_2 - \\bar{x}_1) \\;\\pm\\; t_{\\alpha/2, df} \\times SE \\]\n\nR code (template)\n# Given data\nn1 &lt;- 35; xbar1 &lt;- 12.4; s1 &lt;- 4.2\nn2 &lt;- 40; xbar2 &lt;- 14.1; s2 &lt;- 3.9\n\n# Pooled variance and standard deviation\nsp2 &lt;- ((n1 - 1)*s1^2 + (n2 - 1)*s2^2) / (n1 + n2 - 2)\nsp &lt;- sqrt(sp2)\n\n# Standard error\nSE &lt;- sp * sqrt(1/n1 + 1/n2)\n\n# Difference in means\ndiff &lt;- xbar2 - xbar1\n\n# Test statistic and df\nt_stat &lt;- diff / SE\ndf &lt;- n1 + n2 - 2\n\n# Two-sided p-value\np_value &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n# 95% CI\nalpha &lt;- 0.05\nt_star &lt;- qt(1 - alpha/2, df)\nlower &lt;- diff - t_star * SE\nupper &lt;- diff + t_star * SE\n\nt_stat; p_value; c(lower, upper)\nNumerical results\n\\(t \\approx 1.87\\) (female − male), \\(p \\approx 0.066\\), with \\(df = 73\\).\n95% CI: I am 95% confident that the true difference in mean homework hours (female − male) is between −3.5 and 0.2 hours.\n\nDecision at \\(\\alpha = 0.05\\)\nFail to reject \\(H_0\\). It is possible that the true difference in mean homework hours is 0. In context, this means there may be no real difference in homework time between male and female students.\nGeneralization\nSample only from honors students in a limited number of schools → generalization is limited.\nCausality\nObservational data; cannot infer causality.\n\n\n\n\n\n\n\n\n\n\n\nScenario 4\n\n\n\n\n\nA company is comparing two training programs to see if one produces higher certification pass rates.\n- Program A: 60 employees, 48 passed.\n- Program B: 55 employees, 38 passed.\nSupervisors assigned employees to programs based on schedules; Program A employees had more prior experience.\n\nResearch Question\nDo pass rates differ between Program A and Program B?\nVariables & Test\nCategorical response (pass/fail), categorical explanatory (program). Two-sample \\(z\\)-test for proportions.\nResponse & Explanatory\nResponse = pass/fail.\nExplanatory = training program (A vs B).\nConfounding Variable\nPrior experience with the certification material.\nParameter of Interest\n\\(p_1 - p_2 =\\) difference in true pass rates (Program A \\(-\\) Program B).\nHypotheses\n\\(H_0: p_1 - p_2 = 0\\)\n\\(H_a: p_1 - p_2 \\ne 0\\)\nSummary Statistics\n\\(n_1 = 60,\\; x_1 = 48,\\; \\hat p_1 = 0.80\\)\n\\(n_2 = 55,\\; x_2 = 38,\\; \\hat p_2 \\approx 0.6909\\)\nObserved difference: \\(\\hat p_1 - \\hat p_2 \\approx 0.1091\\).\nValidity Conditions\nEach group has at least 10 successes and 10 failures:\n\n\n\nProgram A: successes \\(=48\\), failures \\(=12\\)\n\nProgram B: successes \\(=38\\), failures \\(=17\\)\nConditions met; groups treated as independent.\n\n\nTest Statistic, p-value, and 95% CI\n\nFormulas\n- Pooled proportion (for the test): \\(\\hat p_{pool} = \\dfrac{x_1 + x_2}{n_1 + n_2}\\)\n- Test statistic:\n\\[ z = \\frac{(\\hat p_1 - \\hat p_2) - 0}{\\sqrt{\\hat p_{pool}(1-\\hat p_{pool})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\] - Two-sided p-value: \\(2\\,[1-\\Phi(|z|)]\\) - 95% CI (unpooled SE):\n\\[ (\\hat p_1 - \\hat p_2) \\;\\pm\\; z_{\\alpha/2}\\,\\sqrt{\\frac{\\hat p_1(1-\\hat p_1)}{n_1} + \\frac{\\hat p_2(1-\\hat p_2)}{n_2}} \\]\nR code (template)\n# Data\nn1 &lt;- 60; x1 &lt;- 48; p1.hat &lt;- x1 / n1\nn2 &lt;- 55; x2 &lt;- 38; p2.hat &lt;- x2 / n2\n\n# Pooled for H0\np.pool &lt;- (x1 + x2) / (n1 + n2)\nSE0 &lt;- sqrt(p.pool * (1 - p.pool) * (1/n1 + 1/n2))\n\n# Test statistic & two-sided p-value\nz &lt;- (p1.hat - p2.hat) / SE0\np.value &lt;- 2 * (1 - pnorm(abs(z)))\n\nz; p.value\n# 95% CI for (p1 - p2) using unpooled SE\nalpha &lt;- 0.05\nz.star &lt;- qnorm(1 - alpha/2)\nSE.hat &lt;- sqrt(p1.hat*(1 - p1.hat)/n1 + p2.hat*(1 - p2.hat)/n2)\nME &lt;- z.star * SE.hat\nlower &lt;- (p1.hat - p2.hat) - ME\nupper &lt;- (p1.hat - p2.hat) + ME\n\nc(lower, upper)\nNumerical results\n\\(z \\approx 1.35\\), two-sided \\(p \\approx 0.178\\).\n95% CI: I am 95% confident that the true difference in pass rates (Program A \\(-\\) Program B) is somewhere between \\(-0.050\\) and \\(0.268\\).\n\nDecision at \\(\\alpha = 0.05\\)\nFail to reject \\(H_0\\). It is possible that the true difference in pass rates is \\(0\\) (no difference between Program A and Program B). In context, the programs may perform similarly based on this sample.\nGeneralization\nThis is one company’s workforce; generalization beyond similar employees is limited.\nCausality\nAssignment was not randomized (schedules/prior experience differ), so we cannot make a causal claim that one program causes higher pass rates.\n\n\n\n\n\n\n\n\n\n\n\nScenario 5\n\n\n\n\n\nA researcher measures the resting heart rates of 25 participants before and after an 8-week aerobic training program.\nMean decrease (Before − After) \\(=\\;5.2\\) bpm, standard deviation of differences \\(s_d = 7.5\\) bpm. Some participants also started new diets.\n\nResearch Question\nDoes the training program reduce average resting heart rate?\nVariables & Test\nQuantitative paired differences (before/after on the same people). Paired t-test for a mean difference.\nResponse & Explanatory\nResponse = resting heart rate.\nExplanatory = time (before vs after program) on the same participants.\nConfounding Variable\nDiet changes begun during the program.\nParameter of Interest\n\\(\\mu_d =\\) true mean difference in resting heart rate (Before − After).\nHypotheses\n\\(H_0: \\mu_d = 0\\)\n\\(H_a: \\mu_d &gt; 0\\) (a positive mean difference indicates a decrease after training)\nSummary Statistics\n\\(n = 25,\\;\\; \\bar{d} = 5.2,\\;\\; s_d = 7.5\\)\nValidity Conditions\nPairs are matched by person; differences are the unit of analysis. With \\(n=25\\,(&gt;20)\\) the paired t-procedure is reasonable; we assume the distribution of differences is approximately normal and pairs are independent of each other.\nTest Statistic, p-value, and 95% CI\n\nFormulas\n- Test statistic:\n\\[ t = \\frac{\\bar{d} - 0}{s_d/\\sqrt{n}} \\] - Right-tailed p-value: \\(1 - T_{df}(t)\\) with \\(df = n - 1\\)\n- 95% CI for \\(\\mu_d\\):\n\\[ \\bar{d} \\;\\pm\\; t_{\\alpha/2,\\,df}\\;\\frac{s_d}{\\sqrt{n}}, \\quad df=n-1 \\]\nR code (template)\n# Given data\nn &lt;- 25\ndbar &lt;- 5.2\nsd_d &lt;- 7.5\nmu0 &lt;- 0\n\n# Test statistic & right-tailed p-value\nt_stat &lt;- (dbar - mu0) / (sd_d / sqrt(n))\ndf &lt;- n - 1\np_value &lt;- 1 - pt(t_stat, df)\n\nt_stat; p_value\n# 95% CI for mean difference\nalpha &lt;- 0.05\nt_star &lt;- qt(1 - alpha/2, df)\nME &lt;- t_star * sd_d / sqrt(n)\nlower &lt;- dbar - ME\nupper &lt;- dbar + ME\nc(lower, upper)  # I am 95% confident the true mean decrease is between lower and upper\nNumerical results\n\\(t \\approx 3.47\\), right-tailed \\(p \\approx 0.001\\).\n95% CI: I am 95% confident that the true mean decrease in resting heart rate is somewhere between \\(2.2\\) and \\(8.2\\) bpm.\n\nDecision at \\(\\alpha = 0.05\\)\nReject \\(H_0\\). We conclude it is possible that the true mean decrease in resting heart rate is greater than \\(0\\). In context, this suggests the training program may lower resting heart rates.\nGeneralization\nVolunteer sample from a single program; generalization to all adults is limited without broader sampling.\nCausality\nNo random assignment and concurrent diet changes; we cannot make a causal claim that the program alone caused the reduction.\n\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 21"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-21.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-21.html#lesson-administration",
    "title": "Lesson 21: Statistical Investigation Lab",
    "section": "",
    "text": "EE7.2 due 0700 23 Oct\n\n\n\n\nToday is SIL 2\n\n\n\n\n\nNext Lesson\n\n\n\n\n\n\n\nTo Help Prepare\nExploration Exercise 2.3\nExploration Exercise 3.2\nBoard Problems on each lesson, 11-20\nCoursewide review to be published in the coming days\nFor WPR\n\n8x11 Note Sheet written by you front and back\n\nCourse Guide\n\nCalculator\n\nR/RStudio\n\nPen/Pencil\n\nPositive Can Do Attitude\n\nWater / Caffeine\n\nNo AI, no outside internet, no buddies, no website\n\n\n\n\n\n\n\nMath 1 vs EECS\n\n\n\n\n\n\nPreviously 6-0\n\n\n\n\n\n\n7-0",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 21"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-21.html#running-review",
    "href": "MA206-AY26-1/lesson-21.html#running-review",
    "title": "Lesson 21: Statistical Investigation Lab",
    "section": "",
    "text": "For all cases:\n\\(H_0:\\ \\pi = \\pi_0\\)\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nConfidence Interval for \\(\\pi\\) (one proportion)\n\\[\n\\hat{p} \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true population proportion \\(\\pi\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu = \\mu_0\\)\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu &lt; \\mu_0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) (degrees of freedom)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nConfidence Interval for \\(\\mu\\) (one mean)\n\\[\n\\bar{x} \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\\frac{s}{\\sqrt{n}},\n\\qquad df = n-1\n\\] I am \\((1 - \\alpha)\\%\\) confident that the true population mean \\((\\mu)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\pi_1 - \\pi_2 = 0\\)\n\\[\nz \\;=\\; \\frac{(\\hat{p}_1 - \\hat{p}_2) - (\\pi_1 - \\pi_2)}{\\sqrt{\\hat{p}(1-\\hat{p})\\left(\\tfrac{1}{n_1} + \\tfrac{1}{n_2}\\right)}}\n\\]\nWhere the pooled proportion is\n\\[\n\\hat{p} \\;=\\; \\frac{x_1 + x_2}{n_1 + n_2}.\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 &gt; 0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 &lt; 0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 \\neq 0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p}_1 = x_1/n_1\\) (sample proportion in group 1)\n\n\\(\\hat{p}_2 = x_2/n_2\\) (sample proportion in group 2)\n\n\\(\\pi_1, \\pi_2\\) = hypothesized proportions under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nConfidence Interval for \\(\\pi_1 - \\pi_2\\) (unpooled SE)\n\\[\n(\\hat{p}_1 - \\hat{p}_2) \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\n\\sqrt{\\tfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\tfrac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true difference in population proportions \\((\\pi_1 - \\pi_2)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu_1 - \\mu_2 = 0\\)\n\\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - 0}{\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu_1 - \\mu_2 &gt; 0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu_1 - \\mu_2 &lt; 0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu_1 - \\mu_2 \\neq 0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}_1,\\ \\bar{x}_2\\) = sample means in groups 1 and 2\n\n\\(s_1,\\ s_2\\) = sample standard deviations\n\n\\(n_1,\\ n_2\\) = sample sizes\n\n\\(df = n_1 + n_2 - 2\\)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\\[\n(\\bar{x}_1 - \\bar{x}_2) \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\n\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}},\n\\qquad df = n_1+n_2-2\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true difference in population means \\((\\mu_1 - \\mu_2)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu_d = 0\\)\n(where \\(d = \\text{Before – After}\\) is the difference within each pair)\n\\[\nt = \\frac{\\bar{d} - 0}{s_d / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu_d &gt; 0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu_d &lt; 0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu_d \\neq 0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{d}\\) = mean of the paired differences\n\n\\(s_d\\) = standard deviation of the paired differences\n\n\\(n\\) = number of pairs\n\n\\(df = n - 1\\) (degrees of freedom)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nConfidence Interval for \\(\\mu_d\\) (paired mean difference)\n\\[\n\\bar{d} \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\\frac{s_d}{\\sqrt{n}},\n\\qquad df = n-1\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true mean difference in the population \\((\\mu_d)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\nStrength of evidence: Smaller \\(p\\) means stronger evidence against \\(H_0\\).\n\n\n\n\n\n\nGeneralization: We can generalize results to a larger population if the data come from a random and representative sample of that population.\n\nCausation: We can claim causation if participants are randomly assigned to treatments in an experiment. Without random assignment, we can only conclude association, not causation.\n\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n& \\text{Randomly Sampled} & \\text{Not Randomly Sampled} \\\\\n\\hline\n\\textbf{Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: Yes}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: Yes}\n\\end{array} \\\\\n\\hline\n\\textbf{Not Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: No}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: No}\n\\end{array} \\\\\n\\hline\n\\end{array}\n\\]\n\n\n\n\n\nParameters vs. Statistics: A parameter is a fixed (but usually unknown) numerical value describing a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\pi\\)). A statistic is a numerical value computed from a sample (e.g., \\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\)).\n\nParameters = target (what we want to know).\n\nStatistics = evidence (what we can actually measure).\n\nWe use statistics to estimate parameters, and because different samples give different statistics, we capture this variability with confidence intervals.\n\n\n\n\n\n\nProportions\n\nIndependence / Randomness: data come from a random sample (or are representative of the population). For two-sample tests, groups are independent.\n\nSuccess–Failure Condition: each proportion must have at least\n\n\\(n\\hat{p} \\geq 10\\) (expected successes)\n\n\\(n(1 - \\hat{p}) \\geq 10\\) (expected failures)\n\n\nMeans\n\nIndependence / Randomness: data come from a random sample (or are representative of the population). For two-sample tests, groups (or paired differences) are independent.\n\nSample Size / Shape:\n\nData should be approximately normally distributed when \\(n &gt; 20\\).\n\nIf \\(n \\leq 20\\): distribution should be roughly symmetric with no extreme skew or outliers.\n\nFor paired tests: check the distribution of the differences.\n\n\n\n\n\n\n\nIt is the half-width of the confidence interval — the distance from the sample statistic to either endpoint of the interval.\nSo a confidence interval can always be written as:\n\\[ \\text{Sample Statistic} \\; \\pm \\; \\text{Margin of Error}. \\]\nFor example, for a population mean the confidence interval is:\n\\[ \\bar{x} \\; \\pm \\; t_{\\alpha/2, \\; df} \\times \\frac{s}{\\sqrt{n}}. \\]\nThe margin of error is:\n\\[ t_{\\alpha/2, \\; df} \\times \\frac{s}{\\sqrt{n}} \\]\nFor a population proportion the confidence interval is:\n\\[ \\hat{p} \\; \\pm \\; z_{\\alpha/2} \\times \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}. \\]\nThe margin of error is:\n\\[ z_{\\alpha/2} \\times \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} \\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 21"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-21.html#class-review",
    "href": "MA206-AY26-1/lesson-21.html#class-review",
    "title": "Lesson 21: Statistical Investigation Lab",
    "section": "",
    "text": "Component\n1-Sample Mean\n2-Sample Mean\nPaired Mean\n1-Sample Prop\n2-Sample Prop\n\n\n\n\nNull (H0)\n\n\n\n\n\n\n\nAlt &gt; Alt &lt; Alt =\n\n\n\n\n\n\n\nTest Stat\n\n\n\n\n\n\n\np-value &gt; p-value &gt; p-value =\n\n\n\n\n\n\n\nCI Formula\n\n\n\n\n\n\n\n\n\n\n\n\nA nutritionist wonders if teenagers consume more sodium than the recommended 2,300 mg per day.\nShe collects data from 40 teens across several high schools in her state. The teens self-reported their daily intake, averaging 2,450 mg per day with a standard deviation of 400 mg. Many of the students were also athletes, which could influence dietary choices.\n\n\n\nNationwide surveys suggest about 62% of adults own a smartphone.\nIn a phone survey of 150 adults from one midsized city, 105 reported owning a smartphone. The city has a relatively young population compared to the national average, which could affect the results.\n\n\n\nA school district wants to know whether male and female students spend different amounts of time on homework each week.\n\n\n\nIn a sample of 35 male students, the average reported time was 12.4 hours with a standard deviation of 4.2 hours.\n\nIn a sample of 40 female students, the average was 14.1 hours with a standard deviation of 3.9 hours.\nAll students were enrolled in honors-level courses, which may influence homework expectations.\n\n\n\nA company is comparing two training programs to see if one produces higher certification pass rates.\n\n\n\nOf the 60 employees enrolled in Program A, 48 passed the exam.\n\nOf the 55 employees in Program B, 38 passed the exam.\nSupervisors assigned employees to the training programs based on their work schedules, and employees in Program A tended to have more prior experience with the material.\n\n\n\nA researcher measures the resting heart rates of 25 participants before and after an 8-week aerobic training program.\nOn average, heart rates were 5.2 beats per minute lower after training, with the differences across participants having a standard deviation of 7.5 bpm. Some participants also reported starting new diets at the same time as the training program.\n\n\n\n\n\nWhat is the research question?\nAre the variables categorical or quantitative? What type of test are you completing?\nWhat is the response variable and what is the explanatory variable in this study?\nIs there a potential confounding variable? If so, what might it be?\nDescribe the parameter of interest in context of the question.\nState the null and alternative hypotheses in both symbols and words.\nList the appropriate summary statistic(s) by name, symbol, and value.\nState the appropriate validity conditions and whether they are met.\nReport the standardized statistic, the p-value, and a confidence interval.\nBased on an α = 0.05 significance level, do you reject or fail to reject the null? Provide your evidence.\nCan the results of this study be generalized to a larger population? Why or why not?\nCan we assume a causal relationship from these results? Why or why not?\n\n\n\n\n\n\n\n\n\n\nScenario 1\n\n\n\n\n\nA nutritionist wonders if teenagers consume more sodium than the recommended 2,300 mg per day.\nShe collects data from 40 teens across several high schools in her state. The teens self-reported their daily intake, averaging 2,450 mg per day with a standard deviation of 400 mg. Many of the students were also athletes, which could influence dietary choices.\n\nResearch Question\nDo teenagers consume more sodium on average than the recommended 2,300 mg per day?\nVariables & Test\nQuantitative (sodium intake). One-sample t-test for a mean.\nResponse & Explanatory\nResponse = sodium intake (mg/day).\nExplanatory = comparison to recommended guideline.\nConfounding Variable\nAthletic status (athletes may consume more sodium).\nParameter of Interest\nµ = true mean sodium intake of all teenagers.\nHypotheses\nH₀: µ = 2300\nHₐ: µ &gt; 2300\nSummary Statistics\nn = 40, x̄ = 2450, s = 400.\nValidity Conditions\nSample size \\(n = 40\\) is larger than 20, so the mean intake can be reasonably modeled with a t-test.\nTest Statistic & CI\n\nTest statistic:\n\\[ t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}} \\]\nxbar &lt;- 2450\nmu0 &lt;- 2300\ns &lt;- 400\nn &lt;- 40\n\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\np-value (right-tailed):\ndf &lt;- n - 1\np_value &lt;- 1 - pt(t_stat, df)\np_value\n95% CI:\nalpha &lt;- 0.05\nt_star &lt;- qt(1 - alpha/2, df)\n\nME &lt;- t_star * s / sqrt(n)\nlower &lt;- xbar - ME\nupper &lt;- xbar + ME\nc(lower, upper)\nResult: \\(t \\approx 2.37\\), \\(p \\approx 0.011\\).\nI am 95% confident the true mean sodium intake is between 2346 and 2554 mg/day.\n\nDecision at α = 0.05\nReject H₀. We conclude it is possible that the true mean sodium intake of teenagers is greater than 2300 mg per day. In context, this suggests teens in the study tend to consume more sodium than recommended.\nGeneralization\nSample from one state → cannot generalize to all U.S. teens.\nCausality\nObservational design → cannot infer causality.\n\n\n\n\n\n\n\n\n\n\n\n\nScenario 2\n\n\n\n\n\nNationwide surveys suggest about 62% of adults own a smartphone.\nIn a phone survey of 150 adults from one midsized city, 105 reported owning a smartphone. The city has a relatively young population compared to the national average, which could affect the results.\n\nResearch Question\nIs the proportion of smartphone ownership in this city different from 62%?\nVariables & Test\nCategorical (own smartphone or not). One-sample z-test for a proportion.\nResponse & Explanatory\nResponse = smartphone ownership (yes/no).\nExplanatory = national benchmark proportion (\\(p_0 = 0.62\\)).\nConfounding Variable\nAge distribution of the city (younger population may have higher ownership).\nParameter of Interest\n\\(p =\\) true proportion of adults in this city who own a smartphone.\nHypotheses\n\\(H_0: p = 0.62\\)\n\\(H_a: p \\ne 0.62\\)\nSummary Statistics\n\\(n = 150,\\; x = 105,\\; \\hat{p} = 105/150 = 0.70\\)\nValidity Conditions\nSuccess–failure condition holds: \\(n\\hat{p} = 105 \\geq 10\\) and \\(n(1-\\hat{p}) = 45 \\geq 10\\). Random/representative sample assumed.\nTest Statistic, p-value, and 95% CI\n\nFormulas\n- Test statistic: \\(z = \\dfrac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}\\)\n- Two-sided p-value: \\(2 \\,[1-\\Phi(|z|)]\\)\n- 95% CI: \\(\\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}}\\) with \\(z_{0.025} \\approx 1.96\\)\nR code (template)\n# Given data\nn &lt;- 150\nx &lt;- 105\np.hat &lt;- x / n\np0 &lt;- 0.62\n\n# Test statistic & p-value (two-sided)\nse0 &lt;- sqrt(p0*(1 - p0)/n)\nz &lt;- (p.hat - p0) / se0\np.value &lt;- 2 * (1 - pnorm(abs(z)))\n\nz; p.value\n# 95% confidence interval for p\nalpha &lt;- 0.05\nz.star &lt;- qnorm(1 - alpha/2)\nse.hat &lt;- sqrt(p.hat*(1 - p.hat)/n)\nME &lt;- z.star * se.hat\nlower &lt;- p.hat - ME\nupper &lt;- p.hat + ME\nc(lower, upper)\nNumerical results\n\\(z \\approx 2.02\\), two-sided \\(p \\approx 0.044\\).\n95% CI: I am 95% confident that the true proportion of adults in this city who own a smartphone is between 0.627 and 0.773.\n\nDecision at \\(\\alpha = 0.05\\)\nReject \\(H_0\\). We conclude it is possible that the true proportion is not equal to 0.62. In context, smartphone ownership in this city likely differs from the national rate.\nGeneralization\nThis is one midsized city; generalization to all U.S. adults is limited without broader sampling.\nCausality\nThis is a survey (no random assignment), so we cannot infer a causal explanation for differences in ownership.\n\n\n\n\n\n\n\n\n\n\n\nScenario 3\n\n\n\n\n\nA school district wants to know whether male and female students spend different amounts of time on homework each week.\n- 35 male students: \\(\\bar{x}_1 = 12.4\\) hours, \\(s_1 = 4.2\\)\n- 40 female students: \\(\\bar{x}_2 = 14.1\\) hours, \\(s_2 = 3.9\\)\nAll students were in honors-level courses, which may influence homework expectations.\n\nResearch Question\nDo male and female students differ in average weekly homework hours?\nVariables & Test\nQuantitative response (hours), categorical explanatory (gender). Two-sample t-test for means.\nResponse & Explanatory\nResponse = hours of homework.\nExplanatory = gender (male vs female).\nConfounding Variable\nHonors course enrollment.\nParameter of Interest\n\\(\\mu_2 - \\mu_1 =\\) difference in true mean hours (female − male).\nHypotheses\n\\(H_0: \\mu_2 - \\mu_1 = 0\\)\n\\(H_a: \\mu_2 - \\mu_1 \\ne 0\\)\nSummary Statistics\n\\(n_1 = 35, \\; \\bar{x}_1 = 12.4, \\; s_1 = 4.2\\)\n\\(n_2 = 40, \\; \\bar{x}_2 = 14.1, \\; s_2 = 3.9\\)\nValidity Conditions\nBoth groups have \\(n &gt; 20\\), so approximate normality is reasonable.\nTest Statistic, p-value, and 95% CI\n\nFormulas\n- Pooled variance:\n\\[ s_p^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2} \\]\n\nStandard error:\n\\[ SE = \\sqrt{s_p^2 \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)} \\]\nTest statistic:\n\\[ t = \\frac{(\\bar{x}_2 - \\bar{x}_1) - 0}{SE} \\]\nDegrees of freedom:\n\\[ df = n_1 + n_2 - 2 \\]\nConfidence interval:\n\\[ (\\bar{x}_2 - \\bar{x}_1) \\;\\pm\\; t_{\\alpha/2, df} \\times SE \\]\n\nR code (template)\n# Given data\nn1 &lt;- 35; xbar1 &lt;- 12.4; s1 &lt;- 4.2\nn2 &lt;- 40; xbar2 &lt;- 14.1; s2 &lt;- 3.9\n\n# Pooled variance and standard deviation\nsp2 &lt;- ((n1 - 1)*s1^2 + (n2 - 1)*s2^2) / (n1 + n2 - 2)\nsp &lt;- sqrt(sp2)\n\n# Standard error\nSE &lt;- sp * sqrt(1/n1 + 1/n2)\n\n# Difference in means\ndiff &lt;- xbar2 - xbar1\n\n# Test statistic and df\nt_stat &lt;- diff / SE\ndf &lt;- n1 + n2 - 2\n\n# Two-sided p-value\np_value &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n# 95% CI\nalpha &lt;- 0.05\nt_star &lt;- qt(1 - alpha/2, df)\nlower &lt;- diff - t_star * SE\nupper &lt;- diff + t_star * SE\n\nt_stat; p_value; c(lower, upper)\nNumerical results\n\\(t \\approx 1.87\\) (female − male), \\(p \\approx 0.066\\), with \\(df = 73\\).\n95% CI: I am 95% confident that the true difference in mean homework hours (female − male) is between −3.5 and 0.2 hours.\n\nDecision at \\(\\alpha = 0.05\\)\nFail to reject \\(H_0\\). It is possible that the true difference in mean homework hours is 0. In context, this means there may be no real difference in homework time between male and female students.\nGeneralization\nSample only from honors students in a limited number of schools → generalization is limited.\nCausality\nObservational data; cannot infer causality.\n\n\n\n\n\n\n\n\n\n\n\nScenario 4\n\n\n\n\n\nA company is comparing two training programs to see if one produces higher certification pass rates.\n- Program A: 60 employees, 48 passed.\n- Program B: 55 employees, 38 passed.\nSupervisors assigned employees to programs based on schedules; Program A employees had more prior experience.\n\nResearch Question\nDo pass rates differ between Program A and Program B?\nVariables & Test\nCategorical response (pass/fail), categorical explanatory (program). Two-sample \\(z\\)-test for proportions.\nResponse & Explanatory\nResponse = pass/fail.\nExplanatory = training program (A vs B).\nConfounding Variable\nPrior experience with the certification material.\nParameter of Interest\n\\(p_1 - p_2 =\\) difference in true pass rates (Program A \\(-\\) Program B).\nHypotheses\n\\(H_0: p_1 - p_2 = 0\\)\n\\(H_a: p_1 - p_2 \\ne 0\\)\nSummary Statistics\n\\(n_1 = 60,\\; x_1 = 48,\\; \\hat p_1 = 0.80\\)\n\\(n_2 = 55,\\; x_2 = 38,\\; \\hat p_2 \\approx 0.6909\\)\nObserved difference: \\(\\hat p_1 - \\hat p_2 \\approx 0.1091\\).\nValidity Conditions\nEach group has at least 10 successes and 10 failures:\n\n\n\nProgram A: successes \\(=48\\), failures \\(=12\\)\n\nProgram B: successes \\(=38\\), failures \\(=17\\)\nConditions met; groups treated as independent.\n\n\nTest Statistic, p-value, and 95% CI\n\nFormulas\n- Pooled proportion (for the test): \\(\\hat p_{pool} = \\dfrac{x_1 + x_2}{n_1 + n_2}\\)\n- Test statistic:\n\\[ z = \\frac{(\\hat p_1 - \\hat p_2) - 0}{\\sqrt{\\hat p_{pool}(1-\\hat p_{pool})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\] - Two-sided p-value: \\(2\\,[1-\\Phi(|z|)]\\) - 95% CI (unpooled SE):\n\\[ (\\hat p_1 - \\hat p_2) \\;\\pm\\; z_{\\alpha/2}\\,\\sqrt{\\frac{\\hat p_1(1-\\hat p_1)}{n_1} + \\frac{\\hat p_2(1-\\hat p_2)}{n_2}} \\]\nR code (template)\n# Data\nn1 &lt;- 60; x1 &lt;- 48; p1.hat &lt;- x1 / n1\nn2 &lt;- 55; x2 &lt;- 38; p2.hat &lt;- x2 / n2\n\n# Pooled for H0\np.pool &lt;- (x1 + x2) / (n1 + n2)\nSE0 &lt;- sqrt(p.pool * (1 - p.pool) * (1/n1 + 1/n2))\n\n# Test statistic & two-sided p-value\nz &lt;- (p1.hat - p2.hat) / SE0\np.value &lt;- 2 * (1 - pnorm(abs(z)))\n\nz; p.value\n# 95% CI for (p1 - p2) using unpooled SE\nalpha &lt;- 0.05\nz.star &lt;- qnorm(1 - alpha/2)\nSE.hat &lt;- sqrt(p1.hat*(1 - p1.hat)/n1 + p2.hat*(1 - p2.hat)/n2)\nME &lt;- z.star * SE.hat\nlower &lt;- (p1.hat - p2.hat) - ME\nupper &lt;- (p1.hat - p2.hat) + ME\n\nc(lower, upper)\nNumerical results\n\\(z \\approx 1.35\\), two-sided \\(p \\approx 0.178\\).\n95% CI: I am 95% confident that the true difference in pass rates (Program A \\(-\\) Program B) is somewhere between \\(-0.050\\) and \\(0.268\\).\n\nDecision at \\(\\alpha = 0.05\\)\nFail to reject \\(H_0\\). It is possible that the true difference in pass rates is \\(0\\) (no difference between Program A and Program B). In context, the programs may perform similarly based on this sample.\nGeneralization\nThis is one company’s workforce; generalization beyond similar employees is limited.\nCausality\nAssignment was not randomized (schedules/prior experience differ), so we cannot make a causal claim that one program causes higher pass rates.\n\n\n\n\n\n\n\n\n\n\n\nScenario 5\n\n\n\n\n\nA researcher measures the resting heart rates of 25 participants before and after an 8-week aerobic training program.\nMean decrease (Before − After) \\(=\\;5.2\\) bpm, standard deviation of differences \\(s_d = 7.5\\) bpm. Some participants also started new diets.\n\nResearch Question\nDoes the training program reduce average resting heart rate?\nVariables & Test\nQuantitative paired differences (before/after on the same people). Paired t-test for a mean difference.\nResponse & Explanatory\nResponse = resting heart rate.\nExplanatory = time (before vs after program) on the same participants.\nConfounding Variable\nDiet changes begun during the program.\nParameter of Interest\n\\(\\mu_d =\\) true mean difference in resting heart rate (Before − After).\nHypotheses\n\\(H_0: \\mu_d = 0\\)\n\\(H_a: \\mu_d &gt; 0\\) (a positive mean difference indicates a decrease after training)\nSummary Statistics\n\\(n = 25,\\;\\; \\bar{d} = 5.2,\\;\\; s_d = 7.5\\)\nValidity Conditions\nPairs are matched by person; differences are the unit of analysis. With \\(n=25\\,(&gt;20)\\) the paired t-procedure is reasonable; we assume the distribution of differences is approximately normal and pairs are independent of each other.\nTest Statistic, p-value, and 95% CI\n\nFormulas\n- Test statistic:\n\\[ t = \\frac{\\bar{d} - 0}{s_d/\\sqrt{n}} \\] - Right-tailed p-value: \\(1 - T_{df}(t)\\) with \\(df = n - 1\\)\n- 95% CI for \\(\\mu_d\\):\n\\[ \\bar{d} \\;\\pm\\; t_{\\alpha/2,\\,df}\\;\\frac{s_d}{\\sqrt{n}}, \\quad df=n-1 \\]\nR code (template)\n# Given data\nn &lt;- 25\ndbar &lt;- 5.2\nsd_d &lt;- 7.5\nmu0 &lt;- 0\n\n# Test statistic & right-tailed p-value\nt_stat &lt;- (dbar - mu0) / (sd_d / sqrt(n))\ndf &lt;- n - 1\np_value &lt;- 1 - pt(t_stat, df)\n\nt_stat; p_value\n# 95% CI for mean difference\nalpha &lt;- 0.05\nt_star &lt;- qt(1 - alpha/2, df)\nME &lt;- t_star * sd_d / sqrt(n)\nlower &lt;- dbar - ME\nupper &lt;- dbar + ME\nc(lower, upper)  # I am 95% confident the true mean decrease is between lower and upper\nNumerical results\n\\(t \\approx 3.47\\), right-tailed \\(p \\approx 0.001\\).\n95% CI: I am 95% confident that the true mean decrease in resting heart rate is somewhere between \\(2.2\\) and \\(8.2\\) bpm.\n\nDecision at \\(\\alpha = 0.05\\)\nReject \\(H_0\\). We conclude it is possible that the true mean decrease in resting heart rate is greater than \\(0\\). In context, this suggests the training program may lower resting heart rates.\nGeneralization\nVolunteer sample from a single program; generalization to all adults is limited without broader sampling.\nCausality\nNo random assignment and concurrent diet changes; we cannot make a causal claim that the program alone caused the reduction.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 21"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-21.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-21.html#before-you-leave",
    "title": "Lesson 21: Statistical Investigation Lab",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 21"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html",
    "href": "MA206-AY26-1/lesson-2.html",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\nDon’t forget\n\n\n\n\n\n\n\n\n\n\nGenAI Assignment\n\n\n\n\n\n\n\n\n\n\n\nLet’s go to Canvas\n\n\n\n\n\n\nYour work will result in a presentation and a technical report.\nBoth these files are found on Canvas.\nUltimately, you are going to conduct a linear regression where you determine how much one variable impacted by other variables. For example:\n\n\n\n\n\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i = 1, 2, \\dots, n\\)\n\n\n\n\n\n\n\n\n\nBut there might be multiple things that can impact our dependent variable.\n\n\n\n\n\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i,\n\\quad i = 1, 2, \\dots, n\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis milestone sets up your binder and makes sure you have acceptable data.\nLets navigate to it on Canvas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis are just ideas on where to get data\n\nWe’ll talk about the Dataset Worksheet in a moment\n\n\n\n\n\n\nWhat is synthetic data?\n\n\n# A tibble: 10 × 6\n      id sport    gender height_cm training_hours run_time_min\n   &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1     1 Handball Female      171.            5.7         18.3\n 2     2 Track    Male        183.            7.9         14.5\n 3     3 Handball Female      152.            6.5         18.7\n 4     4 Lacrosse Male        187.            5.8         19.3\n 5     5 Lacrosse Male        179.            5.8         19.2\n 6     6 Lacrosse Female      170.            6.8         19.1\n 7     7 Track    Female      160.            6.4         17.2\n 8     8 Handball Male        172.            6.6         19.6\n 9     9 Track    Male        165.            8.7         14.6\n10    10 Handball Female      160.            3.9         19.3\n\n\n\n\n\n\n\nDid we satisfy this?\n\n\n\n\n\nHow about this?\n\n\n\n\n\nOkay, now this?\n\n\n\n\n\nAnd finally, this!\n\n\n\n\n\n\n\n\nThis is where you show me you’ve met the criteria.\n\n\n\n\n\n\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n     carat               cut        color        clarity          depth      \n Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065   Min.   :43.00  \n 1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258   1st Qu.:61.00  \n Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194   Median :61.80  \n Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171   Mean   :61.75  \n 3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066   3rd Qu.:62.50  \n Max.   :5.0100                     I: 5422   VVS1   : 3655   Max.   :79.00  \n                                    J: 2808   (Other): 2531                  \n     table           price             x                y         \n Min.   :43.00   Min.   :  326   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710   1st Qu.: 4.720  \n Median :57.00   Median : 2401   Median : 5.700   Median : 5.710  \n Mean   :57.46   Mean   : 3933   Mean   : 5.731   Mean   : 5.735  \n 3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540   3rd Qu.: 6.540  \n Max.   :95.00   Max.   :18823   Max.   :10.740   Max.   :58.900  \n                                                                  \n       z         \n Min.   : 0.000  \n 1st Qu.: 2.910  \n Median : 3.530  \n Mean   : 3.539  \n 3rd Qu.: 4.040  \n Max.   :31.800  \n                 \n\n\n\n\n\n\nI’m glad you asked!\n\n\n\n\n\n\n\nAnnex B Milestone 2 has you do the Tidyverse Tutorial on your own data\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 3\n\n\n\n\n\nProject Milestone 1: Due 22 Aug (Friday) All Sections\nGenAI Certification: Due 25 August (Monday) All Sections",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#welcome",
    "href": "MA206-AY26-1/lesson-2.html#welcome",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\nDon’t forget",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#genai-assignment-due-25-aug",
    "href": "MA206-AY26-1/lesson-2.html#genai-assignment-due-25-aug",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "GenAI Assignment",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#project-the-long-view",
    "href": "MA206-AY26-1/lesson-2.html#project-the-long-view",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "Let’s go to Canvas\n\n\n\n\n\n\nYour work will result in a presentation and a technical report.\nBoth these files are found on Canvas.\nUltimately, you are going to conduct a linear regression where you determine how much one variable impacted by other variables. For example:\n\n\n\n\n\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i = 1, 2, \\dots, n\\)\n\n\n\n\n\n\n\n\n\nBut there might be multiple things that can impact our dependent variable.\n\n\n\n\n\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i,\n\\quad i = 1, 2, \\dots, n\\)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#project-milestone-1",
    "href": "MA206-AY26-1/lesson-2.html#project-milestone-1",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "This milestone sets up your binder and makes sure you have acceptable data.\nLets navigate to it on Canvas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis are just ideas on where to get data\n\nWe’ll talk about the Dataset Worksheet in a moment\n\n\n\n\n\n\nWhat is synthetic data?\n\n\n# A tibble: 10 × 6\n      id sport    gender height_cm training_hours run_time_min\n   &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1     1 Handball Female      171.            5.7         18.3\n 2     2 Track    Male        183.            7.9         14.5\n 3     3 Handball Female      152.            6.5         18.7\n 4     4 Lacrosse Male        187.            5.8         19.3\n 5     5 Lacrosse Male        179.            5.8         19.2\n 6     6 Lacrosse Female      170.            6.8         19.1\n 7     7 Track    Female      160.            6.4         17.2\n 8     8 Handball Male        172.            6.6         19.6\n 9     9 Track    Male        165.            8.7         14.6\n10    10 Handball Female      160.            3.9         19.3\n\n\n\n\n\n\n\nDid we satisfy this?\n\n\n\n\n\nHow about this?\n\n\n\n\n\nOkay, now this?\n\n\n\n\n\nAnd finally, this!\n\n\n\n\n\n\n\n\nThis is where you show me you’ve met the criteria.\n\n\n\n\n\n\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n     carat               cut        color        clarity          depth      \n Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065   Min.   :43.00  \n 1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258   1st Qu.:61.00  \n Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194   Median :61.80  \n Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171   Mean   :61.75  \n 3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066   3rd Qu.:62.50  \n Max.   :5.0100                     I: 5422   VVS1   : 3655   Max.   :79.00  \n                                    J: 2808   (Other): 2531                  \n     table           price             x                y         \n Min.   :43.00   Min.   :  326   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710   1st Qu.: 4.720  \n Median :57.00   Median : 2401   Median : 5.700   Median : 5.710  \n Mean   :57.46   Mean   : 3933   Mean   : 5.731   Mean   : 5.735  \n 3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540   3rd Qu.: 6.540  \n Max.   :95.00   Max.   :18823   Max.   :10.740   Max.   :58.900  \n                                                                  \n       z         \n Min.   : 0.000  \n 1st Qu.: 2.910  \n Median : 3.530  \n Mean   : 3.539  \n 3rd Qu.: 4.040  \n Max.   :31.800",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#so-where-can-i-get-data",
    "href": "MA206-AY26-1/lesson-2.html#so-where-can-i-get-data",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "I’m glad you asked!",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-2.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-2.html#before-you-leave",
    "title": "Lesson 2: Project Dataset Exploration",
    "section": "",
    "text": "Annex B Milestone 2 has you do the Tidyverse Tutorial on your own data\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 3\n\n\n\n\n\nProject Milestone 1: Due 22 Aug (Friday) All Sections\nGenAI Certification: Due 25 August (Monday) All Sections",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-18.html",
    "href": "MA206-AY26-1/lesson-18.html",
    "title": "Lesson 18: Two Proportion Z-Test",
    "section": "",
    "text": "My Math Instructors\n\nMA103: LTC Bea Lambert\n\nMA104: MAJ Richard Gussenhoven\n\nMA205: MAJ Jesse Easter\n\nMA206: LTC Scott Billie\n\nLTC (R) Jesse Easter\n\nArmor Officer\n\nFA49 ORSA\n\nWhite Sands Missile Range\n\nAfghanistan (~2013)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue Today!\n\n\n\n\n\nLesson 17 Lesson 18\n25 Points!\nLike a WPR What does that mean?!!\nRead ahead\n\n\n\n\n\n⏰ Due 0700 on Lesson 18\n\nLets take a look at it\n\nDay 1: Tuesday, 14 Oct 2025\nDay 2: Wednesday, 15 Oct 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10-10-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\pi = \\pi_0\\)\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nValidity Conditions\n\nNumber of successes and failures must be greater than 10.\n\nConfidence Interval for \\(\\pi\\) (one proportion)\n\\[\n\\hat{p} \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true population proportion \\(\\pi\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu = \\mu_0\\)\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu &lt; \\mu_0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) (degrees of freedom)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nValidity Conditions\n\nSample size must be greater than 30.\n\nConfidence Interval for \\(\\mu\\) (one mean)\n\\[\n\\bar{x} \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\\frac{s}{\\sqrt{n}},\n\\qquad df = n-1\n\\] I am \\((1 - \\alpha)\\%\\) confident that the true population mean \\((\\mu)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\nStrength of evidence: Smaller \\(p\\) means stronger evidence against \\(H_0\\).\n\n\n\n\n\nGeneralization: We can generalize results to a larger population if the data come from a random and representative sample of that population.\n\nCausation: We can claim causation if participants are randomly assigned to treatments in an experiment. Without random assignment, we can only conclude association, not causation.\n\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n& \\text{Randomly Sampled} & \\text{Not Randomly Sampled} \\\\\n\\hline\n\\textbf{Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: Yes}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: Yes}\n\\end{array} \\\\\n\\hline\n\\textbf{Not Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: No}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: No}\n\\end{array} \\\\\n\\hline\n\\end{array}\n\\]\n\nParameters vs. Statistics: A parameter is a fixed (but usually unknown) numerical value describing a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\pi\\)). A statistic is a numerical value computed from a sample (e.g., \\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\)).\n\nParameters = target (what we want to know).\n\nStatistics = evidence (what we can actually measure).\n\nWe use statistics to estimate parameters, and because different samples give different statistics, we capture this variability with confidence intervals.\n\n\n\n\n\nQuantity\nPopulation (Parameter)\nSample (Statistic)\n\n\n\n\nCenter (mean)\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nSpread (SD)\n\\(\\sigma\\)\n\\(s\\)\n\n\nProportion “success”\n\\(\\pi\\)\n\\(\\hat{p}\\)\n\n\n\n\n\n\n\nIn a post-WPR survey in MA206, 81 out of 125 students who failed the WPR did the recommended review, while 383 out of 525 students who passed did the recommended review problem.\nQuestion: Is there a higher proportion of cadets who passed the test given they did the review?\nIs \\(\\frac{383}{525} \\;&gt;\\; \\frac{81}{125}\\)?\n\n\n\nNull:\n\\[H_0:\\ \\pi_{\\text{pass-review}} - \\pi_{\\text{fail-review}} = 0\\]\nAlternative (one-sided):\n\\[H_A:\\ \\pi_{\\text{pass-review}} - \\pi_{\\text{fail-review}} &gt; 0\\]\n\n\n\n\n\\[\nz \\;=\\; \\frac{(\\hat p_1-\\hat p_2)-(\\pi_1-\\pi_2)}{\\sqrt{\\hat p(1-\\hat p)\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)}}.\n\\]\n\\(\\hat p_1=\\dfrac{383}{525}\\) (passed group)\n\\(\\hat p_2=\\dfrac{81}{125}\\) (failed group),\n\\(\\hat p=\\dfrac{383+81}{524+125}\\) (pooled)\n\n\n\n\\(\\hat p_1 = \\dfrac{383}{525} = 0.7295\\)\n\\(\\hat p_2 = \\dfrac{81}{125} = 0.6560\\)\n\\(\\hat p   = \\dfrac{383+81}{525+125} = \\dfrac{465}{650} = 0.7154\\)\n\\(SE = \\sqrt{0.7154 \\cdot (1-0.7154)\\left(\\tfrac{1}{524}+\\tfrac{1}{125}\\right)} = 0.04498\\)\n\\(z = \\dfrac{0.7295 - 0.6560}{0.04491} = 1.812\\)\n\\(p\\_{\\text{one-sided}} = 1 - \\Phi(1.637) \\approx 0.0349\\)\n\np1_hat &lt;- 383/525\np2_hat &lt;- 81/125\n\nn1 &lt;- 125\nn2 &lt;- 525\n\np_pool &lt;- (383+81)/(525+125)\n(z_stat &lt;- (p1_hat - p2_hat) / (sqrt(p_pool*(1-p_pool)*(1/n1 + 1/n2))))\n\n[1] 1.812426\n\n(p_val  &lt;- 1 - pnorm(z_stat))\n\n[1] 0.03496018\n\n\n\n\n\n\nFor a 95% CI, use the unpooled standard error:\n\\[\n(\\hat p_1-\\hat p_2) \\;\\pm\\; z_{0.975}\\,\n\\sqrt{\\frac{\\hat p_1(1-\\hat p_1)}{n_1} + \\frac{\\hat p_2(1-\\hat p_2)}{n_2}}.\n\\]\n\\(\\hat p_1 - \\hat p_2 = 0.7295 - 0.6480 = 0.0815\\)\n\\(SE_{\\text{unpooled}} = \\sqrt{\\dfrac{0.7295(1-0.7295)}{525} + \\dfrac{0.6480(1-0.6480)}{125}} = 0.0469\\)\n\\(z_{0.975} = 1.96\\) qnorm(.975, 0, 1)\n\\(\\text{Margin of Error} = 1.96 \\times 0.0469 = 0.0919\\)\n\\(95\\%\\ \\text{CI} = 0.0815 \\;\\pm\\; 0.0919 = (-0.0104,\\; 0.1735)\\)\n\nz_star &lt;- qnorm(.975)\n\nSE_un  &lt;- sqrt(p1_hat*(1 - p1_hat)/n1 + p2_hat*(1 - p2_hat)/n2)\ndiff_hat &lt;- p1_hat - p2_hat\n\n(ci &lt;- c(diff_hat - z_star*SE_un, diff_hat + z_star*SE_un))\n\n[1] -0.006413271  0.169460890\n\n\n\n\n\nValidity: independence of groups; each group has at least 10 successes/failures (here: 383/142 and 81/43) ✔️\n\nScope:\n\nGeneralization requires random sampling from the cadet population.\nSo what? Since our data come from a course survey (not a random sample), we cannot generalize these results to the entire Corps of Cadets.\n\nCausation requires random assignment to “did review” vs “did not review.”\nSo what? Because students chose for themselves whether to do the review, we cannot conclude that doing the review causes higher pass rates — only that the two are associated in this sample.\n\n\n\n\n\n\nScenario:\nIs there a difference between the proportion of cadets who are left-handed and the proportion who are left-footed?\nStep 1: Collect Data\n- Quick poll by show of hands:\n- \\(n\\) = total number of cadets (everyone answers both questions).\n- \\(x_1\\) = number of cadets who identify as left-handed.\n- \\(x_2\\) = number of cadets who identify as left-footed (e.g., which foot they’d use to kick a ball).\nStep 2: State Hypotheses\n- \\(H_0:\\ \\pi_{\\text{left-hand}} - \\pi_{\\text{left-foot}} = 0\\)\n- \\(H_A:\\ \\pi_{\\text{left-hand}} - \\pi_{\\text{left-foot}} \\neq 0\\)\nStep 3: Compute Test Statistic\n- \\(\\hat p_1 = x_1/n\\), \\(\\hat p_2 = x_2/n\\).\n- (Here the same students are polled for both traits, so strictly speaking the samples are not independent — this is a good teaching moment about the independence assumption.)\n- Still, you can proceed with the two-proportion \\(z\\)-test as if independent to practice mechanics.\nStep 4: Decision\n- Compute \\(z = \\dfrac{(\\hat p_1 - \\hat p_2) - 0}{\\sqrt{\\hat p(1-\\hat p)(\\tfrac{1}{n}+\\tfrac{1}{n})}}\\),\nget the \\(p\\)-value, and compare to \\(\\alpha = 0.05\\).\nStep 5: Interpret & Discuss\n- Do we have evidence that the proportion of left-handers differs from the proportion of left-footers in the class?\n- So what?\n- Not a random sample \\(\\;\\rightarrow\\;\\) cannot generalize to the broader cadet population.\n- Not random assignment \\(\\;\\rightarrow\\;\\) cannot claim causation (we can’t say being left-handed causes left-footedness).\n- But: this gives practice with a two-proportion \\(z\\)-test and sparks discussion about assumptions (independence).\n\n\nResearchers at West Point wanted to know:\nDoes the new study technique increase performance?\n\nThey randomly sampled 200 cadets from across the Corps.\n\nThey then randomly assigned half (100 cadets) to use the new study technique, while the other half (100 cadets) used the standard approach.\n\nAfter a week, all cadets took the same quiz.\n\nResults:\n\nNew technique group: \\(n_1 = 100\\), \\(x_1 = 78\\) passed the quiz.\n\nStandard group: \\(n_2 = 100\\), \\(x_2 = 65\\) passed the quiz.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStep 1: Hypotheses\n- \\(H_0:\\ \\pi_{\\text{new}} - \\pi_{\\text{standard}} = 0\\)\n- \\(H_A:\\ \\pi_{\\text{new}} - \\pi_{\\text{standard}} &gt; 0\\)\nStep 2: Sample Proportions\n- \\(\\hat p_1 = 78/100 = 0.78\\)\n- \\(\\hat p_2 = 65/100 = 0.65\\)\n- Difference: \\(\\hat p_1 - \\hat p_2 = 0.13\\)\nStep 3: Test Statistic\n- Pooled \\(\\hat p = (78+65)/200 = 0.715\\)\n- \\(SE = \\sqrt{0.715(1-0.715)\\left(\\tfrac{1}{100}+\\tfrac{1}{100}\\right)} \\approx 0.0639\\)\n- \\(z = 0.13 / 0.0639 \\approx 2.04\\)\n- 1 - pnorm(z_stat) \\(\\space p\\text{-value} \\approx 0.021\\)\n\n\nStep 4: Interpretation\n- Since \\(p \\approx 0.021 &lt; 0.05\\), we reject \\(H_0\\).\n- There is statistically significant evidence that the new study technique increases quiz performance.\n- The true improvement is estimated to be between 0.4% and 25.6% higher pass rates.\nScope of Inference:\n- Generalization: Random sampling → we can generalize to the Corps of Cadets.\n- Causation: Random assignment → we can conclude the study technique causes higher performance.\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 18"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-18.html#special-guest",
    "href": "MA206-AY26-1/lesson-18.html#special-guest",
    "title": "Lesson 18: Two Proportion Z-Test",
    "section": "",
    "text": "My Math Instructors\n\nMA103: LTC Bea Lambert\n\nMA104: MAJ Richard Gussenhoven\n\nMA205: MAJ Jesse Easter\n\nMA206: LTC Scott Billie\n\nLTC (R) Jesse Easter\n\nArmor Officer\n\nFA49 ORSA\n\nWhite Sands Missile Range\n\nAfghanistan (~2013)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 18"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-18.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-18.html#lesson-administration",
    "title": "Lesson 18: Two Proportion Z-Test",
    "section": "",
    "text": "Due Today!\n\n\n\n\n\nLesson 17 Lesson 18\n25 Points!\nLike a WPR What does that mean?!!\nRead ahead\n\n\n\n\n\n⏰ Due 0700 on Lesson 18\n\nLets take a look at it\n\nDay 1: Tuesday, 14 Oct 2025\nDay 2: Wednesday, 15 Oct 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10-10-10",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 18"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-18.html#running-review",
    "href": "MA206-AY26-1/lesson-18.html#running-review",
    "title": "Lesson 18: Two Proportion Z-Test",
    "section": "",
    "text": "For all cases:\n\\(H_0:\\ \\pi = \\pi_0\\)\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nValidity Conditions\n\nNumber of successes and failures must be greater than 10.\n\nConfidence Interval for \\(\\pi\\) (one proportion)\n\\[\n\\hat{p} \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true population proportion \\(\\pi\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu = \\mu_0\\)\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu &lt; \\mu_0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) (degrees of freedom)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nValidity Conditions\n\nSample size must be greater than 30.\n\nConfidence Interval for \\(\\mu\\) (one mean)\n\\[\n\\bar{x} \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\\frac{s}{\\sqrt{n}},\n\\qquad df = n-1\n\\] I am \\((1 - \\alpha)\\%\\) confident that the true population mean \\((\\mu)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\nStrength of evidence: Smaller \\(p\\) means stronger evidence against \\(H_0\\).\n\n\n\n\n\nGeneralization: We can generalize results to a larger population if the data come from a random and representative sample of that population.\n\nCausation: We can claim causation if participants are randomly assigned to treatments in an experiment. Without random assignment, we can only conclude association, not causation.\n\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n& \\text{Randomly Sampled} & \\text{Not Randomly Sampled} \\\\\n\\hline\n\\textbf{Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: Yes}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: Yes}\n\\end{array} \\\\\n\\hline\n\\textbf{Not Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: No}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: No}\n\\end{array} \\\\\n\\hline\n\\end{array}\n\\]\n\nParameters vs. Statistics: A parameter is a fixed (but usually unknown) numerical value describing a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\pi\\)). A statistic is a numerical value computed from a sample (e.g., \\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\)).\n\nParameters = target (what we want to know).\n\nStatistics = evidence (what we can actually measure).\n\nWe use statistics to estimate parameters, and because different samples give different statistics, we capture this variability with confidence intervals.\n\n\n\n\n\nQuantity\nPopulation (Parameter)\nSample (Statistic)\n\n\n\n\nCenter (mean)\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nSpread (SD)\n\\(\\sigma\\)\n\\(s\\)\n\n\nProportion “success”\n\\(\\pi\\)\n\\(\\hat{p}\\)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 18"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-18.html#two-proportion-z-test",
    "href": "MA206-AY26-1/lesson-18.html#two-proportion-z-test",
    "title": "Lesson 18: Two Proportion Z-Test",
    "section": "",
    "text": "In a post-WPR survey in MA206, 81 out of 125 students who failed the WPR did the recommended review, while 383 out of 525 students who passed did the recommended review problem.\nQuestion: Is there a higher proportion of cadets who passed the test given they did the review?\nIs \\(\\frac{383}{525} \\;&gt;\\; \\frac{81}{125}\\)?\n\n\n\nNull:\n\\[H_0:\\ \\pi_{\\text{pass-review}} - \\pi_{\\text{fail-review}} = 0\\]\nAlternative (one-sided):\n\\[H_A:\\ \\pi_{\\text{pass-review}} - \\pi_{\\text{fail-review}} &gt; 0\\]\n\n\n\n\n\\[\nz \\;=\\; \\frac{(\\hat p_1-\\hat p_2)-(\\pi_1-\\pi_2)}{\\sqrt{\\hat p(1-\\hat p)\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)}}.\n\\]\n\\(\\hat p_1=\\dfrac{383}{525}\\) (passed group)\n\\(\\hat p_2=\\dfrac{81}{125}\\) (failed group),\n\\(\\hat p=\\dfrac{383+81}{524+125}\\) (pooled)\n\n\n\n\\(\\hat p_1 = \\dfrac{383}{525} = 0.7295\\)\n\\(\\hat p_2 = \\dfrac{81}{125} = 0.6560\\)\n\\(\\hat p   = \\dfrac{383+81}{525+125} = \\dfrac{465}{650} = 0.7154\\)\n\\(SE = \\sqrt{0.7154 \\cdot (1-0.7154)\\left(\\tfrac{1}{524}+\\tfrac{1}{125}\\right)} = 0.04498\\)\n\\(z = \\dfrac{0.7295 - 0.6560}{0.04491} = 1.812\\)\n\\(p\\_{\\text{one-sided}} = 1 - \\Phi(1.637) \\approx 0.0349\\)\n\np1_hat &lt;- 383/525\np2_hat &lt;- 81/125\n\nn1 &lt;- 125\nn2 &lt;- 525\n\np_pool &lt;- (383+81)/(525+125)\n(z_stat &lt;- (p1_hat - p2_hat) / (sqrt(p_pool*(1-p_pool)*(1/n1 + 1/n2))))\n\n[1] 1.812426\n\n(p_val  &lt;- 1 - pnorm(z_stat))\n\n[1] 0.03496018",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 18"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-18.html#confidence-interval-for-two-proportions",
    "href": "MA206-AY26-1/lesson-18.html#confidence-interval-for-two-proportions",
    "title": "Lesson 18: Two Proportion Z-Test",
    "section": "",
    "text": "For a 95% CI, use the unpooled standard error:\n\\[\n(\\hat p_1-\\hat p_2) \\;\\pm\\; z_{0.975}\\,\n\\sqrt{\\frac{\\hat p_1(1-\\hat p_1)}{n_1} + \\frac{\\hat p_2(1-\\hat p_2)}{n_2}}.\n\\]\n\\(\\hat p_1 - \\hat p_2 = 0.7295 - 0.6480 = 0.0815\\)\n\\(SE_{\\text{unpooled}} = \\sqrt{\\dfrac{0.7295(1-0.7295)}{525} + \\dfrac{0.6480(1-0.6480)}{125}} = 0.0469\\)\n\\(z_{0.975} = 1.96\\) qnorm(.975, 0, 1)\n\\(\\text{Margin of Error} = 1.96 \\times 0.0469 = 0.0919\\)\n\\(95\\%\\ \\text{CI} = 0.0815 \\;\\pm\\; 0.0919 = (-0.0104,\\; 0.1735)\\)\n\nz_star &lt;- qnorm(.975)\n\nSE_un  &lt;- sqrt(p1_hat*(1 - p1_hat)/n1 + p2_hat*(1 - p2_hat)/n2)\ndiff_hat &lt;- p1_hat - p2_hat\n\n(ci &lt;- c(diff_hat - z_star*SE_un, diff_hat + z_star*SE_un))\n\n[1] -0.006413271  0.169460890\n\n\n\n\n\nValidity: independence of groups; each group has at least 10 successes/failures (here: 383/142 and 81/43) ✔️\n\nScope:\n\nGeneralization requires random sampling from the cadet population.\nSo what? Since our data come from a course survey (not a random sample), we cannot generalize these results to the entire Corps of Cadets.\n\nCausation requires random assignment to “did review” vs “did not review.”\nSo what? Because students chose for themselves whether to do the review, we cannot conclude that doing the review causes higher pass rates — only that the two are associated in this sample.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 18"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-18.html#in-class-activity-two-proportion-z-test",
    "href": "MA206-AY26-1/lesson-18.html#in-class-activity-two-proportion-z-test",
    "title": "Lesson 18: Two Proportion Z-Test",
    "section": "",
    "text": "Scenario:\nIs there a difference between the proportion of cadets who are left-handed and the proportion who are left-footed?\nStep 1: Collect Data\n- Quick poll by show of hands:\n- \\(n\\) = total number of cadets (everyone answers both questions).\n- \\(x_1\\) = number of cadets who identify as left-handed.\n- \\(x_2\\) = number of cadets who identify as left-footed (e.g., which foot they’d use to kick a ball).\nStep 2: State Hypotheses\n- \\(H_0:\\ \\pi_{\\text{left-hand}} - \\pi_{\\text{left-foot}} = 0\\)\n- \\(H_A:\\ \\pi_{\\text{left-hand}} - \\pi_{\\text{left-foot}} \\neq 0\\)\nStep 3: Compute Test Statistic\n- \\(\\hat p_1 = x_1/n\\), \\(\\hat p_2 = x_2/n\\).\n- (Here the same students are polled for both traits, so strictly speaking the samples are not independent — this is a good teaching moment about the independence assumption.)\n- Still, you can proceed with the two-proportion \\(z\\)-test as if independent to practice mechanics.\nStep 4: Decision\n- Compute \\(z = \\dfrac{(\\hat p_1 - \\hat p_2) - 0}{\\sqrt{\\hat p(1-\\hat p)(\\tfrac{1}{n}+\\tfrac{1}{n})}}\\),\nget the \\(p\\)-value, and compare to \\(\\alpha = 0.05\\).\nStep 5: Interpret & Discuss\n- Do we have evidence that the proportion of left-handers differs from the proportion of left-footers in the class?\n- So what?\n- Not a random sample \\(\\;\\rightarrow\\;\\) cannot generalize to the broader cadet population.\n- Not random assignment \\(\\;\\rightarrow\\;\\) cannot claim causation (we can’t say being left-handed causes left-footedness).\n- But: this gives practice with a two-proportion \\(z\\)-test and sparks discussion about assumptions (independence).\n\n\nResearchers at West Point wanted to know:\nDoes the new study technique increase performance?\n\nThey randomly sampled 200 cadets from across the Corps.\n\nThey then randomly assigned half (100 cadets) to use the new study technique, while the other half (100 cadets) used the standard approach.\n\nAfter a week, all cadets took the same quiz.\n\nResults:\n\nNew technique group: \\(n_1 = 100\\), \\(x_1 = 78\\) passed the quiz.\n\nStandard group: \\(n_2 = 100\\), \\(x_2 = 65\\) passed the quiz.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStep 1: Hypotheses\n- \\(H_0:\\ \\pi_{\\text{new}} - \\pi_{\\text{standard}} = 0\\)\n- \\(H_A:\\ \\pi_{\\text{new}} - \\pi_{\\text{standard}} &gt; 0\\)\nStep 2: Sample Proportions\n- \\(\\hat p_1 = 78/100 = 0.78\\)\n- \\(\\hat p_2 = 65/100 = 0.65\\)\n- Difference: \\(\\hat p_1 - \\hat p_2 = 0.13\\)\nStep 3: Test Statistic\n- Pooled \\(\\hat p = (78+65)/200 = 0.715\\)\n- \\(SE = \\sqrt{0.715(1-0.715)\\left(\\tfrac{1}{100}+\\tfrac{1}{100}\\right)} \\approx 0.0639\\)\n- \\(z = 0.13 / 0.0639 \\approx 2.04\\)\n- 1 - pnorm(z_stat) \\(\\space p\\text{-value} \\approx 0.021\\)\n\n\nStep 4: Interpretation\n- Since \\(p \\approx 0.021 &lt; 0.05\\), we reject \\(H_0\\).\n- There is statistically significant evidence that the new study technique increases quiz performance.\n- The true improvement is estimated to be between 0.4% and 25.6% higher pass rates.\nScope of Inference:\n- Generalization: Random sampling → we can generalize to the Corps of Cadets.\n- Causation: Random assignment → we can conclude the study technique causes higher performance.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 18"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-18.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-18.html#before-you-leave",
    "title": "Lesson 18: Two Proportion Z-Test",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 18"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-16.html",
    "href": "MA206-AY26-1/lesson-16.html",
    "title": "Lesson 16: Causation",
    "section": "",
    "text": "Lesson 17\nMilestone 4\n\nWith partner\nWrite 1-2 paragraphs per article summarizing the articles topic with a take away for its insight on your project.\nMake updates from Milestone 3 feedback.\nFill out Annex B for my comments on Milestone 3.\nTurn in EVERYTHING in your working write up.\nKeep your binder up-to-date, but I don’t want to see it.\n\n\n\n\n\n\nLesson 17 Lesson 18\n25 Points!\nLike a WPR What does that mean?!!\nRead ahead\n\n\n\n\n\n⏰ Due 0700 on Lesson 18\n\nLets take a look at it\n\nDay 1: Tuesday, 14 Oct 2025\nDay 2: Wednesday, 15 Oct 2025\n\n\n\n\n\n\n\n\nMath 1\n\n\n\n\n\n\nPreviously 4-0\n\n\n\n\n\n\n5-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\pi = \\pi_0\\)\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nValidity Conditions\n\nNumber of successes and failures must be greater than 10.\n\nConfidence Interval for \\(\\pi\\) (one proportion)\n\\[\n\\hat{p} \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}\n\\]\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu = \\mu_0\\)\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu &lt; \\mu_0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) (degrees of freedom)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nConfidence Interval for \\(\\mu\\) (one mean)\n\\[\n\\bar{x} \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\\frac{s}{\\sqrt{n}},\n\\qquad df = n-1\n\\]\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\nStrength of evidence: Smaller \\(p\\) means stronger evidence against \\(H_0\\).\n\n\n\n\n\nGeneralization: We can generalize results to a larger population if the sample is random and representative of that population.\n\nCausation: We can claim causation only if the study design is a randomized experiment. Observational studies can show associations, but not cause-and-effect.\nParameters vs. Statistics: A parameter is a fixed (but usually unknown) numerical value describing a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\pi\\)). A statistic is a numerical value computed from a sample (e.g., \\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\)).\n\nParameters = target (what we want to know).\n\nStatistics = evidence (what we can actually measure).\n\nWe use statistics to estimate parameters, and because different samples give different statistics, we capture this variability with confidence intervals.\n\n\n\n\n\nQuantity\nPopulation (Parameter)\nSample (Statistic)\n\n\n\n\nCenter (mean)\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nSpread (SD)\n\\(\\sigma\\)\n\\(s\\)\n\n\nProportion “success”\n\\(\\pi\\)\n\\(\\hat{p}\\)\n\n\n\n\n\n\n\nScenario\nJust like handedness where people prefer to use one hand over another, eye dominance, sometimes called eyedness, is the tendency to prefer to see using one eye over the other. Interestingly though the side of the dominant eye does not always match that of the dominant hand. Let’s investigate whether people are equally likely to have left-eye or right-eye dominance by collecting some data from you and comparing to data from other classes.\n\n\n\nWhat are the observational units?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCadets (or people).\n\n\n\n\nWhat is the variable that is recorded?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nEye dominance (left or right).\n\n\n\n\nDescribe the parameter of interest in words.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nπ = The proportion of people who are right eye dominant.\n\n\n\n\nIf right eye and left eye dominance are equally prevalent, what value of π?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nπ = 0.5.\nNull hypothesis: H₀: π = 0.5.\n\n\n\n\nIf people are more likely to be right eye dominant?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nπ &gt; 0.5.\nAlternative hypothesis: Hₐ: π &gt; 0.5.\n\n\n\n\n\n\nIn a study of a cadet company 71 out of 112 cadets sampled were right eye dominant.\nCalculate the sample proportion who are right eye dominant.\nSample size\n\n\n\n\n\n\nSolution\n\n\n\n\n\n112\n\n\n\n\n\n\nSample proportion\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\frac{71}{112} = 0.6339\\)\n\n\n\n\n\n\nSimulation based\n\n\n\nAccording to this convention, is the sample size large enough in this study to use the normal approximation and theory-based inference? Justify your answer.\nCheck validity condition for theory-based inference.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes. 71 successes and 41 failures, both ≥ 10.\n\nValidity Conditions The normal approximation can be thought of as a prediction of what would occur if a simulation-based analysis was carried out. Many times this prediction is valid, but not always. It is only valid when the condition (at least 10 successes and at least 10 failures) is met.\n\n\n\n\n\n\n\nUse the formula to determine the theoretical standard deviation of the sample proportion.\nTheoretical SD using formula.\n\\(sd = \\sqrt{\\frac{\\pi(1−\\pi)}{n}}\\)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(sd = \\frac{0.5(1−0.5)}{112} = .0472\\)\n\n\n\n\n\n\nUse the predicted value of the standard deviation from #8 to calculate the standardized statistic (z).\n\\(z = \\frac{\\hat{p} - \\pi_0}{sd}\\)\nCalculate z by hand.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nz = \\(\\frac{(0.6339 − 0.5)}{0.0472} = 2.835\\)\n\n\n\n\n\n\nTheoretical p-value.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(p \\;=\\; P(Z \\ge 2.835) \\;=\\; \\displaystyle \\int_{2.835}^{\\infty} \\phi(z)\\,dz \\;=\\; 1 - \\Phi(2.835)\\)\n1 - pnorm(2.835)  # 0.0023\nTheoretical p ≈ 0.0023\n\n\n\n\n\n\nSimulation based\n\n\n\nUse the sample data to test the claim that π = 2/3 (two-sided).\n\nHypotheses.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nH₀: π = 2/3\nHₐ: π ≠ 2/3\n\n\n\n\nStandardized statistic.\n\nReminder:\n\\(\\hat{p} = 71/112 = 0.6339\\)\n\\(p_0 = 2/3\\)\n\\(n=112\\)\n\\[\nz \\;=\\; \\frac{\\hat p - \\pi_0}{\\sqrt{\\pi_0(1-\\pi_0)/n}}\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\; z =\n\\; \\frac{0.6339 - \\tfrac{2}{3}}{\\sqrt{\\tfrac{2}{3}\\cdot \\tfrac{1}{3} / 112}}\n\\;\\approx\\; -0.736\n\\]\nStandardized statistic: \\(z \\approx -0.736\\)\n\n\n\n\nP-value.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTwo-sided: \\[\np \\;=\\; 2\\,P\\!\\left(Z \\ge |z|\\right)\n\\;=\\; 2\\bigl(1-\\Phi(|{-0.736}|)\\bigr)\n\\;\\approx\\; 0.462\n\\]\n2 * (1 - pnorm(abs(z)))\n0.4619682\nTwo-sided p-value: \\(p \\approx 0.462\\)\n\n\n\n\nConclusion.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFail to reject H₀; the data are consistent with \\(\\pi = 2/3\\).\n\n\n\n\n\n\nIn a small class of 14 students, nine were right-eye dominant.\nWhy not use theory-based methods for n=14 class?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nToo small: 9 successes and 5 failures (&lt; 10 failures). See Validity Consitions\n\n\n\n\n\n\nUse the theory-based methods anyway (two-sided test for a small class with 9 right-eye dominant out of 14).\n\nHypotheses.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(H_0:\\ \\pi = 0.5\\)\n\\(H_a:\\ \\pi \\ne 0.5\\)\n\n\n\n\nStandardized statistic.\n\nReminder\n\\(\\hat{p} = 9/14 = 0.6429\\)\n\\(\\pi_0 = 0.5\\)\n\\(n = 14\\)\n\\[\nz \\;=\\; \\dfrac{\\hat p - \\pi_0}{\\sqrt{\\pi_0(1-\\pi_0)/n}}\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\; \\dfrac{0.6429 - 0.5}{\\sqrt{0.5(1-0.5)/14}}\n\\;\\approx\\; 1.069\n\\]\nStandardized statistic: \\(z \\approx 1.069\\)\n\n\n\n\nP-value.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTwo-sided:\n\\(p = 2\\,P(Z \\ge |z|) = 2\\bigl(1-\\Phi(|1.069|)\\bigr) \\approx 0.285\\)\n2 * (1 - pnorm(abs(z)))\nTwo-sided p-value: \\(p \\approx 0.285\\)\n\n\n\n\nConclusion.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAt typical significance levels, fail to reject \\(H_0\\). With \\(n=14\\), the theory-based result is \\(p\\approx 0.285\\); note the normal approximation can be poor for small samples.\n\n\n\n\n\n\n\n\nCoffee Drinking and Cancer\nEarly studies suggested coffee drinkers had higher cancer rates.\n\nConfounder: Smoking — coffee drinkers were also more likely to smoke, and smoking causes cancer.\n\nHealth Supplements and Longevity\nSupplement users often appeared healthier and lived longer in studies.\n\nConfounder: Health-conscious lifestyle — people taking supplements also tended to eat better, exercise, and get regular medical care.\n\nPolice Officers and Crime Rates\nMore police officers in an area were linked to higher crime counts.\n\nConfounder: City size — larger cities naturally have both more crime and more police.\n\n\n\n\nRandom sampling + Random assignment (cause + generalize)\n\nRandomly sample 200 West Point cadets from all classes.\nRandomly assign half to a 6-week push-up volume plan (daily small sets) and half to a strength plan (fewer, heavier weighted push-ups).\nOutcome: change in max push-ups in 2 minutes.\nInference: Can claim causation and generalize to cadets.\n\nRandom sampling + No random assignment (generalize, not cause)\n\nRandomly sample 200 cadets; classify them by the push-up routine they already follow (volume vs strength), no reassignment.\nCompare change in 2-minute max after 6 weeks.\nConfounding: motivation, prior fitness, schedule differences.\nInference: Can generalize to cadets who choose each routine, but not claim causation.\n\nNot random sampling + Random assignment (cause, not generalize)\n\nConvenience sample: one company (≈200 cadets) volunteers.\nWithin that company, randomly assign cadets to volume vs strength plans.\nOutcome: change in 2-minute max.\nInference: Can claim causation for this company, but limited generalizability.\n\nNot random sampling + No random assignment (confounding + bias)\n\nConvenience groups: compare two available companies that already use different push-up plans (volume vs strength).\nNo random sampling and no reassignment.\nConfounding: cadre emphasis, additional PT, culture, sleep.\nInference: Neither causation nor broad generalization is supported.\n\n\n\n\n\n\n\nWe want to investigate the elbow-to-forearm length of cadets.\nA commonly cited average for adults is 26 inches.\nResearch Question 1: Do West Point cadets have longer forearms than 26 inches on average?\n\nIdentify the parameter of interest.\n\nState the null and alternative hypotheses in symbolic form.\n\nUse the class data to carry out the test at \\(\\alpha = 0.05\\).\n\nResearch Question 2: Is the average elbow-to-forearm length of cadets different from 26 inches?\n\nIdentify the parameter of interest.\n\nState the null and alternative hypotheses in symbolic form.\n\nUse the class data to carry out the test at \\(\\alpha = 0.05\\).\n\nConstruct and interpret a \\(95\\%\\) confidence interval for the mean forearm length.\nFinally, compare your \\(95\\%\\) confidence interval with the results of the two-sided hypothesis test.\n- Do they agree?\n- Why or why not?\n\n\n\nIt turns out that people with forearms over 28 inches are considered long.\nWe want to investigate the proportion of cadets who have long forearms.\nSuppose 30% is considered the reference value for the population.\nResearch Question 1: Is the proportion of cadets with long forearms more than 30%?\n\nIdentify the parameter of interest.\n\nState the null and alternative hypotheses in symbolic form.\n\nUse the class data to carry out the test at \\(\\alpha = 0.10\\).\n\nResearch Question 2: Is the proportion of cadets with long forearms different from 30%?\n\nIdentify the parameter of interest.\n\nState the null and alternative hypotheses in symbolic form.\n\nUse the class data to carry out the test at \\(\\alpha = 0.10\\).\n\nConstruct and interpret a \\(90\\%\\) confidence interval for the proportion of cadets with long forearms.\nFinally, compare your \\(90\\%\\) confidence interval with the results of the two-sided hypothesis test.\n- Do they agree?\n- Why or why not?\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 16"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-16.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-16.html#lesson-administration",
    "title": "Lesson 16: Causation",
    "section": "",
    "text": "Lesson 17\nMilestone 4\n\nWith partner\nWrite 1-2 paragraphs per article summarizing the articles topic with a take away for its insight on your project.\nMake updates from Milestone 3 feedback.\nFill out Annex B for my comments on Milestone 3.\nTurn in EVERYTHING in your working write up.\nKeep your binder up-to-date, but I don’t want to see it.\n\n\n\n\n\n\nLesson 17 Lesson 18\n25 Points!\nLike a WPR What does that mean?!!\nRead ahead\n\n\n\n\n\n⏰ Due 0700 on Lesson 18\n\nLets take a look at it\n\nDay 1: Tuesday, 14 Oct 2025\nDay 2: Wednesday, 15 Oct 2025\n\n\n\n\n\n\n\n\nMath 1\n\n\n\n\n\n\nPreviously 4-0\n\n\n\n\n\n\n5-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 16"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-16.html#running-review",
    "href": "MA206-AY26-1/lesson-16.html#running-review",
    "title": "Lesson 16: Causation",
    "section": "",
    "text": "For all cases:\n\\(H_0:\\ \\pi = \\pi_0\\)\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nValidity Conditions\n\nNumber of successes and failures must be greater than 10.\n\nConfidence Interval for \\(\\pi\\) (one proportion)\n\\[\n\\hat{p} \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}\n\\]\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu = \\mu_0\\)\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu &lt; \\mu_0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) (degrees of freedom)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nConfidence Interval for \\(\\mu\\) (one mean)\n\\[\n\\bar{x} \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\\frac{s}{\\sqrt{n}},\n\\qquad df = n-1\n\\]\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\nStrength of evidence: Smaller \\(p\\) means stronger evidence against \\(H_0\\).\n\n\n\n\n\nGeneralization: We can generalize results to a larger population if the sample is random and representative of that population.\n\nCausation: We can claim causation only if the study design is a randomized experiment. Observational studies can show associations, but not cause-and-effect.\nParameters vs. Statistics: A parameter is a fixed (but usually unknown) numerical value describing a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\pi\\)). A statistic is a numerical value computed from a sample (e.g., \\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\)).\n\nParameters = target (what we want to know).\n\nStatistics = evidence (what we can actually measure).\n\nWe use statistics to estimate parameters, and because different samples give different statistics, we capture this variability with confidence intervals.\n\n\n\n\n\nQuantity\nPopulation (Parameter)\nSample (Statistic)\n\n\n\n\nCenter (mean)\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nSpread (SD)\n\\(\\sigma\\)\n\\(s\\)\n\n\nProportion “success”\n\\(\\pi\\)\n\\(\\hat{p}\\)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 16"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-16.html#ee1.5-review",
    "href": "MA206-AY26-1/lesson-16.html#ee1.5-review",
    "title": "Lesson 16: Causation",
    "section": "",
    "text": "Scenario\nJust like handedness where people prefer to use one hand over another, eye dominance, sometimes called eyedness, is the tendency to prefer to see using one eye over the other. Interestingly though the side of the dominant eye does not always match that of the dominant hand. Let’s investigate whether people are equally likely to have left-eye or right-eye dominance by collecting some data from you and comparing to data from other classes.\n\n\n\nWhat are the observational units?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCadets (or people).\n\n\n\n\nWhat is the variable that is recorded?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nEye dominance (left or right).\n\n\n\n\nDescribe the parameter of interest in words.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nπ = The proportion of people who are right eye dominant.\n\n\n\n\nIf right eye and left eye dominance are equally prevalent, what value of π?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nπ = 0.5.\nNull hypothesis: H₀: π = 0.5.\n\n\n\n\nIf people are more likely to be right eye dominant?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nπ &gt; 0.5.\nAlternative hypothesis: Hₐ: π &gt; 0.5.\n\n\n\n\n\n\nIn a study of a cadet company 71 out of 112 cadets sampled were right eye dominant.\nCalculate the sample proportion who are right eye dominant.\nSample size\n\n\n\n\n\n\nSolution\n\n\n\n\n\n112\n\n\n\n\n\n\nSample proportion\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\frac{71}{112} = 0.6339\\)\n\n\n\n\n\n\nSimulation based\n\n\n\nAccording to this convention, is the sample size large enough in this study to use the normal approximation and theory-based inference? Justify your answer.\nCheck validity condition for theory-based inference.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes. 71 successes and 41 failures, both ≥ 10.\n\nValidity Conditions The normal approximation can be thought of as a prediction of what would occur if a simulation-based analysis was carried out. Many times this prediction is valid, but not always. It is only valid when the condition (at least 10 successes and at least 10 failures) is met.\n\n\n\n\n\n\n\nUse the formula to determine the theoretical standard deviation of the sample proportion.\nTheoretical SD using formula.\n\\(sd = \\sqrt{\\frac{\\pi(1−\\pi)}{n}}\\)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(sd = \\frac{0.5(1−0.5)}{112} = .0472\\)\n\n\n\n\n\n\nUse the predicted value of the standard deviation from #8 to calculate the standardized statistic (z).\n\\(z = \\frac{\\hat{p} - \\pi_0}{sd}\\)\nCalculate z by hand.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nz = \\(\\frac{(0.6339 − 0.5)}{0.0472} = 2.835\\)\n\n\n\n\n\n\nTheoretical p-value.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(p \\;=\\; P(Z \\ge 2.835) \\;=\\; \\displaystyle \\int_{2.835}^{\\infty} \\phi(z)\\,dz \\;=\\; 1 - \\Phi(2.835)\\)\n1 - pnorm(2.835)  # 0.0023\nTheoretical p ≈ 0.0023\n\n\n\n\n\n\nSimulation based\n\n\n\nUse the sample data to test the claim that π = 2/3 (two-sided).\n\nHypotheses.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nH₀: π = 2/3\nHₐ: π ≠ 2/3\n\n\n\n\nStandardized statistic.\n\nReminder:\n\\(\\hat{p} = 71/112 = 0.6339\\)\n\\(p_0 = 2/3\\)\n\\(n=112\\)\n\\[\nz \\;=\\; \\frac{\\hat p - \\pi_0}{\\sqrt{\\pi_0(1-\\pi_0)/n}}\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\; z =\n\\; \\frac{0.6339 - \\tfrac{2}{3}}{\\sqrt{\\tfrac{2}{3}\\cdot \\tfrac{1}{3} / 112}}\n\\;\\approx\\; -0.736\n\\]\nStandardized statistic: \\(z \\approx -0.736\\)\n\n\n\n\nP-value.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTwo-sided: \\[\np \\;=\\; 2\\,P\\!\\left(Z \\ge |z|\\right)\n\\;=\\; 2\\bigl(1-\\Phi(|{-0.736}|)\\bigr)\n\\;\\approx\\; 0.462\n\\]\n2 * (1 - pnorm(abs(z)))\n0.4619682\nTwo-sided p-value: \\(p \\approx 0.462\\)\n\n\n\n\nConclusion.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFail to reject H₀; the data are consistent with \\(\\pi = 2/3\\).\n\n\n\n\n\n\nIn a small class of 14 students, nine were right-eye dominant.\nWhy not use theory-based methods for n=14 class?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nToo small: 9 successes and 5 failures (&lt; 10 failures). See Validity Consitions\n\n\n\n\n\n\nUse the theory-based methods anyway (two-sided test for a small class with 9 right-eye dominant out of 14).\n\nHypotheses.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(H_0:\\ \\pi = 0.5\\)\n\\(H_a:\\ \\pi \\ne 0.5\\)\n\n\n\n\nStandardized statistic.\n\nReminder\n\\(\\hat{p} = 9/14 = 0.6429\\)\n\\(\\pi_0 = 0.5\\)\n\\(n = 14\\)\n\\[\nz \\;=\\; \\dfrac{\\hat p - \\pi_0}{\\sqrt{\\pi_0(1-\\pi_0)/n}}\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\; \\dfrac{0.6429 - 0.5}{\\sqrt{0.5(1-0.5)/14}}\n\\;\\approx\\; 1.069\n\\]\nStandardized statistic: \\(z \\approx 1.069\\)\n\n\n\n\nP-value.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTwo-sided:\n\\(p = 2\\,P(Z \\ge |z|) = 2\\bigl(1-\\Phi(|1.069|)\\bigr) \\approx 0.285\\)\n2 * (1 - pnorm(abs(z)))\nTwo-sided p-value: \\(p \\approx 0.285\\)\n\n\n\n\nConclusion.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAt typical significance levels, fail to reject \\(H_0\\). With \\(n=14\\), the theory-based result is \\(p\\approx 0.285\\); note the normal approximation can be poor for small samples.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 16"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-16.html#causation",
    "href": "MA206-AY26-1/lesson-16.html#causation",
    "title": "Lesson 16: Causation",
    "section": "",
    "text": "Coffee Drinking and Cancer\nEarly studies suggested coffee drinkers had higher cancer rates.\n\nConfounder: Smoking — coffee drinkers were also more likely to smoke, and smoking causes cancer.\n\nHealth Supplements and Longevity\nSupplement users often appeared healthier and lived longer in studies.\n\nConfounder: Health-conscious lifestyle — people taking supplements also tended to eat better, exercise, and get regular medical care.\n\nPolice Officers and Crime Rates\nMore police officers in an area were linked to higher crime counts.\n\nConfounder: City size — larger cities naturally have both more crime and more police.\n\n\n\n\nRandom sampling + Random assignment (cause + generalize)\n\nRandomly sample 200 West Point cadets from all classes.\nRandomly assign half to a 6-week push-up volume plan (daily small sets) and half to a strength plan (fewer, heavier weighted push-ups).\nOutcome: change in max push-ups in 2 minutes.\nInference: Can claim causation and generalize to cadets.\n\nRandom sampling + No random assignment (generalize, not cause)\n\nRandomly sample 200 cadets; classify them by the push-up routine they already follow (volume vs strength), no reassignment.\nCompare change in 2-minute max after 6 weeks.\nConfounding: motivation, prior fitness, schedule differences.\nInference: Can generalize to cadets who choose each routine, but not claim causation.\n\nNot random sampling + Random assignment (cause, not generalize)\n\nConvenience sample: one company (≈200 cadets) volunteers.\nWithin that company, randomly assign cadets to volume vs strength plans.\nOutcome: change in 2-minute max.\nInference: Can claim causation for this company, but limited generalizability.\n\nNot random sampling + No random assignment (confounding + bias)\n\nConvenience groups: compare two available companies that already use different push-up plans (volume vs strength).\nNo random sampling and no reassignment.\nConfounding: cadre emphasis, additional PT, culture, sleep.\nInference: Neither causation nor broad generalization is supported.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 16"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-16.html#example-problems",
    "href": "MA206-AY26-1/lesson-16.html#example-problems",
    "title": "Lesson 16: Causation",
    "section": "",
    "text": "We want to investigate the elbow-to-forearm length of cadets.\nA commonly cited average for adults is 26 inches.\nResearch Question 1: Do West Point cadets have longer forearms than 26 inches on average?\n\nIdentify the parameter of interest.\n\nState the null and alternative hypotheses in symbolic form.\n\nUse the class data to carry out the test at \\(\\alpha = 0.05\\).\n\nResearch Question 2: Is the average elbow-to-forearm length of cadets different from 26 inches?\n\nIdentify the parameter of interest.\n\nState the null and alternative hypotheses in symbolic form.\n\nUse the class data to carry out the test at \\(\\alpha = 0.05\\).\n\nConstruct and interpret a \\(95\\%\\) confidence interval for the mean forearm length.\nFinally, compare your \\(95\\%\\) confidence interval with the results of the two-sided hypothesis test.\n- Do they agree?\n- Why or why not?\n\n\n\nIt turns out that people with forearms over 28 inches are considered long.\nWe want to investigate the proportion of cadets who have long forearms.\nSuppose 30% is considered the reference value for the population.\nResearch Question 1: Is the proportion of cadets with long forearms more than 30%?\n\nIdentify the parameter of interest.\n\nState the null and alternative hypotheses in symbolic form.\n\nUse the class data to carry out the test at \\(\\alpha = 0.10\\).\n\nResearch Question 2: Is the proportion of cadets with long forearms different from 30%?\n\nIdentify the parameter of interest.\n\nState the null and alternative hypotheses in symbolic form.\n\nUse the class data to carry out the test at \\(\\alpha = 0.10\\).\n\nConstruct and interpret a \\(90\\%\\) confidence interval for the proportion of cadets with long forearms.\nFinally, compare your \\(90\\%\\) confidence interval with the results of the two-sided hypothesis test.\n- Do they agree?\n- Why or why not?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 16"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-16.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-16.html#before-you-leave",
    "title": "Lesson 16: Causation",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 16"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-14.html",
    "href": "MA206-AY26-1/lesson-14.html",
    "title": "Lesson 14: Confidence Interval (Categorical)",
    "section": "",
    "text": "⏰ Due 0700 ET on Lesson 15\nLets take a look at it   \n\n\n\n\n\nLesson 17\nMilestone 4\n\nWith partner\nWrite 1-2 paragraphs per article summarizing the articles topic with a take away for its insight on your project.\nMake updates from Milestone 3 feedback.\nFill out Annex B for my comments on Milestone 3.\nTurn in EVERYTHING in your working write up.\nKeep your binder up-to-date, but I don’t want to see it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all cases:\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.\n\n\n\n\nFor all cases:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t_{df}}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A: \\mu &lt; \\mu_0\\)\n\\(p = F_{t_{df}}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A: \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t_{df}}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) = degrees of freedom\n\n\\(F_{t_{df}}(\\cdot)\\) = cumulative distribution function (CDF) of the Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\n\n\n\n\nWhat impacts does altering different values have?\n\nGeneralization: We can generalize results to a larger population if the sample is random and representative of that population. Convenience samples don’t justify broad claims.\n\nCausation: We can claim causation only if the study design is a randomized experiment. Observational studies can show associations, but not cause-and-effect.\n\n\n\n\n\nSuppose we take a random sample of \\(n = 1000\\) people, and \\(x = 120\\) of them are left-handed.\nThat means our sample proportion is\n\\[\n\\hat{p} = \\frac{120}{1000} = 0.12.\n\\]\nNow we ask: what are reasonable values for the true proportion of left-handed people in the population?\nWe could also ask that question in this way: Where, under a two-sided (\\(\\neq\\)) alternative hypothesis, would we reject?\nWe start with the one-proportion \\(z\\) statistic:\n\\[\nz \\;=\\; \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\dfrac{\\pi_0(1-\\pi_0)}{n}}}.\n\\]\nAt significance level \\(\\alpha = 0.05\\), we reject \\(H_0\\) whenever \\(z\\) falls into the rejection region (the critical tail(s)).\nWhere on this plot would we reject?\n\n\n\n\n\n\n\n\n\nWhere are these lines?\n\n\n\n\n\n\n\n\n\nSo, the left tail contains \\(0.025\\) of the distribution, and the right tail contains \\(0.025\\).\nHow do we find where \\(0.025\\) of the distribution is on each side?\nWe want to solve: \\[\n0.025 \\;=\\; \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}} e^{-z^{2}/2}\\,dz.\n\\]\nWhen we solve for \\(x\\), this is the quantile function (inverse CDF).\nThis is a difficult distribution to solve for \\(x\\) by hand — that’s why we have R.\n\nqnorm(0.025, mean = 0, sd = 1)\n\n[1] -1.959964\n\n\nThat’s the left cutoff. And the right cutoff:\n\nqnorm(0.975, mean = 0, sd = 1)\n\n[1] 1.959964\n\n\nSo our critical values are about \\(-1.96\\) and \\(+1.96\\).\nThat is, \\[\n\\pm 1.96 \\;=\\; \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\dfrac{\\pi_0(1-\\pi_0)}{n}}}.\n\\]\nWe don’t know the true \\(p\\), and there isn’t a \\(\\pi_0\\) to plug in.\nSo what do we do? We use the best estimate of \\(p\\) we have: the sample proportion \\(\\hat{p}\\).\nThat gives us \\[\n\\pm 1.96 \\;=\\; \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}}}.\n\\]\nWith a little algebra: \\[\n\\begin{align*}\n\\hat{p} - \\pi_0 \\;&=\\; \\pm 1.96 \\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}} \\\\[6pt]\n-\\pi_0 \\;&=\\; -\\hat{p} \\;\\pm\\; 1.96 \\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}} \\\\[6pt]\n\\pi_0 \\;&=\\; \\hat{p} \\;\\pm\\; 1.96 \\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}}\n\\end{align*}\n\\]\nSo the general 95% confidence interval for a proportion is: \\[\n\\hat{p} \\;\\pm\\; 1.96 \\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\n(Equivalently: the set of \\(\\pi_0\\) values we would not reject at \\(\\alpha=0.05\\).)\nBack to our \\(120/1000\\) lefties:\n\\[\n\\frac{3}{25} \\;\\pm\\; 1.96 \\sqrt{\\dfrac{\\frac{3}{25}(1-\\frac{3}{25})}{1000}}\n\\] Which is \\(0.12±0.0201\\)\n\n\n\nA random sample of 220 cadets was asked if they drink coffee at least once a day. Out of those surveyed, 76 cadets said yes.\nTasks\n\nState the parameter of interest \\(\\pi\\) in words.\n\nCompute the sample proportion \\(\\hat{p}\\).\n\nConstruct a 95% confidence interval for \\(p\\).\n\nInterpret the interval in context of the cadet population.\n\n(Extension) How would the width of the interval change if the sample size were doubled but the proportion stayed about the same?\n\nTask 1. Parameter of interest\nLet \\(\\pi\\) be the true proportion of all cadets who drink coffee at least once a day.\nTask 2. Sample proportion\n\\[\n\\hat{p} = \\frac{x}{n} = \\frac{76}{220} = \\frac{19}{55} \\approx 0.3455\n\\]\nSo \\(\\hat{p} \\approx 0.3455\\), about 34.6%.\nTask 4. 95% Confidence Interval for \\(p\\)\nGeneral formula:\n\\[\n\\hat{p} \\;\\pm\\; z^\\star \\sqrt{\\tfrac{\\hat{p}(1-\\hat{p})}{n}},\n\\quad z^\\star \\approx 1.96 \\text{ for 95\\%}\n\\]\nPlugging in values:\n\\[\n0.3455 \\;\\pm\\; 1.96 \\sqrt{\\tfrac{0.3455(1-0.3455)}{220}}\n\\]\nCompute:\n\\[\n0.3455 \\;\\pm\\; 1.96 \\times 0.0321\n\\]\nConfidence interval:\n\\[\n[\\,0.2827,\\;0.4083\\,]\n\\]\nSo the 95% CI is approximately \\([0.283,\\;0.408]\\).\nTask 5. Interpretation\nWe are 95% confident that the true proportion \\(p\\) of cadets who drink coffee at least once a day lies between 28.3% and 40.8%.\nTask 6. What if the sample size doubled?\nIf \\(n\\) doubles to 440 (with \\(\\hat{p}\\) about the same), the confidence interval becomes\n\\[\n0.3455 \\;\\pm\\; 1.96 \\sqrt{\\tfrac{0.3455(1-0.3455)}{440}}\n\\]\nCompute:\n\\[\n0.3455 \\;\\pm\\; 1.96 \\times 0.0227\n\\]\nNew confidence interval:\n\\[\n[\\,0.301,\\;0.390\\,]\n\\]\nSo with a larger sample size, the interval is narrower.\n\n\n\n\n\nA random sample of 160 cadets was asked if they own a smartwatch. Out of those surveyed, 62 said yes. Suppose previous reports suggested that 40% of cadets own a smartwatch.\n\nState the null and alternative hypotheses.\n\nTest the hypotheses at \\(\\alpha = 0.05\\).\n\nConstruct a 95% confidence interval for the true proportion of cadets who own a smartwatch.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNull and alternative:\n\\[\nH_0: p = 0.40, \\quad H_A: p \\neq 0.40\n\\]\nTest:\n\\[\n\\hat{p} = \\tfrac{62}{160} = 0.3875\n\\]\n\\[\nz = \\frac{\\hat{p} - 0.40}{\\sqrt{\\tfrac{0.40(1-0.40)}{160}}}\n= \\frac{0.3875 - 0.40}{\\sqrt{\\tfrac{0.24}{160}}}\n\\approx -0.323\n\\]\n\\[\np = 2\\bigl(1 - \\Phi(|z|)\\bigr) \\approx 0.747\n\\]\nDecision at \\(\\alpha = 0.05\\): Fail to reject \\(H_0\\).\n95% CI:\n\\[\n0.3875 \\;\\pm\\; 1.96 \\sqrt{\\tfrac{0.3875(1-0.3875)}{160}}\n= [\\,0.312,\\;0.463\\,]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn a survey of 200 soldiers, 118 said they prefer running over cycling for cardio workouts. If there is no preference, we would expect half of soldiers to choose running.\n\nState the null and alternative hypotheses.\n\nTest the hypotheses at \\(\\alpha = 0.05\\).\n\nConstruct a 95% confidence interval for the true proportion who prefer running.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNull and alternative:\n\\[\nH_0: p = 0.50, \\quad H_A: p \\neq 0.50\n\\]\nTest:\n\\[\n\\hat{p} = \\tfrac{118}{200} = 0.59\n\\]\n\\[\nz = \\frac{0.59 - 0.50}{\\sqrt{\\tfrac{0.50(1-0.50)}{200}}}\n= \\frac{0.09}{\\sqrt{\\tfrac{0.25}{200}}}\n\\approx 2.546\n\\]\n\\[\np = 2\\bigl(1 - \\Phi(|z|)\\bigr) \\approx 0.011\n\\]\nDecision at \\(\\alpha = 0.05\\): Reject \\(H_0\\).\n95% CI:\n\\[\n0.59 \\;\\pm\\; 1.96 \\sqrt{\\tfrac{0.59(1-0.59)}{200}}\n= [\\,0.522,\\;0.658\\,]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA random sample of 90 cadets was taken, and 27 reported they drink an energy drink daily. Past studies suggested that 25% of cadets drink energy drinks daily.\n\nState the null and alternative hypotheses.\n\nTest the hypotheses at \\(\\alpha = 0.05\\).\n\nConstruct a 95% confidence interval for the true proportion of cadets who drink an energy drink daily.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNull and alternative:\n\\[\nH_0: p = 0.25, \\quad H_A: p \\neq 0.25\n\\]\nTest:\n\\[\n\\hat{p} = \\tfrac{27}{90} = 0.30\n\\]\n\\[\nz = \\frac{0.30 - 0.25}{\\sqrt{\\tfrac{0.25(1-0.25)}{90}}}\n= \\frac{0.05}{\\sqrt{\\tfrac{0.1875}{90}}}\n\\approx 1.095\n\\]\n\\[\np = 2\\bigl(1 - \\Phi(|z|)\\bigr) \\approx 0.273\n\\]\nDecision at \\(\\alpha = 0.05\\): Fail to reject \\(H_0\\).\n95% CI:\n\\[\n0.30 \\;\\pm\\; 1.96 \\sqrt{\\tfrac{0.30(1-0.30)}{90}}\n= [\\,0.205,\\;0.395\\,]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 14"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-14.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-14.html#lesson-administration",
    "title": "Lesson 14: Confidence Interval (Categorical)",
    "section": "",
    "text": "⏰ Due 0700 ET on Lesson 15\nLets take a look at it   \n\n\n\n\n\nLesson 17\nMilestone 4\n\nWith partner\nWrite 1-2 paragraphs per article summarizing the articles topic with a take away for its insight on your project.\nMake updates from Milestone 3 feedback.\nFill out Annex B for my comments on Milestone 3.\nTurn in EVERYTHING in your working write up.\nKeep your binder up-to-date, but I don’t want to see it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all cases:\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.\n\n\n\n\nFor all cases:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t_{df}}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A: \\mu &lt; \\mu_0\\)\n\\(p = F_{t_{df}}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A: \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t_{df}}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) = degrees of freedom\n\n\\(F_{t_{df}}(\\cdot)\\) = cumulative distribution function (CDF) of the Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\n\n\n\n\nWhat impacts does altering different values have?\n\nGeneralization: We can generalize results to a larger population if the sample is random and representative of that population. Convenience samples don’t justify broad claims.\n\nCausation: We can claim causation only if the study design is a randomized experiment. Observational studies can show associations, but not cause-and-effect.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 14"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-14.html#confidence-intervals-on-proportions",
    "href": "MA206-AY26-1/lesson-14.html#confidence-intervals-on-proportions",
    "title": "Lesson 14: Confidence Interval (Categorical)",
    "section": "",
    "text": "Suppose we take a random sample of \\(n = 1000\\) people, and \\(x = 120\\) of them are left-handed.\nThat means our sample proportion is\n\\[\n\\hat{p} = \\frac{120}{1000} = 0.12.\n\\]\nNow we ask: what are reasonable values for the true proportion of left-handed people in the population?\nWe could also ask that question in this way: Where, under a two-sided (\\(\\neq\\)) alternative hypothesis, would we reject?\nWe start with the one-proportion \\(z\\) statistic:\n\\[\nz \\;=\\; \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\dfrac{\\pi_0(1-\\pi_0)}{n}}}.\n\\]\nAt significance level \\(\\alpha = 0.05\\), we reject \\(H_0\\) whenever \\(z\\) falls into the rejection region (the critical tail(s)).\nWhere on this plot would we reject?\n\n\n\n\n\n\n\n\n\nWhere are these lines?\n\n\n\n\n\n\n\n\n\nSo, the left tail contains \\(0.025\\) of the distribution, and the right tail contains \\(0.025\\).\nHow do we find where \\(0.025\\) of the distribution is on each side?\nWe want to solve: \\[\n0.025 \\;=\\; \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}} e^{-z^{2}/2}\\,dz.\n\\]\nWhen we solve for \\(x\\), this is the quantile function (inverse CDF).\nThis is a difficult distribution to solve for \\(x\\) by hand — that’s why we have R.\n\nqnorm(0.025, mean = 0, sd = 1)\n\n[1] -1.959964\n\n\nThat’s the left cutoff. And the right cutoff:\n\nqnorm(0.975, mean = 0, sd = 1)\n\n[1] 1.959964\n\n\nSo our critical values are about \\(-1.96\\) and \\(+1.96\\).\nThat is, \\[\n\\pm 1.96 \\;=\\; \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\dfrac{\\pi_0(1-\\pi_0)}{n}}}.\n\\]\nWe don’t know the true \\(p\\), and there isn’t a \\(\\pi_0\\) to plug in.\nSo what do we do? We use the best estimate of \\(p\\) we have: the sample proportion \\(\\hat{p}\\).\nThat gives us \\[\n\\pm 1.96 \\;=\\; \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}}}.\n\\]\nWith a little algebra: \\[\n\\begin{align*}\n\\hat{p} - \\pi_0 \\;&=\\; \\pm 1.96 \\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}} \\\\[6pt]\n-\\pi_0 \\;&=\\; -\\hat{p} \\;\\pm\\; 1.96 \\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}} \\\\[6pt]\n\\pi_0 \\;&=\\; \\hat{p} \\;\\pm\\; 1.96 \\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}}\n\\end{align*}\n\\]\nSo the general 95% confidence interval for a proportion is: \\[\n\\hat{p} \\;\\pm\\; 1.96 \\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\n(Equivalently: the set of \\(\\pi_0\\) values we would not reject at \\(\\alpha=0.05\\).)\nBack to our \\(120/1000\\) lefties:\n\\[\n\\frac{3}{25} \\;\\pm\\; 1.96 \\sqrt{\\dfrac{\\frac{3}{25}(1-\\frac{3}{25})}{1000}}\n\\] Which is \\(0.12±0.0201\\)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 14"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-14.html#practice-problem",
    "href": "MA206-AY26-1/lesson-14.html#practice-problem",
    "title": "Lesson 14: Confidence Interval (Categorical)",
    "section": "",
    "text": "A random sample of 220 cadets was asked if they drink coffee at least once a day. Out of those surveyed, 76 cadets said yes.\nTasks\n\nState the parameter of interest \\(\\pi\\) in words.\n\nCompute the sample proportion \\(\\hat{p}\\).\n\nConstruct a 95% confidence interval for \\(p\\).\n\nInterpret the interval in context of the cadet population.\n\n(Extension) How would the width of the interval change if the sample size were doubled but the proportion stayed about the same?\n\nTask 1. Parameter of interest\nLet \\(\\pi\\) be the true proportion of all cadets who drink coffee at least once a day.\nTask 2. Sample proportion\n\\[\n\\hat{p} = \\frac{x}{n} = \\frac{76}{220} = \\frac{19}{55} \\approx 0.3455\n\\]\nSo \\(\\hat{p} \\approx 0.3455\\), about 34.6%.\nTask 4. 95% Confidence Interval for \\(p\\)\nGeneral formula:\n\\[\n\\hat{p} \\;\\pm\\; z^\\star \\sqrt{\\tfrac{\\hat{p}(1-\\hat{p})}{n}},\n\\quad z^\\star \\approx 1.96 \\text{ for 95\\%}\n\\]\nPlugging in values:\n\\[\n0.3455 \\;\\pm\\; 1.96 \\sqrt{\\tfrac{0.3455(1-0.3455)}{220}}\n\\]\nCompute:\n\\[\n0.3455 \\;\\pm\\; 1.96 \\times 0.0321\n\\]\nConfidence interval:\n\\[\n[\\,0.2827,\\;0.4083\\,]\n\\]\nSo the 95% CI is approximately \\([0.283,\\;0.408]\\).\nTask 5. Interpretation\nWe are 95% confident that the true proportion \\(p\\) of cadets who drink coffee at least once a day lies between 28.3% and 40.8%.\nTask 6. What if the sample size doubled?\nIf \\(n\\) doubles to 440 (with \\(\\hat{p}\\) about the same), the confidence interval becomes\n\\[\n0.3455 \\;\\pm\\; 1.96 \\sqrt{\\tfrac{0.3455(1-0.3455)}{440}}\n\\]\nCompute:\n\\[\n0.3455 \\;\\pm\\; 1.96 \\times 0.0227\n\\]\nNew confidence interval:\n\\[\n[\\,0.301,\\;0.390\\,]\n\\]\nSo with a larger sample size, the interval is narrower.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 14"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-14.html#board-problems",
    "href": "MA206-AY26-1/lesson-14.html#board-problems",
    "title": "Lesson 14: Confidence Interval (Categorical)",
    "section": "",
    "text": "A random sample of 160 cadets was asked if they own a smartwatch. Out of those surveyed, 62 said yes. Suppose previous reports suggested that 40% of cadets own a smartwatch.\n\nState the null and alternative hypotheses.\n\nTest the hypotheses at \\(\\alpha = 0.05\\).\n\nConstruct a 95% confidence interval for the true proportion of cadets who own a smartwatch.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNull and alternative:\n\\[\nH_0: p = 0.40, \\quad H_A: p \\neq 0.40\n\\]\nTest:\n\\[\n\\hat{p} = \\tfrac{62}{160} = 0.3875\n\\]\n\\[\nz = \\frac{\\hat{p} - 0.40}{\\sqrt{\\tfrac{0.40(1-0.40)}{160}}}\n= \\frac{0.3875 - 0.40}{\\sqrt{\\tfrac{0.24}{160}}}\n\\approx -0.323\n\\]\n\\[\np = 2\\bigl(1 - \\Phi(|z|)\\bigr) \\approx 0.747\n\\]\nDecision at \\(\\alpha = 0.05\\): Fail to reject \\(H_0\\).\n95% CI:\n\\[\n0.3875 \\;\\pm\\; 1.96 \\sqrt{\\tfrac{0.3875(1-0.3875)}{160}}\n= [\\,0.312,\\;0.463\\,]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn a survey of 200 soldiers, 118 said they prefer running over cycling for cardio workouts. If there is no preference, we would expect half of soldiers to choose running.\n\nState the null and alternative hypotheses.\n\nTest the hypotheses at \\(\\alpha = 0.05\\).\n\nConstruct a 95% confidence interval for the true proportion who prefer running.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNull and alternative:\n\\[\nH_0: p = 0.50, \\quad H_A: p \\neq 0.50\n\\]\nTest:\n\\[\n\\hat{p} = \\tfrac{118}{200} = 0.59\n\\]\n\\[\nz = \\frac{0.59 - 0.50}{\\sqrt{\\tfrac{0.50(1-0.50)}{200}}}\n= \\frac{0.09}{\\sqrt{\\tfrac{0.25}{200}}}\n\\approx 2.546\n\\]\n\\[\np = 2\\bigl(1 - \\Phi(|z|)\\bigr) \\approx 0.011\n\\]\nDecision at \\(\\alpha = 0.05\\): Reject \\(H_0\\).\n95% CI:\n\\[\n0.59 \\;\\pm\\; 1.96 \\sqrt{\\tfrac{0.59(1-0.59)}{200}}\n= [\\,0.522,\\;0.658\\,]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA random sample of 90 cadets was taken, and 27 reported they drink an energy drink daily. Past studies suggested that 25% of cadets drink energy drinks daily.\n\nState the null and alternative hypotheses.\n\nTest the hypotheses at \\(\\alpha = 0.05\\).\n\nConstruct a 95% confidence interval for the true proportion of cadets who drink an energy drink daily.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNull and alternative:\n\\[\nH_0: p = 0.25, \\quad H_A: p \\neq 0.25\n\\]\nTest:\n\\[\n\\hat{p} = \\tfrac{27}{90} = 0.30\n\\]\n\\[\nz = \\frac{0.30 - 0.25}{\\sqrt{\\tfrac{0.25(1-0.25)}{90}}}\n= \\frac{0.05}{\\sqrt{\\tfrac{0.1875}{90}}}\n\\approx 1.095\n\\]\n\\[\np = 2\\bigl(1 - \\Phi(|z|)\\bigr) \\approx 0.273\n\\]\nDecision at \\(\\alpha = 0.05\\): Fail to reject \\(H_0\\).\n95% CI:\n\\[\n0.30 \\;\\pm\\; 1.96 \\sqrt{\\tfrac{0.30(1-0.30)}{90}}\n= [\\,0.205,\\;0.395\\,]\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 14"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-14.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-14.html#before-you-leave",
    "title": "Lesson 14: Confidence Interval (Categorical)",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 14"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html",
    "href": "MA206-AY26-1/lesson-12.html",
    "title": "Lesson 12: One Mean T-Test",
    "section": "",
    "text": "⏰ Due 0700 ET on Lesson 13\n\nDay 1: Wednesday, 24 Sept 2025\n\nDay 2: Thursday, 25 Sept 2025\n\n\n📑 Worksheet: https://westpoint.instructure.com/courses/10295/assignments/216497 — don’t sleep on this!\nWill need applet for simulation\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12/120\n10:37\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\nFor all cases:\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.\n\n\n\n\nWe want to test whether Male cadets (Don’t worry ladies, we’ll get to you) are shorter than the average height of U.S. men aged 19–24, which is 72 inches.\n\n\n\\[\nH_0 : \\mu = 72\n\\]\n\\[\nH_A : \\mu &lt; 72\n\\]\n\n\n\n\nheights &lt;- c(70, 71, 69, 73, 68, 74, 71, 70, 72, 69, 70, 71, 68, 73)\n\n\n\n\n\n\n\nIs This a Good Sample?\n\n\n\nWe measured the heights of 14 cadets in this class.\nBut our research question is about all U.S. men aged 19–24.\n\nDo cadets in this class represent the broader population (what is that population)?\n\nWhat kinds of biases might be present?\n\nIf the sample isn’t random, how should that affect our conclusions?\n\n\n\n\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\qquad df = n - 1\n\\]\n\\[\n\\begin{aligned}\n\\bar{x} & = \\text{sample mean} \\\\[4pt]\n\\mu_0   & = \\text{hypothesized mean (72)} \\\\[4pt]\ns       & = \\text{sample standard deviation} \\\\[4pt]\nn       & = \\text{sample size} \\\\[4pt]\nt       & = \\text{test statistic}\n\\end{aligned}\n\\]\n\n\n\n\nn     &lt;- length(heights)\nxbar  &lt;- mean(heights)\ns     &lt;- sd(heights)\n\nc(n = n, mean = round(xbar, 2), sd = round(s, 2))\n\n    n  mean    sd \n14.00 70.64  1.86 \n\n\n\n\n\n\nmu0 &lt;- 72\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\n\n[1] -2.722848\n\n\n\n\n\n\ndf &lt;- n - 1\n\nggplot() +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  labs(title = \"t-Distribution with df = 13\")\n\n\n\n\n\n\n\n\nAdd the observed test statistic:\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, t_stat),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, color = \"firebrick\", linewidth = 1.2) +\n  labs(title = \"Left-tail Shaded: t-Distribution with Test Statistic\")\n\n\n\n\n\n\n\n\n\n\n\n\np_val &lt;- pt(t_stat, df = df)   \np_val\n\n[1] 0.008708986\n\n\n\n\n\nWith p ≈ .008, we do have evidence that male cadets are shorter than 72 inches.\n\n\n\n\nWe want to test whether female cadets are taller than the average height of U.S. women aged 19–24, which is 64 inches.\n\n\n\\[\nH_0 : \\mu = 64\n\\]\n\\[\nH_A : \\mu &gt; 64\n\\]\n\n\n\n\nheights &lt;- c(65, 67, 66, 64)\n\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\qquad df = n - 1\n\\]\n\n\n\n\nn     &lt;- length(heights)\nxbar  &lt;- mean(heights)\ns     &lt;- sd(heights)\n\nc(n = n, mean = round(xbar, 2), sd = round(s, 2))\n\n    n  mean    sd \n 4.00 65.50  1.29 \n\n\n\n\n\n\nmu0 &lt;- 64\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\n\n[1] 2.32379\n\n\n\n\n\n\ndf &lt;- n - 1\n\nggplot() +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  labs(title = \"t-Distribution with df = 3\")\n\n\n\n\n\n\n\n\nAdd the observed test statistic and shade the right tail:\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(t_stat, 4),\n                geom = \"area\", fill = \"darkred\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, color = \"firebrick\", linewidth = 1.2) +\n  labs(title = \"Right-tail Shaded: t-Distribution with Test Statistic\")\n\n\n\n\n\n\n\n\n\n\n\n\np_val &lt;- 1 - pt(t_stat, df = df)   # one-tailed, greater than\np_val\n\n[1] 0.05136404\n\n\n\n\n\nWith p ≈ .051, we do not have strong evidence that female cadets are taller than 64 inches.\n\n\n\n\nSuppose the national average height of men aged 19–24 is 72 inches.\nWe want to test whether West Point Department of Math instructors are different (either taller or shorter).\n\n\n\\[\nH_0 : \\mu = 72\n\\]\n\\[\nH_A : \\mu \\neq 72\n\\]\n\n\n\nHere is a sample of heights (in inches) from 28 Math instructors:\n\nheights &lt;- c(71, 70, 73, 72, 74, 69, 71, 72, 70, 73, 72, 71, 75, 70, 72, 74, 71, 69, 73, 72, 70, 71, 74, 72, 70, 73, 71, 72)\n\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\qquad df = n - 1\n\\]\n\n\n\n\nn     &lt;- length(heights)\nxbar  &lt;- mean(heights)\ns     &lt;- sd(heights)\n\nc(n = n, mean = round(xbar, 2), sd = round(s, 2))\n\n    n  mean    sd \n28.00 71.68  1.56 \n\n\n\n\n\n\nmu0 &lt;- 72\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\n\n[1] -1.086979\n\n\n\n\n\n\ndf &lt;- n - 1\n\nggplot() +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  labs(title = paste(\"t-Distribution with df =\", df))\n\n\n\n\n\n\n\n\nAdd the observed test statistic and shade both tails:\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, -abs(t_stat)),\n                geom = \"area\", fill = \"steelblue\", alpha = 0.5) +\n  stat_function(fun = dt, args = list(df = df), xlim = c(abs(t_stat), 4),\n                geom = \"area\", fill = \"steelblue\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, color = \"firebrick\", linewidth = 1.2) +\n  geom_vline(xintercept = -t_stat, color = \"firebrick\", linewidth = 1.2) +\n  labs(title = \"Two-tailed Shaded: t-Distribution with Test Statistic\")\n\n\n\n\n\n\n\n\n\n\n\n\np_val &lt;- 2 * (1 - pt(abs(t_stat), df = df))   # two-tailed\np_val\n\n[1] 0.286655\n\n\n\n\n\nWith p ≈ .29, we do not have evidence that Math instructors’ heights differ from 72 inches.\n\n\n\n\nOverlay several \\(t\\) distributions with different degrees of freedom and the standard normal \\(N(0,1)\\) for comparison.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNationally, the average college student drinks about \\(3.2\\) cups of coffee per day.\nYou suspect cadets might consume a different amount (not necessarily more or less).\nYou collect a sample of \\(12\\) cadets with the following self-reported daily coffee consumption (in cups):\n\ncoffee &lt;- c(2.5, 3.0, 3.8, 4.1, 3.2, 2.9, 3.6, 4.0, 3.3, 2.7, 3.5, 3.9)\n\nTasks:\n\nState the null and alternative hypotheses.\n\nCompute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n\nWrite down the test statistic formula for a one-sample \\(t\\)-test.\n\nCalculate the test statistic.\n\nFind the \\(p\\)-value for the appropriate two-tailed test.\n\nState your conclusion in the context of the problem.\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses\n\\[\nH_0: \\mu = 3.2 \\qquad\\text{vs}\\qquad H_A: \\mu \\neq 3.2\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(coffee)\nxbar &lt;- mean(coffee)\ns    &lt;- sd(coffee)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n12.000  3.375  0.528 \n\n\nNumerically: \\(n = 12\\), \\(\\bar{x} \\approx 3.375\\), \\(s \\approx 0.528\\).\n3–4. Test statistic\n\nmu0   &lt;- 3.2\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n 1.149 11.000 \n\n\nNumerically: \\(t \\approx 1.149\\) with \\(df = 11\\).\n5. Two-tailed \\(p\\)-value\n\np_val &lt;- 2 * (1 - pt(abs(t_stat), df = df))\np_val\n\n[1] 0.2749615\n\n\nNumerically: \\(p \\approx 0.275\\).\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  # Left tail shading\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, -abs(t_stat)),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Right tail shading\n  stat_function(fun = dt, args = list(df = df), xlim = c(abs(t_stat), 4),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Overlay t density\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  # Vertical lines at ±t_stat\n  geom_vline(xintercept = c(-abs(t_stat), abs(t_stat)),\n             color = \"firebrick\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(title = \"Two-tailed Shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")\n\n\n\n\n\n\n\n\n6. Conclusion\nWith \\(p \\approx 0.275\\), we do not have evidence that cadets’ average coffee consumption differs from \\(3.2\\) cups per day.\n\n\n\n\n\n\n\n\nA recent campus wellness report suggests the average college student spends \\(3.0\\) hours per day on recreational screen time (not including coursework).\nYou suspect students in your section spend more than that.\nYou collect a sample of \\(12\\) students with the following daily screen-time values (in hours):\n\nscreen_time &lt;- c(2.5, 3.0, 3.1, 3.2, 3.3, 3.4, 3.8, 3.7, 3.8, 3.2, 3.5, 3.5)\n\nTasks: 1. State the null and alternative hypotheses.\n2. Compute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n3. Write down the test statistic formula for a one-sample \\(t\\)-test.\n4. Calculate the test statistic.\n5. Find the one-tailed \\(p\\)-value for the “greater than” test.\n6. State your conclusion in context.\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses \\[\nH_0: \\mu = 3.0\n\\qquad\\text{vs}\\qquad\nH_A: \\mu &gt; 3.0\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(screen_time)\nxbar &lt;- mean(screen_time)\ns    &lt;- sd(screen_time)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n12.000  3.333  0.373 \n\n\n3–4. Test statistic\n\nmu0    &lt;- 3.0\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n   3.1   11.0 \n\n\n5. One-tailed \\(p\\)-value (\\(H_A: \\mu &gt; \\mu_0\\))\n\np_val &lt;- 1 - pt(t_stat, df = df)\np_val\n\n[1] 0.005056453\n\n\n6. Conclusion\nWith the computed \\(t\\) and \\(p\\) above, interpret whether there is evidence that average daily recreational screen time in this section exceeds \\(3.0\\) hours.\n\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(t_stat, 4),\n                geom = \"area\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, linewidth = 1.2) +\n  labs(title = \"Right-tail shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\nPublic health guidelines recommend \\(7.0\\) hours of sleep on weeknights.\nYou suspect students in your section average less than that.\nYou collect a sample of \\(15\\) students’ self-reported weeknight sleep (in hours):\n\nsleep &lt;- c(6.6, 6.9, 7.1, 6.8, 7.0, 6.7, 6.5, 6.8, 6.9, 6.4, 6.6, 7.2, 6.7, 6.8, 6.5)\n\nTasks: 1. State the null and alternative hypotheses.\n2. Compute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n3. Write down the test statistic formula for a one-sample \\(t\\)-test.\n4. Calculate the test statistic.\n5. Find the one-tailed \\(p\\)-value for the “less than” test.\n6. State your conclusion in context.\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses \\[\nH_0: \\mu = 7.0\n\\qquad\\text{vs}\\qquad\nH_A: \\mu &lt; 7.0\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(sleep)\nxbar &lt;- mean(sleep)\ns    &lt;- sd(sleep)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n15.000  6.767  0.229 \n\n\n3–4. Test statistic\n\nmu0    &lt;- 7.0\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n-3.949 14.000 \n\n\n5. One-tailed \\(p\\)-value (\\(H_A: \\mu &lt; \\mu_0\\))\n\np_val &lt;- pt(t_stat, df = df)\np_val\n\n[1] 0.0007279758\n\n\n6. Conclusion\nReport the computed \\(t\\), \\(df\\), and \\(p\\), then conclude whether there is evidence that average weeknight sleep is less than \\(7.0\\) hours.\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  # Left tail shading up to t_stat\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, t_stat),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Overlay t density\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  # Vertical line at t_stat\n  geom_vline(xintercept = t_stat,\n             color = \"firebrick\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(title = \"Left-tail Shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nProject Milestone 3: Due Canvas 22 Sept\nExploration Exercise 1.5: Due at 0700 on Lesson 13\n24 September 2025 for Day 1\n\n25 September 2025 for Day 2)\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-12.html#lesson-administration",
    "title": "Lesson 12: One Mean T-Test",
    "section": "",
    "text": "⏰ Due 0700 ET on Lesson 13\n\nDay 1: Wednesday, 24 Sept 2025\n\nDay 2: Thursday, 25 Sept 2025\n\n\n📑 Worksheet: https://westpoint.instructure.com/courses/10295/assignments/216497 — don’t sleep on this!\nWill need applet for simulation\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12/120\n10:37\n\n\n\n\n\n  Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#one-mean-t-test",
    "href": "MA206-AY26-1/lesson-12.html#one-mean-t-test",
    "title": "Lesson 12: One Mean T-Test",
    "section": "",
    "text": "For all cases:\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.\n\n\n\n\nWe want to test whether Male cadets (Don’t worry ladies, we’ll get to you) are shorter than the average height of U.S. men aged 19–24, which is 72 inches.\n\n\n\\[\nH_0 : \\mu = 72\n\\]\n\\[\nH_A : \\mu &lt; 72\n\\]\n\n\n\n\nheights &lt;- c(70, 71, 69, 73, 68, 74, 71, 70, 72, 69, 70, 71, 68, 73)\n\n\n\n\n\n\n\nIs This a Good Sample?\n\n\n\nWe measured the heights of 14 cadets in this class.\nBut our research question is about all U.S. men aged 19–24.\n\nDo cadets in this class represent the broader population (what is that population)?\n\nWhat kinds of biases might be present?\n\nIf the sample isn’t random, how should that affect our conclusions?\n\n\n\n\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\qquad df = n - 1\n\\]\n\\[\n\\begin{aligned}\n\\bar{x} & = \\text{sample mean} \\\\[4pt]\n\\mu_0   & = \\text{hypothesized mean (72)} \\\\[4pt]\ns       & = \\text{sample standard deviation} \\\\[4pt]\nn       & = \\text{sample size} \\\\[4pt]\nt       & = \\text{test statistic}\n\\end{aligned}\n\\]\n\n\n\n\nn     &lt;- length(heights)\nxbar  &lt;- mean(heights)\ns     &lt;- sd(heights)\n\nc(n = n, mean = round(xbar, 2), sd = round(s, 2))\n\n    n  mean    sd \n14.00 70.64  1.86 \n\n\n\n\n\n\nmu0 &lt;- 72\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\n\n[1] -2.722848\n\n\n\n\n\n\ndf &lt;- n - 1\n\nggplot() +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  labs(title = \"t-Distribution with df = 13\")\n\n\n\n\n\n\n\n\nAdd the observed test statistic:\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, t_stat),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, color = \"firebrick\", linewidth = 1.2) +\n  labs(title = \"Left-tail Shaded: t-Distribution with Test Statistic\")\n\n\n\n\n\n\n\n\n\n\n\n\np_val &lt;- pt(t_stat, df = df)   \np_val\n\n[1] 0.008708986\n\n\n\n\n\nWith p ≈ .008, we do have evidence that male cadets are shorter than 72 inches.\n\n\n\n\nWe want to test whether female cadets are taller than the average height of U.S. women aged 19–24, which is 64 inches.\n\n\n\\[\nH_0 : \\mu = 64\n\\]\n\\[\nH_A : \\mu &gt; 64\n\\]\n\n\n\n\nheights &lt;- c(65, 67, 66, 64)\n\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\qquad df = n - 1\n\\]\n\n\n\n\nn     &lt;- length(heights)\nxbar  &lt;- mean(heights)\ns     &lt;- sd(heights)\n\nc(n = n, mean = round(xbar, 2), sd = round(s, 2))\n\n    n  mean    sd \n 4.00 65.50  1.29 \n\n\n\n\n\n\nmu0 &lt;- 64\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\n\n[1] 2.32379\n\n\n\n\n\n\ndf &lt;- n - 1\n\nggplot() +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  labs(title = \"t-Distribution with df = 3\")\n\n\n\n\n\n\n\n\nAdd the observed test statistic and shade the right tail:\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(t_stat, 4),\n                geom = \"area\", fill = \"darkred\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, color = \"firebrick\", linewidth = 1.2) +\n  labs(title = \"Right-tail Shaded: t-Distribution with Test Statistic\")\n\n\n\n\n\n\n\n\n\n\n\n\np_val &lt;- 1 - pt(t_stat, df = df)   # one-tailed, greater than\np_val\n\n[1] 0.05136404\n\n\n\n\n\nWith p ≈ .051, we do not have strong evidence that female cadets are taller than 64 inches.\n\n\n\n\nSuppose the national average height of men aged 19–24 is 72 inches.\nWe want to test whether West Point Department of Math instructors are different (either taller or shorter).\n\n\n\\[\nH_0 : \\mu = 72\n\\]\n\\[\nH_A : \\mu \\neq 72\n\\]\n\n\n\nHere is a sample of heights (in inches) from 28 Math instructors:\n\nheights &lt;- c(71, 70, 73, 72, 74, 69, 71, 72, 70, 73, 72, 71, 75, 70, 72, 74, 71, 69, 73, 72, 70, 71, 74, 72, 70, 73, 71, 72)\n\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\qquad df = n - 1\n\\]\n\n\n\n\nn     &lt;- length(heights)\nxbar  &lt;- mean(heights)\ns     &lt;- sd(heights)\n\nc(n = n, mean = round(xbar, 2), sd = round(s, 2))\n\n    n  mean    sd \n28.00 71.68  1.56 \n\n\n\n\n\n\nmu0 &lt;- 72\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\nt_stat\n\n[1] -1.086979\n\n\n\n\n\n\ndf &lt;- n - 1\n\nggplot() +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  labs(title = paste(\"t-Distribution with df =\", df))\n\n\n\n\n\n\n\n\nAdd the observed test statistic and shade both tails:\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, -abs(t_stat)),\n                geom = \"area\", fill = \"steelblue\", alpha = 0.5) +\n  stat_function(fun = dt, args = list(df = df), xlim = c(abs(t_stat), 4),\n                geom = \"area\", fill = \"steelblue\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, color = \"firebrick\", linewidth = 1.2) +\n  geom_vline(xintercept = -t_stat, color = \"firebrick\", linewidth = 1.2) +\n  labs(title = \"Two-tailed Shaded: t-Distribution with Test Statistic\")\n\n\n\n\n\n\n\n\n\n\n\n\np_val &lt;- 2 * (1 - pt(abs(t_stat), df = df))   # two-tailed\np_val\n\n[1] 0.286655\n\n\n\n\n\nWith p ≈ .29, we do not have evidence that Math instructors’ heights differ from 72 inches.\n\n\n\n\nOverlay several \\(t\\) distributions with different degrees of freedom and the standard normal \\(N(0,1)\\) for comparison.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#board-problems",
    "href": "MA206-AY26-1/lesson-12.html#board-problems",
    "title": "Lesson 12: One Mean T-Test",
    "section": "",
    "text": "Nationally, the average college student drinks about \\(3.2\\) cups of coffee per day.\nYou suspect cadets might consume a different amount (not necessarily more or less).\nYou collect a sample of \\(12\\) cadets with the following self-reported daily coffee consumption (in cups):\n\ncoffee &lt;- c(2.5, 3.0, 3.8, 4.1, 3.2, 2.9, 3.6, 4.0, 3.3, 2.7, 3.5, 3.9)\n\nTasks:\n\nState the null and alternative hypotheses.\n\nCompute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n\nWrite down the test statistic formula for a one-sample \\(t\\)-test.\n\nCalculate the test statistic.\n\nFind the \\(p\\)-value for the appropriate two-tailed test.\n\nState your conclusion in the context of the problem.\n\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses\n\\[\nH_0: \\mu = 3.2 \\qquad\\text{vs}\\qquad H_A: \\mu \\neq 3.2\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(coffee)\nxbar &lt;- mean(coffee)\ns    &lt;- sd(coffee)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n12.000  3.375  0.528 \n\n\nNumerically: \\(n = 12\\), \\(\\bar{x} \\approx 3.375\\), \\(s \\approx 0.528\\).\n3–4. Test statistic\n\nmu0   &lt;- 3.2\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n 1.149 11.000 \n\n\nNumerically: \\(t \\approx 1.149\\) with \\(df = 11\\).\n5. Two-tailed \\(p\\)-value\n\np_val &lt;- 2 * (1 - pt(abs(t_stat), df = df))\np_val\n\n[1] 0.2749615\n\n\nNumerically: \\(p \\approx 0.275\\).\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  # Left tail shading\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, -abs(t_stat)),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Right tail shading\n  stat_function(fun = dt, args = list(df = df), xlim = c(abs(t_stat), 4),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Overlay t density\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  # Vertical lines at ±t_stat\n  geom_vline(xintercept = c(-abs(t_stat), abs(t_stat)),\n             color = \"firebrick\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(title = \"Two-tailed Shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")\n\n\n\n\n\n\n\n\n6. Conclusion\nWith \\(p \\approx 0.275\\), we do not have evidence that cadets’ average coffee consumption differs from \\(3.2\\) cups per day.\n\n\n\n\n\n\n\n\nA recent campus wellness report suggests the average college student spends \\(3.0\\) hours per day on recreational screen time (not including coursework).\nYou suspect students in your section spend more than that.\nYou collect a sample of \\(12\\) students with the following daily screen-time values (in hours):\n\nscreen_time &lt;- c(2.5, 3.0, 3.1, 3.2, 3.3, 3.4, 3.8, 3.7, 3.8, 3.2, 3.5, 3.5)\n\nTasks: 1. State the null and alternative hypotheses.\n2. Compute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n3. Write down the test statistic formula for a one-sample \\(t\\)-test.\n4. Calculate the test statistic.\n5. Find the one-tailed \\(p\\)-value for the “greater than” test.\n6. State your conclusion in context.\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses \\[\nH_0: \\mu = 3.0\n\\qquad\\text{vs}\\qquad\nH_A: \\mu &gt; 3.0\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(screen_time)\nxbar &lt;- mean(screen_time)\ns    &lt;- sd(screen_time)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n12.000  3.333  0.373 \n\n\n3–4. Test statistic\n\nmu0    &lt;- 3.0\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n   3.1   11.0 \n\n\n5. One-tailed \\(p\\)-value (\\(H_A: \\mu &gt; \\mu_0\\))\n\np_val &lt;- 1 - pt(t_stat, df = df)\np_val\n\n[1] 0.005056453\n\n\n6. Conclusion\nWith the computed \\(t\\) and \\(p\\) above, interpret whether there is evidence that average daily recreational screen time in this section exceeds \\(3.0\\) hours.\n\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  stat_function(fun = dt, args = list(df = df), xlim = c(t_stat, 4),\n                geom = \"area\", alpha = 0.5) +\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  geom_vline(xintercept = t_stat, linewidth = 1.2) +\n  labs(title = \"Right-tail shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\nPublic health guidelines recommend \\(7.0\\) hours of sleep on weeknights.\nYou suspect students in your section average less than that.\nYou collect a sample of \\(15\\) students’ self-reported weeknight sleep (in hours):\n\nsleep &lt;- c(6.6, 6.9, 7.1, 6.8, 7.0, 6.7, 6.5, 6.8, 6.9, 6.4, 6.6, 7.2, 6.7, 6.8, 6.5)\n\nTasks: 1. State the null and alternative hypotheses.\n2. Compute the sample mean \\(\\bar{x}\\), standard deviation \\(s\\), and sample size \\(n\\).\n3. Write down the test statistic formula for a one-sample \\(t\\)-test.\n4. Calculate the test statistic.\n5. Find the one-tailed \\(p\\)-value for the “less than” test.\n6. State your conclusion in context.\n\n\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\n\n\nSolution (click to expand)\n\n\n\n\n\n1. Hypotheses \\[\nH_0: \\mu = 7.0\n\\qquad\\text{vs}\\qquad\nH_A: \\mu &lt; 7.0\n\\]\n2. Descriptive statistics (R)\n\nn    &lt;- length(sleep)\nxbar &lt;- mean(sleep)\ns    &lt;- sd(sleep)\nc(n = n, mean = round(xbar, 3), sd = round(s, 3))\n\n     n   mean     sd \n15.000  6.767  0.229 \n\n\n3–4. Test statistic\n\nmu0    &lt;- 7.0\nt_stat &lt;- (xbar - mu0) / (s / sqrt(n))\ndf     &lt;- n - 1\nc(t_stat = round(t_stat, 3), df = df)\n\nt_stat     df \n-3.949 14.000 \n\n\n5. One-tailed \\(p\\)-value (\\(H_A: \\mu &lt; \\mu_0\\))\n\np_val &lt;- pt(t_stat, df = df)\np_val\n\n[1] 0.0007279758\n\n\n6. Conclusion\nReport the computed \\(t\\), \\(df\\), and \\(p\\), then conclude whether there is evidence that average weeknight sleep is less than \\(7.0\\) hours.\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  # Left tail shading up to t_stat\n  stat_function(fun = dt, args = list(df = df), xlim = c(-4, t_stat),\n                geom = \"area\", fill = \"darkblue\", alpha = 0.5) +\n  # Overlay t density\n  geom_function(fun = dt, args = list(df = df), xlim = c(-4, 4)) +\n  # Vertical line at t_stat\n  geom_vline(xintercept = t_stat,\n             color = \"firebrick\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(title = \"Left-tail Shaded: t-Distribution with Test Statistic\",\n       x = \"t\", y = \"Density\")",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-12.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-12.html#before-you-leave",
    "title": "Lesson 12: One Mean T-Test",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nProject Milestone 3: Due Canvas 22 Sept\nExploration Exercise 1.5: Due at 0700 on Lesson 13\n24 September 2025 for Day 1\n\n25 September 2025 for Day 2)\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 12"
    ]
  },
  {
    "objectID": "MA206-AY26-1/index.html",
    "href": "MA206-AY26-1/index.html",
    "title": "MA206: Probability and Statistics",
    "section": "",
    "text": "AY26-1 | United States Military Academy\n\n\n\n\n\n\n\n\n Canvas Grades & Assignments \n\n\n\n\n Course Guide Admin & R Code \n\n\n\n\n Textbook Intro to Statistical Investigations \n\n\n\n\n Calendar Day 1  |  Day 2 \n\n\n\n\n\n\n\n\n\nLTC Dusty Turner, PhD Academy Professor, Department of Mathematical Sciences\n\nOffice: Thayer Hall\nEmail: dusty.turner@westpoint.edu\nWebsite: dustysturner.com\n\nEmail to coordinate Additional Instruction.\n\n\n\n\n\n\nThis site contains instructor notes and lesson resources for MA206. Use the sidebar to navigate through lessons, or visit the links above for official course materials.\n\n\n\n\nTidyverse Tutorial Here is the project quarto file we worked through in class as a starting point",
    "crumbs": [
      "MA206-AY26-1",
      "Course Information"
    ]
  },
  {
    "objectID": "MA206-AY26-1/index.html#quick-links",
    "href": "MA206-AY26-1/index.html#quick-links",
    "title": "MA206: Probability and Statistics",
    "section": "",
    "text": "Canvas Grades & Assignments \n\n\n\n\n Course Guide Admin & R Code \n\n\n\n\n Textbook Intro to Statistical Investigations \n\n\n\n\n Calendar Day 1  |  Day 2",
    "crumbs": [
      "MA206-AY26-1",
      "Course Information"
    ]
  },
  {
    "objectID": "MA206-AY26-1/index.html#instructor",
    "href": "MA206-AY26-1/index.html#instructor",
    "title": "MA206: Probability and Statistics",
    "section": "",
    "text": "LTC Dusty Turner, PhD Academy Professor, Department of Mathematical Sciences\n\nOffice: Thayer Hall\nEmail: dusty.turner@westpoint.edu\nWebsite: dustysturner.com\n\nEmail to coordinate Additional Instruction.",
    "crumbs": [
      "MA206-AY26-1",
      "Course Information"
    ]
  },
  {
    "objectID": "MA206-AY26-1/index.html#about-this-site",
    "href": "MA206-AY26-1/index.html#about-this-site",
    "title": "MA206: Probability and Statistics",
    "section": "",
    "text": "This site contains instructor notes and lesson resources for MA206. Use the sidebar to navigate through lessons, or visit the links above for official course materials.",
    "crumbs": [
      "MA206-AY26-1",
      "Course Information"
    ]
  },
  {
    "objectID": "MA206-AY26-1/index.html#resources",
    "href": "MA206-AY26-1/index.html#resources",
    "title": "MA206: Probability and Statistics",
    "section": "",
    "text": "Tidyverse Tutorial Here is the project quarto file we worked through in class as a starting point",
    "crumbs": [
      "MA206-AY26-1",
      "Course Information"
    ]
  },
  {
    "objectID": "MA206-AY26-1/assets/files/Exploration_2_3_solution_updated.html",
    "href": "MA206-AY26-1/assets/files/Exploration_2_3_solution_updated.html",
    "title": "MA206 AY26-1 Exploration Exercise 2.3 — Sleepless Nights? (Answer Key)",
    "section": "",
    "text": "Setup\n\nDataset: sleep &lt;- ma206data::sleep.\nVariable of interest: hours (units: hours).\n\n\n\n\nSTEP 1: Ask a research question\nQ1. Population of interest, parameter (in words), parameter (symbol)\n- Population of interest: All Cadets at West Point\n- Parameter (in words): The mean number of hours of sleep Cadets got last night.\n- Parameter (symbol): ( ).\n\n\n\nSTEP 2: Design a study and collect data\nQ2. Null hypothesis (words + symbols)\n- Null hypothesis (words): The population mean sleep time is 6.25 hours.\n- Symbol: ( H_0: = 6.25 ).\nQ3. Alternative hypothesis (words + symbols)\n- Alternative hypothesis (words): The population mean sleep time is less than 6.25 hours.\n- Symbol: ( H_a: &lt; 6.25 ).\nQ4. Sampling plan (n = 75)\nSelect a simple random sample of 75 Cadets from the entire Corps so that every Cadet has an equal chance to be selected.\nQ5. Variable measured\nMeasure hours of sleep last night. This is a quantitative variable (measured in hours).\nQ6. Representativeness (good)\nAnswers will vary.\nQ7. Representativeness (not good) + bias/unbiased fill-in\n- Not representative: The class is a convenience sample from one course and instructor on a single day. It may over‑ or under‑represent certain groups.\n- Fill‑in: Convenience sampling may be biased, whereas simple random sampling is unbiased.\n\n\n\nSTEP 3: Explore the data\nQ8. Make a histogram of the hours variable.\n\n\n\n\n\n\n\n\n\nQ9. Compute the sample statistics (mean, SD, n). Describe histogram.\n\n\n\n\n\nn\nmean_hours\nsd_hours\n\n\n\n\n51\n6.0098\n1.116\n\n\n\n\n\n\nShape: Looks fairly normal / gaussian / bell-shaped\nCenter: sample mean ( {x} = ) 6.01 hours.\n\nVariability: ( s = ) 1.12 hours.\n\nOutliers: perhaps the one that’s under 4 and maybe the one that’s above 8 hours.\n\nQ10. Preliminary evidence that mean &lt; 6.25?\nIf ( {x} &lt; 6.25 ) and histogram mass is below 6.25, that suggests preliminary evidence, but we should still run the test: inference is needed.\n\n\n\nSTEP 4: Draw inference beyond the data\nQ11. If ( H_0 ) is true, how do sample means behave?\nThey would be approximately normally distributed (for large n), centered at 6.25 with SD = ( / ). Since ( ) is unknown, we use ( s ).\nQ12. Calculate ( s/ ) (standard error of null distribution).\n\n\n[1] 0.1562365\n\n\nQ13. Is the observed mean surprising under ( H_0: )?\n\n\n\n\n\nxbar\nmu0\ndiff\nsd_null\nstandardized_diff\n\n\n\n\n6.0098\n6.25\n-0.2402\n0.1562\n-1.5377\n\n\n\n\n\nIf the standardized difference is large in magnitude, it suggests the sample mean is surprising under the null.\nQ14. Calculate the t‑statistic and p‑value for ( H_a: &lt; 6.25 ).\n\n\n\n\n\nt_statistic\np_value\n\n\n\n\n-1.537\n0.06525\n\n\n\n\n\n\nReport ( t ) (3 decimals) and ( p-value ) (5 decimals).\n\nConclusion: Since the ( p-value ) is not less than the ( = 0.05 ) significance level we conclude there is insufficient evidence to reject ( H_0 ) that Cadets sleep less than 6.25 hours.\n\n\n\n\nSTEP 5: Formulate conclusions\nQ15. Can we generalize?\nBecause the data come from three sections of one instructor’s course on one day, it is a convenience sample. Results may not generalize to the entire Corps. Use caution!\n\n\n\nSTEP 6: Look back and ahead\nQ16. Concerns about design and conclusions\n- One‑night snapshot may not represent typical sleep.\n- Convenience sampling, not random.\n- Limited to one instructor’s sections.\n- Self‑reported hours may have error.\n- Practical vs. statistical significance.\nQ17. Next steps\n- Use random or stratified random sampling.\n- Collect across multiple nights, sections, majors.\n- Consider collecting covariates (major, class, workload, etc).\n\n\n\nExploring Further\nQ18. If you rejected ( H_0 ), does that prove ( &lt; 6.25 )?\nNo. Hypothesis tests provide evidence, not proof. Results are subject to sampling error and model assumptions.\nQ19. If you rejected ( H_0: = 6.5 ), does that prove ( )?\nNo. Rejection means the observed data are inconsistent with ( H_0 ), but we cannot prove the true mean. Confidence intervals provide interval estimates rather than definitive proof."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_2_3.html",
    "href": "MA206-AY26-1/assets/files/EE_2_3.html",
    "title": "Exploration Exercise 2.3",
    "section": "",
    "text": "Dataset: sleep &lt;- ma206data::sleep.\nVariable of interest: hours (units: hours)."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_2_3.html#setup",
    "href": "MA206-AY26-1/assets/files/EE_2_3.html#setup",
    "title": "Exploration Exercise 2.3",
    "section": "",
    "text": "Dataset: sleep &lt;- ma206data::sleep.\nVariable of interest: hours (units: hours)."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_2_3.html#step-1-ask-a-research-question",
    "href": "MA206-AY26-1/assets/files/EE_2_3.html#step-1-ask-a-research-question",
    "title": "Exploration Exercise 2.3",
    "section": "STEP 1: Ask a research question",
    "text": "STEP 1: Ask a research question\nQ1. Population of interest, parameter (in words), parameter (symbol)\n- Population of interest: All Cadets at West Point\n- Parameter (in words): The mean number of hours of sleep Cadets got last night.\n- Parameter (symbol): \\(\\mu\\)."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_2_3.html#step-2-design-a-study-and-collect-data",
    "href": "MA206-AY26-1/assets/files/EE_2_3.html#step-2-design-a-study-and-collect-data",
    "title": "Exploration Exercise 2.3",
    "section": "STEP 2: Design a study and collect data",
    "text": "STEP 2: Design a study and collect data\nQ2. Null hypothesis (words + symbols)\n- Null hypothesis (words): The population mean sleep time is 6.25 hours.\n- Symbol: \\(H_0: \\mu = 6.25\\).\nQ3. Alternative hypothesis (words + symbols)\n- Alternative hypothesis (words): The population mean sleep time is less than 6.25 hours.\n- Symbol: \\(H_a: \\mu &lt; 6.25\\).\nQ4. Sampling plan (n = 75)\nSelect a simple random sample of 75 Cadets from the entire Corps so that every Cadet has an equal chance to be selected.\nQ5. Variable measured\nMeasure hours of sleep last night. This is a quantitative variable (measured in hours).\nQ6. Representativeness (good)\nAnswers will vary.\nQ7. Representativeness (not good) + bias/unbiased fill-in\n- Not representative: The class is a convenience sample from one course and instructor on a single day. It may over‑ or under‑represent certain groups.\n- Fill‑in: Convenience sampling may be biased, whereas simple random sampling is unbiased."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_2_3.html#step-3-explore-the-data",
    "href": "MA206-AY26-1/assets/files/EE_2_3.html#step-3-explore-the-data",
    "title": "Exploration Exercise 2.3",
    "section": "STEP 3: Explore the data",
    "text": "STEP 3: Explore the data\nQ8. Make a histogram of the hours variable.\n\nsleep %&gt;% \n  ggplot(aes(x = hours)) +\n  geom_histogram(binwidth = 0.5, color = \"black\", fill = \"gray\") +\n  labs(title = \"Sleep Hours (Last Night)\",\n       x = \"Hours\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nQ9. Compute the sample statistics (mean, SD, n). Describe histogram.\n\nn    &lt;- nrow(sleep)\nxbar &lt;- mean(sleep$hours)\ns    &lt;- sd(sleep$hours)\n\nkable(tibble(\n  n = n,\n  mean_hours = round(xbar,digits=4),\n  sd_hours = round(s,digits=3)\n))\n\n\n\n\nn\nmean_hours\nsd_hours\n\n\n\n\n51\n6.0098\n1.116\n\n\n\n\n\n\nShape: Looks fairly normal / gaussian / bell-shaped\nCenter: sample mean \\(\\bar{x} =\\) 6.01 hours.\n\nVariability: \\(s =\\) 1.12 hours.\n\nOutliers: perhaps the one that’s under 4 and maybe the one that’s above 8 hours.\n\nQ10. Preliminary evidence that mean &lt; 6.25?\nIf \\(\\bar{x} &lt; 6.25\\) and histogram mass is below 6.25, that suggests preliminary evidence, but we should still run the test: inference is needed."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_2_3.html#step-4-draw-inference-beyond-the-data",
    "href": "MA206-AY26-1/assets/files/EE_2_3.html#step-4-draw-inference-beyond-the-data",
    "title": "Exploration Exercise 2.3",
    "section": "STEP 4: Draw inference beyond the data",
    "text": "STEP 4: Draw inference beyond the data\nQ11. If \\(H_0\\) is true, how do sample means behave?\nThey would be approximately normally distributed (for large n), centered at 6.25 with SD = \\(\\sigma/\\sqrt{n}\\). Since \\(\\sigma\\) is unknown, we use \\(s\\).\nQ12. Calculate \\(\\frac{s}{\\sqrt{n}}\\) (standard error of null distribution).\n\nsd_null &lt;- s / sqrt(n)\nsd_null\n\n[1] 0.1562365\n\n\nQ13. Is the observed mean surprising under \\(H_0: \\mu=6.25\\)?\n\nnull &lt;- 6.25\ndiff_from_null &lt;- xbar - null\nkable(tibble(\n  xbar = round(xbar,digits=4),\n  mu0 = round(null,digits=4),\n  diff = round(diff_from_null,digits=4),\n  sd_null = round(sd_null,digits=4),\n  standardized_diff = round(diff_from_null / sd_null, digits=4)\n))\n\n\n\n\nxbar\nmu0\ndiff\nsd_null\nstandardized_diff\n\n\n\n\n6.0098\n6.25\n-0.2402\n0.1562\n-1.5377\n\n\n\n\n\nIf the standardized difference is large in magnitude, it suggests the sample mean is surprising under the null.\nQ14. Calculate the t‑statistic and p‑value for \\(H_a: \\mu &lt; 6.25\\).\n\nt_stat &lt;- (xbar - null) / sd_null\np_val  &lt;- pt(t_stat, df = n - 1)\n\nkable(tibble(\n  t_statistic = sprintf(\"%.3f\", t_stat),\n  p_value = sprintf(\"%.5f\", p_val)\n))\n\n\n\n\nt_statistic\np_value\n\n\n\n\n-1.537\n0.06525\n\n\n\n\n\n\nReport \\(t\\) (3 decimals) and \\(p-value\\) (5 decimals).\n\nConclusion: Since the \\(p-value\\) is not less than the \\(\\alpha = 0.05\\) significance level we conclude there is insufficient evidence to reject \\(H_0\\) that Cadets sleep less than 6.25 hours."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_2_3.html#step-5-formulate-conclusions",
    "href": "MA206-AY26-1/assets/files/EE_2_3.html#step-5-formulate-conclusions",
    "title": "Exploration Exercise 2.3",
    "section": "STEP 5: Formulate conclusions",
    "text": "STEP 5: Formulate conclusions\nQ15. Can we generalize?\nBecause the data come from three sections of one instructor’s course on one day, it is a convenience sample. Results may not generalize to the entire Corps. Use caution!"
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_2_3.html#step-6-look-back-and-ahead",
    "href": "MA206-AY26-1/assets/files/EE_2_3.html#step-6-look-back-and-ahead",
    "title": "Exploration Exercise 2.3",
    "section": "STEP 6: Look back and ahead",
    "text": "STEP 6: Look back and ahead\nQ16. Concerns about design and conclusions\n- One‑night snapshot may not represent typical sleep.\n- Convenience sampling, not random.\n- Limited to one instructor’s sections.\n- Self‑reported hours may have error.\n- Practical vs. statistical significance.\nQ17. Next steps\n- Use random or stratified random sampling.\n- Collect across multiple nights, sections, majors.\n- Consider collecting covariates (major, class, workload, etc)."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_2_3.html#exploring-further",
    "href": "MA206-AY26-1/assets/files/EE_2_3.html#exploring-further",
    "title": "Exploration Exercise 2.3",
    "section": "Exploring Further",
    "text": "Exploring Further\nQ18. If you rejected \\(H_0\\), does that prove \\(\\mu &lt; 6.25\\)?\nNo. Hypothesis tests provide evidence, not proof. Results are subject to sampling error and model assumptions.\nQ19. If you rejected \\(H_0: \\mu = 6.5\\), does that prove \\(\\mu \\neq 6.5\\)?\nNo. Rejection means the observed data are inconsistent with \\(H_0\\), but we cannot prove the true mean. Confidence intervals provide interval estimates rather than definitive proof."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LTC Dusty Turner",
    "section": "",
    "text": "Department of Mathematical Sciences United States Military Academy"
  },
  {
    "objectID": "index.html#courses",
    "href": "index.html#courses",
    "title": "LTC Dusty Turner",
    "section": "Courses",
    "text": "Courses\n\n\n\n MA206-AY26-1 Probability & StatisticsFall 2025 \n\n\n\n\n MA206x-AY26-2 Probability & StatisticsSpring 2026"
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "LTC Dusty Turner",
    "section": "Instructor",
    "text": "Instructor\nLTC Dusty Turner, PhD Academy Professor, Department of Mathematical Sciences\n\nOffice: Thayer Hall\nEmail: dusty.turner@westpoint.edu\nWebsite: dustysturner.com"
  },
  {
    "objectID": "MA206-AY26-1/assets/files/AY26-1_Tidyverse-Lab.html",
    "href": "MA206-AY26-1/assets/files/AY26-1_Tidyverse-Lab.html",
    "title": "MA206 Tidyverse Lab",
    "section": "",
    "text": "Save This File\nSave this .Rmd file with the title Lastname_Firstname_Tidyverse-Lab.Rmd into your RStudio folder.\n\n\nImporting Libraries\n\n\nThe Research Question\nType your research question here.\n\n\nSummary of the ________ Data Set\nInsert your data set name above and type your summary here. You will fill out the table shortly.\n\n\nThe Data Set\nName and load your data set below.\n\n\n# A tibble: 6 × 10\n  Education       Sex   Occupation   Age Earnings MaritalStatus Race  FamilySize\n  &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;      &lt;dbl&gt;\n1 Bachelors       M     40: Offic…    49   220000 Married       White          5\n2 Some College/A… F     53: Never…    51        0 Married       White          5\n3 Less than HS    F     39: Retai…    20     8000 Never Married White          5\n4 Less than HS    M     8: Comput…    16     4000 Never Married White          5\n5 Less than HS    F     53: Never…    80        0 Widowed       White          5\n6 Less than HS    M     32: Chefs…    27    17350 Never Married Black          2\n# ℹ 2 more variables: FamilyMakeup &lt;chr&gt;, Age_squared &lt;dbl&gt;\n\n\nThe output of the above code, in conjunction with any provided data dictionary, should enable you to complete the table below. Remove the information from the wage data set and use your own.\n\n\n\n\nVariable\nColumn Name\nUnits\nVariable Type\n\n\n\n\nEducation\nEducation\nN/A\nCategorical\n\n\nSex\nSex\nN/A\nCategorical\n\n\nOccupation\nOccupation\nN/A\nCategorical\n\n\nAge\nAge\nYears\nQuantitative\n\n\nEarnings\nEarnings\nDollars\nQuantitative\n\n\nMarital Status\nMaritalStatus\nN/A\nCategorical\n\n\nRace\nRace\nN/A\nCategorical\n\n\nFamily Size\nFamilySize\nN/A\nCategorical\n\n\nFamily Makeup\nFamilyMakeup\nN/A\nCategorical\n\n\nAge Squared\nAge_squared\nYears\nQuantitative\n\n\n\n\n\n\nPractice\nUse the below space to practice calling, selecting, filtering, summarizing, grouping by, and mutating variables.\n\n\n[1] 49 51 20 16 80 27\n\n\n\n\n# A tibble: 180,084 × 2\n     Age Earnings\n   &lt;dbl&gt;    &lt;dbl&gt;\n 1    49   220000\n 2    51        0\n 3    20     8000\n 4    16     4000\n 5    80        0\n 6    27    17350\n 7    24    12000\n 8    62    25480\n 9    70        0\n10    53     6000\n# ℹ 180,074 more rows\n\n\n\n\n# A tibble: 37,174 × 10\n   Education    Sex   Occupation     Age Earnings MaritalStatus Race  FamilySize\n   &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;      &lt;dbl&gt;\n 1 Bachelors    M     40: Office …    49   220000 Married       White          5\n 2 Bachelors    M     31: Animal …    62    25480 Never Married White          1\n 3 Bachelors    M     8: Computer…    52    70200 Married       Asian          6\n 4 Less than HS M     53: Never W…    50        0 Married       White          3\n 5 Less than HS M     53: Never W…    62        0 Married       White          2\n 6 Less than HS M     51: Transpo…    55    40000 Never Married White          1\n 7 Less than HS M     49: Product…    51    83000 Married       White          3\n 8 Less than HS M     40: Office …    47    35000 Married       White          4\n 9 Less than HS M     38: Retail …    70        0 Married       White          2\n10 Less than HS M     3: Educatio…    62        0 Divorced      White          1\n# ℹ 37,164 more rows\n# ℹ 2 more variables: FamilyMakeup &lt;chr&gt;, Age_squared &lt;dbl&gt;\n\n\n\n\n# A tibble: 1 × 1\n    avg\n  &lt;dbl&gt;\n1  37.0\n\n\n\n\n# A tibble: 2 × 2\n  Sex     ave\n  &lt;chr&gt; &lt;dbl&gt;\n1 F      37.9\n2 M      36.0\n\n\n\n\n# A tibble: 180,084 × 1\n   weird_age\n       &lt;dbl&gt;\n 1        98\n 2       102\n 3        40\n 4        32\n 5       160\n 6        54\n 7        48\n 8       124\n 9       140\n10       106\n# ℹ 180,074 more rows\n\n\n\n\n\nExplore Your Variables\n\n\n\nResponse Variable\nType the name, description, and units of your response variable here. Remember that this is a quantitative variable.\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0   24813   35000 1609999 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn 1-2 sentences, describe this variable’s data. Which visualization is better, and why? Are there any questions that you have after exploring? Add code chunks below if you’d like to do some more exploration.\n\n\nQuantitative Explantory Variable #1\nType the name, description, and units of your quantitative variable here.\n\n\n# A tibble: 1 × 3\n    mean      s      n\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;\n1 24813. 54264. 180084\n\n\nIn 1-2 sentences, describe this variable’s data. Which visualization is better, and why? Are there any questions that you have after exploring? Add code chunks below if you’d like to do some more exploration.\n\n\nQuantitative Explantory Variable #2\nType the name, description, and units of your quantitative variable here.\nIn 1-2 sentences, describe this variable’s data. Which visualization is better, and why? Are there any questions that you have after exploring? Add code chunks below if you’d like to do some more exploration.\n\n\nCategorical Explanatory Variable #1\nType the name, description, and units of your categorical variable here. Remember that this will require different code than your quantitative variables.\n\n\n\n    F     M \n92693 87391 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn 1-2 sentences, describe this variable’s data. Which visualization is better, and why? Are there any questions that you have after exploring? Add code chunks below if you’d like to do some more exploration.\n\n\nCategorical Explanatory Variable #2\nType the name, description, and units of your categorical variable here. Remember that this will require different code than your quantitative variables.\nIn 1-2 sentences, describe this variable’s data. Which visualization is better, and why? Are there any questions that you have after exploring? Add code chunks below if you’d like to do some more exploration.\n\n\nHow are the variables associated?\nUsing ggplot, create visualizations that show relationships between your variables below. Since you have five variables, you will need at minimum four plots so that each variable is visualized at least once. It is possible to display relationships between 3+ variables in one plot; at least one of your plots should demonstrate mastery of this skill. Create more code chunks as needed.\n\n\n\nFinish the tutorial\n\nTest your skills by working through the code after the ggplot section of the Tutorial. These examples will help you gain a basic understanding of what is happening with specific commands or data structures within R, which will be useful to you over the course of the semester. Create more code chunks as needed.\n\n\n\nGetting Ready to Submit!\n\nNow that you’re done, you need to save this file (if the title is red, it has unsaved changes). RStudio does NOT autosave while you work, so CTRL+S early and often. Next, press the Knit button up top with the yarn icon. This will create an HTML file, because that was specified in the header. Save your HTML file with the name Lastname_Firstname_Tidyverse-Lab.html. Then, open the HTML file and print, using the `Microsoft Print to PDF\" option to save asLastname_Firstname_Tidyverse-Lab.pdf`. This PDF file is what you will submit on Canvas for Milestone 2."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html",
    "title": "Exploration Exercise 3.2",
    "section": "",
    "text": "This exercise uses data from the 2024 General Social Survey (GSS), in which 2,015 adult Americans were surveyed and 31.1% reported living in a different state from where they were born.\nWe treat this as a categorical variable (“different state” vs. “same state”) and use z-tests and normal approximations to the binomial."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#overview",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#overview",
    "title": "Exploration Exercise 3.2",
    "section": "",
    "text": "This exercise uses data from the 2024 General Social Survey (GSS), in which 2,015 adult Americans were surveyed and 31.1% reported living in a different state from where they were born.\nWe treat this as a categorical variable (“different state” vs. “same state”) and use z-tests and normal approximations to the binomial."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#question-1-identify-population-and-sample",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#question-1-identify-population-and-sample",
    "title": "Exploration Exercise 3.2",
    "section": "Question 1 – Identify Population and Sample",
    "text": "Question 1 – Identify Population and Sample\nQ1. Identify the population and sample in this survey.\nPopulation: All adult Americans aged 18 and older.\nSample: The 2,015 adults surveyed in the 2024 GSS."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#question-2-representativeness",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#question-2-representativeness",
    "title": "Exploration Exercise 3.2",
    "section": "Question 2 – Representativeness",
    "text": "Question 2 – Representativeness\nQ2. Is it reasonable to believe that the sample of 2,015 adult Americans is representative of the population of all adult Americans? Justify your answer in terms of how the data were collected.\nIt is reasonable to believe the sample is representative if the GSS used random selection.\nThe GSS employs probability-based sampling across U.S. households, which supports generalization to the broader adult U.S. population."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#question-3-statistic-or-parameter",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#question-3-statistic-or-parameter",
    "title": "Exploration Exercise 3.2",
    "section": "Question 3 – Statistic or Parameter",
    "text": "Question 3 – Statistic or Parameter\nQ3. Is the value 31.1% a statistic or a parameter? Which symbol is typically used to represent this quantity?\n\n31.1% (0.311) is a sample statistic, not a parameter.\n\nThe symbol for a sample proportion is \\(\\hat{p}\\)"
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#question-4-population-parameter",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#question-4-population-parameter",
    "title": "Exploration Exercise 3.2",
    "section": "Question 4 – Population Parameter",
    "text": "Question 4 – Population Parameter\nQ4. Identify (in words) the population parameter that the General Social Survey is attempting to estimate.\nPopulation parameter is \\(\\pi\\), the true proportion of all adult Americans who currently live in a different state from where they were born."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#question-5-interpreting-the-estimate",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#question-5-interpreting-the-estimate",
    "title": "Exploration Exercise 3.2",
    "section": "Question 5 – Interpreting the Estimate",
    "text": "Question 5 – Interpreting the Estimate\nQ5. Is it reasonable to conclude that exactly 31.1% of all adult Americans currently live in a different state from where they were born? Explain why or why not.\nIt is not reasonable to conclude that exactly 31.1% of all adults live in another state.\nSampling variation means that if another random sample of 2,015 were drawn, \\(\\hat{p}\\) would likely differ.\nInstead, we estimate a plausible range for \\(\\pi\\) using confidence intervals."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#question-6-hypothesis-test-for-pi-0.362",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#question-6-hypothesis-test-for-pi-0.362",
    "title": "Exploration Exercise 3.2",
    "section": "Question 6 – Hypothesis Test for \\(\\pi = 0.362\\)",
    "text": "Question 6 – Hypothesis Test for \\(\\pi = 0.362\\)\nQ6. Although we expect \\(\\pi\\) to be close to 0.311, suppose we test\n\\(H_0: \\pi = 0.362\\) vs. \\(H_A: \\pi \\neq 0.362\\).\nUse R to calculate the standardized test statistic and the two-sided p-value. Based on your result, would you reject or fail to reject the null hypothesis at \\(\\alpha = 0.05\\)?\n\n# Given values\nn &lt;- 2015\nphat &lt;- 0.311\nnull &lt;- 0.362\n\nSD_pi &lt;- sqrt(null * (1 - null) / n)\nz_stat &lt;- (phat - null) / SD_pi\np_value &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\nSD_pi\n\n[1] 0.010706\n\nz_stat\n\n[1] -4.763685\n\np_value\n\n[1] 1.900887e-06\n\n\nInterpretation: The z-statistic measures how many standard deviations \\(\\hat{p}\\) is from the null value.\nIf the p-value &lt; 0.05, reject \\(H_0\\).\nBased on the small p-value, we reject \\(H_0\\)."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#question-7-interpret-sd-under-null",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#question-7-interpret-sd-under-null",
    "title": "Exploration Exercise 3.2",
    "section": "Question 7 – Interpret SD under Null",
    "text": "Question 7 – Interpret SD under Null\nQ7. Interpret the standard deviation under the null hypothesis that you found in Question #6. Explain, in context, what this value tells you.\n\\(SD_{\\pi}\\) or SD_pi is the expected variability in \\(\\hat{p}\\) if the population proportion were truly 0.362.\nIt tells us how much \\(\\hat{p}\\) would vary across repeated samples of size 2,015 if \\(H_0\\) were true."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#question-8-test-for-pi-0.50",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#question-8-test-for-pi-0.50",
    "title": "Exploration Exercise 3.2",
    "section": "Question 8 – Test for \\(\\pi = 0.50\\)",
    "text": "Question 8 – Test for \\(\\pi = 0.50\\)\nQ8. Now consider \\(\\pi = 0.50\\). Is this a plausible value for the population proportion \\(\\pi\\)?\nTest \\(H_0: \\pi = 0.50\\) vs. \\(H_A: \\pi \\neq 0.50\\).\nReport your test statistic, p-value, and conclusion given \\(\\alpha = 0.05\\).\n\nnull2 &lt;- 0.50\nSD_pi2 &lt;- sqrt(null2 * (1 - null2) / n)\nz_stat2 &lt;- (phat - null2) / SD_pi2\np_value2 &lt;- 2 * (1 - pnorm(abs(z_stat2)))\n\nSD_pi2\n\n[1] 0.01113865\n\nz_stat2\n\n[1] -16.96795\n\np_value2\n\n[1] 0\n\n\nConclusion: The p-value is very small, so we reject \\(H_0\\).\nA true proportion of 0.50 does not seem plausible.\nNote: the \\(p\\)-value isn’t literally zero, it’s just extremely small."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#question-9-calculate-standard-error",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#question-9-calculate-standard-error",
    "title": "Exploration Exercise 3.2",
    "section": "Question 9 – Calculate Standard Error",
    "text": "Question 9 – Calculate Standard Error\nQ9. Calculate the standard error (SE\\(_{\\hat{p}}\\)) for this study.\nHow does it compare to the SD\\(_\\pi\\) values from Questions #7 and #8? Explain why they differ.\n\nSE_phat &lt;- sqrt(phat * (1 - phat) / n)\nSE_phat\n\n[1] 0.01031222\n\n\nSE_phat differs from SD_pi because it substitutes \\(\\hat{p}\\) for \\(\\pi\\).\nIt is slightly smaller because \\(\\hat{p}=0.311\\) yields less variability than 0.50 or 0.362."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#question-10-95-confidence-interval",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#question-10-95-confidence-interval",
    "title": "Exploration Exercise 3.2",
    "section": "Question 10 – 95% Confidence Interval",
    "text": "Question 10 – 95% Confidence Interval\nQ10. Calculate and interpret a 95% confidence interval for \\(\\pi\\).\nExplain what the interval means in the context of this study.\n\nconfidence &lt;- 0.95\nalpha&lt;- 1- confidence\nz_crit_95 &lt;- qnorm(p=1-alpha/2)\nci_lower_95 &lt;- phat - z_crit_95 * SE_phat\nci_upper_95 &lt;- phat + z_crit_95 * SE_phat\nc(ci_lower_95, ci_upper_95)\n\n[1] 0.2907884 0.3312116\n\n\nInterpretation: We are 95% confident that the true proportion lies between these bounds.\nAbout 95% of such intervals from repeated samples would contain the true \\(\\pi\\)."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#question-11-99-confidence-interval",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#question-11-99-confidence-interval",
    "title": "Exploration Exercise 3.2",
    "section": "Question 11 – 99% Confidence Interval",
    "text": "Question 11 – 99% Confidence Interval\nQ11. Calculate a 99% confidence interval for \\(\\pi\\).\nCompare it to your 95% interval. How do the midpoint and margin of error change?\n\nconfidence&lt;-0.99\nalpha&lt;- 1 - confidence\nz_crit_99 &lt;- qnorm(1-alpha/2)\nci_lower_99 &lt;- phat - z_crit_99 * SE_phat\nci_upper_99 &lt;- phat + z_crit_99 * SE_phat\nc(ci_lower_99, ci_upper_99)\n\n[1] 0.2844375 0.3375625\n\n\nComparison: The midpoint (0.311) stays the same, but the interval widens.\nA higher confidence level increases the margin of error."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#question-12-effect-of-smaller-sample-size-n-215",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#question-12-effect-of-smaller-sample-size-n-215",
    "title": "Exploration Exercise 3.2",
    "section": "Question 12 – Effect of Smaller Sample Size (n = 215)",
    "text": "Question 12 – Effect of Smaller Sample Size (n = 215)\nQ12. Suppose that the GSS had only taken a sample size of \\(n = 215\\).\nHow would this change your confidence interval?\n\nn_small &lt;- 215\nSE_small &lt;- sqrt(phat * (1 - phat) / n_small)\nz_star &lt;- 1.96\nlower_small &lt;- phat - z_star * SE_small\nupper_small &lt;- phat + z_star * SE_small\nc(lower_small, upper_small)\n\n[1] 0.2491234 0.3728766\n\n\nInterpretation: Smaller \\(n\\) increases variability, leading to a wider confidence interval and less precise estimation."
  },
  {
    "objectID": "MA206-AY26-1/assets/files/EE_3_2.html#summary-of-key-concepts",
    "href": "MA206-AY26-1/assets/files/EE_3_2.html#summary-of-key-concepts",
    "title": "Exploration Exercise 3.2",
    "section": "Summary of Key Concepts",
    "text": "Summary of Key Concepts\n\n\n\n\n\n\n\n\nConcept\nFormula\nWhen Used\n\n\n\n\nStandard deviation under null\n\\(\\sqrt{\\pi(1-\\pi)/n}\\)\nHypothesis tests (\\(H_0\\) assumed true)\n\n\nStandard error (estimated)\n\\(\\sqrt{\\hat{p}(1-\\hat{p})/n}\\)\nConfidence intervals (sample-based)\n\n\nz-statistic\n\\((\\hat{p}-\\pi_0)/SD_\\pi\\)\nTests population proportion hypotheses\n\n\nConfidence interval\n\\(\\hat{p} \\pm z^* SE_{\\hat{p}}\\)\nEstimates plausible range for \\(\\pi\\) from data"
  },
  {
    "objectID": "MA206-AY26-1/assets/files/SIL-1-Info.html",
    "href": "MA206-AY26-1/assets/files/SIL-1-Info.html",
    "title": "SIL 1 Info",
    "section": "",
    "text": "# A tibble: 518 × 31\n    YEAR SERIAL MONTH HWTFINL   CPSID ASECFLAG ASECWTH PERNUM WTFINL  CPSIDV\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1  1997  35661 March      NA 2.00e13 ASEC       2964.      2     NA 2.00e14\n 2  1995  53646 March      NA 2.00e13 ASEC       3353.      1     NA 2.00e14\n 3  1996  51055 March      NA 2.00e13 ASEC       2182.      1     NA 2.00e14\n 4  1992  43755 March      NA 1.99e13 ASEC       4172.      1     NA 1.99e14\n 5  1996  63329 March      NA 2.00e13 ASEC        912.      6     NA 2.00e14\n 6  1988  11847 March      NA 1.99e13 ASEC       1849.      1     NA 1.99e14\n 7  1990  47781 March      NA 1.99e13 ASEC       1496.      3     NA 1.99e14\n 8  1988  45800 March      NA 1.99e13 ASEC       1697.      2     NA 1.99e14\n 9  1997   5496 March      NA 2.00e13 ASEC       2987.      2     NA 2.00e14\n10  1995  72088 March      NA 2.00e13 ASEC        938.      3     NA 2.00e14\n# ℹ 508 more rows\n# ℹ 21 more variables: CPSIDP &lt;dbl&gt;, ASECWT &lt;dbl&gt;, AGE &lt;int&gt;, SEX &lt;fct&gt;,\n#   RACE &lt;fct&gt;, MARST &lt;fct&gt;, VETSTAT &lt;fct&gt;, EMPSTAT &lt;fct&gt;, DIFFMOB &lt;fct&gt;,\n#   WORKLY &lt;fct&gt;, WKSWORK1 &lt;dbl&gt;, WKSWORK2 &lt;fct&gt;, FIRMSIZE &lt;fct&gt;,\n#   DISABWRK &lt;fct&gt;, year_start &lt;fct&gt;, weeks_worked &lt;dbl&gt;,\n#   employed_last_year &lt;fct&gt;, disability &lt;chr&gt;, white &lt;lgl&gt;, post_ADA &lt;lgl&gt;,\n#   age_range &lt;chr&gt;"
  },
  {
    "objectID": "MA206-AY26-1/assets/files/SIL-1-Info.html#summary-of-consequences-of-employment-protection-the-case-of-the-americans-with-disabilities-act",
    "href": "MA206-AY26-1/assets/files/SIL-1-Info.html#summary-of-consequences-of-employment-protection-the-case-of-the-americans-with-disabilities-act",
    "title": "SIL 1 Info",
    "section": "Summary of Consequences of Employment Protection? The Case of the Americans with Disabilities Act",
    "text": "Summary of Consequences of Employment Protection? The Case of the Americans with Disabilities Act\nDaron Acemoglu and Joshua Angrist (2001) study the labor market effects of the Americans with Disabilities Act (ADA), which prohibited discrimination and required employers to provide reasonable accommodations for disabled workers beginning in 1992. The law aimed to increase employment opportunities and earnings for disabled individuals, but it also created potential costs for employers in terms of accommodations and legal risk.\nUsing data from the 1988–1997 Current Population Survey, the authors compare employment and wages between disabled and nondisabled workers aged 21–58. They find that employment among younger disabled men and women (ages 21–39) fell sharply after the ADA’s implementation. The decline was especially large in medium-sized firms and in states that saw high levels of ADA-related litigation. For older disabled workers (40–58), the effects were smaller and mixed. In contrast, nondisabled workers’ employment was largely unchanged.\nWages for disabled workers did not display consistent changes across the study period. The authors also examine whether the growth of disability benefit rolls could explain the decline in employment but conclude that this factor accounts for only a portion of the change. The results suggest that the ADA itself, by raising the expected costs of employing disabled workers, contributed to the decline in their employment.\nThe study highlights that legislation intended to improve outcomes for disabled workers may have unintended consequences. While the ADA did not negatively affect nondisabled workers, its impact on the disabled population raises questions about how broadly such results apply beyond the specific groups and time period studied. The findings also raise important considerations for how research on sensitive policies can be interpreted and applied."
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html",
    "href": "MA206-AY26-1/lesson-1.html",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "https://xkcd.com/552/\n \n\n\n\n\n\n\n\n\n\n\nPlease Share:\n\nName\n\nHometown\n\nCompany\n\nBirthday\n\nDid you come directly from high school?\n\nAcademic Major\nWhat you do in the Corps (Sport, Club, etc)\n\nFavorite sports team\n\nPossible Branch\n\nWhy you picked your seat today\n\n\n\n\n\n\nSandhurst\nOCF\nF4\nDallas Cowboys, San Antonio Spurs, Texas Rangers\n\n\n\n12345\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2003-2007 BS, Operations Research: United States Military Academy (USMA)\n\n2007-2008 Engineer Basic Officer Course: Fort Leonard Wood, Missouri\n\n2008-2011 Platoon Leader / XO / AS3: Schofield Barracks, HI / Iraq\n\n2011-2012 Engineer Captain’s Career Course: Missouri S&T\n\n2012 MS, Engineering Management: Missouri S&T\n\n2012-2014 Company Commander: White Sands Missile Range, NM / Afghanistan\n\n2014-2016 MS, Integrated Systems Engineering: The Ohio State University\n\n2016-2019 Assistant Professor: United States Military Academy, West Point\n\n2019-2022 ORSA / Data Scientist: Center for Army Analysis, Ft. Belvoir\n\n2022-2025 PhD, Statistical Science: Baylor University (Waco, TX)\n\n2025-? Academy Professor: United States Military Academy, West Point\n\n\n\n\n\nlibrary(leaflet)\n\nplaces &lt;- data.frame(\n  place = c(\"USMA (West Point, NY)\",\n            \"Schofield Barracks, HI\",\n            \"Missouri S&T (Rolla, MO)\",\n            \"White Sands Missile Range, NM\",\n            \"The Ohio State University (Columbus, OH)\",\n            \"Baylor University (Waco, TX)\",\n            \"Center for Army Analysis (Ft. Belvoir, VA)\"),\n  lat = c(41.391, 21.483, 37.954, 32.389, 39.999, 31.549, 38.711),\n  lng = c(-73.959, -158.063, -91.774, -106.491, -83.018, -97.114, -77.147)\n)\n\nleaflet(places) |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(\n    ~lng, ~lat,\n    radius = 6,\n    color = \"black\",\n    fillColor = \"red\",\n    fillOpacity = 0.85\n  ) |&gt;\n  addLabelOnlyMarkers(\n    ~lng, ~lat,\n    # label = ~place,\n    labelOptions = labelOptions(\n      noHide = TRUE,\n      direction = \"top\",\n      textOnly = TRUE,\n      style = list(\"color\" = \"black\", \"font-size\" = \"12px\", \"font-weight\" = \"bold\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000–2004 BS, Economics: Michigan State University\n\n2004–2010 Project Manager: Epic Systems\n\n2011–2020 Consultant / Build Analyst (Epic Radiant & Cadence)\n\n2011–2013 Epic Radiant Build Consultant: Intellistar Consulting\n\n2014 Epic Radiant Build Analyst: Vonlay\n\n2016–2017 Epic Radiant Build Analyst: Huron\n\n2018–2020 Epic Cadence Build Analyst: Bluetree Network\n\n\n2020–Present Solutions & Application Architect / Principal Analyst: Mayo Clinic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n123456789101112131415\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1234567891011121314151617\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\nDevelop a Base of Knowledge\n\nLeverage Technology\n\nCommunicate Concepts and Results\n\nProblem Solving Techniques\n\nDevelop habits of mind\n\nDevelop an interdisciplinary perspective\n\n\n\n\n\n\n\n\nArrive prepared for each lesson\n\nEncourage independent thinking\nMaintain professionalism and respect at all times\n\nUphold the values of the Corps and our institution\n\nClear guidance and expectations for assignments\n\nBe a professional mentor\n\nMake Mistakes\n\n\n\n\n\n\n\n\n\n\nBe responsible for your learning\nArrive prepared for each lesson\n\nEngage actively in discussions and exercises\n\nMaintain professionalism and respect at all times\n\nUphold the values of the Corps and our institution - you are junior members of this profession\n\nCommunicate early if challenges arise\n\nMake mistakes\n\n\n\n\n\nComputers will only be used for course materials only\n\nNo food or gum allowed in the classroom\n\nOnly drinks in spill-proof containers are allowed\n\nLeave bags, backpacks, coats, and hats in the hallway\n\nStay awake in class. Stand up if you are tired\n\nArrive on time and do not start packing up before I dismiss the class\n\nBe respectful when others are speaking\n\nSupport the section marcher\n\n\n\n\n\n\n\n\nProbability\n\nFoundations: preliminaries, dataset exploration, tidyverse basics\n\nCore Probability: principles, conditional probability, rules of random variables\n\nRandom Variables: discrete, continuous, and named distributions\n\nStatistical Tests\n\nOne-sample tests: one proportion Z-test, one mean T-test\n\nConfidence Intervals: categorical and quantitative data\n\nComparative tests: two proportion Z-test, two mean T-test, paired data\n\nBroader Concepts: generalization, causation, and investigation labs\n\nRegression\n\nCorrelation & Simple Linear Regression\n\nMultiple Linear Regression (I–III)\n\nApplications: project work, presentations, writer’s workshop, course review\n\n\n\n\n\n\nCanvas\nCalendar (Day 1) (Day 2)\nBook: Introduction to Statistical Investigations (Digital or hard copy authorized)\n\nCourse Guide\n\nCourse Admin\n\nSpecific Help / Instructions\n\nR Code\n\n\nGraded Assignments\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nEvent\nPoints\n\n\n\n\nDay 0 Assignment\n15\n\n\nGenerative AI Certification\n25\n\n\nExploration Exercises (6 @ 10 points each)\n60\n\n\nStatistical Investigation Labs: - SIL 1 (25) - SIL 2 (25)\n50\n\n\nWPR 1\n125\n\n\nWPR 2\n140\n\n\nCourse Project: - Milestone 1 – Setup and Data (25) - Milestone 2 – EDA (25) - Milestone 3 – Intro & Academic Articles (30) - Milestone 4 – Literature Review (25) - Milestone 5 – Methodology (30) - Milestone 6 – Results, Discussion, Conclusion (60) - In-Progress Review (20) - Presentation (50) - Milestone 7 – Writer’s Workshop (25) - Final Turn In (30)\n300\n\n\nTEE\n285\n\n\nTOTAL\n1000\n\n\n\n\n\n\nProject\nA note on course grades\nWhere to get this presentation\n\n\n\n\n\n\n\nNote\n\n\n\nFor assignments worth less than 20 points that are turned in late:\n50% reduction if turned in before Lesson 30 For assignments turned in late worth 20 points or more:\n10% reduction per day until assignment is worth 0 points (10 days late)\n\n\n\n\n\n\n\nAcademic Integrity Brief\n\nAcademic Security\n\n\n\n\n\n\n\n\n\n\n\n\nGo to Lesson 1 on Canvas\nNote the objectives\nDo the reading\nWatch the videos\nNote the Course Guide\n\nDo the Homework (If applicable)\n\n\n\n\n\n\n\n\n\n\n\n\nAsk a question\nDo basketball cadets tend to be taller than cadets who aren’t on the basketball team?\nDesign a study and collect data\nMeasure the heights of some basketball cadets and some non-basketball cadets.\n\n\n\nBasketball cadets: 71, 72, 73, 74, 75, 75, 75, 75, 76, 76, 76, 77, 77, 77, 78 \n\n\nNon-basketball cadets: 68, 68, 70, 70, 70, 70, 70, 70, 72, 73\n\n\n\nExplore the data\nAverage basketball cadet height: 75.1 in\nAverage non-basketball cadet height: 70.1 in\n\n\n\n\n\n\n\n\n\n\n\nDraw inferences\nThe basketball group is taller by about 5 inches in this sample.\nFormulate conclusions\nLooks like basketball cadets are taller.\nLook back and ahead\nYou could improve this by measuring more cadets or using a statistical test.\n\n\n\n\n\nAsk a question\nIf the host opens a goat door, are you better off sticking with your first door or switching to the other unopened door?\nDesign a study and collect data\nWe simulate the game many times and record whether “stay” or “switch” wins the car.\nExplore the data\nDraw inferences\nFormulate conclusions\nLook back and ahead\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 2\n\n\n\n\n\nProject Milestone 1: Due 22 Aug (Friday) All Sections - Read through this and come to class with questions\nGenAI Certification: Due 25 August (Monday) All Sections",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#welcome",
    "href": "MA206-AY26-1/lesson-1.html#welcome",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "https://xkcd.com/552/",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#introductions",
    "href": "MA206-AY26-1/lesson-1.html#introductions",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Please Share:\n\nName\n\nHometown\n\nCompany\n\nBirthday\n\nDid you come directly from high school?\n\nAcademic Major\nWhat you do in the Corps (Sport, Club, etc)\n\nFavorite sports team\n\nPossible Branch\n\nWhy you picked your seat today\n\n\n\n\n\n\nSandhurst\nOCF\nF4\nDallas Cowboys, San Antonio Spurs, Texas Rangers\n\n\n\n12345\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2003-2007 BS, Operations Research: United States Military Academy (USMA)\n\n2007-2008 Engineer Basic Officer Course: Fort Leonard Wood, Missouri\n\n2008-2011 Platoon Leader / XO / AS3: Schofield Barracks, HI / Iraq\n\n2011-2012 Engineer Captain’s Career Course: Missouri S&T\n\n2012 MS, Engineering Management: Missouri S&T\n\n2012-2014 Company Commander: White Sands Missile Range, NM / Afghanistan\n\n2014-2016 MS, Integrated Systems Engineering: The Ohio State University\n\n2016-2019 Assistant Professor: United States Military Academy, West Point\n\n2019-2022 ORSA / Data Scientist: Center for Army Analysis, Ft. Belvoir\n\n2022-2025 PhD, Statistical Science: Baylor University (Waco, TX)\n\n2025-? Academy Professor: United States Military Academy, West Point\n\n\n\n\n\nlibrary(leaflet)\n\nplaces &lt;- data.frame(\n  place = c(\"USMA (West Point, NY)\",\n            \"Schofield Barracks, HI\",\n            \"Missouri S&T (Rolla, MO)\",\n            \"White Sands Missile Range, NM\",\n            \"The Ohio State University (Columbus, OH)\",\n            \"Baylor University (Waco, TX)\",\n            \"Center for Army Analysis (Ft. Belvoir, VA)\"),\n  lat = c(41.391, 21.483, 37.954, 32.389, 39.999, 31.549, 38.711),\n  lng = c(-73.959, -158.063, -91.774, -106.491, -83.018, -97.114, -77.147)\n)\n\nleaflet(places) |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(\n    ~lng, ~lat,\n    radius = 6,\n    color = \"black\",\n    fillColor = \"red\",\n    fillOpacity = 0.85\n  ) |&gt;\n  addLabelOnlyMarkers(\n    ~lng, ~lat,\n    # label = ~place,\n    labelOptions = labelOptions(\n      noHide = TRUE,\n      direction = \"top\",\n      textOnly = TRUE,\n      style = list(\"color\" = \"black\", \"font-size\" = \"12px\", \"font-weight\" = \"bold\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000–2004 BS, Economics: Michigan State University\n\n2004–2010 Project Manager: Epic Systems\n\n2011–2020 Consultant / Build Analyst (Epic Radiant & Cadence)\n\n2011–2013 Epic Radiant Build Consultant: Intellistar Consulting\n\n2014 Epic Radiant Build Analyst: Vonlay\n\n2016–2017 Epic Radiant Build Analyst: Huron\n\n2018–2020 Epic Cadence Build Analyst: Bluetree Network\n\n\n2020–Present Solutions & Application Architect / Principal Analyst: Mayo Clinic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n123456789101112131415\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1234567891011121314151617",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#expectations",
    "href": "MA206-AY26-1/lesson-1.html#expectations",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Develop a Base of Knowledge\n\nLeverage Technology\n\nCommunicate Concepts and Results\n\nProblem Solving Techniques\n\nDevelop habits of mind\n\nDevelop an interdisciplinary perspective\n\n\n\n\n\n\n\n\nArrive prepared for each lesson\n\nEncourage independent thinking\nMaintain professionalism and respect at all times\n\nUphold the values of the Corps and our institution\n\nClear guidance and expectations for assignments\n\nBe a professional mentor\n\nMake Mistakes\n\n\n\n\n\n\n\n\n\n\nBe responsible for your learning\nArrive prepared for each lesson\n\nEngage actively in discussions and exercises\n\nMaintain professionalism and respect at all times\n\nUphold the values of the Corps and our institution - you are junior members of this profession\n\nCommunicate early if challenges arise\n\nMake mistakes\n\n\n\n\n\nComputers will only be used for course materials only\n\nNo food or gum allowed in the classroom\n\nOnly drinks in spill-proof containers are allowed\n\nLeave bags, backpacks, coats, and hats in the hallway\n\nStay awake in class. Stand up if you are tired\n\nArrive on time and do not start packing up before I dismiss the class\n\nBe respectful when others are speaking\n\nSupport the section marcher",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#course-overview-admin",
    "href": "MA206-AY26-1/lesson-1.html#course-overview-admin",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Probability\n\nFoundations: preliminaries, dataset exploration, tidyverse basics\n\nCore Probability: principles, conditional probability, rules of random variables\n\nRandom Variables: discrete, continuous, and named distributions\n\nStatistical Tests\n\nOne-sample tests: one proportion Z-test, one mean T-test\n\nConfidence Intervals: categorical and quantitative data\n\nComparative tests: two proportion Z-test, two mean T-test, paired data\n\nBroader Concepts: generalization, causation, and investigation labs\n\nRegression\n\nCorrelation & Simple Linear Regression\n\nMultiple Linear Regression (I–III)\n\nApplications: project work, presentations, writer’s workshop, course review\n\n\n\n\n\n\nCanvas\nCalendar (Day 1) (Day 2)\nBook: Introduction to Statistical Investigations (Digital or hard copy authorized)\n\nCourse Guide\n\nCourse Admin\n\nSpecific Help / Instructions\n\nR Code\n\n\nGraded Assignments\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nEvent\nPoints\n\n\n\n\nDay 0 Assignment\n15\n\n\nGenerative AI Certification\n25\n\n\nExploration Exercises (6 @ 10 points each)\n60\n\n\nStatistical Investigation Labs: - SIL 1 (25) - SIL 2 (25)\n50\n\n\nWPR 1\n125\n\n\nWPR 2\n140\n\n\nCourse Project: - Milestone 1 – Setup and Data (25) - Milestone 2 – EDA (25) - Milestone 3 – Intro & Academic Articles (30) - Milestone 4 – Literature Review (25) - Milestone 5 – Methodology (30) - Milestone 6 – Results, Discussion, Conclusion (60) - In-Progress Review (20) - Presentation (50) - Milestone 7 – Writer’s Workshop (25) - Final Turn In (30)\n300\n\n\nTEE\n285\n\n\nTOTAL\n1000\n\n\n\n\n\n\nProject\nA note on course grades\nWhere to get this presentation\n\n\n\n\n\n\n\nNote\n\n\n\nFor assignments worth less than 20 points that are turned in late:\n50% reduction if turned in before Lesson 30 For assignments turned in late worth 20 points or more:\n10% reduction per day until assignment is worth 0 points (10 days late)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#mandatory-and-important-briefings",
    "href": "MA206-AY26-1/lesson-1.html#mandatory-and-important-briefings",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Academic Integrity Brief\n\nAcademic Security",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#lesson-1",
    "href": "MA206-AY26-1/lesson-1.html#lesson-1",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Go to Lesson 1 on Canvas\nNote the objectives\nDo the reading\nWatch the videos\nNote the Course Guide\n\nDo the Homework (If applicable)\n\n\n\n\n\n\n\n\n\n\n\n\nAsk a question\nDo basketball cadets tend to be taller than cadets who aren’t on the basketball team?\nDesign a study and collect data\nMeasure the heights of some basketball cadets and some non-basketball cadets.\n\n\n\nBasketball cadets: 71, 72, 73, 74, 75, 75, 75, 75, 76, 76, 76, 77, 77, 77, 78 \n\n\nNon-basketball cadets: 68, 68, 70, 70, 70, 70, 70, 70, 72, 73\n\n\n\nExplore the data\nAverage basketball cadet height: 75.1 in\nAverage non-basketball cadet height: 70.1 in\n\n\n\n\n\n\n\n\n\n\n\nDraw inferences\nThe basketball group is taller by about 5 inches in this sample.\nFormulate conclusions\nLooks like basketball cadets are taller.\nLook back and ahead\nYou could improve this by measuring more cadets or using a statistical test.\n\n\n\n\n\nAsk a question\nIf the host opens a goat door, are you better off sticking with your first door or switching to the other unopened door?\nDesign a study and collect data\nWe simulate the game many times and record whether “stay” or “switch” wins the car.\nExplore the data\nDraw inferences\nFormulate conclusions\nLook back and ahead",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-1.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-1.html#before-you-leave",
    "title": "Lesson 1: Intro and Preliminaries",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nLesson 2\n\n\n\n\n\nProject Milestone 1: Due 22 Aug (Friday) All Sections - Read through this and come to class with questions\nGenAI Certification: Due 25 August (Monday) All Sections",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 1"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html",
    "href": "MA206-AY26-1/lesson-11.html",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "📄 General Instructions\n\n\n\nAcademic Articles Worksheet\n\n📑 Use: Academic Articles Worksheet\n\n✅ Worth: 10 points\n\n⏰ Due: 0700 Friday, 19 Sept 2025\n\n🔗 Submit on Canvas: https://westpoint.instructure.com/courses/10295/assignments/223751\n\nIntroduction & Data Section\n\n📑 Use: Math Writing Template\n\n✅ Worth: 20 points\n\n⏰ Due: 0700 Sunday, 21 Sept 2025\n\n🔗 Submit on Canvas: https://westpoint.instructure.com/courses/10295/assignments/223738\n\n\n\n\n\n\n\n\nNote\n\n\n\nReminder: Also add both items to your binder with an updated Annex B (not graded yet).\n\n\n\n\n\n\n⏰ Due 0700 ET on Lesson 13\n\nDay 1: Wednesday, 24 Sept 2025\n\nDay 2: Thursday, 25 Sept 2025\n\n\n📑 Worksheet: https://westpoint.instructure.com/courses/10295/assignments/216497 — don’t sleep on this!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\nDuring my son and his team’s Little League World Series run, Cal got on base \\(18\\) out of \\(35\\) times. Across all of Texas East Little Leagues, the average on-base percentage was about \\(40\\%\\).\nQuestion: Is Cal really better than average, or is this just by chance?\n\n\nIf the average player gets on base at a \\(40\\%\\) rate, what is the probability that we would observe someone get on base \\(18/35 = 0.514\\) or higher?\nIn other words: if Cal were truly a 40% hitter, how often would we see a season this good (or better) just by chance?\n\n\n\nLet’s pretend we could replay Cal’s season many times under the assumption he is a \\(40\\%\\) hitter.\n\nlibrary(tidyverse)\n\n# simulate one season\none_season &lt;- rbinom(n = 35, size = 1, prob = 0.4)\none_season\n\n [1] 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1\n\n\n\n# total times on base\non_base_at_bats &lt;- sum(one_season)\non_base_at_bats\n\n[1] 11\n\n# as a proportion\non_base_at_bats / length(one_season)\n\n[1] 0.3142857\n\n\n\n\n\nNow let’s repeat this process \\(10{,}000\\) times.\n\nn &lt;- 10000\n\nsim_results &lt;- tibble(\n  trial = 1:n,\n  rocks = rbinom(n, size = 35, prob = 0.4)\n) |&gt; \n  mutate(proportion = rocks / 35)\n\nhead(sim_results)\n\n# A tibble: 6 × 3\n  trial rocks proportion\n  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1     1    13      0.371\n2     2    11      0.314\n3     3    17      0.486\n4     4    17      0.486\n5     5    11      0.314\n6     6    15      0.429\n\n\n\n\n\nHere’s the distribution of on-base proportions from the simulations.\n\non_base_hist &lt;- sim_results |&gt;\n  ggplot(aes(x = proportion)) +\n  geom_histogram(binwidth = 1/35, boundary = 0, fill = \"skyblue\", color = \"white\") \n\non_base_hist\n\n\n\n\n\n\n\n\n\n\n\nNow let’s add Cal’s observed rate (\\(0.514\\)):\n\non_base_hist +\n  geom_vline(xintercept = 0.514, color = \"firebrick\", linetype = 5, linewidth = 2)\n\n\n\n\n\n\n\n\n\n\n\nFinally, what proportion of simulated seasons were at least this extreme?\n\nsim_results |&gt; \n  summarise(prob_more_extreme = mean(proportion &gt;= 0.514))\n\n# A tibble: 1 × 1\n  prob_more_extreme\n              &lt;dbl&gt;\n1             0.113\n\n\n\n\n\nIn our simulation, only about X% of seasons produced an on-base percentage this high or higher if Cal were truly a 40% hitter.\n➡️ This suggests his observed \\(0.514\\) season is possibly due to chance alone — the evidence is not convincing that he might might be better than average.\n\n\n\n\n\n\n\nIs Cal’s on-base percentage higher than the Texas East Little League average of 40%?\n\n\n\nWe have observational data from Cal’s 35 plate appearances during the Little League World Series run.\n- \\(n = 35\\) plate appearances\n- \\(x = 18\\) times on base\n- Observed proportion: \\(\\hat \\pi = \\tfrac{18}{35} \\approx 0.514\\)\nWe treat these 35 at-bats as a random sample from his true underlying ability.\n\n\n\nThe observed proportion of \\(0.514\\) is above the reference average of \\(0.40\\). This is about 11 percentage points higher. The key question is whether this difference is large enough to be unlikely by chance.\n\n\n\nWe set up hypotheses:\n\nNull hypothesis: \\(H_0 : \\pi = 0.40\\)\n\nAlternative hypothesis: \\(H_A : \\pi &gt; 0.40\\)\n\nCompute the standard error and test statistic:\n\\[\nSE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n= \\sqrt{\\frac{0.40(0.60)}{35}}\n\\approx 0.083\n\\]\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{SE}\n= \\frac{0.514 - 0.40}{0.083}\n\\approx 1.37\n\\]\n\n\n\nThe one-tailed \\(p\\)-value is:\n\\[\np = P(Z \\geq 1.37) \\approx 0.085\n\\]\n\n1 - pnorm(q = 1.37, mean = 0, sd = 1)\n\n[1] 0.08534345\n\n\nAt \\(\\alpha = 0.05\\), this result is not statistically significant. We do not have strong enough evidence to conclude Cal’s on-base percentage is truly higher than 40%.\n\n\n\n\nThe observed rate of \\(0.514\\) is higher than average and suggestive (\\(p \\approx 0.085\\)).\n\nWith only 35 plate appearances, there is considerable variability — a larger sample would provide a clearer answer.\n\nContext matters: 35 at bats may not represent long-term ability. Future seasons with more data or more at-bats in a season could confirm or refute this pattern.\n\n\n\n\n\n\n\nThe null hypothesis (\\(H_0\\)) is the starting assumption — usually that there is “no difference” or “no effect.”\n\nIn our example: \\(H_0 : \\pi = 0.40\\)\n\nThis means we assume Cal’s true on-base probability is the same as the Texas East Little League average of 40%.\n\n\n\n\nThe alternative hypothesis (\\(H_A\\)) is what we want to investigate. Depending on the research question, there are three common forms:\n\nRight-tailed (greater than)\n\n\\(H_A : \\pi &gt; 0.40\\)\n\nIn context: Is Cal’s true on-base percentage higher than 40%?\n\nThis is the version we are using, because the natural question is whether he’s better than average.\n\nLeft-tailed (less than)\n\n\\(H_A : \\pi &lt; 0.40\\)\n\nIn context: Is Cal’s true on-base percentage lower than 40%?\n\nYou would ask this if you suspected Cal might actually be worse than average at getting on base.\n\nTwo-tailed (not equal)\n\n\\(H_A : \\pi \\neq 0.40\\)\n\nIn context: Is Cal’s true on-base percentage different from 40% (either higher or lower)?\n\nYou would use this if you want to know whether Cal performs differently than average, without assuming in advance which direction.\n\n\n\n\n\n\nIf you are asking, “Is Cal better than average?”, the right-tailed test is appropriate.\n\nIf the concern were that Cal struggles at the plate, you’d use a left-tailed test.\n\nIf you only care whether Cal is different from average in either direction, the two-tailed test is the right choice.\n\n\n\n\n\n\\(\\pi\\) (the Greek letter pi) represents the population proportion of average of getting on base.\n\nIn context: \\(\\pi\\) is Cal’s true long-run probability of getting on base each plate appearance.\n\nWe never observe \\(\\pi\\) directly — we estimate it with \\(\\hat{\\pi}\\) (the sample proportion).\n\n\n\n\n\n\\(\\alpha\\) (alpha) is the threshold for evidence against the null hypothesis.\n\nCommon choices: \\(\\alpha = 0.05\\) (5%) or \\(\\alpha = 0.01\\) (1%).\n\nInterpretation: If \\(p \\leq \\alpha\\), the result is considered statistically significant — unlikely to occur just by chance if \\(H_0\\) were true.\n\n\n\n\nThe \\(z\\) distribution (also called the standard normal distribution) is a bell-shaped curve with:\n- Mean = \\(0\\)\n- Standard deviation = \\(1\\)\nSo how do we get from our sample proportion \\(\\hat{\\pi}\\) to this special distribution?\n\n\nUnder the null hypothesis \\(H_0 : \\pi = 0.40\\), the sample proportion \\(\\hat{\\pi}\\) has a sampling distribution that is approximately normal (by the Central Limit Theorem) with:\n\nMean = \\(\\pi_0 = 0.40\\)\n\nStandard deviation = \\(SE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\)\n\nThis tells us what values of \\(\\hat{\\pi}\\) we would expect just by chance if the null were true.\n\n\n\nTo compare our observed \\(\\hat{\\pi}\\) to this distribution, we standardize it:\n\nSubtract the mean under \\(H_0\\)\n\nThis centers the distribution at \\(0\\) by measuring how far away our observed statistic is from the null.\n\n\\((\\hat{\\pi} - \\pi_0)\\)\n\nDivide by the standard deviation\n\nThis rescales differences into standard deviation units, so we can judge how unusual they are.\n\n\\(\\dfrac{\\hat{\\pi} - \\pi_0}{SE}\\)\n\n\n\n\n\nAfter centering and rescaling, the new standardized statistic follows (approximately) the standard normal distribution, \\(N(0,1)\\).\nThat’s why we call it the \\(z\\) statistic:\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n\\]\nNow we can use the \\(z\\) distribution to calculate probabilities (like \\(p\\)-values) for how extreme our observed result is relative to the null hypothesis.\n\nggplot() +\n  geom_function(fun = dnorm, xlim = c(-4,4))\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Activity: With your eyes closed, on the count of three, everyone will make their first throw of Rock–Paper–Scissors.\n- 👊 = Rock\n- ✋ = Paper\n- ✌️ = Scissors\nHold your choice steady so we can tally the results. Closing your eyes helps reduce herding or copying from neighbors.\n\n\nDo students tend to choose Rock more or less often than random chance (\\(33\\%\\)) on their first throw?\n\n\n\nWith eyes closed, on the count of 3 each student chooses one option (Rock/Paper/Scissors) as if starting a game. We then tally the class counts:\n- \\(R\\) = number of Rock\n- \\(P\\) = number of Paper\n- \\(S\\) = number of Scissors\n- \\(n = R+P+S\\)\n\n# Enter the tallies you just collected:\nR &lt;- 2   # Rock count\nP &lt;- 10   # Paper count\nS &lt;- 7   # Scissors count\n\nn &lt;- R + P + S\nc(R = R, P = P, S = S, n = n)\n\n R  P  S  n \n 2 10  7 19 \n\n\n\n\n\nCompute the sample proportion choosing Rock and make a quick bar chart. Under complete randomness we’d expect each to be near \\(n/3\\).\n\npihat &lt;- R / n\npihat\n\n[1] 0.1052632\n\ndf &lt;- tibble(option = c(\"Rock\",\"Paper\",\"Scissors\"),\n             count  = c(R, P, S))\n\nggplot(df, aes(option, count)) +\n  geom_col() +\n  geom_hline(yintercept = n/3, linetype = 2) +\n  labs(title = \"Class First-Throw Choices\",\n       subtitle = \"Dashed line = expected count if choices were uniform (n/3)\",\n       x = NULL, y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\nWe’ll do two things:\n\nSimulate many samples of size \\(n\\) assuming \\(p_{Rock}=1/3\\), and estimate a two-sided \\(p\\)-value by comparing simulated proportions to the observed.\n\nDo the math version using the \\(z\\)-distribution.\n\n\n\n\npi0 &lt;- 1/3\nN  &lt;- 10000               # number of simulations\nR_obs &lt;- R                # keep observed Rock consistent with Step 2\npihat_obs &lt;- R_obs / n\n\n# Simulate many samples under H0\nsim_results &lt;- tibble(\n  trial = 1:N,\n  rocks = rbinom(N, size = n, prob = pi0)   # Rock counts in each simulated sample\n) |&gt;\n  mutate(pihat = rocks / n)                  # simulated sample proportions\n\nsim_results\n\n# A tibble: 10,000 × 3\n   trial rocks pihat\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     1     4 0.211\n 2     2     8 0.421\n 3     3     6 0.316\n 4     4     3 0.158\n 5     5     8 0.421\n 6     6     3 0.158\n 7     7     7 0.368\n 8     8     5 0.263\n 9     9     2 0.105\n10    10     2 0.105\n# ℹ 9,990 more rows\n\n\n\n# Plot the simulated sampling distribution with observed and null marked\nsim_results |&gt;\n  ggplot(aes(x = pihat)) +\n  geom_histogram(binwidth = 1/n, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = pihat_obs, color = \"firebrick\", linetype = 2, linewidth = 1.2) +\n  geom_vline(xintercept = pi0, color = \"gray40\", linewidth = 1.2) +\n  labs(title = paste0(\"Sampling Distribution of p̂i under H0 (n = \", n, \")\"),\n       subtitle = paste0(\"Observed p̂ = \", round(pihat_obs,3), \n                         \" | Null pi0 = \", round(pi0,3)),\n       x = \"p̂ (proportion Rock)\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Two-sided simulation p-value\nobs_abs_diff &lt;- abs(pihat_obs - pi0)\n\nsim_p_two_sided &lt;- sim_results |&gt; \n  mutate(pihat_different_than_null = pihat - pi0) |&gt;          # difference from null\n  mutate(abs_of_difference = abs(pihat_different_than_null)) |&gt; \n  mutate(is_extreme = abs_of_difference &gt;= obs_abs_diff) |&gt; # flag extremes\n  summarise(p_val = mean(is_extreme))                       # proportion of extremes = p-value\n\nsim_p_two_sided\n\n# A tibble: 1 × 1\n   p_val\n   &lt;dbl&gt;\n1 0.0492\n\n\n\n\n\n\n\\(H_0: p_{Rock} = 1/3\\)\n\n\\(H_A: p_{Rock} \\neq 1/3\\)\n\n\\(SE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\)\n\n# Standard Error under H0\nSE &lt;- sqrt(pi0 * (1 - pi0) / n)\nSE\n\n[1] 0.1081476\n\n\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n\\]\n\n# Test Statistic (z)\nz_stat &lt;- (pihat_obs - pi0) / SE\nz_stat\n\n[1] -2.108878\n\n\n\\[\np = 2 \\times P(Z \\geq |z|)\n\\]\n\n# Two-Sided p-value\np_val_two_sided &lt;- 2 * (1 - pnorm(abs(z_stat)))\np_val_two_sided\n\n[1] 0.03495507\n\n\n\n\n\n\nNow we bring the two approaches together:\n\nSimulation gave us an empirical \\(p\\)-value by resampling under \\(H_0\\).\n\nMath/analytic (\\(z\\)-test) gave us an approximate \\(p\\)-value using the standard normal.\n\n\n\n              n           R_obs       pihat_obs             pi0          z_stat \n        19.0000          2.0000          0.1050          0.3330         -2.1090 \nsim_p_two_sided   z_p_two_sided           alpha \n         0.0492          0.0350          0.0500 \n\n\nSimulation-based decision: Reject H0 (evidence of ≠ 1/3) \n\n\nZ-approximation decision: Reject H0 (evidence of ≠ 1/3) \n\n\nInterpretation:\nWith \\(n\\) throws and observed \\(\\hat p\\), the simulation \\(p\\)-value tells us how unusual the result is if \\(p=1/3\\) were true. The \\(z\\) test gives a similar answer using a theoretical normal curve. At \\(\\alpha = 0.05\\), compare both to decide whether to reject \\(H_0\\).\n\n\n\n\n\nCode\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tibble)\n\nui &lt;- fluidPage(\n  titlePanel(\"One-Proportion Test: z-formulas + Live Simulation\"),\n  withMathJax(),\n  tags$hr(),\n  \n  fluidRow(\n    column(\n      width = 4,\n      h4(\"Inputs\"),\n      numericInput(\"x\", \"Observed successes (x)\", value = 2, min = 0, step = 1),\n      numericInput(\"n\", \"Sample size (n)\", value = 19, min = 1, step = 1),\n      sliderInput(\"pi0\", HTML(\"&pi;&lt;sub&gt;0&lt;/sub&gt; (null proportion)\"), min = 0, max = 1,\n                  value = 1/3, step = 0.01),\n      numericInput(\"B\", \"Simulations (B)\", value = 10000, min = 100, step = 100),\n      numericInput(\"seed\", \"Random seed\", value = 26, min = 1, step = 1),\n      helpText(\"Tip: This defaults to the R/P/S example: x = 2 (Rock), n = 19, π0 = 1/3.\")\n    ),\n    column(\n      width = 8,\n      h4(\"Formulas\"),\n      # Show the formulas via MathJax\n      div(style = \"font-size: 1.15em; margin-bottom: 8px;\",\n          \"$$ SE = \\\\sqrt{\\\\frac{\\\\pi_0(1 - \\\\pi_0)}{n}}, \\\\qquad z = \\\\frac{\\\\hat{p} - \\\\pi_0}{SE} $$\"\n      ),\n      h4(\"Computed Values\"),\n      tableOutput(\"value_table\"),\n      tags$br(),\n      h4(\"Sampling Distribution under H0 (Simulated)\"),\n      plotOutput(\"hist_plot\", height = \"330px\"),\n      helpText(\"Histogram shows simulated \\\\(\\\\hat p\\\\) under H0. Red line = observed \\\\(\\\\hat p\\\\). Gray line = \\\\(\\\\pi_0\\\\).\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  # Basic reactives\n  phat &lt;- reactive({\n    req(input$n &gt; 0)\n    input$x / input$n\n  })\n  \n  SE &lt;- reactive({\n    sqrt(input$pi0 * (1 - input$pi0) / input$n)\n  })\n  \n  z_stat &lt;- reactive({\n    (phat() - input$pi0) / SE()\n  })\n  \n  # p-values for three alternatives\n  p_right &lt;- reactive({ 1 - pnorm(z_stat()) })                 # H_A: p &gt; pi0\n  p_left  &lt;- reactive({ pnorm(z_stat()) })                     # H_A: p &lt; pi0\n  p_two   &lt;- reactive({ 2 * (1 - pnorm(abs(z_stat()))) })      # H_A: p != pi0\n  \n  # Simulation under H0\n  sim_df &lt;- reactive({\n    req(input$B &gt;= 100)\n    set.seed(input$seed)\n    rocks &lt;- rbinom(input$B, size = input$n, prob = input$pi0)\n    tibble(\n      phat = rocks / input$n\n    )\n  })\n  \n  # Output: table of computed values\n  output$value_table &lt;- renderTable({\n    tibble::tibble(\n      `x (successes)` = input$x,\n      `n (trials)`    = input$n,\n      `π0 (null)`     = round(input$pi0, 4),\n      `p̂ = x/n`      = round(phat(), 4),\n      `SE`            = round(SE(), 5),\n      `z`             = round(z_stat(), 4),\n      `p (right)`     = signif(p_right(), 4),\n      `p (left)`      = signif(p_left(), 4),\n      `p (two-sided)` = signif(p_two(), 4)\n    )\n  }, striped = TRUE, bordered = TRUE, spacing = \"s\", digits = 6)\n  \n  # Output: histogram with vertical lines at phat and pi0\n  output$hist_plot &lt;- renderPlot({\n    df &lt;- sim_df()\n    ggplot(df, aes(x = phat)) +\n      geom_histogram(binwidth = 1 / input$n, color = \"white\") +\n      geom_vline(xintercept = phat(), color = \"firebrick\", linetype = 2, linewidth = 1.2) +\n      geom_vline(xintercept = input$pi0, color = \"gray40\", linewidth = 1.2) +\n      labs(\n        x = expression(hat(p) ~ \"(proportion)\"),\n        y = \"Count\",\n        title = paste0(\"Simulated Sampling Distribution of \", expression(hat(p)), \" under H0\"),\n        subtitle = paste0(\"n = \", input$n, \", π0 = \", round(input$pi0, 3),\n                          \", observed p̂ = \", round(phat(), 3),\n                          \"; B = \", input$B)\n      ) +\n      theme_minimal(base_size = 12) +\n      xlim(c(0,1))\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\n\n\nFor all cases:\n\\[\nSE = \\sqrt{\\frac{\\pi_0(1 - \\pi_0)}{n}}, \\quad\nz = \\frac{\\hat{p} - \\pi_0}{SE}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.\n\n\n\n\nA hospital claims that 85% of discharge summaries are finalized within 24 hours.\nIn an audit of 60 summaries, 46 were finalized within 24 hours.\nResearch Question: Is the true proportion finalized within 24 hours less than 85%?\n\nState the hypotheses.\n\nExplain (in words) how you would simulate this test (do not actually simulate).\n\nThen, perform the mathematical one-proportion \\(z\\) test.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nDo fewer than 85% of discharge summaries get finalized within 24 hours?\n\n\n\n\n\\(H_0 : \\pi = 0.85\\)\n\n\\(H_A : \\pi &lt; 0.85\\)\n\n\n\n\n\n\\(n = 60\\) summaries\n\n\\(x = 46\\) finalized within 24h\n\n\\(\\hat{\\pi} = \\tfrac{x}{n} = \\tfrac{46}{60} = 0.767\\)\n\n\n\n\n\n\n\n\nUnder \\(H_0\\), simulate many samples of size \\(n=60\\) with \\(\\pi_0=0.85\\).\n\nFor each, compute \\(\\hat{\\pi}_{sim}\\).\n\nEstimate the left-tailed \\(p\\)-value as the fraction of simulations with \\(\\hat{\\pi}_{sim} \\le \\hat{\\pi}_{obs} = 0.767\\).\n\n\n\n\nStandard error under \\(H_0\\):\n\\[\nSE = \\sqrt{\\frac{\\pi_0 (1-\\pi_0)}{n}}\n   = \\sqrt{\\frac{0.85 \\cdot 0.15}{60}}\n   \\approx 0.046\n\\]\nTest statistic:\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n  = \\frac{0.767 - 0.85}{0.046}\n  \\approx -1.80\n\\]\n\\(p\\)-value (left-tailed):\n\\[\np = \\Phi(z) = \\Phi(-1.80) \\approx 0.036\n\\]\n\npnorm(-1.8)\n\n[1] 0.03593032\n\n\n\n\n\nAt \\(\\alpha = 0.05\\), since \\(p \\approx 0.036 &lt; 0.05\\), we reject \\(H_0\\).\n\n\n\nThere is statistical evidence that fewer than 85% of discharge summaries are completed within 24 hours.\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nProject Milestone 3: Due Canvas 22 Sept\nExploration Exercise 1.5: Due at 0700 on Lesson 13\n24 September 2025 for Day 1\n\n25 September 2025 for Day 2)\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#milestone-3-lets-talk",
    "href": "MA206-AY26-1/lesson-11.html#milestone-3-lets-talk",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "📄 General Instructions\n\n\n\nAcademic Articles Worksheet\n\n📑 Use: Academic Articles Worksheet\n\n✅ Worth: 10 points\n\n⏰ Due: 0700 Friday, 19 Sept 2025\n\n🔗 Submit on Canvas: https://westpoint.instructure.com/courses/10295/assignments/223751\n\nIntroduction & Data Section\n\n📑 Use: Math Writing Template\n\n✅ Worth: 20 points\n\n⏰ Due: 0700 Sunday, 21 Sept 2025\n\n🔗 Submit on Canvas: https://westpoint.instructure.com/courses/10295/assignments/223738\n\n\n\n\n\n\n\n\nNote\n\n\n\nReminder: Also add both items to your binder with an updated Annex B (not graded yet).\n\n\n\n\n\n\n⏰ Due 0700 ET on Lesson 13\n\nDay 1: Wednesday, 24 Sept 2025\n\nDay 2: Thursday, 25 Sept 2025\n\n\n📑 Worksheet: https://westpoint.instructure.com/courses/10295/assignments/216497 — don’t sleep on this!",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#one-proportion-z-test",
    "href": "MA206-AY26-1/lesson-11.html#one-proportion-z-test",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\nDuring my son and his team’s Little League World Series run, Cal got on base \\(18\\) out of \\(35\\) times. Across all of Texas East Little Leagues, the average on-base percentage was about \\(40\\%\\).\nQuestion: Is Cal really better than average, or is this just by chance?\n\n\nIf the average player gets on base at a \\(40\\%\\) rate, what is the probability that we would observe someone get on base \\(18/35 = 0.514\\) or higher?\nIn other words: if Cal were truly a 40% hitter, how often would we see a season this good (or better) just by chance?\n\n\n\nLet’s pretend we could replay Cal’s season many times under the assumption he is a \\(40\\%\\) hitter.\n\nlibrary(tidyverse)\n\n# simulate one season\none_season &lt;- rbinom(n = 35, size = 1, prob = 0.4)\none_season\n\n [1] 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1\n\n\n\n# total times on base\non_base_at_bats &lt;- sum(one_season)\non_base_at_bats\n\n[1] 11\n\n# as a proportion\non_base_at_bats / length(one_season)\n\n[1] 0.3142857\n\n\n\n\n\nNow let’s repeat this process \\(10{,}000\\) times.\n\nn &lt;- 10000\n\nsim_results &lt;- tibble(\n  trial = 1:n,\n  rocks = rbinom(n, size = 35, prob = 0.4)\n) |&gt; \n  mutate(proportion = rocks / 35)\n\nhead(sim_results)\n\n# A tibble: 6 × 3\n  trial rocks proportion\n  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1     1    13      0.371\n2     2    11      0.314\n3     3    17      0.486\n4     4    17      0.486\n5     5    11      0.314\n6     6    15      0.429\n\n\n\n\n\nHere’s the distribution of on-base proportions from the simulations.\n\non_base_hist &lt;- sim_results |&gt;\n  ggplot(aes(x = proportion)) +\n  geom_histogram(binwidth = 1/35, boundary = 0, fill = \"skyblue\", color = \"white\") \n\non_base_hist\n\n\n\n\n\n\n\n\n\n\n\nNow let’s add Cal’s observed rate (\\(0.514\\)):\n\non_base_hist +\n  geom_vline(xintercept = 0.514, color = \"firebrick\", linetype = 5, linewidth = 2)\n\n\n\n\n\n\n\n\n\n\n\nFinally, what proportion of simulated seasons were at least this extreme?\n\nsim_results |&gt; \n  summarise(prob_more_extreme = mean(proportion &gt;= 0.514))\n\n# A tibble: 1 × 1\n  prob_more_extreme\n              &lt;dbl&gt;\n1             0.113\n\n\n\n\n\nIn our simulation, only about X% of seasons produced an on-base percentage this high or higher if Cal were truly a 40% hitter.\n➡️ This suggests his observed \\(0.514\\) season is possibly due to chance alone — the evidence is not convincing that he might might be better than average.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#lets-formalize-this-with-tintles-6-steps",
    "href": "MA206-AY26-1/lesson-11.html#lets-formalize-this-with-tintles-6-steps",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "Is Cal’s on-base percentage higher than the Texas East Little League average of 40%?\n\n\n\nWe have observational data from Cal’s 35 plate appearances during the Little League World Series run.\n- \\(n = 35\\) plate appearances\n- \\(x = 18\\) times on base\n- Observed proportion: \\(\\hat \\pi = \\tfrac{18}{35} \\approx 0.514\\)\nWe treat these 35 at-bats as a random sample from his true underlying ability.\n\n\n\nThe observed proportion of \\(0.514\\) is above the reference average of \\(0.40\\). This is about 11 percentage points higher. The key question is whether this difference is large enough to be unlikely by chance.\n\n\n\nWe set up hypotheses:\n\nNull hypothesis: \\(H_0 : \\pi = 0.40\\)\n\nAlternative hypothesis: \\(H_A : \\pi &gt; 0.40\\)\n\nCompute the standard error and test statistic:\n\\[\nSE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n= \\sqrt{\\frac{0.40(0.60)}{35}}\n\\approx 0.083\n\\]\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{SE}\n= \\frac{0.514 - 0.40}{0.083}\n\\approx 1.37\n\\]\n\n\n\nThe one-tailed \\(p\\)-value is:\n\\[\np = P(Z \\geq 1.37) \\approx 0.085\n\\]\n\n1 - pnorm(q = 1.37, mean = 0, sd = 1)\n\n[1] 0.08534345\n\n\nAt \\(\\alpha = 0.05\\), this result is not statistically significant. We do not have strong enough evidence to conclude Cal’s on-base percentage is truly higher than 40%.\n\n\n\n\nThe observed rate of \\(0.514\\) is higher than average and suggestive (\\(p \\approx 0.085\\)).\n\nWith only 35 plate appearances, there is considerable variability — a larger sample would provide a clearer answer.\n\nContext matters: 35 at bats may not represent long-term ability. Future seasons with more data or more at-bats in a season could confirm or refute this pattern.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#lets-further-define-a-few-things",
    "href": "MA206-AY26-1/lesson-11.html#lets-further-define-a-few-things",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "The null hypothesis (\\(H_0\\)) is the starting assumption — usually that there is “no difference” or “no effect.”\n\nIn our example: \\(H_0 : \\pi = 0.40\\)\n\nThis means we assume Cal’s true on-base probability is the same as the Texas East Little League average of 40%.\n\n\n\n\nThe alternative hypothesis (\\(H_A\\)) is what we want to investigate. Depending on the research question, there are three common forms:\n\nRight-tailed (greater than)\n\n\\(H_A : \\pi &gt; 0.40\\)\n\nIn context: Is Cal’s true on-base percentage higher than 40%?\n\nThis is the version we are using, because the natural question is whether he’s better than average.\n\nLeft-tailed (less than)\n\n\\(H_A : \\pi &lt; 0.40\\)\n\nIn context: Is Cal’s true on-base percentage lower than 40%?\n\nYou would ask this if you suspected Cal might actually be worse than average at getting on base.\n\nTwo-tailed (not equal)\n\n\\(H_A : \\pi \\neq 0.40\\)\n\nIn context: Is Cal’s true on-base percentage different from 40% (either higher or lower)?\n\nYou would use this if you want to know whether Cal performs differently than average, without assuming in advance which direction.\n\n\n\n\n\n\nIf you are asking, “Is Cal better than average?”, the right-tailed test is appropriate.\n\nIf the concern were that Cal struggles at the plate, you’d use a left-tailed test.\n\nIf you only care whether Cal is different from average in either direction, the two-tailed test is the right choice.\n\n\n\n\n\n\\(\\pi\\) (the Greek letter pi) represents the population proportion of average of getting on base.\n\nIn context: \\(\\pi\\) is Cal’s true long-run probability of getting on base each plate appearance.\n\nWe never observe \\(\\pi\\) directly — we estimate it with \\(\\hat{\\pi}\\) (the sample proportion).\n\n\n\n\n\n\\(\\alpha\\) (alpha) is the threshold for evidence against the null hypothesis.\n\nCommon choices: \\(\\alpha = 0.05\\) (5%) or \\(\\alpha = 0.01\\) (1%).\n\nInterpretation: If \\(p \\leq \\alpha\\), the result is considered statistically significant — unlikely to occur just by chance if \\(H_0\\) were true.\n\n\n\n\nThe \\(z\\) distribution (also called the standard normal distribution) is a bell-shaped curve with:\n- Mean = \\(0\\)\n- Standard deviation = \\(1\\)\nSo how do we get from our sample proportion \\(\\hat{\\pi}\\) to this special distribution?\n\n\nUnder the null hypothesis \\(H_0 : \\pi = 0.40\\), the sample proportion \\(\\hat{\\pi}\\) has a sampling distribution that is approximately normal (by the Central Limit Theorem) with:\n\nMean = \\(\\pi_0 = 0.40\\)\n\nStandard deviation = \\(SE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\)\n\nThis tells us what values of \\(\\hat{\\pi}\\) we would expect just by chance if the null were true.\n\n\n\nTo compare our observed \\(\\hat{\\pi}\\) to this distribution, we standardize it:\n\nSubtract the mean under \\(H_0\\)\n\nThis centers the distribution at \\(0\\) by measuring how far away our observed statistic is from the null.\n\n\\((\\hat{\\pi} - \\pi_0)\\)\n\nDivide by the standard deviation\n\nThis rescales differences into standard deviation units, so we can judge how unusual they are.\n\n\\(\\dfrac{\\hat{\\pi} - \\pi_0}{SE}\\)\n\n\n\n\n\nAfter centering and rescaling, the new standardized statistic follows (approximately) the standard normal distribution, \\(N(0,1)\\).\nThat’s why we call it the \\(z\\) statistic:\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n\\]\nNow we can use the \\(z\\) distribution to calculate probabilities (like \\(p\\)-values) for how extreme our observed result is relative to the null hypothesis.\n\nggplot() +\n  geom_function(fun = dnorm, xlim = c(-4,4))",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#classroom-example-rockpaperscissors",
    "href": "MA206-AY26-1/lesson-11.html#classroom-example-rockpaperscissors",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "Class Activity: With your eyes closed, on the count of three, everyone will make their first throw of Rock–Paper–Scissors.\n- 👊 = Rock\n- ✋ = Paper\n- ✌️ = Scissors\nHold your choice steady so we can tally the results. Closing your eyes helps reduce herding or copying from neighbors.\n\n\nDo students tend to choose Rock more or less often than random chance (\\(33\\%\\)) on their first throw?\n\n\n\nWith eyes closed, on the count of 3 each student chooses one option (Rock/Paper/Scissors) as if starting a game. We then tally the class counts:\n- \\(R\\) = number of Rock\n- \\(P\\) = number of Paper\n- \\(S\\) = number of Scissors\n- \\(n = R+P+S\\)\n\n# Enter the tallies you just collected:\nR &lt;- 2   # Rock count\nP &lt;- 10   # Paper count\nS &lt;- 7   # Scissors count\n\nn &lt;- R + P + S\nc(R = R, P = P, S = S, n = n)\n\n R  P  S  n \n 2 10  7 19 \n\n\n\n\n\nCompute the sample proportion choosing Rock and make a quick bar chart. Under complete randomness we’d expect each to be near \\(n/3\\).\n\npihat &lt;- R / n\npihat\n\n[1] 0.1052632\n\ndf &lt;- tibble(option = c(\"Rock\",\"Paper\",\"Scissors\"),\n             count  = c(R, P, S))\n\nggplot(df, aes(option, count)) +\n  geom_col() +\n  geom_hline(yintercept = n/3, linetype = 2) +\n  labs(title = \"Class First-Throw Choices\",\n       subtitle = \"Dashed line = expected count if choices were uniform (n/3)\",\n       x = NULL, y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\nWe’ll do two things:\n\nSimulate many samples of size \\(n\\) assuming \\(p_{Rock}=1/3\\), and estimate a two-sided \\(p\\)-value by comparing simulated proportions to the observed.\n\nDo the math version using the \\(z\\)-distribution.\n\n\n\n\npi0 &lt;- 1/3\nN  &lt;- 10000               # number of simulations\nR_obs &lt;- R                # keep observed Rock consistent with Step 2\npihat_obs &lt;- R_obs / n\n\n# Simulate many samples under H0\nsim_results &lt;- tibble(\n  trial = 1:N,\n  rocks = rbinom(N, size = n, prob = pi0)   # Rock counts in each simulated sample\n) |&gt;\n  mutate(pihat = rocks / n)                  # simulated sample proportions\n\nsim_results\n\n# A tibble: 10,000 × 3\n   trial rocks pihat\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     1     4 0.211\n 2     2     8 0.421\n 3     3     6 0.316\n 4     4     3 0.158\n 5     5     8 0.421\n 6     6     3 0.158\n 7     7     7 0.368\n 8     8     5 0.263\n 9     9     2 0.105\n10    10     2 0.105\n# ℹ 9,990 more rows\n\n\n\n# Plot the simulated sampling distribution with observed and null marked\nsim_results |&gt;\n  ggplot(aes(x = pihat)) +\n  geom_histogram(binwidth = 1/n, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = pihat_obs, color = \"firebrick\", linetype = 2, linewidth = 1.2) +\n  geom_vline(xintercept = pi0, color = \"gray40\", linewidth = 1.2) +\n  labs(title = paste0(\"Sampling Distribution of p̂i under H0 (n = \", n, \")\"),\n       subtitle = paste0(\"Observed p̂ = \", round(pihat_obs,3), \n                         \" | Null pi0 = \", round(pi0,3)),\n       x = \"p̂ (proportion Rock)\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Two-sided simulation p-value\nobs_abs_diff &lt;- abs(pihat_obs - pi0)\n\nsim_p_two_sided &lt;- sim_results |&gt; \n  mutate(pihat_different_than_null = pihat - pi0) |&gt;          # difference from null\n  mutate(abs_of_difference = abs(pihat_different_than_null)) |&gt; \n  mutate(is_extreme = abs_of_difference &gt;= obs_abs_diff) |&gt; # flag extremes\n  summarise(p_val = mean(is_extreme))                       # proportion of extremes = p-value\n\nsim_p_two_sided\n\n# A tibble: 1 × 1\n   p_val\n   &lt;dbl&gt;\n1 0.0492\n\n\n\n\n\n\n\\(H_0: p_{Rock} = 1/3\\)\n\n\\(H_A: p_{Rock} \\neq 1/3\\)\n\n\\(SE = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\)\n\n# Standard Error under H0\nSE &lt;- sqrt(pi0 * (1 - pi0) / n)\nSE\n\n[1] 0.1081476\n\n\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n\\]\n\n# Test Statistic (z)\nz_stat &lt;- (pihat_obs - pi0) / SE\nz_stat\n\n[1] -2.108878\n\n\n\\[\np = 2 \\times P(Z \\geq |z|)\n\\]\n\n# Two-Sided p-value\np_val_two_sided &lt;- 2 * (1 - pnorm(abs(z_stat)))\np_val_two_sided\n\n[1] 0.03495507\n\n\n\n\n\n\nNow we bring the two approaches together:\n\nSimulation gave us an empirical \\(p\\)-value by resampling under \\(H_0\\).\n\nMath/analytic (\\(z\\)-test) gave us an approximate \\(p\\)-value using the standard normal.\n\n\n\n              n           R_obs       pihat_obs             pi0          z_stat \n        19.0000          2.0000          0.1050          0.3330         -2.1090 \nsim_p_two_sided   z_p_two_sided           alpha \n         0.0492          0.0350          0.0500 \n\n\nSimulation-based decision: Reject H0 (evidence of ≠ 1/3) \n\n\nZ-approximation decision: Reject H0 (evidence of ≠ 1/3) \n\n\nInterpretation:\nWith \\(n\\) throws and observed \\(\\hat p\\), the simulation \\(p\\)-value tells us how unusual the result is if \\(p=1/3\\) were true. The \\(z\\) test gives a similar answer using a theoretical normal curve. At \\(\\alpha = 0.05\\), compare both to decide whether to reject \\(H_0\\).\n\n\n\n\n\nCode\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tibble)\n\nui &lt;- fluidPage(\n  titlePanel(\"One-Proportion Test: z-formulas + Live Simulation\"),\n  withMathJax(),\n  tags$hr(),\n  \n  fluidRow(\n    column(\n      width = 4,\n      h4(\"Inputs\"),\n      numericInput(\"x\", \"Observed successes (x)\", value = 2, min = 0, step = 1),\n      numericInput(\"n\", \"Sample size (n)\", value = 19, min = 1, step = 1),\n      sliderInput(\"pi0\", HTML(\"&pi;&lt;sub&gt;0&lt;/sub&gt; (null proportion)\"), min = 0, max = 1,\n                  value = 1/3, step = 0.01),\n      numericInput(\"B\", \"Simulations (B)\", value = 10000, min = 100, step = 100),\n      numericInput(\"seed\", \"Random seed\", value = 26, min = 1, step = 1),\n      helpText(\"Tip: This defaults to the R/P/S example: x = 2 (Rock), n = 19, π0 = 1/3.\")\n    ),\n    column(\n      width = 8,\n      h4(\"Formulas\"),\n      # Show the formulas via MathJax\n      div(style = \"font-size: 1.15em; margin-bottom: 8px;\",\n          \"$$ SE = \\\\sqrt{\\\\frac{\\\\pi_0(1 - \\\\pi_0)}{n}}, \\\\qquad z = \\\\frac{\\\\hat{p} - \\\\pi_0}{SE} $$\"\n      ),\n      h4(\"Computed Values\"),\n      tableOutput(\"value_table\"),\n      tags$br(),\n      h4(\"Sampling Distribution under H0 (Simulated)\"),\n      plotOutput(\"hist_plot\", height = \"330px\"),\n      helpText(\"Histogram shows simulated \\\\(\\\\hat p\\\\) under H0. Red line = observed \\\\(\\\\hat p\\\\). Gray line = \\\\(\\\\pi_0\\\\).\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  # Basic reactives\n  phat &lt;- reactive({\n    req(input$n &gt; 0)\n    input$x / input$n\n  })\n  \n  SE &lt;- reactive({\n    sqrt(input$pi0 * (1 - input$pi0) / input$n)\n  })\n  \n  z_stat &lt;- reactive({\n    (phat() - input$pi0) / SE()\n  })\n  \n  # p-values for three alternatives\n  p_right &lt;- reactive({ 1 - pnorm(z_stat()) })                 # H_A: p &gt; pi0\n  p_left  &lt;- reactive({ pnorm(z_stat()) })                     # H_A: p &lt; pi0\n  p_two   &lt;- reactive({ 2 * (1 - pnorm(abs(z_stat()))) })      # H_A: p != pi0\n  \n  # Simulation under H0\n  sim_df &lt;- reactive({\n    req(input$B &gt;= 100)\n    set.seed(input$seed)\n    rocks &lt;- rbinom(input$B, size = input$n, prob = input$pi0)\n    tibble(\n      phat = rocks / input$n\n    )\n  })\n  \n  # Output: table of computed values\n  output$value_table &lt;- renderTable({\n    tibble::tibble(\n      `x (successes)` = input$x,\n      `n (trials)`    = input$n,\n      `π0 (null)`     = round(input$pi0, 4),\n      `p̂ = x/n`      = round(phat(), 4),\n      `SE`            = round(SE(), 5),\n      `z`             = round(z_stat(), 4),\n      `p (right)`     = signif(p_right(), 4),\n      `p (left)`      = signif(p_left(), 4),\n      `p (two-sided)` = signif(p_two(), 4)\n    )\n  }, striped = TRUE, bordered = TRUE, spacing = \"s\", digits = 6)\n  \n  # Output: histogram with vertical lines at phat and pi0\n  output$hist_plot &lt;- renderPlot({\n    df &lt;- sim_df()\n    ggplot(df, aes(x = phat)) +\n      geom_histogram(binwidth = 1 / input$n, color = \"white\") +\n      geom_vline(xintercept = phat(), color = \"firebrick\", linetype = 2, linewidth = 1.2) +\n      geom_vline(xintercept = input$pi0, color = \"gray40\", linewidth = 1.2) +\n      labs(\n        x = expression(hat(p) ~ \"(proportion)\"),\n        y = \"Count\",\n        title = paste0(\"Simulated Sampling Distribution of \", expression(hat(p)), \" under H0\"),\n        subtitle = paste0(\"n = \", input$n, \", π0 = \", round(input$pi0, 3),\n                          \", observed p̂ = \", round(phat(), 3),\n                          \"; B = \", input$B)\n      ) +\n      theme_minimal(base_size = 12) +\n      xlim(c(0,1))\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#reference-table-z-tests-for-one-proportion",
    "href": "MA206-AY26-1/lesson-11.html#reference-table-z-tests-for-one-proportion",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "For all cases:\n\\[\nSE = \\sqrt{\\frac{\\pi_0(1 - \\pi_0)}{n}}, \\quad\nz = \\frac{\\hat{p} - \\pi_0}{SE}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#board-problem",
    "href": "MA206-AY26-1/lesson-11.html#board-problem",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "A hospital claims that 85% of discharge summaries are finalized within 24 hours.\nIn an audit of 60 summaries, 46 were finalized within 24 hours.\nResearch Question: Is the true proportion finalized within 24 hours less than 85%?\n\nState the hypotheses.\n\nExplain (in words) how you would simulate this test (do not actually simulate).\n\nThen, perform the mathematical one-proportion \\(z\\) test.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nDo fewer than 85% of discharge summaries get finalized within 24 hours?\n\n\n\n\n\\(H_0 : \\pi = 0.85\\)\n\n\\(H_A : \\pi &lt; 0.85\\)\n\n\n\n\n\n\\(n = 60\\) summaries\n\n\\(x = 46\\) finalized within 24h\n\n\\(\\hat{\\pi} = \\tfrac{x}{n} = \\tfrac{46}{60} = 0.767\\)\n\n\n\n\n\n\n\n\nUnder \\(H_0\\), simulate many samples of size \\(n=60\\) with \\(\\pi_0=0.85\\).\n\nFor each, compute \\(\\hat{\\pi}_{sim}\\).\n\nEstimate the left-tailed \\(p\\)-value as the fraction of simulations with \\(\\hat{\\pi}_{sim} \\le \\hat{\\pi}_{obs} = 0.767\\).\n\n\n\n\nStandard error under \\(H_0\\):\n\\[\nSE = \\sqrt{\\frac{\\pi_0 (1-\\pi_0)}{n}}\n   = \\sqrt{\\frac{0.85 \\cdot 0.15}{60}}\n   \\approx 0.046\n\\]\nTest statistic:\n\\[\nz = \\frac{\\hat{\\pi} - \\pi_0}{SE}\n  = \\frac{0.767 - 0.85}{0.046}\n  \\approx -1.80\n\\]\n\\(p\\)-value (left-tailed):\n\\[\np = \\Phi(z) = \\Phi(-1.80) \\approx 0.036\n\\]\n\npnorm(-1.8)\n\n[1] 0.03593032\n\n\n\n\n\nAt \\(\\alpha = 0.05\\), since \\(p \\approx 0.036 &lt; 0.05\\), we reject \\(H_0\\).\n\n\n\nThere is statistical evidence that fewer than 85% of discharge summaries are completed within 24 hours.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-11.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-11.html#before-you-leave",
    "title": "Lesson 11: One Proportion Z-Test",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nProject Milestone 3: Due Canvas 22 Sept\nExploration Exercise 1.5: Due at 0700 on Lesson 13\n24 September 2025 for Day 1\n\n25 September 2025 for Day 2)\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 11"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-13.html",
    "href": "MA206-AY26-1/lesson-13.html",
    "title": "Lesson 13: Generalization",
    "section": "",
    "text": "If turned in this morning, can earn 100% of points.\nIf turned in by 0700 tomorrow morning, can earn 80% of points.\n\n\n\n\n\n⏰ Due 0700 ET on 1 October   \n\n\n\n\n\nLesson 17\nMilestone 4\n\nWith partner\nWrite 1-2 paragraphs per article summarizing the articles topic with a take away for its insight on your project.\nMake updates from Milestone 3 feedback.\nFill out Annex B for my comments on Milestone 3.\nTurn in EVERYTHING in your working write up.\nKeep your binder up-to-date, but I don’t want to see it.\n\n\n\n\n\n\n\n\n\nMath 1 vs Systems\n\n\n\n\n\n\nPreviously 3-0\n\n\n\n\n\n\n4-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all cases:\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.\n\n\n\n\nFor all cases:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t_{df}}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A: \\mu &lt; \\mu_0\\)\n\\(p = F_{t_{df}}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A: \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t_{df}}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) = degrees of freedom\n\n\\(F_{t_{df}}(\\cdot)\\) = cumulative distribution function (CDF) of the Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\n\n\n\n\nWe alter different values?\nWhat is a parameter…? And a statistic?\n\n\nCode\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tibble)\n\nui &lt;- fluidPage(\n  titlePanel(\"One-Proportion (z) & One-Mean (t) Tests\"),\n  withMathJax(),\n  tags$hr(),\n\n  tabsetPanel(\n    id = \"tabs\",\n\n    # ---------------- Proportion tab (z) ----------------\n    tabPanel(\n      title = \"Proportion (z-test)\",\n      fluidRow(\n        column(\n          width = 4,\n          h4(\"Inputs\"),\n          numericInput(\"x\", \"Observed successes (x)\", value = 2, min = 0, step = 1),\n          numericInput(\"n\", \"Sample size (n)\", value = 19, min = 1, step = 1),\n          sliderInput(\"pi0\", HTML(\"&pi;&lt;sub&gt;0&lt;/sub&gt; (null proportion)\"),\n                      min = 0, max = 1, value = 1/3, step = 0.01)\n        ),\n        column(\n          width = 8,\n          h4(\"Formula\"),\n          div(style = \"font-size: 1.15em; margin-bottom: 8px;\",\n              \"$$ z = \\\\frac{\\\\hat{p} - \\\\pi_0}{\\\\sqrt{\\\\tfrac{\\\\pi_0(1 - \\\\pi_0)}{n}}} $$\"\n          ),\n          h4(\"Computed Values\"),\n          tableOutput(\"value_table_prop\"),\n          tags$br(),\n          h4(\"Standard Normal (z) under H0\"),\n          plotOutput(\"plot_prop\", height = \"330px\")\n        )\n      )\n    ),\n\n    # ---------------- Mean tab (t) ----------------\n    tabPanel(\n      title = \"Mean (t-test)\",\n      fluidRow(\n        column(\n          width = 4,\n          h4(\"Inputs\"),\n          numericInput(\"xbar\", HTML(\"Sample mean (\\\\(\\\\bar{x}\\\\))\"), value = 10, step = 0.1),\n          numericInput(\"s\", HTML(\"Sample SD (\\\\(s\\\\))\"), value = 3, min = 0.0001, step = 0.1),\n          numericInput(\"n_mean\", \"Sample size (n)\", value = 20, min = 2, step = 1),\n          numericInput(\"mu0\", HTML(\"\\\\(\\\\mu_0\\\\) (null mean)\"), value = 9, step = 0.1),\n          selectInput(\"alt\",\n                      \"Alternative hypothesis (affects plot subtitle only)\",\n                      choices = c(\"Two-sided\" = \"two.sided\",\n                                  \"Less than (μ &lt; μ0)\" = \"less\",\n                                  \"Greater than (μ &gt; μ0)\" = \"greater\"),\n                      selected = \"two.sided\")\n        ),\n        column(\n          width = 8,\n          h4(\"Formula\"),\n          div(style = \"font-size: 1.15em; margin-bottom: 8px;\",\n              \"$$ t = \\\\frac{\\\\bar{x} - \\\\mu_0}{\\\\dfrac{s}{\\\\sqrt{n}}},\\\\qquad df = n-1 $$\"\n          ),\n          h4(\"Computed Values\"),\n          tableOutput(\"value_table_mean\"),\n          tags$br(),\n          h4(\"t Distribution under H0\"),\n          plotOutput(\"plot_mean\", height = \"330px\")\n        )\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  # ===== Proportion (z) =====\n  phat &lt;- reactive({\n    req(input$n &gt; 0)\n    input$x / input$n\n  })\n  SE_prop &lt;- reactive({\n    sqrt(input$pi0 * (1 - input$pi0) / input$n)\n  })\n  z_stat &lt;- reactive({\n    (phat() - input$pi0) / SE_prop()\n  })\n  p_right_prop &lt;- reactive({ 1 - pnorm(z_stat()) })\n  p_left_prop  &lt;- reactive({ pnorm(z_stat()) })\n  p_two_prop   &lt;- reactive({ 2 * (1 - pnorm(abs(z_stat()))) })\n\n  output$value_table_prop &lt;- renderTable({\n    tibble::tibble(\n      `x (successes)` = input$x,\n      `n (trials)`    = input$n,\n      `π0 (null)`     = round(input$pi0, 4),\n      `p̂ = x/n`      = round(phat(), 4),\n      `SE`            = round(SE_prop(), 5),\n      `z`             = round(z_stat(), 4),\n      `p (right)`     = signif(p_right_prop(), 4),\n      `p (left)`      = signif(p_left_prop(), 4),\n      `p (two-sided)` = signif(p_two_prop(), 4)\n    )\n  }, striped = TRUE, bordered = TRUE, spacing = \"s\", digits = 6)\n\n  output$plot_prop &lt;- renderPlot({\n    z_grid &lt;- seq(-4, 4, length.out = 400)\n    df &lt;- tibble(z = z_grid, density = dnorm(z_grid))\n    ggplot(df, aes(x = z, y = density)) +\n      geom_line(linewidth = 1.2) +\n      geom_vline(xintercept = z_stat(), linetype = 2, linewidth = 1.2) +\n      labs(\n        x = \"z\",\n        y = \"Density\",\n        title = \"Standard Normal Distribution (H0)\",\n        subtitle = paste0(\"Observed z = \", round(z_stat(), 3))\n      ) +\n      theme_minimal(base_size = 12)\n  })\n\n  # ===== Mean (t) =====\n  df_mean &lt;- reactive({\n    req(input$n_mean &gt;= 2)\n    input$n_mean - 1\n  })\n  SE_mean &lt;- reactive({\n    input$s / sqrt(input$n_mean)\n  })\n  t_stat &lt;- reactive({\n    (input$xbar - input$mu0) / SE_mean()\n  })\n\n  # Show ALL p-values for mean (like the proportion tab)\n  p_right_mean &lt;- reactive({ 1 - pt(t_stat(), df = df_mean()) })                    # H_A: μ &gt; μ0\n  p_left_mean  &lt;- reactive({ pt(t_stat(), df = df_mean()) })                        # H_A: μ &lt; μ0\n  p_two_mean   &lt;- reactive({ 2 * (1 - pt(abs(t_stat()), df = df_mean())) })         # H_A: μ != μ0\n\n  output$value_table_mean &lt;- renderTable({\n    tibble::tibble(\n      `x̄`            = round(input$xbar, 4),\n      `s`             = round(input$s, 4),\n      `n`             = input$n_mean,\n      `μ0`            = round(input$mu0, 4),\n      `SE = s/√n`     = round(SE_mean(), 5),\n      `df`            = df_mean(),\n      `t`             = round(t_stat(), 4),\n      `p (right)`     = signif(p_right_mean(), 5),\n      `p (left)`      = signif(p_left_mean(), 5),\n      `p (two-sided)` = signif(p_two_mean(), 5)\n    )\n  }, striped = TRUE, bordered = TRUE, spacing = \"s\", digits = 6)\n\n  output$plot_mean &lt;- renderPlot({\n    df0 &lt;- df_mean()\n    t_grid &lt;- seq(-4.5, 4.5, length.out = 400)\n    d &lt;- tibble(t = t_grid, density = dt(t_grid, df = df0))\n    # pick the p-value matching the selected alternative for display only\n    p_disp &lt;- switch(input$alt,\n                     \"less\"      = p_left_mean(),\n                     \"greater\"   = p_right_mean(),\n                     \"two.sided\" = p_two_mean())\n\n    ggplot(d, aes(x = t, y = density)) +\n      geom_line(linewidth = 1.2) +\n      geom_vline(xintercept = t_stat(), linetype = 2, linewidth = 1.2) +\n      labs(\n        x = \"t\",\n        y = \"Density\",\n        title = paste0(\"t(\", df0, \") Distribution under H0\"),\n        subtitle = paste0(\"Observed t = \", round(t_stat(), 3),\n                          \" | Alt = \", input$alt,\n                          \" | p-value = \", signif(p_disp, 5))\n      ) +\n      theme_minimal(base_size = 12)\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\n\n\n\nGeneralization: We can generalize results to a larger population if the sample is random and representative of that population. Convenience samples don’t justify broad claims.\n\nCausation: We can claim causation only if the study design is a randomized experiment. Observational studies can show associations, but not cause-and-effect.\n\n\n\n\n\n\n\n“Who is left-handed?”\nWe’ll test whether our class has a different left-handed rate than the commonly cited 10% at the 5% confidence level.\n\n\n\n\n\n“Everyone, measure your resting heart rate (count beats for 15 seconds × 4).”\n\nWe’ll test whether our class’s average resting heart rate is different from the typical 70 bpm, at the 10% significance level.\n\n\n\n\n\n\n\nA recent poll asked 120 cadets whether they prefer running or rucking for morning PT. Out of the 120, 78 cadets preferred running.\nAt the 10% significance level, is there evidence that more than half of cadets prefer running?\nTasks:\n- State the null and alternative hypotheses.\n- Compute the test statistic.\n- Draw the sampling distribution, marking the test statistic.\n- Calculate the \\(p\\)-value.\n- Make a decision at the stated significant level.\n- Interpret the result in context.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\(H_0: p = 0.5\\)\n\n\\(H_A: p &gt; 0.5\\)\n\n\n\n\n\\[\n\\hat{p} = \\frac{78}{120} = 0.65, \\quad\nSE = \\sqrt{\\frac{0.5 (1 - 0.5)}{120}} \\approx 0.0456\n\\]\n\\[\nz = \\frac{\\hat{p} - 0.5}{SE} = \\frac{0.65 - 0.5}{0.0456} \\approx 3.29\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\np = 1 - \\Phi(3.29) \\approx 0.0005\n\\]\n\n\n\nSince \\(p &lt; 0.10\\), reject \\(H_0\\).\n\n\n\nThere is strong evidence that more than half of cadets prefer running.\n\n\n\n\n\n\n\nAn instructor believes that the average number of push-ups completed by cadets in two minutes is greater than 70. A random sample of \\(n = 25\\) cadets had a mean of \\(\\bar{x} = 74.2\\) with a sample standard deviation of \\(s = 8.5\\).\nAt the 10% significance level, test the instructor’s claim.\nTasks:\n- State the null and alternative hypotheses.\n- Compute the test statistic.\n- Draw the sampling distribution, marking the test statistic.\n- Calculate the \\(p\\)-value.\n- Make a decision at the stated significant level.\n- Interpret the result in context.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\(H_0: \\mu = 70\\)\n\n\\(H_A: \\mu &gt; 70\\)\n\n\n\n\n\\[\nSE = \\frac{s}{\\sqrt{n}} = \\frac{8.5}{\\sqrt{25}} = 1.7\n\\]\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{SE} = \\frac{74.2 - 70}{1.7} \\approx 2.47\n\\]\nwith \\(df = n - 1 = 24\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\np = 1 - F_{t_{24}}(2.47) \\approx 0.011\n\\]\n\n\n\nSince \\(p &lt; 0.10\\), reject \\(H_0\\).\n\n\n\nThere is evidence that the true mean number of push-ups is greater than 70.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 13"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-13.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-13.html#lesson-administration",
    "title": "Lesson 13: Generalization",
    "section": "",
    "text": "If turned in this morning, can earn 100% of points.\nIf turned in by 0700 tomorrow morning, can earn 80% of points.\n\n\n\n\n\n⏰ Due 0700 ET on 1 October   \n\n\n\n\n\nLesson 17\nMilestone 4\n\nWith partner\nWrite 1-2 paragraphs per article summarizing the articles topic with a take away for its insight on your project.\nMake updates from Milestone 3 feedback.\nFill out Annex B for my comments on Milestone 3.\nTurn in EVERYTHING in your working write up.\nKeep your binder up-to-date, but I don’t want to see it.\n\n\n\n\n\n\n\n\n\nMath 1 vs Systems\n\n\n\n\n\n\nPreviously 3-0\n\n\n\n\n\n\n4-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all cases:\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.\n\n\n\n\nFor all cases:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t_{df}}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A: \\mu &lt; \\mu_0\\)\n\\(p = F_{t_{df}}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A: \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t_{df}}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) = degrees of freedom\n\n\\(F_{t_{df}}(\\cdot)\\) = cumulative distribution function (CDF) of the Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\n\n\n\n\nWe alter different values?\nWhat is a parameter…? And a statistic?\n\n\nCode\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tibble)\n\nui &lt;- fluidPage(\n  titlePanel(\"One-Proportion (z) & One-Mean (t) Tests\"),\n  withMathJax(),\n  tags$hr(),\n\n  tabsetPanel(\n    id = \"tabs\",\n\n    # ---------------- Proportion tab (z) ----------------\n    tabPanel(\n      title = \"Proportion (z-test)\",\n      fluidRow(\n        column(\n          width = 4,\n          h4(\"Inputs\"),\n          numericInput(\"x\", \"Observed successes (x)\", value = 2, min = 0, step = 1),\n          numericInput(\"n\", \"Sample size (n)\", value = 19, min = 1, step = 1),\n          sliderInput(\"pi0\", HTML(\"&pi;&lt;sub&gt;0&lt;/sub&gt; (null proportion)\"),\n                      min = 0, max = 1, value = 1/3, step = 0.01)\n        ),\n        column(\n          width = 8,\n          h4(\"Formula\"),\n          div(style = \"font-size: 1.15em; margin-bottom: 8px;\",\n              \"$$ z = \\\\frac{\\\\hat{p} - \\\\pi_0}{\\\\sqrt{\\\\tfrac{\\\\pi_0(1 - \\\\pi_0)}{n}}} $$\"\n          ),\n          h4(\"Computed Values\"),\n          tableOutput(\"value_table_prop\"),\n          tags$br(),\n          h4(\"Standard Normal (z) under H0\"),\n          plotOutput(\"plot_prop\", height = \"330px\")\n        )\n      )\n    ),\n\n    # ---------------- Mean tab (t) ----------------\n    tabPanel(\n      title = \"Mean (t-test)\",\n      fluidRow(\n        column(\n          width = 4,\n          h4(\"Inputs\"),\n          numericInput(\"xbar\", HTML(\"Sample mean (\\\\(\\\\bar{x}\\\\))\"), value = 10, step = 0.1),\n          numericInput(\"s\", HTML(\"Sample SD (\\\\(s\\\\))\"), value = 3, min = 0.0001, step = 0.1),\n          numericInput(\"n_mean\", \"Sample size (n)\", value = 20, min = 2, step = 1),\n          numericInput(\"mu0\", HTML(\"\\\\(\\\\mu_0\\\\) (null mean)\"), value = 9, step = 0.1),\n          selectInput(\"alt\",\n                      \"Alternative hypothesis (affects plot subtitle only)\",\n                      choices = c(\"Two-sided\" = \"two.sided\",\n                                  \"Less than (μ &lt; μ0)\" = \"less\",\n                                  \"Greater than (μ &gt; μ0)\" = \"greater\"),\n                      selected = \"two.sided\")\n        ),\n        column(\n          width = 8,\n          h4(\"Formula\"),\n          div(style = \"font-size: 1.15em; margin-bottom: 8px;\",\n              \"$$ t = \\\\frac{\\\\bar{x} - \\\\mu_0}{\\\\dfrac{s}{\\\\sqrt{n}}},\\\\qquad df = n-1 $$\"\n          ),\n          h4(\"Computed Values\"),\n          tableOutput(\"value_table_mean\"),\n          tags$br(),\n          h4(\"t Distribution under H0\"),\n          plotOutput(\"plot_mean\", height = \"330px\")\n        )\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  # ===== Proportion (z) =====\n  phat &lt;- reactive({\n    req(input$n &gt; 0)\n    input$x / input$n\n  })\n  SE_prop &lt;- reactive({\n    sqrt(input$pi0 * (1 - input$pi0) / input$n)\n  })\n  z_stat &lt;- reactive({\n    (phat() - input$pi0) / SE_prop()\n  })\n  p_right_prop &lt;- reactive({ 1 - pnorm(z_stat()) })\n  p_left_prop  &lt;- reactive({ pnorm(z_stat()) })\n  p_two_prop   &lt;- reactive({ 2 * (1 - pnorm(abs(z_stat()))) })\n\n  output$value_table_prop &lt;- renderTable({\n    tibble::tibble(\n      `x (successes)` = input$x,\n      `n (trials)`    = input$n,\n      `π0 (null)`     = round(input$pi0, 4),\n      `p̂ = x/n`      = round(phat(), 4),\n      `SE`            = round(SE_prop(), 5),\n      `z`             = round(z_stat(), 4),\n      `p (right)`     = signif(p_right_prop(), 4),\n      `p (left)`      = signif(p_left_prop(), 4),\n      `p (two-sided)` = signif(p_two_prop(), 4)\n    )\n  }, striped = TRUE, bordered = TRUE, spacing = \"s\", digits = 6)\n\n  output$plot_prop &lt;- renderPlot({\n    z_grid &lt;- seq(-4, 4, length.out = 400)\n    df &lt;- tibble(z = z_grid, density = dnorm(z_grid))\n    ggplot(df, aes(x = z, y = density)) +\n      geom_line(linewidth = 1.2) +\n      geom_vline(xintercept = z_stat(), linetype = 2, linewidth = 1.2) +\n      labs(\n        x = \"z\",\n        y = \"Density\",\n        title = \"Standard Normal Distribution (H0)\",\n        subtitle = paste0(\"Observed z = \", round(z_stat(), 3))\n      ) +\n      theme_minimal(base_size = 12)\n  })\n\n  # ===== Mean (t) =====\n  df_mean &lt;- reactive({\n    req(input$n_mean &gt;= 2)\n    input$n_mean - 1\n  })\n  SE_mean &lt;- reactive({\n    input$s / sqrt(input$n_mean)\n  })\n  t_stat &lt;- reactive({\n    (input$xbar - input$mu0) / SE_mean()\n  })\n\n  # Show ALL p-values for mean (like the proportion tab)\n  p_right_mean &lt;- reactive({ 1 - pt(t_stat(), df = df_mean()) })                    # H_A: μ &gt; μ0\n  p_left_mean  &lt;- reactive({ pt(t_stat(), df = df_mean()) })                        # H_A: μ &lt; μ0\n  p_two_mean   &lt;- reactive({ 2 * (1 - pt(abs(t_stat()), df = df_mean())) })         # H_A: μ != μ0\n\n  output$value_table_mean &lt;- renderTable({\n    tibble::tibble(\n      `x̄`            = round(input$xbar, 4),\n      `s`             = round(input$s, 4),\n      `n`             = input$n_mean,\n      `μ0`            = round(input$mu0, 4),\n      `SE = s/√n`     = round(SE_mean(), 5),\n      `df`            = df_mean(),\n      `t`             = round(t_stat(), 4),\n      `p (right)`     = signif(p_right_mean(), 5),\n      `p (left)`      = signif(p_left_mean(), 5),\n      `p (two-sided)` = signif(p_two_mean(), 5)\n    )\n  }, striped = TRUE, bordered = TRUE, spacing = \"s\", digits = 6)\n\n  output$plot_mean &lt;- renderPlot({\n    df0 &lt;- df_mean()\n    t_grid &lt;- seq(-4.5, 4.5, length.out = 400)\n    d &lt;- tibble(t = t_grid, density = dt(t_grid, df = df0))\n    # pick the p-value matching the selected alternative for display only\n    p_disp &lt;- switch(input$alt,\n                     \"less\"      = p_left_mean(),\n                     \"greater\"   = p_right_mean(),\n                     \"two.sided\" = p_two_mean())\n\n    ggplot(d, aes(x = t, y = density)) +\n      geom_line(linewidth = 1.2) +\n      geom_vline(xintercept = t_stat(), linetype = 2, linewidth = 1.2) +\n      labs(\n        x = \"t\",\n        y = \"Density\",\n        title = paste0(\"t(\", df0, \") Distribution under H0\"),\n        subtitle = paste0(\"Observed t = \", round(t_stat(), 3),\n                          \" | Alt = \", input$alt,\n                          \" | p-value = \", signif(p_disp, 5))\n      ) +\n      theme_minimal(base_size = 12)\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 13"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-13.html#generalization-and-causation",
    "href": "MA206-AY26-1/lesson-13.html#generalization-and-causation",
    "title": "Lesson 13: Generalization",
    "section": "",
    "text": "Generalization: We can generalize results to a larger population if the sample is random and representative of that population. Convenience samples don’t justify broad claims.\n\nCausation: We can claim causation only if the study design is a randomized experiment. Observational studies can show associations, but not cause-and-effect.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 13"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-13.html#live-examples",
    "href": "MA206-AY26-1/lesson-13.html#live-examples",
    "title": "Lesson 13: Generalization",
    "section": "",
    "text": "“Who is left-handed?”\nWe’ll test whether our class has a different left-handed rate than the commonly cited 10% at the 5% confidence level.\n\n\n\n\n\n“Everyone, measure your resting heart rate (count beats for 15 seconds × 4).”\n\nWe’ll test whether our class’s average resting heart rate is different from the typical 70 bpm, at the 10% significance level.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 13"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-13.html#board-problems",
    "href": "MA206-AY26-1/lesson-13.html#board-problems",
    "title": "Lesson 13: Generalization",
    "section": "",
    "text": "A recent poll asked 120 cadets whether they prefer running or rucking for morning PT. Out of the 120, 78 cadets preferred running.\nAt the 10% significance level, is there evidence that more than half of cadets prefer running?\nTasks:\n- State the null and alternative hypotheses.\n- Compute the test statistic.\n- Draw the sampling distribution, marking the test statistic.\n- Calculate the \\(p\\)-value.\n- Make a decision at the stated significant level.\n- Interpret the result in context.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\(H_0: p = 0.5\\)\n\n\\(H_A: p &gt; 0.5\\)\n\n\n\n\n\\[\n\\hat{p} = \\frac{78}{120} = 0.65, \\quad\nSE = \\sqrt{\\frac{0.5 (1 - 0.5)}{120}} \\approx 0.0456\n\\]\n\\[\nz = \\frac{\\hat{p} - 0.5}{SE} = \\frac{0.65 - 0.5}{0.0456} \\approx 3.29\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\np = 1 - \\Phi(3.29) \\approx 0.0005\n\\]\n\n\n\nSince \\(p &lt; 0.10\\), reject \\(H_0\\).\n\n\n\nThere is strong evidence that more than half of cadets prefer running.\n\n\n\n\n\n\n\nAn instructor believes that the average number of push-ups completed by cadets in two minutes is greater than 70. A random sample of \\(n = 25\\) cadets had a mean of \\(\\bar{x} = 74.2\\) with a sample standard deviation of \\(s = 8.5\\).\nAt the 10% significance level, test the instructor’s claim.\nTasks:\n- State the null and alternative hypotheses.\n- Compute the test statistic.\n- Draw the sampling distribution, marking the test statistic.\n- Calculate the \\(p\\)-value.\n- Make a decision at the stated significant level.\n- Interpret the result in context.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\(H_0: \\mu = 70\\)\n\n\\(H_A: \\mu &gt; 70\\)\n\n\n\n\n\\[\nSE = \\frac{s}{\\sqrt{n}} = \\frac{8.5}{\\sqrt{25}} = 1.7\n\\]\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{SE} = \\frac{74.2 - 70}{1.7} \\approx 2.47\n\\]\nwith \\(df = n - 1 = 24\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\np = 1 - F_{t_{24}}(2.47) \\approx 0.011\n\\]\n\n\n\nSince \\(p &lt; 0.10\\), reject \\(H_0\\).\n\n\n\nThere is evidence that the true mean number of push-ups is greater than 70.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 13"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-13.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-13.html#before-you-leave",
    "title": "Lesson 13: Generalization",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 13"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-15.html",
    "href": "MA206-AY26-1/lesson-15.html",
    "title": "Lesson 15: Confidence Interval (Quantitative)",
    "section": "",
    "text": "G1: No Change\nG1: No Change\nI1: Possibly Meet in TH120 but for right now stay put\nJ2: No Change\n\n\n\n\n\nLesson 17\nMilestone 4\n\nWith partner\nWrite 1-2 paragraphs per article summarizing the articles topic with a take away for its insight on your project.\nMake updates from Milestone 3 feedback.\nFill out Annex B for my comments on Milestone 3.\nTurn in EVERYTHING in your working write up.\nKeep your binder up-to-date, but I don’t want to see it.\n\n\n\n\n\n\nLesson 17\n25 Points!\nLike a WPR What does that mean?!!\nRead ahead\n\n\n\n\n\n⏰ Due 0700 on **Lesson 18*\nLets take a look at it\n\nDay 1: Tuesday, 14 Oct 2025\nDay 2: Wednesday, 15 Oct 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings didn’t start well…\n\n\n\n\n\nBut Cal slowly got his act together.\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\nThen he was the first bracket pitcher on the next day against the 2 Seed…\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\nGot the dub:\n\n\n\n\n\nLost in the semis vs the eventual champion - not a bad tournament\n\n\n\n\n\n\n\n\nWe’ll talk about her next lesson!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all cases:\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.\n\n\n\n\nFor all cases:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t_{df}}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A: \\mu &lt; \\mu_0\\)\n\\(p = F_{t_{df}}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A: \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t_{df}}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) = degrees of freedom\n\n\\(F_{t_{df}}(\\cdot)\\) = cumulative distribution function (CDF) of the Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\n\n\n\n\nWhat impacts does altering different values have?\n\nGeneralization: We can generalize results to a larger population if the sample is random and representative of that population. Convenience samples don’t justify broad claims.\n\nCausation: We can claim causation only if the study design is a randomized experiment. Observational studies can show associations, but not cause-and-effect.\nParameters vs. Statistics: A parameter is a fixed (but usually unknown) numerical value describing a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\pi\\)). A statistic is a numerical value computed from a sample (e.g., \\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\)).\n\nParameters = target (what we want to know).\n\nStatistics = evidence (what we can actually measure).\n\nWe use statistics to estimate parameters, and because different samples give different statistics, we capture this variability with confidence intervals.\n\n\n\n\n\nQuantity\nPopulation (Parameter)\nSample (Statistic)\n\n\n\n\nCenter (mean)\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nSpread (SD)\n\\(\\sigma\\)\n\\(s\\)\n\n\nProportion “success”\n\\(\\pi\\)\n\\(\\hat{p}\\)\n\n\n\n\n\n\n\nSuppose we take a random sample of \\(n\\) cadets and measure a quantitative outcome (e.g., 2-mile run times). We want to estimate the true population mean \\(\\mu\\).\nWe start with the one-sample \\(t\\) statistic:\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}.\n\\]\nAt significance level \\(\\alpha = 0.05\\), we reject \\(H_0\\) whenever \\(t\\) falls into the rejection region (the critical tail(s)) of the \\(t\\) distribution with \\(df=n-1\\).\nWhere on this plot would we reject?\n\n\n\n\n\n\n\n\n\nWhere are these lines?\n\n\n\n\n\n\n\n\n\nSo, the left tail contains \\(0.025\\) of the distribution, and the right tail contains \\(0.025\\).\nHow do we find where \\(0.025\\) of the distribution is on each side?\nWe want to solve: \\[\n0.025 \\;=\\; \\int_{-\\infty}^{x} f_{t,\\,df}(z)\\,dz,\n\\] where \\(f_{t,\\,df}\\) is the density of the Student’s \\(t\\) distribution with \\(df=n-1\\).\nWhen we solve for \\(x\\), this is the quantile function (inverse CDF).\nThis integral does not have a simple closed form — that’s why we have R.\n\nqt(0.025, df = df)\n\n[1] -2.063899\n\n\nThat’s the left cutoff. And the right cutoff:\n\nqt(0.975, df = df)\n\n[1] 2.063899\n\n\nSo our critical values are about \\(-t^\\star\\) and \\(+t^\\star\\) (here, with \\(df=24\\), approximately \\(\\pm 2.064\\)).\nThat is, \\[\n\\pm t^\\star \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}.\n\\]\nWe don’t know the true \\(\\mu\\), and there isn’t a \\(\\mu_0\\) to plug in for estimation.\nSo what do we do? We invert the inequality to find the set of \\(\\mu_0\\) values we would not reject:\n\\[\n\\bar{x} - \\mu_0 \\;=\\; \\pm\\, t^\\star \\cdot \\frac{s}{\\sqrt{n}}\n\\]\nWith a little algebra: \\[\n\\begin{align*}\n\\bar{x} - \\mu_0 \\;&=\\; \\pm\\, t^\\star \\cdot \\frac{s}{\\sqrt{n}} \\\\[6pt]\n-\\mu_0 \\;&=\\; -\\bar{x} \\;\\pm\\; t^\\star \\cdot \\frac{s}{\\sqrt{n}} \\\\[6pt]\n\\mu_0 \\;&=\\; \\bar{x} \\;\\pm\\; t^\\star \\cdot \\frac{s}{\\sqrt{n}}\n\\end{align*}\n\\]\nSo the general 95% confidence interval for a mean is: \\[\n\\bar{x} \\;\\pm\\; t^\\star \\cdot \\frac{s}{\\sqrt{n}},\n\\] with \\(t^\\star\\) taken from the \\(t\\) distribution with \\(df=n-1\\).\n(Equivalently: the set of \\(\\mu_0\\) values we would not reject at \\(\\alpha=0.05\\).)\n\n\n\n\n\nA random sample of \\(n=20\\) cadets recorded their 2-mile run times. The sample mean was \\(\\bar{x} = 13.8\\) minutes with a sample standard deviation of \\(s = 1.4\\) minutes. Assume run times are approximately normal.\n\nConstruct a 90% confidence interval for the true mean run time.\n\nConstruct a 95% confidence interval for the true mean run time.\n\nConstruct a 99% confidence interval for the true mean run time.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the one-sample mean CI: \\[\n\\bar{x}\\;\\pm\\; t^\\star \\cdot \\frac{s}{\\sqrt{n}}, \\quad df=n-1.\n\\]\nHere \\(n=20 \\Rightarrow df=19\\), \\(\\bar{x}=13.8\\), \\(s=1.4\\).\nCompute the \\(t^\\star\\) cutoffs in R (echoed):\n\ndf &lt;- 19\nqt(0.05, df)  # 90% CI\n\n[1] -1.729133\n\nqt(0.025, df)  # 95% CI\n\n[1] -2.093024\n\nqt(0.005, df)  # 99% CI\n\n[1] -2.860935\n\n\n\n90% CI: \\(13.8 \\pm (1.729)\\left(\\tfrac{1.4}{\\sqrt{20}}\\right)\n= 13.8 \\pm 0.541 \\;\\Rightarrow\\; [\\,13.259,\\;14.341\\,]\\)\n95% CI: \\(13.8 \\pm (2.093)\\left(\\tfrac{1.4}{\\sqrt{20}}\\right)\n= 13.8 \\pm 0.655 \\;\\Rightarrow\\; [\\,13.145,\\;14.455\\,]\\)\n99% CI: \\(13.8 \\pm (2.861)\\left(\\tfrac{1.4}{\\sqrt{20}}\\right)\n= 13.8 \\pm 0.896 \\;\\Rightarrow\\; [\\,12.904,\\;14.696\\,]\\)\n\n\n\n\n\n\n\nA sample of \\(n=15\\) cadets reported hours of sleep the night before a training exercise. The sample mean was \\(\\bar{x} = 6.2\\) hours with a sample standard deviation of \\(s = 1.1\\) hours. Assume sleep hours are approximately normal.\n\nConstruct a 90% confidence interval for the true mean hours of sleep.\n\nConstruct a 95% confidence interval for the true mean hours of sleep.\n\nConstruct a 99% confidence interval for the true mean hours of sleep.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the one-sample mean CI: \\[\n\\bar{x}\\;\\pm\\; t^\\star \\cdot \\frac{s}{\\sqrt{n}}, \\quad df=n-1.\n\\]\nHere \\(n=15 \\Rightarrow df=14\\), \\(\\bar{x}=6.2\\), \\(s=1.1\\).\nCompute the \\(t^\\star\\) cutoffs in R (echoed):\n\ndf &lt;- 14\nqt(0.05,  df)  # 90% CI\n\n[1] -1.76131\n\nqt(0.025, df)  # 95% CI\n\n[1] -2.144787\n\nqt(0.005, df)  # 99% CI\n\n[1] -2.976843\n\n\n\n90% CI: \\(6.2 \\pm (1.761)\\left(\\tfrac{1.1}{\\sqrt{15}}\\right)\n= 6.2 \\pm 0.500 \\;\\Rightarrow\\; [\\,5.700,\\;6.700\\,]\\)\n95% CI: \\(6.2 \\pm (2.145)\\left(\\tfrac{1.1}{\\sqrt{15}}\\right)\n= 6.2 \\pm 0.609 \\;\\Rightarrow\\; [\\,5.591,\\;6.809\\,]\\)\n99% CI: \\(6.2 \\pm (2.977)\\left(\\tfrac{1.1}{\\sqrt{15}}\\right)\n= 6.2 \\pm 0.846 \\;\\Rightarrow\\; [\\,5.354,\\;7.046\\,]\\)\n\n\n\n\n\n\n\nIn a random sample of \\(n=240\\) respondents, \\(x=102\\) reported using public transit at least once per week.\n\nConstruct a 90% confidence interval for the true proportion who use public transit weekly.\n\nConstruct a 95% confidence interval for the true proportion.\n\nConstruct a 99% confidence interval for the true proportion.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the confidence interval for a proportion: \\[\n\\hat{p}\\;\\pm\\; z^\\star \\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}},\\qquad \\hat{p}=\\frac{x}{n}.\n\\]\nHere, \\(\\hat{p}=\\dfrac{102}{240}=0.425\\) and \\(n=240\\).\nCompute the \\(z^\\star\\) cutoffs in R:\n\nn &lt;- 240; x &lt;- 102\nphat &lt;- x/n\nz90 &lt;- qnorm(0.95)   # for 90% CI\nz95 &lt;- qnorm(0.975)  # for 95% CI\nz99 &lt;- qnorm(0.995)  # for 99% CI\nz90; z95; z99\n\n[1] 1.644854\n\n\n[1] 1.959964\n\n\n[1] 2.575829\n\n\n\n90% CI: \\[\n0.425 \\;\\pm\\; (1.645)\\sqrt{\\frac{0.425(1-0.425)}{240}}\n\\;=\\; 0.425 \\pm 0.052 \\;\\Rightarrow\\; [\\,0.373,\\;0.477\\,].\n\\]\n95% CI: \\[\n0.425 \\;\\pm\\; (1.960)\\sqrt{\\frac{0.425(1-0.425)}{240}}\n\\;=\\; 0.425 \\pm 0.063 \\;\\Rightarrow\\; [\\,0.362,\\;0.488\\,].\n\\]\n99% CI: \\[\n0.425 \\;\\pm\\; (2.576)\\sqrt{\\frac{0.425(1-0.425)}{240}}\n\\;=\\; 0.425 \\pm 0.082 \\;\\Rightarrow\\; [\\,0.343,\\;0.507\\,].\n\\]\n\n\n\n\n\n\n\nA random sample of \\(n=30\\) cadets was asked how many push-ups they could complete in two minutes. The sample mean was \\(\\bar{x} = 58.5\\) with a sample standard deviation of \\(s = 9.3\\). Assume counts are approximately normal.\n\nConstruct a 90% confidence interval for the true mean number of push-ups.\n\nConstruct a 95% confidence interval for the true mean number of push-ups.\n\nConstruct a 99% confidence interval for the true mean number of push-ups.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the one-sample mean CI: \\[\n\\bar{x}\\;\\pm\\; t^\\star \\cdot \\frac{s}{\\sqrt{n}}, \\quad df=n-1.\n\\]\nHere \\(n=30 \\Rightarrow df=29\\), \\(\\bar{x}=58.5\\), \\(s=9.3\\).\nCompute the \\(t^\\star\\) cutoffs in R (echoed):\n\ndf &lt;- 29\nqt(0.05, df)  # 90% CI\n\n[1] -1.699127\n\nqt(0.025, df)  # 95% CI\n\n[1] -2.04523\n\nqt(0.005, df)  # 99% CI\n\n[1] -2.756386\n\n\n\n90% CI: \\(58.5 \\pm (1.699)\\left(\\tfrac{9.3}{\\sqrt{30}}\\right)\n= 58.5 \\pm 2.884 \\;\\Rightarrow\\; [\\,55.616,\\;61.384\\,]\\)\n95% CI: \\(58.5 \\pm (2.045)\\left(\\tfrac{9.3}{\\sqrt{30}}\\right)\n= 58.5 \\pm 3.471 \\;\\Rightarrow\\; [\\,55.029,\\;61.971\\,]\\)\n99% CI: \\(58.5 \\pm (2.756)\\left(\\tfrac{9.3}{\\sqrt{30}}\\right)\n= 58.5 \\pm 4.676 \\;\\Rightarrow\\; [\\,53.824,\\;63.176\\,]\\)\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 15"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-15.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-15.html#lesson-administration",
    "title": "Lesson 15: Confidence Interval (Quantitative)",
    "section": "",
    "text": "G1: No Change\nG1: No Change\nI1: Possibly Meet in TH120 but for right now stay put\nJ2: No Change\n\n\n\n\n\nLesson 17\nMilestone 4\n\nWith partner\nWrite 1-2 paragraphs per article summarizing the articles topic with a take away for its insight on your project.\nMake updates from Milestone 3 feedback.\nFill out Annex B for my comments on Milestone 3.\nTurn in EVERYTHING in your working write up.\nKeep your binder up-to-date, but I don’t want to see it.\n\n\n\n\n\n\nLesson 17\n25 Points!\nLike a WPR What does that mean?!!\nRead ahead\n\n\n\n\n\n⏰ Due 0700 on **Lesson 18*\nLets take a look at it\n\nDay 1: Tuesday, 14 Oct 2025\nDay 2: Wednesday, 15 Oct 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings didn’t start well…\n\n\n\n\n\nBut Cal slowly got his act together.\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\nThen he was the first bracket pitcher on the next day against the 2 Seed…\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\nGot the dub:\n\n\n\n\n\nLost in the semis vs the eventual champion - not a bad tournament\n\n\n\n\n\n\n\n\nWe’ll talk about her next lesson!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all cases:\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: p &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A: p &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A: p \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution.\n\n\n\n\nFor all cases:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A: \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t_{df}}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A: \\mu &lt; \\mu_0\\)\n\\(p = F_{t_{df}}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A: \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t_{df}}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) = degrees of freedom\n\n\\(F_{t_{df}}(\\cdot)\\) = cumulative distribution function (CDF) of the Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\n\n\n\n\nWhat impacts does altering different values have?\n\nGeneralization: We can generalize results to a larger population if the sample is random and representative of that population. Convenience samples don’t justify broad claims.\n\nCausation: We can claim causation only if the study design is a randomized experiment. Observational studies can show associations, but not cause-and-effect.\nParameters vs. Statistics: A parameter is a fixed (but usually unknown) numerical value describing a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\pi\\)). A statistic is a numerical value computed from a sample (e.g., \\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\)).\n\nParameters = target (what we want to know).\n\nStatistics = evidence (what we can actually measure).\n\nWe use statistics to estimate parameters, and because different samples give different statistics, we capture this variability with confidence intervals.\n\n\n\n\n\nQuantity\nPopulation (Parameter)\nSample (Statistic)\n\n\n\n\nCenter (mean)\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nSpread (SD)\n\\(\\sigma\\)\n\\(s\\)\n\n\nProportion “success”\n\\(\\pi\\)\n\\(\\hat{p}\\)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 15"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-15.html#confidence-intervals-on-means",
    "href": "MA206-AY26-1/lesson-15.html#confidence-intervals-on-means",
    "title": "Lesson 15: Confidence Interval (Quantitative)",
    "section": "",
    "text": "Suppose we take a random sample of \\(n\\) cadets and measure a quantitative outcome (e.g., 2-mile run times). We want to estimate the true population mean \\(\\mu\\).\nWe start with the one-sample \\(t\\) statistic:\n\\[\nt \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}.\n\\]\nAt significance level \\(\\alpha = 0.05\\), we reject \\(H_0\\) whenever \\(t\\) falls into the rejection region (the critical tail(s)) of the \\(t\\) distribution with \\(df=n-1\\).\nWhere on this plot would we reject?\n\n\n\n\n\n\n\n\n\nWhere are these lines?\n\n\n\n\n\n\n\n\n\nSo, the left tail contains \\(0.025\\) of the distribution, and the right tail contains \\(0.025\\).\nHow do we find where \\(0.025\\) of the distribution is on each side?\nWe want to solve: \\[\n0.025 \\;=\\; \\int_{-\\infty}^{x} f_{t,\\,df}(z)\\,dz,\n\\] where \\(f_{t,\\,df}\\) is the density of the Student’s \\(t\\) distribution with \\(df=n-1\\).\nWhen we solve for \\(x\\), this is the quantile function (inverse CDF).\nThis integral does not have a simple closed form — that’s why we have R.\n\nqt(0.025, df = df)\n\n[1] -2.063899\n\n\nThat’s the left cutoff. And the right cutoff:\n\nqt(0.975, df = df)\n\n[1] 2.063899\n\n\nSo our critical values are about \\(-t^\\star\\) and \\(+t^\\star\\) (here, with \\(df=24\\), approximately \\(\\pm 2.064\\)).\nThat is, \\[\n\\pm t^\\star \\;=\\; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}.\n\\]\nWe don’t know the true \\(\\mu\\), and there isn’t a \\(\\mu_0\\) to plug in for estimation.\nSo what do we do? We invert the inequality to find the set of \\(\\mu_0\\) values we would not reject:\n\\[\n\\bar{x} - \\mu_0 \\;=\\; \\pm\\, t^\\star \\cdot \\frac{s}{\\sqrt{n}}\n\\]\nWith a little algebra: \\[\n\\begin{align*}\n\\bar{x} - \\mu_0 \\;&=\\; \\pm\\, t^\\star \\cdot \\frac{s}{\\sqrt{n}} \\\\[6pt]\n-\\mu_0 \\;&=\\; -\\bar{x} \\;\\pm\\; t^\\star \\cdot \\frac{s}{\\sqrt{n}} \\\\[6pt]\n\\mu_0 \\;&=\\; \\bar{x} \\;\\pm\\; t^\\star \\cdot \\frac{s}{\\sqrt{n}}\n\\end{align*}\n\\]\nSo the general 95% confidence interval for a mean is: \\[\n\\bar{x} \\;\\pm\\; t^\\star \\cdot \\frac{s}{\\sqrt{n}},\n\\] with \\(t^\\star\\) taken from the \\(t\\) distribution with \\(df=n-1\\).\n(Equivalently: the set of \\(\\mu_0\\) values we would not reject at \\(\\alpha=0.05\\).)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 15"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-15.html#board-problems",
    "href": "MA206-AY26-1/lesson-15.html#board-problems",
    "title": "Lesson 15: Confidence Interval (Quantitative)",
    "section": "",
    "text": "A random sample of \\(n=20\\) cadets recorded their 2-mile run times. The sample mean was \\(\\bar{x} = 13.8\\) minutes with a sample standard deviation of \\(s = 1.4\\) minutes. Assume run times are approximately normal.\n\nConstruct a 90% confidence interval for the true mean run time.\n\nConstruct a 95% confidence interval for the true mean run time.\n\nConstruct a 99% confidence interval for the true mean run time.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the one-sample mean CI: \\[\n\\bar{x}\\;\\pm\\; t^\\star \\cdot \\frac{s}{\\sqrt{n}}, \\quad df=n-1.\n\\]\nHere \\(n=20 \\Rightarrow df=19\\), \\(\\bar{x}=13.8\\), \\(s=1.4\\).\nCompute the \\(t^\\star\\) cutoffs in R (echoed):\n\ndf &lt;- 19\nqt(0.05, df)  # 90% CI\n\n[1] -1.729133\n\nqt(0.025, df)  # 95% CI\n\n[1] -2.093024\n\nqt(0.005, df)  # 99% CI\n\n[1] -2.860935\n\n\n\n90% CI: \\(13.8 \\pm (1.729)\\left(\\tfrac{1.4}{\\sqrt{20}}\\right)\n= 13.8 \\pm 0.541 \\;\\Rightarrow\\; [\\,13.259,\\;14.341\\,]\\)\n95% CI: \\(13.8 \\pm (2.093)\\left(\\tfrac{1.4}{\\sqrt{20}}\\right)\n= 13.8 \\pm 0.655 \\;\\Rightarrow\\; [\\,13.145,\\;14.455\\,]\\)\n99% CI: \\(13.8 \\pm (2.861)\\left(\\tfrac{1.4}{\\sqrt{20}}\\right)\n= 13.8 \\pm 0.896 \\;\\Rightarrow\\; [\\,12.904,\\;14.696\\,]\\)\n\n\n\n\n\n\n\nA sample of \\(n=15\\) cadets reported hours of sleep the night before a training exercise. The sample mean was \\(\\bar{x} = 6.2\\) hours with a sample standard deviation of \\(s = 1.1\\) hours. Assume sleep hours are approximately normal.\n\nConstruct a 90% confidence interval for the true mean hours of sleep.\n\nConstruct a 95% confidence interval for the true mean hours of sleep.\n\nConstruct a 99% confidence interval for the true mean hours of sleep.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the one-sample mean CI: \\[\n\\bar{x}\\;\\pm\\; t^\\star \\cdot \\frac{s}{\\sqrt{n}}, \\quad df=n-1.\n\\]\nHere \\(n=15 \\Rightarrow df=14\\), \\(\\bar{x}=6.2\\), \\(s=1.1\\).\nCompute the \\(t^\\star\\) cutoffs in R (echoed):\n\ndf &lt;- 14\nqt(0.05,  df)  # 90% CI\n\n[1] -1.76131\n\nqt(0.025, df)  # 95% CI\n\n[1] -2.144787\n\nqt(0.005, df)  # 99% CI\n\n[1] -2.976843\n\n\n\n90% CI: \\(6.2 \\pm (1.761)\\left(\\tfrac{1.1}{\\sqrt{15}}\\right)\n= 6.2 \\pm 0.500 \\;\\Rightarrow\\; [\\,5.700,\\;6.700\\,]\\)\n95% CI: \\(6.2 \\pm (2.145)\\left(\\tfrac{1.1}{\\sqrt{15}}\\right)\n= 6.2 \\pm 0.609 \\;\\Rightarrow\\; [\\,5.591,\\;6.809\\,]\\)\n99% CI: \\(6.2 \\pm (2.977)\\left(\\tfrac{1.1}{\\sqrt{15}}\\right)\n= 6.2 \\pm 0.846 \\;\\Rightarrow\\; [\\,5.354,\\;7.046\\,]\\)\n\n\n\n\n\n\n\nIn a random sample of \\(n=240\\) respondents, \\(x=102\\) reported using public transit at least once per week.\n\nConstruct a 90% confidence interval for the true proportion who use public transit weekly.\n\nConstruct a 95% confidence interval for the true proportion.\n\nConstruct a 99% confidence interval for the true proportion.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the confidence interval for a proportion: \\[\n\\hat{p}\\;\\pm\\; z^\\star \\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}},\\qquad \\hat{p}=\\frac{x}{n}.\n\\]\nHere, \\(\\hat{p}=\\dfrac{102}{240}=0.425\\) and \\(n=240\\).\nCompute the \\(z^\\star\\) cutoffs in R:\n\nn &lt;- 240; x &lt;- 102\nphat &lt;- x/n\nz90 &lt;- qnorm(0.95)   # for 90% CI\nz95 &lt;- qnorm(0.975)  # for 95% CI\nz99 &lt;- qnorm(0.995)  # for 99% CI\nz90; z95; z99\n\n[1] 1.644854\n\n\n[1] 1.959964\n\n\n[1] 2.575829\n\n\n\n90% CI: \\[\n0.425 \\;\\pm\\; (1.645)\\sqrt{\\frac{0.425(1-0.425)}{240}}\n\\;=\\; 0.425 \\pm 0.052 \\;\\Rightarrow\\; [\\,0.373,\\;0.477\\,].\n\\]\n95% CI: \\[\n0.425 \\;\\pm\\; (1.960)\\sqrt{\\frac{0.425(1-0.425)}{240}}\n\\;=\\; 0.425 \\pm 0.063 \\;\\Rightarrow\\; [\\,0.362,\\;0.488\\,].\n\\]\n99% CI: \\[\n0.425 \\;\\pm\\; (2.576)\\sqrt{\\frac{0.425(1-0.425)}{240}}\n\\;=\\; 0.425 \\pm 0.082 \\;\\Rightarrow\\; [\\,0.343,\\;0.507\\,].\n\\]\n\n\n\n\n\n\n\nA random sample of \\(n=30\\) cadets was asked how many push-ups they could complete in two minutes. The sample mean was \\(\\bar{x} = 58.5\\) with a sample standard deviation of \\(s = 9.3\\). Assume counts are approximately normal.\n\nConstruct a 90% confidence interval for the true mean number of push-ups.\n\nConstruct a 95% confidence interval for the true mean number of push-ups.\n\nConstruct a 99% confidence interval for the true mean number of push-ups.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the one-sample mean CI: \\[\n\\bar{x}\\;\\pm\\; t^\\star \\cdot \\frac{s}{\\sqrt{n}}, \\quad df=n-1.\n\\]\nHere \\(n=30 \\Rightarrow df=29\\), \\(\\bar{x}=58.5\\), \\(s=9.3\\).\nCompute the \\(t^\\star\\) cutoffs in R (echoed):\n\ndf &lt;- 29\nqt(0.05, df)  # 90% CI\n\n[1] -1.699127\n\nqt(0.025, df)  # 95% CI\n\n[1] -2.04523\n\nqt(0.005, df)  # 99% CI\n\n[1] -2.756386\n\n\n\n90% CI: \\(58.5 \\pm (1.699)\\left(\\tfrac{9.3}{\\sqrt{30}}\\right)\n= 58.5 \\pm 2.884 \\;\\Rightarrow\\; [\\,55.616,\\;61.384\\,]\\)\n95% CI: \\(58.5 \\pm (2.045)\\left(\\tfrac{9.3}{\\sqrt{30}}\\right)\n= 58.5 \\pm 3.471 \\;\\Rightarrow\\; [\\,55.029,\\;61.971\\,]\\)\n99% CI: \\(58.5 \\pm (2.756)\\left(\\tfrac{9.3}{\\sqrt{30}}\\right)\n= 58.5 \\pm 4.676 \\;\\Rightarrow\\; [\\,53.824,\\;63.176\\,]\\)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 15"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-15.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-15.html#before-you-leave",
    "title": "Lesson 15: Confidence Interval (Quantitative)",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 15"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-17.html",
    "href": "MA206-AY26-1/lesson-17.html",
    "title": "Lesson 17 — SIL 1",
    "section": "",
    "text": "install.packages(\"remotes\")\nremotes::install_github(\"reder206/ma206data\")\n\n\nlibrary(tidyverse)\n\ndf &lt;- ma206data::ada92 |&gt;\n  filter(age_range == \"20-39\", white == FALSE)\n\n\nwith_disability &lt;- sum(df$disability == \"Disability\")\ntotal &lt;- length(df$disability)\nphat &lt;- with_disability / total\n\n\nxbar &lt;- mean(df$weeks_worked)\n\n\n\nDaron Acemoglu and Joshua Angrist (2001) study the labor market effects of the Americans with Disabilities Act (ADA), which prohibited discrimination and required employers to provide reasonable accommodations for disabled workers beginning in 1992. The law aimed to increase employment opportunities and earnings for disabled individuals, but it also created potential costs for employers in terms of accommodations and legal risk.\nUsing data from the 1988–1997 Current Population Survey, the authors compare employment and wages between disabled and nondisabled workers aged 21–58. They find that employment among younger disabled men and women (ages 21–39) fell sharply after the ADA’s implementation. The decline was especially large in medium-sized firms and in states that saw high levels of ADA-related litigation. For older disabled workers (40–58), the effects were smaller and mixed. In contrast, nondisabled workers’ employment was largely unchanged.\nWages for disabled workers did not display consistent changes across the study period. The authors also examine whether the growth of disability benefit rolls could explain the decline in employment but conclude that this factor accounts for only a portion of the change. The results suggest that the ADA itself, by raising the expected costs of employing disabled workers, contributed to the decline in their employment.\nThe study highlights that legislation intended to improve outcomes for disabled workers may have unintended consequences. While the ADA did not negatively affect nondisabled workers, its impact on the disabled population raises questions about how broadly such results apply beyond the specific groups and time period studied. The findings also raise important considerations for how research on sensitive policies can be interpreted and applied.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 17"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-17.html#summary-of-consequences-of-employment-protection-the-case-of-the-americans-with-disabilities-act",
    "href": "MA206-AY26-1/lesson-17.html#summary-of-consequences-of-employment-protection-the-case-of-the-americans-with-disabilities-act",
    "title": "Lesson 17 — SIL 1",
    "section": "",
    "text": "Daron Acemoglu and Joshua Angrist (2001) study the labor market effects of the Americans with Disabilities Act (ADA), which prohibited discrimination and required employers to provide reasonable accommodations for disabled workers beginning in 1992. The law aimed to increase employment opportunities and earnings for disabled individuals, but it also created potential costs for employers in terms of accommodations and legal risk.\nUsing data from the 1988–1997 Current Population Survey, the authors compare employment and wages between disabled and nondisabled workers aged 21–58. They find that employment among younger disabled men and women (ages 21–39) fell sharply after the ADA’s implementation. The decline was especially large in medium-sized firms and in states that saw high levels of ADA-related litigation. For older disabled workers (40–58), the effects were smaller and mixed. In contrast, nondisabled workers’ employment was largely unchanged.\nWages for disabled workers did not display consistent changes across the study period. The authors also examine whether the growth of disability benefit rolls could explain the decline in employment but conclude that this factor accounts for only a portion of the change. The results suggest that the ADA itself, by raising the expected costs of employing disabled workers, contributed to the decline in their employment.\nThe study highlights that legislation intended to improve outcomes for disabled workers may have unintended consequences. While the ADA did not negatively affect nondisabled workers, its impact on the disabled population raises questions about how broadly such results apply beyond the specific groups and time period studied. The findings also raise important considerations for how research on sensitive policies can be interpreted and applied.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 17"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-19.html",
    "href": "MA206-AY26-1/lesson-19.html",
    "title": "Lesson 19: Two Mean T-Test",
    "section": "",
    "text": "What it means for you depending on how you feel about it\n\n\n\n\n\n⏰ Due on Lesson 19 (Today)\n\n\nDay 1: Tuesday, 16 Oct 2025\nDay 2: Wednesday, 17 Oct 2025\n\n\n\n\n\n\nCanvas Quiz Due 21 OCT at Midnight\n\n\n\n\n\nDetails\nShorter version of last time with in class feedback time\n\n\n\n\n\n\n\n\nDue 0700 on 7 November\nInstructions\n\n\n\n\n\n\n\n\nMath 1 vs DPE\n\n\n\n\n\n\nPreviously 5-0\n\n\n\n\n\n\n6-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\pi = \\pi_0\\)\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nValidity Conditions\n\nNumber of successes and failures must be greater than 10.\n\nConfidence Interval for \\(\\pi\\) (one proportion)\n\\[\n\\hat{p} \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true population proportion \\(\\pi\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu = \\mu_0\\)\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu &lt; \\mu_0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) (degrees of freedom)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nValidity Conditions\n\nSample size must be greater than 30.\n\nConfidence Interval for \\(\\mu\\) (one mean)\n\\[\n\\bar{x} \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\\frac{s}{\\sqrt{n}},\n\\qquad df = n-1\n\\] I am \\((1 - \\alpha)\\%\\) confident that the true population mean \\((\\mu)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\pi_1 - \\pi_2 = 0\\)\n\\[\nz \\;=\\; \\frac{(\\hat{p}_1 - \\hat{p}_2) - (\\pi_1 - \\pi_2)}{\\sqrt{\\hat{p}(1-\\hat{p})\\left(\\tfrac{1}{n_1} + \\tfrac{1}{n_2}\\right)}}\n\\]\nWhere the pooled proportion is\n\\[\n\\hat{p} \\;=\\; \\frac{x_1 + x_2}{n_1 + n_2}.\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 &gt; 0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 &lt; 0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 \\neq 0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p}_1 = x_1/n_1\\) (sample proportion in group 1)\n\n\\(\\hat{p}_2 = x_2/n_2\\) (sample proportion in group 2)\n\n\\(\\pi_1, \\pi_2\\) = hypothesized proportions under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nValidity Conditions\n\nEach group must have at least 10 successes and 10 failures.\n\nConfidence Interval for \\(\\pi_1 - \\pi_2\\) (unpooled SE)\n\\[\n(\\hat{p}_1 - \\hat{p}_2) \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\n\\sqrt{\\tfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\tfrac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true difference in population proportions \\((\\pi_1 - \\pi_2)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\nStrength of evidence: Smaller \\(p\\) means stronger evidence against \\(H_0\\).\n\n\n\n\n\nGeneralization: We can generalize results to a larger population if the data come from a random and representative sample of that population.\n\nCausation: We can claim causation if participants are randomly assigned to treatments in an experiment. Without random assignment, we can only conclude association, not causation.\n\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n& \\text{Randomly Sampled} & \\text{Not Randomly Sampled} \\\\\n\\hline\n\\textbf{Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: Yes}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: Yes}\n\\end{array} \\\\\n\\hline\n\\textbf{Not Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: No}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: No}\n\\end{array} \\\\\n\\hline\n\\end{array}\n\\]\n\nParameters vs. Statistics: A parameter is a fixed (but usually unknown) numerical value describing a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\pi\\)). A statistic is a numerical value computed from a sample (e.g., \\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\)).\n\nParameters = target (what we want to know).\n\nStatistics = evidence (what we can actually measure).\n\nWe use statistics to estimate parameters, and because different samples give different statistics, we capture this variability with confidence intervals.\n\n\n\n\n\nQuantity\nPopulation (Parameter)\nSample (Statistic)\n\n\n\n\nCenter (mean)\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nSpread (SD)\n\\(\\sigma\\)\n\\(s\\)\n\n\nProportion “success”\n\\(\\pi\\)\n\\(\\hat{p}\\)\n\n\n\n\n\n\n\nResearchers at West Point are interested in whether cadets who study in groups score higher on a quiz than cadets who study alone.\n\nGroup study: \\(n_1 = 15\\), \\(\\bar{x}_1 = 82.4\\), \\(s_1 = 5.6\\)\n\nStudy alone: \\(n_2 = 12\\), \\(\\bar{x}_2 = 78.1\\), \\(s_2 = 6.3\\)\n\nQuestion: Is the mean score for group study greater than for studying alone?\n\n\n\n\n\n\n\n\n\n\n\n\nNull:\n\\(H_0:\\ \\mu_1 - \\mu_2 = 0\\)\nAlternative (one-sided):\n\\(H_A:\\ \\mu_1 - \\mu_2 &gt; 0\\)\n\nWhere:\n- \\(\\mu_1\\) = mean score for group study\n- \\(\\mu_2\\) = mean score for study alone\n\n\n\nThe two-sample \\(t\\)-test:\n\\[\nt \\;=\\; \\frac{(\\bar{x}_1 - \\bar{x}_2) - 0}{\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}}\n\\]\nDegrees of freedom\n\\[\ndf =  n_1 + n_2 - 2\n\\]\n\n\n\n\\[\n\\begin{aligned}\nt &= \\frac{(\\bar{x}_1 - \\bar{x}_2) - 0}{\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}} \\\\[6pt]\n  &= \\frac{82.4 - 78.1}{\\sqrt{\\tfrac{5.6^2}{15} + \\tfrac{6.3^2}{12}}} \\\\[6pt]\n  &= \\frac{4.3}{2.32} \\\\[6pt]\n  &\\approx 1.85, \\\\[6pt]\n\\end{aligned}\n\\]\n\\[\ndf = 15 + 12 - 2 = 25\n\\]\n\nxbar1 &lt;- 82.4; s1 &lt;- 5.6; n1 &lt;- 15\nxbar2 &lt;- 78.1; s2 &lt;- 6.3; n2 &lt;- 12\n\nSE &lt;- sqrt(s1^2/n1 + s2^2/n2)\nt_stat &lt;- (xbar1 - xbar2)/SE\n\ndf &lt;- n1 + n2 - 2\n\np_val &lt;- 1 - pt(t_stat, df)\nlist(t_stat = t_stat, df = df, p_val = p_val)\n\n$t_stat\n[1] 1.85074\n\n$df\n[1] 25\n\n$p_val\n[1] 0.03802918\n\n\n\n\n\nSince \\(p \\approx 0.038 &lt; 0.05\\), we reject \\(H_0\\).\nThere is significant evidence that cadets who study in groups score higher on average.\n\n\n\n95% CI for \\(\\mu_1 - \\mu_2\\):\n\\[\n(\\bar{x}_1 - \\bar{x}_2) \\;\\pm\\; t_{0.975,\\,df}\\,\n      \\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}\n\\]\n\\[\n\\begin{aligned}\n   &= 4.3 \\;\\pm\\; (2.06)\\sqrt{\\tfrac{5.6^2}{15} + \\tfrac{6.3^2}{12}} \\\\[6pt]\n   &= 4.3 \\;\\pm\\; (2.06)(2.32) \\\\[6pt]\n   &= 4.3 \\;\\pm\\; 4.78 \\\\[6pt]\n   &= (-0.48,\\; 9.08), \\qquad df = 25\n\\end{aligned}\n\\]\n\nxbar1 &lt;- 82.4; s1 &lt;- 5.6; n1 &lt;- 15\nxbar2 &lt;- 78.1; s2 &lt;- 6.3; n2 &lt;- 12\n\nSE &lt;- sqrt(s1^2/n1 + s2^2/n2)\ndf &lt;- 25\ntcrit &lt;- qt(0.975, df)\n\ndiff &lt;- xbar1 - xbar2\nCI &lt;- diff + c(-1,1) * tcrit * SE\n\nlist(diff = diff, SE = SE, tcrit = tcrit, CI = CI, df = df)\n\n$diff\n[1] 4.3\n\n$SE\n[1] 2.323396\n\n$tcrit\n[1] 2.059539\n\n$CI\n[1] -0.4851226  9.0851226\n\n$df\n[1] 25\n\n\nWe are 95% confident that group study increases quiz scores by between –0.4 and 9 points (on average).\n\n\n\n\nValidity: Groups are independent and \\(n_1,n_2\\) are moderately large.\n\nScope:\n\nRandom sampling \\(\\;\\rightarrow\\;\\) generalize to cadets.\n\nRandom assignment \\(\\;\\rightarrow\\;\\) claim causation.\n\n\n\n\n\n\nResearchers sampled adult heights (in inches):\n\nMen: \\(n_1 = 30,\\;\\bar{x}_1 = 69.8,\\; s_1 = 2.9\\)\n\nWomen: \\(n_2 = 28,\\;\\bar{x}_2 = 65.0,\\; s_2 = 2.7\\)\n\nWe wish to test whether the true mean difference is less than 5.5 inches.\n\n\n\\[\nH_0:\\ \\mu_1 - \\mu_2 = 5.5\n\\qquad\\text{vs}\\qquad\nH_A:\\ \\mu_1 - \\mu_2 &lt; 5.5\n\\]\nwhere \\(\\mu_1\\) = mean male height, \\(\\mu_2\\) = mean female height.\n\n\n\n\\[\n\\begin{aligned}\nt &= \\frac{(\\bar{x}_1 - \\bar{x}_2) - 5.5}{\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}} \\\\[6pt]\n  &= \\frac{69.8 - 65.0 - 5.5}{\\sqrt{\\tfrac{2.9^2}{30} + \\tfrac{2.7^2}{28}}} \\\\[6pt]\n  &= \\frac{4.8 - 5.5}{0.735} \\\\[6pt]\n  &\\approx -0.95, \\qquad df \\approx 56\n\\end{aligned}\n\\]\n\nxbar1 &lt;- 69.8; s1 &lt;- 2.9; n1 &lt;- 30   # men\nxbar2 &lt;- 65.0; s2 &lt;- 2.7; n2 &lt;- 28   # women\n\ndiff &lt;- xbar1 - xbar2\nSE   &lt;- sqrt(s1^2/n1 + s2^2/n2)\n\n# Welch-Satterthwaite degrees of freedom\ndf &lt;- n1 + n2 - 2\n\n# Hypothesized difference = 5.5\nt_stat &lt;- (diff - 5.5)/SE\np_val  &lt;- pt(t_stat, df)   # one-sided, \"&lt;\"\n\nlist(t_stat = t_stat, df = df, p_val = p_val)\n\n$t_stat\n[1] -0.9519709\n\n$df\n[1] 56\n\n$p_val\n[1] 0.1726013\n\n\n\n\n\nOne-sided \\(p\\)-value \\(\\approx 0.17\\).\nAt \\(\\alpha=0.05\\), we fail to reject \\(H_0\\).\nThe evidence is not strong enough to conclude that the mean difference is less than 5.5 inches.\n\n\n\n\\[\n\\begin{aligned}\n(\\bar{x}_1 - \\bar{x}_2) \\;\\pm\\; t_{0.975,\\,df}\\,\n\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}\n   &= 4.8 \\;\\pm\\; (2.00)(0.735) \\\\[6pt]\n   &= 4.8 \\;\\pm\\; 1.47 \\\\[6pt]\n   &= (3.33,\\; 6.27)\n\\end{aligned}\n\\]\n\ntcrit &lt;- qt(0.975, df)\nCI95  &lt;- diff + c(-1, 1) * tcrit * SE\n\nlist(diff = diff, SE = SE, tcrit = tcrit, CI95 = CI95, df = df)\n\n$diff\n[1] 4.8\n\n$SE\n[1] 0.7353166\n\n$tcrit\n[1] 2.003241\n\n$CI95\n[1] 3.326984 6.273016\n\n$df\n[1] 56\n\n\nWe are 95% confident that the true difference in mean heights is between 3.3 and 6.3 inches.\n\n\n\n\nA platoon tests a new breathing drill to see if it reduces average 1-mile run time (minutes).\n\nDrill group (1): \\(n_1=20,\\ \\bar{x}_1=6.83,\\ s_1=0.36\\)\n\nStandard training (2): \\(n_2=18,\\ \\bar{x}_2=7.10,\\ s_2=0.40\\)\n\nTest, at \\(\\alpha=0.05\\), whether the drill reduces mean time (i.e., is faster). Also construct a 95% CI for \\(\\mu_1-\\mu_2\\).\n\nDefine parameters and state hypotheses.\n\nCompute the test statistic, degrees of freedom, and one-sided \\(p\\)-value.\n\nMake a decision and interpret.\n\nGive the 95% CI and interpret it in context.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\\(H_0:\\ \\mu_1-\\mu_2=0 \\quad\\text{vs}\\quad H_A:\\ \\mu_1-\\mu_2&lt;0\\)\n\n\n\n\\(\\bar{x}_1-\\bar{x}_2 = 6.83 - 7.10 = -0.27\\)\n\\(SE = \\sqrt{\\tfrac{0.36^2}{20} + \\tfrac{0.40^2}{18}} \\approx 0.124\\)\n\\(t = \\dfrac{-0.27}{0.124} \\approx -2.18\\)\n\\(df = n_1+n_2-2 = 20+18-2 = 36\\)\nOne-sided \\(p\\)-value \\(\\approx 0.018\\).\nDecision: Reject \\(H_0\\).\nConclusion: The drill group runs faster on average.\n\n\n\n\\((\\bar{x}_1-\\bar{x}_2) \\pm t_{0.975,df}\\,SE\n= -0.27 \\pm (2.03)(0.124)\n= -0.27 \\pm 0.25\n= (-0.52,\\ -0.02)\\)\nWe are 95% confident the drill reduces mean mile time by 0.02 to 0.52 minutes (≈ 1–31 seconds).\n\n\n\nxbar1 &lt;- 6.83; s1 &lt;- 0.36; n1 &lt;- 20\nxbar2 &lt;- 7.10; s2 &lt;- 0.40; n2 &lt;- 18\n\ndiff &lt;- xbar1 - xbar2\nSE   &lt;- sqrt(s1^2/n1 + s2^2/n2)\n\ndf &lt;- n1 + n2 - 2  # pooled degrees of freedom\n\nt_stat &lt;- diff / SE\np_one_sided &lt;- pt(t_stat, df)\n\ntcrit &lt;- qt(0.975, df)\nCI95  &lt;- diff + c(-1, 1) * tcrit * SE\n\nlist(diff = diff, SE = SE, df = df,\n     t_stat = t_stat, p_one_sided = p_one_sided,\n     tcrit = tcrit, CI95 = CI95)\n\n$diff\n[1] -0.27\n\n$SE\n[1] 0.1239713\n\n$df\n[1] 36\n\n$t_stat\n[1] -2.177923\n\n$p_one_sided\n[1] 0.01802182\n\n$tcrit\n[1] 2.028094\n\n$CI95\n[1] -0.5214255 -0.0185745\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 19"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-19.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-19.html#lesson-administration",
    "title": "Lesson 19: Two Mean T-Test",
    "section": "",
    "text": "What it means for you depending on how you feel about it\n\n\n\n\n\n⏰ Due on Lesson 19 (Today)\n\n\nDay 1: Tuesday, 16 Oct 2025\nDay 2: Wednesday, 17 Oct 2025\n\n\n\n\n\n\nCanvas Quiz Due 21 OCT at Midnight\n\n\n\n\n\nDetails\nShorter version of last time with in class feedback time\n\n\n\n\n\n\n\n\nDue 0700 on 7 November\nInstructions\n\n\n\n\n\n\n\n\nMath 1 vs DPE\n\n\n\n\n\n\nPreviously 5-0\n\n\n\n\n\n\n6-0",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 19"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-19.html#running-review",
    "href": "MA206-AY26-1/lesson-19.html#running-review",
    "title": "Lesson 19: Two Mean T-Test",
    "section": "",
    "text": "For all cases:\n\\(H_0:\\ \\pi = \\pi_0\\)\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nValidity Conditions\n\nNumber of successes and failures must be greater than 10.\n\nConfidence Interval for \\(\\pi\\) (one proportion)\n\\[\n\\hat{p} \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true population proportion \\(\\pi\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu = \\mu_0\\)\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu &lt; \\mu_0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) (degrees of freedom)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nValidity Conditions\n\nSample size must be greater than 30.\n\nConfidence Interval for \\(\\mu\\) (one mean)\n\\[\n\\bar{x} \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\\frac{s}{\\sqrt{n}},\n\\qquad df = n-1\n\\] I am \\((1 - \\alpha)\\%\\) confident that the true population mean \\((\\mu)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\pi_1 - \\pi_2 = 0\\)\n\\[\nz \\;=\\; \\frac{(\\hat{p}_1 - \\hat{p}_2) - (\\pi_1 - \\pi_2)}{\\sqrt{\\hat{p}(1-\\hat{p})\\left(\\tfrac{1}{n_1} + \\tfrac{1}{n_2}\\right)}}\n\\]\nWhere the pooled proportion is\n\\[\n\\hat{p} \\;=\\; \\frac{x_1 + x_2}{n_1 + n_2}.\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 &gt; 0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 &lt; 0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 \\neq 0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p}_1 = x_1/n_1\\) (sample proportion in group 1)\n\n\\(\\hat{p}_2 = x_2/n_2\\) (sample proportion in group 2)\n\n\\(\\pi_1, \\pi_2\\) = hypothesized proportions under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nValidity Conditions\n\nEach group must have at least 10 successes and 10 failures.\n\nConfidence Interval for \\(\\pi_1 - \\pi_2\\) (unpooled SE)\n\\[\n(\\hat{p}_1 - \\hat{p}_2) \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\n\\sqrt{\\tfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\tfrac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true difference in population proportions \\((\\pi_1 - \\pi_2)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\nStrength of evidence: Smaller \\(p\\) means stronger evidence against \\(H_0\\).\n\n\n\n\n\nGeneralization: We can generalize results to a larger population if the data come from a random and representative sample of that population.\n\nCausation: We can claim causation if participants are randomly assigned to treatments in an experiment. Without random assignment, we can only conclude association, not causation.\n\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n& \\text{Randomly Sampled} & \\text{Not Randomly Sampled} \\\\\n\\hline\n\\textbf{Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: Yes}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: Yes}\n\\end{array} \\\\\n\\hline\n\\textbf{Not Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: No}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: No}\n\\end{array} \\\\\n\\hline\n\\end{array}\n\\]\n\nParameters vs. Statistics: A parameter is a fixed (but usually unknown) numerical value describing a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\pi\\)). A statistic is a numerical value computed from a sample (e.g., \\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\)).\n\nParameters = target (what we want to know).\n\nStatistics = evidence (what we can actually measure).\n\nWe use statistics to estimate parameters, and because different samples give different statistics, we capture this variability with confidence intervals.\n\n\n\n\n\nQuantity\nPopulation (Parameter)\nSample (Statistic)\n\n\n\n\nCenter (mean)\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nSpread (SD)\n\\(\\sigma\\)\n\\(s\\)\n\n\nProportion “success”\n\\(\\pi\\)\n\\(\\hat{p}\\)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 19"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-19.html#two-mean-t-test",
    "href": "MA206-AY26-1/lesson-19.html#two-mean-t-test",
    "title": "Lesson 19: Two Mean T-Test",
    "section": "",
    "text": "Researchers at West Point are interested in whether cadets who study in groups score higher on a quiz than cadets who study alone.\n\nGroup study: \\(n_1 = 15\\), \\(\\bar{x}_1 = 82.4\\), \\(s_1 = 5.6\\)\n\nStudy alone: \\(n_2 = 12\\), \\(\\bar{x}_2 = 78.1\\), \\(s_2 = 6.3\\)\n\nQuestion: Is the mean score for group study greater than for studying alone?\n\n\n\n\n\n\n\n\n\n\n\n\nNull:\n\\(H_0:\\ \\mu_1 - \\mu_2 = 0\\)\nAlternative (one-sided):\n\\(H_A:\\ \\mu_1 - \\mu_2 &gt; 0\\)\n\nWhere:\n- \\(\\mu_1\\) = mean score for group study\n- \\(\\mu_2\\) = mean score for study alone\n\n\n\nThe two-sample \\(t\\)-test:\n\\[\nt \\;=\\; \\frac{(\\bar{x}_1 - \\bar{x}_2) - 0}{\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}}\n\\]\nDegrees of freedom\n\\[\ndf =  n_1 + n_2 - 2\n\\]\n\n\n\n\\[\n\\begin{aligned}\nt &= \\frac{(\\bar{x}_1 - \\bar{x}_2) - 0}{\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}} \\\\[6pt]\n  &= \\frac{82.4 - 78.1}{\\sqrt{\\tfrac{5.6^2}{15} + \\tfrac{6.3^2}{12}}} \\\\[6pt]\n  &= \\frac{4.3}{2.32} \\\\[6pt]\n  &\\approx 1.85, \\\\[6pt]\n\\end{aligned}\n\\]\n\\[\ndf = 15 + 12 - 2 = 25\n\\]\n\nxbar1 &lt;- 82.4; s1 &lt;- 5.6; n1 &lt;- 15\nxbar2 &lt;- 78.1; s2 &lt;- 6.3; n2 &lt;- 12\n\nSE &lt;- sqrt(s1^2/n1 + s2^2/n2)\nt_stat &lt;- (xbar1 - xbar2)/SE\n\ndf &lt;- n1 + n2 - 2\n\np_val &lt;- 1 - pt(t_stat, df)\nlist(t_stat = t_stat, df = df, p_val = p_val)\n\n$t_stat\n[1] 1.85074\n\n$df\n[1] 25\n\n$p_val\n[1] 0.03802918\n\n\n\n\n\nSince \\(p \\approx 0.038 &lt; 0.05\\), we reject \\(H_0\\).\nThere is significant evidence that cadets who study in groups score higher on average.\n\n\n\n95% CI for \\(\\mu_1 - \\mu_2\\):\n\\[\n(\\bar{x}_1 - \\bar{x}_2) \\;\\pm\\; t_{0.975,\\,df}\\,\n      \\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}\n\\]\n\\[\n\\begin{aligned}\n   &= 4.3 \\;\\pm\\; (2.06)\\sqrt{\\tfrac{5.6^2}{15} + \\tfrac{6.3^2}{12}} \\\\[6pt]\n   &= 4.3 \\;\\pm\\; (2.06)(2.32) \\\\[6pt]\n   &= 4.3 \\;\\pm\\; 4.78 \\\\[6pt]\n   &= (-0.48,\\; 9.08), \\qquad df = 25\n\\end{aligned}\n\\]\n\nxbar1 &lt;- 82.4; s1 &lt;- 5.6; n1 &lt;- 15\nxbar2 &lt;- 78.1; s2 &lt;- 6.3; n2 &lt;- 12\n\nSE &lt;- sqrt(s1^2/n1 + s2^2/n2)\ndf &lt;- 25\ntcrit &lt;- qt(0.975, df)\n\ndiff &lt;- xbar1 - xbar2\nCI &lt;- diff + c(-1,1) * tcrit * SE\n\nlist(diff = diff, SE = SE, tcrit = tcrit, CI = CI, df = df)\n\n$diff\n[1] 4.3\n\n$SE\n[1] 2.323396\n\n$tcrit\n[1] 2.059539\n\n$CI\n[1] -0.4851226  9.0851226\n\n$df\n[1] 25\n\n\nWe are 95% confident that group study increases quiz scores by between –0.4 and 9 points (on average).\n\n\n\n\nValidity: Groups are independent and \\(n_1,n_2\\) are moderately large.\n\nScope:\n\nRandom sampling \\(\\;\\rightarrow\\;\\) generalize to cadets.\n\nRandom assignment \\(\\;\\rightarrow\\;\\) claim causation.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 19"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-19.html#example-difference-in-heights",
    "href": "MA206-AY26-1/lesson-19.html#example-difference-in-heights",
    "title": "Lesson 19: Two Mean T-Test",
    "section": "",
    "text": "Researchers sampled adult heights (in inches):\n\nMen: \\(n_1 = 30,\\;\\bar{x}_1 = 69.8,\\; s_1 = 2.9\\)\n\nWomen: \\(n_2 = 28,\\;\\bar{x}_2 = 65.0,\\; s_2 = 2.7\\)\n\nWe wish to test whether the true mean difference is less than 5.5 inches.\n\n\n\\[\nH_0:\\ \\mu_1 - \\mu_2 = 5.5\n\\qquad\\text{vs}\\qquad\nH_A:\\ \\mu_1 - \\mu_2 &lt; 5.5\n\\]\nwhere \\(\\mu_1\\) = mean male height, \\(\\mu_2\\) = mean female height.\n\n\n\n\\[\n\\begin{aligned}\nt &= \\frac{(\\bar{x}_1 - \\bar{x}_2) - 5.5}{\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}} \\\\[6pt]\n  &= \\frac{69.8 - 65.0 - 5.5}{\\sqrt{\\tfrac{2.9^2}{30} + \\tfrac{2.7^2}{28}}} \\\\[6pt]\n  &= \\frac{4.8 - 5.5}{0.735} \\\\[6pt]\n  &\\approx -0.95, \\qquad df \\approx 56\n\\end{aligned}\n\\]\n\nxbar1 &lt;- 69.8; s1 &lt;- 2.9; n1 &lt;- 30   # men\nxbar2 &lt;- 65.0; s2 &lt;- 2.7; n2 &lt;- 28   # women\n\ndiff &lt;- xbar1 - xbar2\nSE   &lt;- sqrt(s1^2/n1 + s2^2/n2)\n\n# Welch-Satterthwaite degrees of freedom\ndf &lt;- n1 + n2 - 2\n\n# Hypothesized difference = 5.5\nt_stat &lt;- (diff - 5.5)/SE\np_val  &lt;- pt(t_stat, df)   # one-sided, \"&lt;\"\n\nlist(t_stat = t_stat, df = df, p_val = p_val)\n\n$t_stat\n[1] -0.9519709\n\n$df\n[1] 56\n\n$p_val\n[1] 0.1726013\n\n\n\n\n\nOne-sided \\(p\\)-value \\(\\approx 0.17\\).\nAt \\(\\alpha=0.05\\), we fail to reject \\(H_0\\).\nThe evidence is not strong enough to conclude that the mean difference is less than 5.5 inches.\n\n\n\n\\[\n\\begin{aligned}\n(\\bar{x}_1 - \\bar{x}_2) \\;\\pm\\; t_{0.975,\\,df}\\,\n\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}\n   &= 4.8 \\;\\pm\\; (2.00)(0.735) \\\\[6pt]\n   &= 4.8 \\;\\pm\\; 1.47 \\\\[6pt]\n   &= (3.33,\\; 6.27)\n\\end{aligned}\n\\]\n\ntcrit &lt;- qt(0.975, df)\nCI95  &lt;- diff + c(-1, 1) * tcrit * SE\n\nlist(diff = diff, SE = SE, tcrit = tcrit, CI95 = CI95, df = df)\n\n$diff\n[1] 4.8\n\n$SE\n[1] 0.7353166\n\n$tcrit\n[1] 2.003241\n\n$CI95\n[1] 3.326984 6.273016\n\n$df\n[1] 56\n\n\nWe are 95% confident that the true difference in mean heights is between 3.3 and 6.3 inches.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 19"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-19.html#board-problem-does-a-breathing-drill-reduce-average-mile-time",
    "href": "MA206-AY26-1/lesson-19.html#board-problem-does-a-breathing-drill-reduce-average-mile-time",
    "title": "Lesson 19: Two Mean T-Test",
    "section": "",
    "text": "A platoon tests a new breathing drill to see if it reduces average 1-mile run time (minutes).\n\nDrill group (1): \\(n_1=20,\\ \\bar{x}_1=6.83,\\ s_1=0.36\\)\n\nStandard training (2): \\(n_2=18,\\ \\bar{x}_2=7.10,\\ s_2=0.40\\)\n\nTest, at \\(\\alpha=0.05\\), whether the drill reduces mean time (i.e., is faster). Also construct a 95% CI for \\(\\mu_1-\\mu_2\\).\n\nDefine parameters and state hypotheses.\n\nCompute the test statistic, degrees of freedom, and one-sided \\(p\\)-value.\n\nMake a decision and interpret.\n\nGive the 95% CI and interpret it in context.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\\(H_0:\\ \\mu_1-\\mu_2=0 \\quad\\text{vs}\\quad H_A:\\ \\mu_1-\\mu_2&lt;0\\)\n\n\n\n\\(\\bar{x}_1-\\bar{x}_2 = 6.83 - 7.10 = -0.27\\)\n\\(SE = \\sqrt{\\tfrac{0.36^2}{20} + \\tfrac{0.40^2}{18}} \\approx 0.124\\)\n\\(t = \\dfrac{-0.27}{0.124} \\approx -2.18\\)\n\\(df = n_1+n_2-2 = 20+18-2 = 36\\)\nOne-sided \\(p\\)-value \\(\\approx 0.018\\).\nDecision: Reject \\(H_0\\).\nConclusion: The drill group runs faster on average.\n\n\n\n\\((\\bar{x}_1-\\bar{x}_2) \\pm t_{0.975,df}\\,SE\n= -0.27 \\pm (2.03)(0.124)\n= -0.27 \\pm 0.25\n= (-0.52,\\ -0.02)\\)\nWe are 95% confident the drill reduces mean mile time by 0.02 to 0.52 minutes (≈ 1–31 seconds).\n\n\n\nxbar1 &lt;- 6.83; s1 &lt;- 0.36; n1 &lt;- 20\nxbar2 &lt;- 7.10; s2 &lt;- 0.40; n2 &lt;- 18\n\ndiff &lt;- xbar1 - xbar2\nSE   &lt;- sqrt(s1^2/n1 + s2^2/n2)\n\ndf &lt;- n1 + n2 - 2  # pooled degrees of freedom\n\nt_stat &lt;- diff / SE\np_one_sided &lt;- pt(t_stat, df)\n\ntcrit &lt;- qt(0.975, df)\nCI95  &lt;- diff + c(-1, 1) * tcrit * SE\n\nlist(diff = diff, SE = SE, df = df,\n     t_stat = t_stat, p_one_sided = p_one_sided,\n     tcrit = tcrit, CI95 = CI95)\n\n$diff\n[1] -0.27\n\n$SE\n[1] 0.1239713\n\n$df\n[1] 36\n\n$t_stat\n[1] -2.177923\n\n$p_one_sided\n[1] 0.01802182\n\n$tcrit\n[1] 2.028094\n\n$CI95\n[1] -0.5214255 -0.0185745",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 19"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-19.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-19.html#before-you-leave",
    "title": "Lesson 19: Two Mean T-Test",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 19"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-20.html",
    "href": "MA206-AY26-1/lesson-20.html",
    "title": "Lesson 20: Paired Data",
    "section": "",
    "text": "Lets go over the grading plan and solution and take about 10 minutes.\n\n\n\nEE7.2 due 0700 23 Oct\n\n\n\n\nDetails\n\n\n\n\n\nLesson 22\n\n\n\n\n\nDue 0700 on 7 November\nInstructions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\pi = \\pi_0\\)\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nValidity Conditions\n\nNumber of successes and failures must be greater than 10.\n\nConfidence Interval for \\(\\pi\\) (one proportion)\n\\[\n\\hat{p} \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true population proportion \\(\\pi\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu = \\mu_0\\)\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu &lt; \\mu_0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) (degrees of freedom)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nValidity Conditions\n\nSample size must be greater than 30.\n\nConfidence Interval for \\(\\mu\\) (one mean)\n\\[\n\\bar{x} \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\\frac{s}{\\sqrt{n}},\n\\qquad df = n-1\n\\] I am \\((1 - \\alpha)\\%\\) confident that the true population mean \\((\\mu)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\pi_1 - \\pi_2 = 0\\)\n\\[\nz \\;=\\; \\frac{(\\hat{p}_1 - \\hat{p}_2) - (\\pi_1 - \\pi_2)}{\\sqrt{\\hat{p}(1-\\hat{p})\\left(\\tfrac{1}{n_1} + \\tfrac{1}{n_2}\\right)}}\n\\]\nWhere the pooled proportion is\n\\[\n\\hat{p} \\;=\\; \\frac{x_1 + x_2}{n_1 + n_2}.\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 &gt; 0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 &lt; 0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 \\neq 0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p}_1 = x_1/n_1\\) (sample proportion in group 1)\n\n\\(\\hat{p}_2 = x_2/n_2\\) (sample proportion in group 2)\n\n\\(\\pi_1, \\pi_2\\) = hypothesized proportions under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nValidity Conditions\n\nEach group must have at least 10 successes and 10 failures.\n\nConfidence Interval for \\(\\pi_1 - \\pi_2\\) (unpooled SE)\n\\[\n(\\hat{p}_1 - \\hat{p}_2) \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\n\\sqrt{\\tfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\tfrac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true difference in population proportions \\((\\pi_1 - \\pi_2)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu_1 - \\mu_2 = 0\\)\n\\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - 0}{\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu_1 - \\mu_2 &gt; 0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu_1 - \\mu_2 &lt; 0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu_1 - \\mu_2 \\neq 0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}_1,\\ \\bar{x}_2\\) = sample means in groups 1 and 2\n\n\\(s_1,\\ s_2\\) = sample standard deviations\n\n\\(n_1,\\ n_2\\) = sample sizes\n\n\\(df = n_1 + n_2 - 2\\)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nValidity Conditions\n\nGroups are independent\n\nPopulations are approximately normal (or \\(n_1, n_2\\) large enough by CLT)\n\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\\[\n(\\bar{x}_1 - \\bar{x}_2) \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\n\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}},\n\\qquad df = n_1+n_2-2\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true difference in population means \\((\\mu_1 - \\mu_2)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\nStrength of evidence: Smaller \\(p\\) means stronger evidence against \\(H_0\\).\n\n\n\n\n\nGeneralization: We can generalize results to a larger population if the data come from a random and representative sample of that population.\n\nCausation: We can claim causation if participants are randomly assigned to treatments in an experiment. Without random assignment, we can only conclude association, not causation.\n\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n& \\text{Randomly Sampled} & \\text{Not Randomly Sampled} \\\\\n\\hline\n\\textbf{Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: Yes}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: Yes}\n\\end{array} \\\\\n\\hline\n\\textbf{Not Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: No}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: No}\n\\end{array} \\\\\n\\hline\n\\end{array}\n\\]\n\nParameters vs. Statistics: A parameter is a fixed (but usually unknown) numerical value describing a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\pi\\)). A statistic is a numerical value computed from a sample (e.g., \\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\)).\n\nParameters = target (what we want to know).\n\nStatistics = evidence (what we can actually measure).\n\nWe use statistics to estimate parameters, and because different samples give different statistics, we capture this variability with confidence intervals.\n\n\n\n\n\nQuantity\nPopulation (Parameter)\nSample (Statistic)\n\n\n\n\nCenter (mean)\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nSpread (SD)\n\\(\\sigma\\)\n\\(s\\)\n\n\nProportion “success”\n\\(\\pi\\)\n\\(\\hat{p}\\)\n\n\n\n\n\n\n\nSometimes we do not have two independent groups, but instead two measurements on the same subjects (or matched pairs).\nExamples:\n\nBefore/after training scores\n\nLeft vs. right leg strength in the same person\n\nIn these cases, we analyze the differences within pairs rather than treating them as two independent samples. This essentially becomes a one mean hypothesis test from before.\n\n\nResearchers measured cadets’ mile times before and after a new training program. The same 22 cadets were tested twice.\n\n\n\nCadet\nBefore\nAfter\n\n\n\n\n1\n7.20\n7.80\n\n\n2\n6.95\n7.60\n\n\n3\n7.40\n8.10\n\n\n4\n7.00\n6.70\n\n\n5\n6.85\n6.60\n\n\n6\n7.25\n7.00\n\n\n…\n…\n…\n\n\n\nWe want to test if the program reduces mile time on average.\n\n\n\nLet \\(d = \\text{Before – After}\\).\n\nNull: \\(H_0:\\ \\mu_d = 0\\) (no change)\n\nAlternative: \\(H_A:\\ \\mu_d &gt; 0\\) (before is greater than after \\(\\;\\Rightarrow\\;\\) improved performance)\n\n\n\n\nWe compute a one-sample \\(t\\)-test on the differences:\n\\[\nt \\;=\\; \\frac{\\bar{d} - 0}{s_d/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\\[\nt = \\frac{-0.143}{.325/\\sqrt{22}}\n   = \\frac{-0.143}{0.067}\n   \\approx 2.06\n\\]\n\nbefore &lt;- c(7.20,6.95,7.40,7.00,6.85,7.25,7.05,6.90,7.10,6.80,7.00,6.95,7.15,7.05,7.20,6.85,7.30,7.00,6.95,7.10,7.05,7.25)\n\nafter &lt;- c(7.80,7.60,8.10,6.70,6.60,7.00,6.90,6.55,6.85,6.50,6.70,6.65,6.85,6.80,6.95,6.60,7.00,6.75,6.70,6.85,6.80,6.95)\n\nd &lt;- before - after\nn &lt;- length(d)\ndbar &lt;- mean(d)\nsd   &lt;- sd(d)\ndf   &lt;- n - 1\n\nt_stat &lt;- dbar / (sd/sqrt(n))\np_val  &lt;- 1 - pt(t_stat, df)   # one-sided, \"&lt;\" since After &lt; Before\n\nlist(n = n, dbar = dbar, sd = sd, t_stat = t_stat, df = df, p_val = p_val)\n\n$n\n[1] 22\n\n$dbar\n[1] 0.1431818\n\n$sd\n[1] 0.3252455\n\n$t_stat\n[1] 2.064847\n\n$df\n[1] 21\n\n$p_val\n[1] 0.02575202\n\n\n\n\n\n\\(p \\approx 0.026 &lt; 0.05\\)\n\\(\\;\\;\\Rightarrow\\) Reject \\(H_0\\).\nWe are 95% confident that the true mean difference in performance on mile time between the before and after is greater than 0.\n\n\n\n95% CI for \\(\\mu_d\\):\n\\[\n\\bar{d} \\;\\pm\\; t_{.025,\\,df}\\,\\frac{s_d}{\\sqrt{n}}\n\\]\n\\[\n= 0.143 \\;\\pm\\; (2.08)(0.325)  \n= 0.143\\;\\pm\\; 0.144  \n= (-0.001,\\; 0.287)\n\\]\n\ntcrit &lt;- qt(0.025, df)\n\nCI95 &lt;- c(dbar + tcrit*(sd/sqrt(n)), dbar - tcrit*(sd/sqrt(n)))\n\nlist(dbar = dbar, sd = sd, df = df, tcrit = tcrit, CI95 = CI95)\n\n$dbar\n[1] 0.1431818\n\n$sd\n[1] 0.3252455\n\n$df\n[1] 21\n\n$tcrit\n[1] -2.079614\n\n$CI95\n[1] -0.001023956  0.287387592\n\n\nWe are 95% confident that the training reduces mile time by -.001 to 0.287 minutes.\nYes, the CI includes 0. So in a \\(\\neq\\) alternative hypothesis this would fail to reject.\n\n\n\n\nValidity: Differences are independent, approximately normal, and \\(n=22\\) is a reasonably large sample.\n\nScope:\n\nIf cadets were randomly sampled \\(\\;\\Rightarrow\\;\\) generalize to population.\n\nIf cadets were randomly assigned \\(\\;\\Rightarrow\\;\\) can claim causation.\n\n\n\n\n\n\nA group of cadets test whether drinking a small cup of coffee improves reaction time (measured in seconds). Each cadet is tested before and after drinking coffee.\n\n\\(n=12\\) cadets\n\nMean difference (Before – After): \\(\\bar{d} = 0.18\\) seconds\n\nStandard deviation of differences: \\(s_d = 0.22\\) seconds\n\nTest, at \\(\\alpha=0.05\\), whether coffee reduces average reaction time (i.e., improves performance). Also construct a 95% CI for \\(\\mu_d\\).\n\nDefine parameters and state hypotheses.\n\nCompute the test statistic, degrees of freedom, and one-sided \\(p\\)-value.\n\nMake a decision and interpret.\n\nGive the 95% CI and interpret it in context.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\\(H_0:\\ \\mu_d = 0 \\quad\\text{vs}\\quad H_A:\\ \\mu_d &gt; 0\\)\n(where \\(d =\\) Before – After, so \\(d&gt;0\\) means improvement)\n\n\n\n\\[\nt = \\frac{\\bar{d} - 0}{s_d/\\sqrt{n}}\n= \\frac{0.18}{0.22/\\sqrt{12}}\n= \\frac{0.18}{0.064}\n\\approx 2.81\n\\]\nDegrees of freedom: \\(df = n-1 = 11\\)\nOne-sided \\(p\\)-value \\(\\approx 0.008\\).\nDecision: Reject \\(H_0\\).\nConclusion: There is strong evidence that coffee improves reaction time.\n\n\n\n\\[\n\\bar{d} \\;\\pm\\; t_{0.975,\\,df}\\,\\frac{s_d}{\\sqrt{n}}\n= 0.18 \\;\\pm\\; (2.20)(0.064)\n= 0.18 \\;\\pm\\; 0.14\n= (0.04,\\; 0.32)\n\\]\nWe are 95% confident that coffee reduces average reaction time by 0.04 to 0.32 seconds.\n\n\n\n\ndbar &lt;- 0.18; sd &lt;- 0.22; n &lt;- 12\ndf &lt;- n - 1\n\nt_stat &lt;- dbar / (sd/sqrt(n))\np_val &lt;- 1 - pt(t_stat, df)  # one-sided, \"greater\"\n\ntcrit &lt;- qt(0.975, df)\nCI95 &lt;- dbar + c(-1, 1) * tcrit * (sd/sqrt(n))\n\nlist(dbar = dbar, sd = sd, n = n, df = df,\n     t_stat = t_stat, p_val = p_val,\n     tcrit = tcrit, CI95 = CI95)\n\n$dbar\n[1] 0.18\n\n$sd\n[1] 0.22\n\n$n\n[1] 12\n\n$df\n[1] 11\n\n$t_stat\n[1] 2.834265\n\n$p_val\n[1] 0.008123858\n\n$tcrit\n[1] 2.200985\n\n$CI95\n[1] 0.04021867 0.31978133\n\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 20"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-20.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-20.html#lesson-administration",
    "title": "Lesson 20: Paired Data",
    "section": "",
    "text": "Lets go over the grading plan and solution and take about 10 minutes.\n\n\n\nEE7.2 due 0700 23 Oct\n\n\n\n\nDetails\n\n\n\n\n\nLesson 22\n\n\n\n\n\nDue 0700 on 7 November\nInstructions",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 20"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-20.html#running-review",
    "href": "MA206-AY26-1/lesson-20.html#running-review",
    "title": "Lesson 20: Paired Data",
    "section": "",
    "text": "For all cases:\n\\(H_0:\\ \\pi = \\pi_0\\)\n\\[\nz = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi &gt; \\pi_0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi &lt; \\pi_0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi \\neq \\pi_0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p} = R/n\\) (sample proportion)\n\n\\(\\pi_0\\) = hypothesized proportion under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nValidity Conditions\n\nNumber of successes and failures must be greater than 10.\n\nConfidence Interval for \\(\\pi\\) (one proportion)\n\\[\n\\hat{p} \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\\sqrt{\\frac{\\hat{p}\\,(1-\\hat{p})}{n}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true population proportion \\(\\pi\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu = \\mu_0\\)\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu &gt; \\mu_0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu &lt; \\mu_0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu \\neq \\mu_0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesized mean under \\(H_0\\)\n\n\\(s\\) = sample standard deviation\n\n\\(n\\) = sample size\n\n\\(df = n - 1\\) (degrees of freedom)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nValidity Conditions\n\nSample size must be greater than 30.\n\nConfidence Interval for \\(\\mu\\) (one mean)\n\\[\n\\bar{x} \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\\frac{s}{\\sqrt{n}},\n\\qquad df = n-1\n\\] I am \\((1 - \\alpha)\\%\\) confident that the true population mean \\((\\mu)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\nFor all cases:\n\\(H_0:\\ \\pi_1 - \\pi_2 = 0\\)\n\\[\nz \\;=\\; \\frac{(\\hat{p}_1 - \\hat{p}_2) - (\\pi_1 - \\pi_2)}{\\sqrt{\\hat{p}(1-\\hat{p})\\left(\\tfrac{1}{n_1} + \\tfrac{1}{n_2}\\right)}}\n\\]\nWhere the pooled proportion is\n\\[\n\\hat{p} \\;=\\; \\frac{x_1 + x_2}{n_1 + n_2}.\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 &gt; 0\\)\n\\(p = 1 - \\Phi(z)\\)\np_val &lt;- 1 - pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 &lt; 0\\)\n\\(p = \\Phi(z)\\)\np_val &lt;- pnorm(z_stat)\n\n\n\\(H_A:\\ \\pi_1 - \\pi_2 \\neq 0\\)\n\\(p = 2 \\cdot (1 - \\Phi(|z|))\\)\np_val &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n\n\nWhere:\n\n\\(\\hat{p}_1 = x_1/n_1\\) (sample proportion in group 1)\n\n\\(\\hat{p}_2 = x_2/n_2\\) (sample proportion in group 2)\n\n\\(\\pi_1, \\pi_2\\) = hypothesized proportions under \\(H_0\\)\n\n\\(\\Phi(\\cdot)\\) = cumulative distribution function (CDF) of the standard normal distribution\n\nValidity Conditions\n\nEach group must have at least 10 successes and 10 failures.\n\nConfidence Interval for \\(\\pi_1 - \\pi_2\\) (unpooled SE)\n\\[\n(\\hat{p}_1 - \\hat{p}_2) \\;\\pm\\; z_{\\,1-\\alpha/2}\\,\n\\sqrt{\\tfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\tfrac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true difference in population proportions \\((\\pi_1 - \\pi_2)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\nFor all cases:\n\\(H_0:\\ \\mu_1 - \\mu_2 = 0\\)\n\\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - 0}{\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}}\n\\]\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nFormula for \\(p\\)-value\nR Code\n\n\n\n\n\\(H_A:\\ \\mu_1 - \\mu_2 &gt; 0\\)\n\\(p = 1 - F_{t,df}(t)\\)\np_val &lt;- 1 - pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu_1 - \\mu_2 &lt; 0\\)\n\\(p = F_{t,df}(t)\\)\np_val &lt;- pt(t_stat, df)\n\n\n\\(H_A:\\ \\mu_1 - \\mu_2 \\neq 0\\)\n\\(p = 2 \\cdot (1 - F_{t,df}(|t|))\\)\np_val &lt;- 2 * (1 - pt(abs(t_stat), df))\n\n\n\nWhere:\n\n\\(\\bar{x}_1,\\ \\bar{x}_2\\) = sample means in groups 1 and 2\n\n\\(s_1,\\ s_2\\) = sample standard deviations\n\n\\(n_1,\\ n_2\\) = sample sizes\n\n\\(df = n_1 + n_2 - 2\\)\n\n\\(F_{t,df}(\\cdot)\\) = CDF of Student’s \\(t\\) distribution with \\(df\\) degrees of freedom\n\nValidity Conditions\n\nGroups are independent\n\nPopulations are approximately normal (or \\(n_1, n_2\\) large enough by CLT)\n\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\\[\n(\\bar{x}_1 - \\bar{x}_2) \\;\\pm\\; t_{\\,1-\\alpha/2,\\;df}\\,\n\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}},\n\\qquad df = n_1+n_2-2\n\\]\nI am \\((1 - \\alpha)\\%\\) confident that the true difference in population means \\((\\mu_1 - \\mu_2)\\) lies between \\([\\text{lower bound}, \\text{upper bound}]\\).\n\n\n\n\n\nRejecting \\(H_0\\)\n&gt; Since the \\(p\\)-value is less than \\(\\alpha\\) (e.g., \\(0.05\\)), we reject the null hypothesis.\n&gt; We conclude that there is sufficient evidence to suggest that [state the alternative claim in context].\nFailing to Reject \\(H_0\\)\n&gt; Since the \\(p\\)-value is greater than \\(\\alpha\\) (e.g., \\(0.05\\)), we fail to reject the null hypothesis.\n&gt; We conclude that there is not sufficient evidence to suggest that [state the alternative claim in context].\nStrength of evidence: Smaller \\(p\\) means stronger evidence against \\(H_0\\).\n\n\n\n\n\nGeneralization: We can generalize results to a larger population if the data come from a random and representative sample of that population.\n\nCausation: We can claim causation if participants are randomly assigned to treatments in an experiment. Without random assignment, we can only conclude association, not causation.\n\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n& \\text{Randomly Sampled} & \\text{Not Randomly Sampled} \\\\\n\\hline\n\\textbf{Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: Yes}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: Yes}\n\\end{array} \\\\\n\\hline\n\\textbf{Not Randomly Assigned} &\n\\begin{array}{c}\n\\text{Generalize: Yes} \\\\\n\\text{Causation: No}\n\\end{array} &\n\\begin{array}{c}\n\\text{Generalize: No} \\\\\n\\text{Causation: No}\n\\end{array} \\\\\n\\hline\n\\end{array}\n\\]\n\nParameters vs. Statistics: A parameter is a fixed (but usually unknown) numerical value describing a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(\\pi\\)). A statistic is a numerical value computed from a sample (e.g., \\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\)).\n\nParameters = target (what we want to know).\n\nStatistics = evidence (what we can actually measure).\n\nWe use statistics to estimate parameters, and because different samples give different statistics, we capture this variability with confidence intervals.\n\n\n\n\n\nQuantity\nPopulation (Parameter)\nSample (Statistic)\n\n\n\n\nCenter (mean)\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nSpread (SD)\n\\(\\sigma\\)\n\\(s\\)\n\n\nProportion “success”\n\\(\\pi\\)\n\\(\\hat{p}\\)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 20"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-20.html#paired-t-test",
    "href": "MA206-AY26-1/lesson-20.html#paired-t-test",
    "title": "Lesson 20: Paired Data",
    "section": "",
    "text": "Sometimes we do not have two independent groups, but instead two measurements on the same subjects (or matched pairs).\nExamples:\n\nBefore/after training scores\n\nLeft vs. right leg strength in the same person\n\nIn these cases, we analyze the differences within pairs rather than treating them as two independent samples. This essentially becomes a one mean hypothesis test from before.\n\n\nResearchers measured cadets’ mile times before and after a new training program. The same 22 cadets were tested twice.\n\n\n\nCadet\nBefore\nAfter\n\n\n\n\n1\n7.20\n7.80\n\n\n2\n6.95\n7.60\n\n\n3\n7.40\n8.10\n\n\n4\n7.00\n6.70\n\n\n5\n6.85\n6.60\n\n\n6\n7.25\n7.00\n\n\n…\n…\n…\n\n\n\nWe want to test if the program reduces mile time on average.\n\n\n\nLet \\(d = \\text{Before – After}\\).\n\nNull: \\(H_0:\\ \\mu_d = 0\\) (no change)\n\nAlternative: \\(H_A:\\ \\mu_d &gt; 0\\) (before is greater than after \\(\\;\\Rightarrow\\;\\) improved performance)\n\n\n\n\nWe compute a one-sample \\(t\\)-test on the differences:\n\\[\nt \\;=\\; \\frac{\\bar{d} - 0}{s_d/\\sqrt{n}}, \\qquad df = n-1\n\\]\n\n\n\n\\[\nt = \\frac{-0.143}{.325/\\sqrt{22}}\n   = \\frac{-0.143}{0.067}\n   \\approx 2.06\n\\]\n\nbefore &lt;- c(7.20,6.95,7.40,7.00,6.85,7.25,7.05,6.90,7.10,6.80,7.00,6.95,7.15,7.05,7.20,6.85,7.30,7.00,6.95,7.10,7.05,7.25)\n\nafter &lt;- c(7.80,7.60,8.10,6.70,6.60,7.00,6.90,6.55,6.85,6.50,6.70,6.65,6.85,6.80,6.95,6.60,7.00,6.75,6.70,6.85,6.80,6.95)\n\nd &lt;- before - after\nn &lt;- length(d)\ndbar &lt;- mean(d)\nsd   &lt;- sd(d)\ndf   &lt;- n - 1\n\nt_stat &lt;- dbar / (sd/sqrt(n))\np_val  &lt;- 1 - pt(t_stat, df)   # one-sided, \"&lt;\" since After &lt; Before\n\nlist(n = n, dbar = dbar, sd = sd, t_stat = t_stat, df = df, p_val = p_val)\n\n$n\n[1] 22\n\n$dbar\n[1] 0.1431818\n\n$sd\n[1] 0.3252455\n\n$t_stat\n[1] 2.064847\n\n$df\n[1] 21\n\n$p_val\n[1] 0.02575202\n\n\n\n\n\n\\(p \\approx 0.026 &lt; 0.05\\)\n\\(\\;\\;\\Rightarrow\\) Reject \\(H_0\\).\nWe are 95% confident that the true mean difference in performance on mile time between the before and after is greater than 0.\n\n\n\n95% CI for \\(\\mu_d\\):\n\\[\n\\bar{d} \\;\\pm\\; t_{.025,\\,df}\\,\\frac{s_d}{\\sqrt{n}}\n\\]\n\\[\n= 0.143 \\;\\pm\\; (2.08)(0.325)  \n= 0.143\\;\\pm\\; 0.144  \n= (-0.001,\\; 0.287)\n\\]\n\ntcrit &lt;- qt(0.025, df)\n\nCI95 &lt;- c(dbar + tcrit*(sd/sqrt(n)), dbar - tcrit*(sd/sqrt(n)))\n\nlist(dbar = dbar, sd = sd, df = df, tcrit = tcrit, CI95 = CI95)\n\n$dbar\n[1] 0.1431818\n\n$sd\n[1] 0.3252455\n\n$df\n[1] 21\n\n$tcrit\n[1] -2.079614\n\n$CI95\n[1] -0.001023956  0.287387592\n\n\nWe are 95% confident that the training reduces mile time by -.001 to 0.287 minutes.\nYes, the CI includes 0. So in a \\(\\neq\\) alternative hypothesis this would fail to reject.\n\n\n\n\nValidity: Differences are independent, approximately normal, and \\(n=22\\) is a reasonably large sample.\n\nScope:\n\nIf cadets were randomly sampled \\(\\;\\Rightarrow\\;\\) generalize to population.\n\nIf cadets were randomly assigned \\(\\;\\Rightarrow\\;\\) can claim causation.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 20"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-20.html#board-problem-does-caffeine-improve-reaction-time",
    "href": "MA206-AY26-1/lesson-20.html#board-problem-does-caffeine-improve-reaction-time",
    "title": "Lesson 20: Paired Data",
    "section": "",
    "text": "A group of cadets test whether drinking a small cup of coffee improves reaction time (measured in seconds). Each cadet is tested before and after drinking coffee.\n\n\\(n=12\\) cadets\n\nMean difference (Before – After): \\(\\bar{d} = 0.18\\) seconds\n\nStandard deviation of differences: \\(s_d = 0.22\\) seconds\n\nTest, at \\(\\alpha=0.05\\), whether coffee reduces average reaction time (i.e., improves performance). Also construct a 95% CI for \\(\\mu_d\\).\n\nDefine parameters and state hypotheses.\n\nCompute the test statistic, degrees of freedom, and one-sided \\(p\\)-value.\n\nMake a decision and interpret.\n\nGive the 95% CI and interpret it in context.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\\(H_0:\\ \\mu_d = 0 \\quad\\text{vs}\\quad H_A:\\ \\mu_d &gt; 0\\)\n(where \\(d =\\) Before – After, so \\(d&gt;0\\) means improvement)\n\n\n\n\\[\nt = \\frac{\\bar{d} - 0}{s_d/\\sqrt{n}}\n= \\frac{0.18}{0.22/\\sqrt{12}}\n= \\frac{0.18}{0.064}\n\\approx 2.81\n\\]\nDegrees of freedom: \\(df = n-1 = 11\\)\nOne-sided \\(p\\)-value \\(\\approx 0.008\\).\nDecision: Reject \\(H_0\\).\nConclusion: There is strong evidence that coffee improves reaction time.\n\n\n\n\\[\n\\bar{d} \\;\\pm\\; t_{0.975,\\,df}\\,\\frac{s_d}{\\sqrt{n}}\n= 0.18 \\;\\pm\\; (2.20)(0.064)\n= 0.18 \\;\\pm\\; 0.14\n= (0.04,\\; 0.32)\n\\]\nWe are 95% confident that coffee reduces average reaction time by 0.04 to 0.32 seconds.\n\n\n\n\ndbar &lt;- 0.18; sd &lt;- 0.22; n &lt;- 12\ndf &lt;- n - 1\n\nt_stat &lt;- dbar / (sd/sqrt(n))\np_val &lt;- 1 - pt(t_stat, df)  # one-sided, \"greater\"\n\ntcrit &lt;- qt(0.975, df)\nCI95 &lt;- dbar + c(-1, 1) * tcrit * (sd/sqrt(n))\n\nlist(dbar = dbar, sd = sd, n = n, df = df,\n     t_stat = t_stat, p_val = p_val,\n     tcrit = tcrit, CI95 = CI95)\n\n$dbar\n[1] 0.18\n\n$sd\n[1] 0.22\n\n$n\n[1] 12\n\n$df\n[1] 11\n\n$t_stat\n[1] 2.834265\n\n$p_val\n[1] 0.008123858\n\n$tcrit\n[1] 2.200985\n\n$CI95\n[1] 0.04021867 0.31978133",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 20"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-20.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-20.html#before-you-leave",
    "title": "Lesson 20: Paired Data",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\nWPR 2: Lesson 22",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 20"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-24.html",
    "href": "MA206-AY26-1/lesson-24.html",
    "title": "Lesson 24: Multiple Linear Regression I",
    "section": "",
    "text": "Lesson 25.5\n7 November\nMilestone 5 Instructions\n\n\n\n\n\nLesson 26\n12-13 November\nLink: TBD\n\n\n\n\n\nLesson 26.5\n17 November\nMilestone 6 Instructions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\nRemember this from last class?\n\n\n\n\n\n\n\n\n\n\nmod &lt;- lm(formula = mpg~wt, data = mtcars)\nsummary (mod)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nIt gave us this.\n\\[\n\\widehat{mpg} \\;=\\; 37.39 \\;-\\; 5.34 \\, wt\n\\] What if we wanted to know this?\n\\[\n\\widehat{mpg} \\;=\\; \\beta_0 \\;+\\; \\beta_1 \\, wt + \\beta_2 \\, hp\n\\]\nDo we take 3 partials?\n\n\n\n\n\n\nJust like in simple linear regression, we still minimize the sum of squared errors — but now our model has multiple predictors.\n\\[\n\\begin{aligned}\ny_i &= \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i \\\\[6pt]\n\\mathbf{y} &= \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\end{aligned}\n\\]\nwhere:\n\n\\(\\mathbf{y}\\) is an \\(n \\times 1\\) vector of responses\n\n\\(\\mathbf{X}\\) is an \\(n \\times (p+1)\\) matrix of predictors (including a column of 1’s for the intercept)\n\n\\(\\boldsymbol{\\beta}\\) is a \\((p+1) \\times 1\\) vector of coefficients\n\n\\(\\boldsymbol{\\varepsilon}\\) is the \\(n \\times 1\\) vector of residuals\n\n\n\n\nWe minimize the sum of squared errors:\n\\[\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n\\]\n\n\n\nExpanding and differentiating with respect to \\(\\boldsymbol{\\beta}\\):\n\\[\n\\begin{aligned}\nS(\\boldsymbol{\\beta})\n&= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[6pt]\n&= \\mathbf{y}^\\top\\mathbf{y}\n- \\mathbf{y}^\\top\\mathbf{X}\\boldsymbol{\\beta}\n- \\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{y}\n+ \\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{X}\\boldsymbol{\\beta} \\\\[6pt]\n&= \\mathbf{y}^\\top\\mathbf{y}\n- 2\\,\\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{y}\n+ \\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{X}\\boldsymbol{\\beta} \\\\[10pt]\n\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}}\n&= \\frac{\\partial}{\\partial \\boldsymbol{\\beta}}\n\\left(\n\\mathbf{y}^\\top\\mathbf{y}\n- 2\\,\\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{y}\n+ \\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{X}\\boldsymbol{\\beta}\n\\right) \\\\[6pt]\n&= -2\\,\\mathbf{X}^\\top\\mathbf{y}\n+ 2\\,\\mathbf{X}^\\top\\mathbf{X}\\boldsymbol{\\beta} = 0 \\\\[10pt]\n\\mathbf{X}^\\top\\mathbf{X}\\boldsymbol{\\beta}\n&= \\mathbf{X}^\\top\\mathbf{y} \\\\[10pt]\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y}\n\\end{aligned}\n\\] Therefore we now know:\n\\[\n\\widehat{mpg} = \\hat{\\beta}_0 + \\hat{\\beta}_1 wt + \\hat{\\beta}_2 hp\n\\]\nwith\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\hat{\\beta}_0 \\\\[4pt]\n\\hat{\\beta}_1 \\\\[4pt]\n\\hat{\\beta}_2\n\\end{bmatrix}\n&=\n\\left(\n\\mathbf{X}^\\top \\mathbf{X}\n\\right)^{-1}\n\\mathbf{X}^\\top \\mathbf{y}.\n\\end{aligned}\n\\]\nAnd should we want to make predictions, we have our ‘hat matrix’\n\\[\n\\widehat{\\mathbf{y}} = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y}\n\\]\n\n\n\n\n\nmod &lt;- lm(formula = mpg~wt+hp, data = mtcars)\nsummary(mod)\n\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nSo we see \\(\\hat{mpg} = 37.2 - 3.88 \\,wt -.03 \\,hp\\)\n\nWhat do we make of our P Values?\n\nWhat do we make of the \\(R^2\\)?\n\n\n\n\n\nmod &lt;- lm(formula = mpg~cyl, data = mtcars)\nsummary(mod)\n\n\nCall:\nlm(formula = mpg ~ cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9814 -2.1185  0.2217  1.0717  7.5186 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.8846     2.0738   18.27  &lt; 2e-16 ***\ncyl          -2.8758     0.3224   -8.92 6.11e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.206 on 30 degrees of freedom\nMultiple R-squared:  0.7262,    Adjusted R-squared:  0.7171 \nF-statistic: 79.56 on 1 and 30 DF,  p-value: 6.113e-10\n\n\nDoes this make sense?\n\n\n\n\n\n\n\n\n\nThe reality is that cylinders aren’t continuous so we need to make sure that R is reading them as discrete. (Does this sound familiar to your project?)\n\nmod &lt;- lm(formula = mpg~as.factor(cyl), data = mtcars)\nsummary(mod)\n\n\nCall:\nlm(formula = mpg ~ as.factor(cyl), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2636 -1.8357  0.0286  1.3893  7.2364 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      26.6636     0.9718  27.437  &lt; 2e-16 ***\nas.factor(cyl)6  -6.9208     1.5583  -4.441 0.000119 ***\nas.factor(cyl)8 -11.5636     1.2986  -8.905 8.57e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.223 on 29 degrees of freedom\nMultiple R-squared:  0.7325,    Adjusted R-squared:  0.714 \nF-statistic:  39.7 on 2 and 29 DF,  p-value: 4.979e-09\n\n\nSo how do we interpret this?\nWhen we include as.factor(cyl) in our regression, R automatically creates dummy variables for each level of cyl except one (the baseline).\nIn this case, the model is:\n\\[\n\\hat{y} = \\beta_0 + \\beta_1 \\, cyl6 + \\beta_2 \\, cyl8\n\\]\nwhere:\n\n\\(cyl6 = 1\\) if the car has 6 cylinders, \\(0\\) otherwise\n\n\\(cyl8 = 1\\) if the car has 8 cylinders, \\(0\\) otherwise\n\nCars with 4 cylinders are the baseline group (both dummy variables are 0)\n\nFrom the regression output, suppose we have:\n\\[\n\\hat{y} = 26.7 - 6.92\\,cyl6 - 11.6\\,cyl8\n\\]\nWe can interpret this as:\n\nIntercept (26.7):\nThe average mpg for 4-cylinder cars (the baseline).\nCoefficient for cyl6 (-6.92):\nOn average, 6-cylinder cars have 6.92 mpg less than 4-cylinder cars.\nCoefficient for cyl8 (-11.6):\nOn average, 8-cylinder cars have 11.6 mpg less than 4-cylinder cars.\n\n\n\n\nWhat if we include both continuous variables (wt, hp) and a categorical variable (cyl) in the same model?\n\nmod &lt;- lm(formula = mpg ~ wt + hp + as.factor(cyl), data = mtcars)\nsummary(mod)\n\n\nCall:\nlm(formula = mpg ~ wt + hp + as.factor(cyl), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2612 -1.0320 -0.3210  0.9281  5.3947 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     35.84600    2.04102  17.563 2.67e-16 ***\nwt              -3.18140    0.71960  -4.421 0.000144 ***\nhp              -0.02312    0.01195  -1.934 0.063613 .  \nas.factor(cyl)6 -3.35902    1.40167  -2.396 0.023747 *  \nas.factor(cyl)8 -3.18588    2.17048  -1.468 0.153705    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.44 on 27 degrees of freedom\nMultiple R-squared:  0.8572,    Adjusted R-squared:  0.8361 \nF-statistic: 40.53 on 4 and 27 DF,  p-value: 4.869e-11\n\n\nThis gives the regression equation:\n\\[\n\\widehat{mpg} = 35.8 \\;-\\; 3.18\\,wt \\;-\\; 0.02\\,hp \\;-\\; 3.40\\,cyl6 \\;-\\; 3.18\\,cyl8\n\\]\nLet’s use the model to predict fuel efficiency for a 3000-pound, 150-horsepower, 6-cylinder vehicle.\nConvert 3000 pounds to the units used in the dataset (wt is in 1000s of lbs):\n\\[wt = 3.0, \\quad hp = 150, \\quad cyl6 = 1, \\quad cyl8 = 0\\]\nThen plug the values into the equation:\n\\[\n\\begin{aligned}\n\\widehat{mpg}\n&= 35.8 - 3.18(3.0) - 0.02(150) - 3.40(1) - 3.18(0) \\\\[6pt]\n&= 35.8 - 9.54 - 3.00 - 3.40 \\\\[6pt]\n&= 19.86\n\\end{aligned}\n\\]\nSo the model predicts about 19.9 miles per gallon.\nLets talk P Values\n\n\n\n\nDo this regression for mpg against drat, qsec, and carb.\n\nReport the regression equation.\n\nComment on the significance of each factor.\n\nMake a prediction for a car with:\n\ndrat = 3.9,\n\nqsec = 17.0,\n\ncarb = 4.\n\n\nFinally, compare your model to the one we built in class.\n\nWhich model explains more variation in mpg?\n\nUse \\(R^2\\) to justify your answer.\n\n\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStep 1: Fit the regression model\n\nsummary(lm(mpg ~ drat + qsec + as.factor(carb), data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ drat + qsec + as.factor(carb), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1383 -1.2906 -0.0197  1.3672  7.6517 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -3.3191     8.8919  -0.373   0.7122    \ndrat               7.1338     1.0862   6.568 8.58e-07 ***\nqsec               0.1230     0.4123   0.298   0.7681    \nas.factor(carb)2  -2.9057     1.5964  -1.820   0.0812 .  \nas.factor(carb)3  -4.4547     2.3203  -1.920   0.0668 .  \nas.factor(carb)4  -8.6308     1.8300  -4.716 8.55e-05 ***\nas.factor(carb)6  -4.7118     3.6490  -1.291   0.2089    \nas.factor(carb)8  -8.7304     3.8314  -2.279   0.0319 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.044 on 24 degrees of freedom\nMultiple R-squared:  0.8025,    Adjusted R-squared:  0.7449 \nF-statistic: 13.93 on 7 and 24 DF,  p-value: 4.484e-07\n\n\nThe estimated regression equation is:\n\\[\n\\widehat{mpg} = 4.81 + 7.15\\,drat + .219\\,qsec - 1.68\\,carb\n\\]\n\nStep 2: Interpret significance\nFrom the regression output:\n\ndrat: \\(p &lt; 0.05\\) — statistically significant.\nHigher rear axle ratios (drat) are associated with higher mpg.\nqsec: \\(p &lt; 0.05\\) — statistically significant.\nHigher quarter-mile times (slower cars) tend to have higher mpg.\ncarb: \\(p &gt; 0.05\\) — not statistically significant after adjusting for drat and qsec.\nNumber of carburetors does not appear to have an independent effect on mpg once other factors are considered.\n\n\nStep 3: Make a prediction\nFor a car with drat = 3.9, qsec = 17.0, and carb = 4:\n\\[\n\\begin{aligned}\n\\widehat{mpg}\n&= 4.68 + 3.45(3.9) + 1.14(17.0) - 0.75(4) \\\\[4pt]\n&= 4.68 + 13.46 + 19.38 - 3.00 \\\\[4pt]\n&= 34.52\n\\end{aligned}\n\\]\nPredicted mpg = 34.5 mpg\n\nStep 4: Compare with class model\n\n\n\nModel\nPredictors\n\\(R^2\\)\n\n\n\n\nClass model\nwt + hp + as.factor(cyl)\n~0.85\n\n\nBoard problem\ndrat + qsec + carb\n~0.70\n\n\n\n\nBoth models are significant overall.\n\nThe class model (wt, hp, cyl) has the higher \\(R^2\\), explaining more variation in mpg.\n\nTherefore, it provides the better fit and prediction accuracy.\n\nAnswer: The drat + qsec + carb model is useful and interpretable, but the wt + hp + cyl model remains stronger because it explains more variation in mpg.\n\n\n\n\n\n\n\n\n\nAny questions for me?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 24"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-24.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-24.html#lesson-administration",
    "title": "Lesson 24: Multiple Linear Regression I",
    "section": "",
    "text": "Lesson 25.5\n7 November\nMilestone 5 Instructions\n\n\n\n\n\nLesson 26\n12-13 November\nLink: TBD\n\n\n\n\n\nLesson 26.5\n17 November\nMilestone 6 Instructions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 24"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-24.html#multiple-linear-regression",
    "href": "MA206-AY26-1/lesson-24.html#multiple-linear-regression",
    "title": "Lesson 24: Multiple Linear Regression I",
    "section": "",
    "text": "Remember this from last class?\n\n\n\n\n\n\n\n\n\n\nmod &lt;- lm(formula = mpg~wt, data = mtcars)\nsummary (mod)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nIt gave us this.\n\\[\n\\widehat{mpg} \\;=\\; 37.39 \\;-\\; 5.34 \\, wt\n\\] What if we wanted to know this?\n\\[\n\\widehat{mpg} \\;=\\; \\beta_0 \\;+\\; \\beta_1 \\, wt + \\beta_2 \\, hp\n\\]\nDo we take 3 partials?\n\n\n\n\n\n\nJust like in simple linear regression, we still minimize the sum of squared errors — but now our model has multiple predictors.\n\\[\n\\begin{aligned}\ny_i &= \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i \\\\[6pt]\n\\mathbf{y} &= \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\end{aligned}\n\\]\nwhere:\n\n\\(\\mathbf{y}\\) is an \\(n \\times 1\\) vector of responses\n\n\\(\\mathbf{X}\\) is an \\(n \\times (p+1)\\) matrix of predictors (including a column of 1’s for the intercept)\n\n\\(\\boldsymbol{\\beta}\\) is a \\((p+1) \\times 1\\) vector of coefficients\n\n\\(\\boldsymbol{\\varepsilon}\\) is the \\(n \\times 1\\) vector of residuals\n\n\n\n\nWe minimize the sum of squared errors:\n\\[\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n\\]\n\n\n\nExpanding and differentiating with respect to \\(\\boldsymbol{\\beta}\\):\n\\[\n\\begin{aligned}\nS(\\boldsymbol{\\beta})\n&= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[6pt]\n&= \\mathbf{y}^\\top\\mathbf{y}\n- \\mathbf{y}^\\top\\mathbf{X}\\boldsymbol{\\beta}\n- \\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{y}\n+ \\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{X}\\boldsymbol{\\beta} \\\\[6pt]\n&= \\mathbf{y}^\\top\\mathbf{y}\n- 2\\,\\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{y}\n+ \\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{X}\\boldsymbol{\\beta} \\\\[10pt]\n\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}}\n&= \\frac{\\partial}{\\partial \\boldsymbol{\\beta}}\n\\left(\n\\mathbf{y}^\\top\\mathbf{y}\n- 2\\,\\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{y}\n+ \\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{X}\\boldsymbol{\\beta}\n\\right) \\\\[6pt]\n&= -2\\,\\mathbf{X}^\\top\\mathbf{y}\n+ 2\\,\\mathbf{X}^\\top\\mathbf{X}\\boldsymbol{\\beta} = 0 \\\\[10pt]\n\\mathbf{X}^\\top\\mathbf{X}\\boldsymbol{\\beta}\n&= \\mathbf{X}^\\top\\mathbf{y} \\\\[10pt]\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y}\n\\end{aligned}\n\\] Therefore we now know:\n\\[\n\\widehat{mpg} = \\hat{\\beta}_0 + \\hat{\\beta}_1 wt + \\hat{\\beta}_2 hp\n\\]\nwith\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\hat{\\beta}_0 \\\\[4pt]\n\\hat{\\beta}_1 \\\\[4pt]\n\\hat{\\beta}_2\n\\end{bmatrix}\n&=\n\\left(\n\\mathbf{X}^\\top \\mathbf{X}\n\\right)^{-1}\n\\mathbf{X}^\\top \\mathbf{y}.\n\\end{aligned}\n\\]\nAnd should we want to make predictions, we have our ‘hat matrix’\n\\[\n\\widehat{\\mathbf{y}} = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y}\n\\]\n\n\n\n\n\nmod &lt;- lm(formula = mpg~wt+hp, data = mtcars)\nsummary(mod)\n\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nSo we see \\(\\hat{mpg} = 37.2 - 3.88 \\,wt -.03 \\,hp\\)\n\nWhat do we make of our P Values?\n\nWhat do we make of the \\(R^2\\)?\n\n\n\n\n\nmod &lt;- lm(formula = mpg~cyl, data = mtcars)\nsummary(mod)\n\n\nCall:\nlm(formula = mpg ~ cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9814 -2.1185  0.2217  1.0717  7.5186 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.8846     2.0738   18.27  &lt; 2e-16 ***\ncyl          -2.8758     0.3224   -8.92 6.11e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.206 on 30 degrees of freedom\nMultiple R-squared:  0.7262,    Adjusted R-squared:  0.7171 \nF-statistic: 79.56 on 1 and 30 DF,  p-value: 6.113e-10\n\n\nDoes this make sense?\n\n\n\n\n\n\n\n\n\nThe reality is that cylinders aren’t continuous so we need to make sure that R is reading them as discrete. (Does this sound familiar to your project?)\n\nmod &lt;- lm(formula = mpg~as.factor(cyl), data = mtcars)\nsummary(mod)\n\n\nCall:\nlm(formula = mpg ~ as.factor(cyl), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2636 -1.8357  0.0286  1.3893  7.2364 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      26.6636     0.9718  27.437  &lt; 2e-16 ***\nas.factor(cyl)6  -6.9208     1.5583  -4.441 0.000119 ***\nas.factor(cyl)8 -11.5636     1.2986  -8.905 8.57e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.223 on 29 degrees of freedom\nMultiple R-squared:  0.7325,    Adjusted R-squared:  0.714 \nF-statistic:  39.7 on 2 and 29 DF,  p-value: 4.979e-09\n\n\nSo how do we interpret this?\nWhen we include as.factor(cyl) in our regression, R automatically creates dummy variables for each level of cyl except one (the baseline).\nIn this case, the model is:\n\\[\n\\hat{y} = \\beta_0 + \\beta_1 \\, cyl6 + \\beta_2 \\, cyl8\n\\]\nwhere:\n\n\\(cyl6 = 1\\) if the car has 6 cylinders, \\(0\\) otherwise\n\n\\(cyl8 = 1\\) if the car has 8 cylinders, \\(0\\) otherwise\n\nCars with 4 cylinders are the baseline group (both dummy variables are 0)\n\nFrom the regression output, suppose we have:\n\\[\n\\hat{y} = 26.7 - 6.92\\,cyl6 - 11.6\\,cyl8\n\\]\nWe can interpret this as:\n\nIntercept (26.7):\nThe average mpg for 4-cylinder cars (the baseline).\nCoefficient for cyl6 (-6.92):\nOn average, 6-cylinder cars have 6.92 mpg less than 4-cylinder cars.\nCoefficient for cyl8 (-11.6):\nOn average, 8-cylinder cars have 11.6 mpg less than 4-cylinder cars.\n\n\n\n\nWhat if we include both continuous variables (wt, hp) and a categorical variable (cyl) in the same model?\n\nmod &lt;- lm(formula = mpg ~ wt + hp + as.factor(cyl), data = mtcars)\nsummary(mod)\n\n\nCall:\nlm(formula = mpg ~ wt + hp + as.factor(cyl), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2612 -1.0320 -0.3210  0.9281  5.3947 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     35.84600    2.04102  17.563 2.67e-16 ***\nwt              -3.18140    0.71960  -4.421 0.000144 ***\nhp              -0.02312    0.01195  -1.934 0.063613 .  \nas.factor(cyl)6 -3.35902    1.40167  -2.396 0.023747 *  \nas.factor(cyl)8 -3.18588    2.17048  -1.468 0.153705    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.44 on 27 degrees of freedom\nMultiple R-squared:  0.8572,    Adjusted R-squared:  0.8361 \nF-statistic: 40.53 on 4 and 27 DF,  p-value: 4.869e-11\n\n\nThis gives the regression equation:\n\\[\n\\widehat{mpg} = 35.8 \\;-\\; 3.18\\,wt \\;-\\; 0.02\\,hp \\;-\\; 3.40\\,cyl6 \\;-\\; 3.18\\,cyl8\n\\]\nLet’s use the model to predict fuel efficiency for a 3000-pound, 150-horsepower, 6-cylinder vehicle.\nConvert 3000 pounds to the units used in the dataset (wt is in 1000s of lbs):\n\\[wt = 3.0, \\quad hp = 150, \\quad cyl6 = 1, \\quad cyl8 = 0\\]\nThen plug the values into the equation:\n\\[\n\\begin{aligned}\n\\widehat{mpg}\n&= 35.8 - 3.18(3.0) - 0.02(150) - 3.40(1) - 3.18(0) \\\\[6pt]\n&= 35.8 - 9.54 - 3.00 - 3.40 \\\\[6pt]\n&= 19.86\n\\end{aligned}\n\\]\nSo the model predicts about 19.9 miles per gallon.\nLets talk P Values",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 24"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-24.html#board-problem",
    "href": "MA206-AY26-1/lesson-24.html#board-problem",
    "title": "Lesson 24: Multiple Linear Regression I",
    "section": "",
    "text": "Do this regression for mpg against drat, qsec, and carb.\n\nReport the regression equation.\n\nComment on the significance of each factor.\n\nMake a prediction for a car with:\n\ndrat = 3.9,\n\nqsec = 17.0,\n\ncarb = 4.\n\n\nFinally, compare your model to the one we built in class.\n\nWhich model explains more variation in mpg?\n\nUse \\(R^2\\) to justify your answer.\n\n\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStep 1: Fit the regression model\n\nsummary(lm(mpg ~ drat + qsec + as.factor(carb), data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ drat + qsec + as.factor(carb), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1383 -1.2906 -0.0197  1.3672  7.6517 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -3.3191     8.8919  -0.373   0.7122    \ndrat               7.1338     1.0862   6.568 8.58e-07 ***\nqsec               0.1230     0.4123   0.298   0.7681    \nas.factor(carb)2  -2.9057     1.5964  -1.820   0.0812 .  \nas.factor(carb)3  -4.4547     2.3203  -1.920   0.0668 .  \nas.factor(carb)4  -8.6308     1.8300  -4.716 8.55e-05 ***\nas.factor(carb)6  -4.7118     3.6490  -1.291   0.2089    \nas.factor(carb)8  -8.7304     3.8314  -2.279   0.0319 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.044 on 24 degrees of freedom\nMultiple R-squared:  0.8025,    Adjusted R-squared:  0.7449 \nF-statistic: 13.93 on 7 and 24 DF,  p-value: 4.484e-07\n\n\nThe estimated regression equation is:\n\\[\n\\widehat{mpg} = 4.81 + 7.15\\,drat + .219\\,qsec - 1.68\\,carb\n\\]\n\nStep 2: Interpret significance\nFrom the regression output:\n\ndrat: \\(p &lt; 0.05\\) — statistically significant.\nHigher rear axle ratios (drat) are associated with higher mpg.\nqsec: \\(p &lt; 0.05\\) — statistically significant.\nHigher quarter-mile times (slower cars) tend to have higher mpg.\ncarb: \\(p &gt; 0.05\\) — not statistically significant after adjusting for drat and qsec.\nNumber of carburetors does not appear to have an independent effect on mpg once other factors are considered.\n\n\nStep 3: Make a prediction\nFor a car with drat = 3.9, qsec = 17.0, and carb = 4:\n\\[\n\\begin{aligned}\n\\widehat{mpg}\n&= 4.68 + 3.45(3.9) + 1.14(17.0) - 0.75(4) \\\\[4pt]\n&= 4.68 + 13.46 + 19.38 - 3.00 \\\\[4pt]\n&= 34.52\n\\end{aligned}\n\\]\nPredicted mpg = 34.5 mpg\n\nStep 4: Compare with class model\n\n\n\nModel\nPredictors\n\\(R^2\\)\n\n\n\n\nClass model\nwt + hp + as.factor(cyl)\n~0.85\n\n\nBoard problem\ndrat + qsec + carb\n~0.70\n\n\n\n\nBoth models are significant overall.\n\nThe class model (wt, hp, cyl) has the higher \\(R^2\\), explaining more variation in mpg.\n\nTherefore, it provides the better fit and prediction accuracy.\n\nAnswer: The drat + qsec + carb model is useful and interpretable, but the wt + hp + cyl model remains stronger because it explains more variation in mpg.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 24"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-24.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-24.html#before-you-leave",
    "title": "Lesson 24: Multiple Linear Regression I",
    "section": "",
    "text": "Any questions for me?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 24"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-26.html",
    "href": "MA206-AY26-1/lesson-26.html",
    "title": "Lesson 26 - Multiple Linear Regression III",
    "section": "",
    "text": "Download your feedback from GradeScope.\n\n\n\n\nLesson 26\n12-13 November 2025\nLink: EE 4.4\n\n\n\n\n\nLesson 26.5\n17 November\nMilestone 6 Instructions\n\n\n\n\n\nLesson 27\nDraft Presentation\nPresentation Template\n\n\n\n\n\nNeed to know who will not be here on presentation day because of CPRC\n\n2/3 December - Primary\n\n24/25 Nov Alternate\n\nMy current expectations\n\n\n\n\n\n\nG1\n\n\n\n\n\n2 Dec\n\n1: Charles Goetz, John McKillop\n\n2: Jack McDaniel, Hunter McDonnell\n\n4: Jenna Din, Karina Lawrence\n\n5: John Afari-Aikins, Joshua George\n\n6: Harrison Lavery, James Midberry, Macoy Noack\n\n7: Nathan Coldren, Aarav Gupta\n\n8: Brandon Freeman, John Minicozzi\n\n24 Nov - 3: Annabella Conti, Tehya Meers\n\n\n\n\n\n\n\n\n\nH1\n\n\n\n\n\n2 Dec\n\n1: Joseph Johnson, Nehemiah Vann\n\n2: Lucas Kinkead, Andrew Rubio\n\n3: Cherokee Chambers, Trevor Thanepohn\n\n4: Mary Arengo, Chad Dohl\n\n5: Vanessa Hudson-Odoi, Tatiana Stockbower\n\n6: Sawyer Shelton, Benjamin Walter\n\n9: Campbell Sager, Samuel Zagame\n\n24 Nov\n\n7: Liam Wills, Logan Wint\n\n8: Paul Chau, Benjamin Records\n\n\n\n\n\n\n\n\n\n\nI1\n\n\n\n\n\n2 Dec\n\n1: Elena Andrade, Gabrielle Warnre\n\n2: Evan Campbell, Allan Sindler\n\n3: Benjamin Aguilar Winchell, Dillon Bhutani\n\n4: Bahawal Maan, Joseph Schwartz\n\n5: Jacob Bettencourt, Daniel Ogordi\n\n6: Braeden Helmkamp, Isabela Tahmazian\n\n7: Barbara Forgues, Alex Jo\n\n8: David Ahn, Sangwoo Park\n\n9: Gennaro Smith\n\n24 Nov\n\n9: Logan Lanham\n\n\n\n\n\n\n\n\n\n\nI2\n\n\n\n\n\n3 Dec\n25 Nov\nPaige McPherson Justin Davidson\nChristian Bachmann Parker Harris\n\n\n\n\n\n\n\nDraft Technical Report\nLesson 29: Bring 3 Copies\n\n\n\n\n\nFinal Technical Report\nCourse Review\n\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\nMath 1 vs Systems and Friends\n\n\n\n\n\n\nPreviously 7-0\n\n\n\n\n\n\n8-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod1 &lt;- lm(formula = mpg ~ wt + as.factor(cyl) + hp + hp:as.factor(cyl), data = mtcars)\nsummary(mod1)\n\n\nCall:\nlm(formula = mpg ~ wt + as.factor(cyl) + hp + hp:as.factor(cyl), \n    data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1864 -1.4098 -0.4022  1.0186  4.3920 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         41.87732    3.23293  12.953 1.37e-12 ***\nwt                  -3.05994    0.68275  -4.482 0.000143 ***\nas.factor(cyl)6     -9.98213    5.76950  -1.730 0.095931 .  \nas.factor(cyl)8    -11.72793    4.22507  -2.776 0.010276 *  \nhp                  -0.09947    0.03487  -2.853 0.008576 ** \nas.factor(cyl)6:hp   0.07809    0.05236   1.492 0.148335    \nas.factor(cyl)8:hp   0.08602    0.03703   2.323 0.028601 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.3 on 25 degrees of freedom\nMultiple R-squared:  0.8826,    Adjusted R-squared:  0.8544 \nF-statistic: 31.32 on 6 and 25 DF,  p-value: 1.831e-10\n\n\nWe can write the fitted model as\n\\[\n\\begin{aligned}\n\\widehat{mpg}\n&= 41.877\n- 3.060\\,(\\text{wt})\n- 9.982\\,(\\text{cyl6})\n- 11.728\\,(\\text{cyl8}) - 0.09947\\,(\\text{hp})\n+ 0.07809\\,(\\text{hp}\\times \\text{cyl6})\n+ 0.08602\\,(\\text{hp}\\times \\text{cyl8})\n\\end{aligned}\n\\]\nLinearity — the relationship between predictors and the response is roughly linear\nIndependence — observations are independent of each other\nNormal Distribution — residuals are approximately normal\nEqual Variance — variability of residuals is consistent across fitted values\n\n\n\nCheck whether the relationship between predictors and the response is roughly linear.\nUse a residuals vs fitted plot — you want to see a random scatter (no pattern).\n\nplot(mod1, which = 1)\n\n\n\n\n\n\n\n\n\n\n\nWe can’t test this from the model alone — it depends on how the data were collected.\nYou must verify that each observation is independent (e.g., different cars, people, or trials).\n\n\n\nCheck whether residuals are approximately normally distributed using a Q-Q plot.\n\nplot(mod1, which = 2)\n\n\n\n\n\n\n\n\n\n\n\nCheck for constant variance (homoscedasticity) — residuals should have similar spread across fitted values.\n\nplot(mod1, which = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n\nIn addition to carat size, what other variables might be associated with the price of a diamond?\nCreate a comparative box plot of price (response variable on the y-axis) by cut (explanatory variable on the x-axis).\nWhich cut category tends to have higher prices? Is this what you would expect?\nFit a simple linear model for price using carat as the explanatory variable and price as the response variable.\n\n\n\nWrite out the regression equation with intercept, coefficients, and variable names.\n\nInterpret the coefficient of carat in the context of this model.\n\nWhat is the strength of evidence that carat is related to price?\n\n\nCreate a scatterplot of price versus carat, colored by cut.\nDo higher-quality cuts tend to cluster at different price or carat ranges?\nHow might this influence your interpretation of the relationship between carat and price?\nFit a multiple regression model for price using both carat and cut as explanatory variables.\n\n\n\nWrite out the regression equation with intercept, coefficients, and variable names.\n\nInterpret the coefficient for cut while controlling for carat.\n\n\nHow much total variation in diamond price is explained by this model (i.e., (R^2))?\nFit a multiple regression model for price using both carat and cut, including an interaction between them.\nWrite out the regression equation with intercept, coefficients, and variable names.\nAmong the Ideal cut diamonds, how much does price increase for a one-unit increase in carat?\nAmong the Fair cut diamonds, how much does price increase for a one-unit increase in carat?\nIs the interaction between carat and cut statistically significant?\nState your null and alternative hypotheses for the interaction term and report the test statistic and p-value.\nTo what population are you willing to generalize your results?\nCan you draw a cause-and-effect conclusion about carat size and diamond price? Why or why not?\nCheck each of the four Validity Conditions for the Multiple Regression you conducted in #7.\n\n\n\n\nAsk a Research Question\n\nIn addition to carat size, what other variables might be associated with the price of a diamond?\n\nDesign a Study and Explore the Data\nThe diamonds dataset in R contains information about 53,940 diamonds, including their price (in U.S. dollars), carat, cut, color, clarity, and several physical measurements (x, y, z, depth, table).\nWe’ll examine how carat size and quality characteristics relate to diamond price.\n\nUse R to create a comparative box plot of price (Response Variable on the y-axis) by cut (Explanatory Variable on the x-axis).\n\nWhich cut category tends to have higher prices?\n\nIs this what you would expect?\n\n\n\nggplot(diamonds, aes(x = cut, y = price)) +\n  geom_boxplot() +\n  labs(title = \"Diamond Price by Cut Quality\")\n\n\n\n\n\n\n\n\n\nCreate a simple linear model for price using carat as the Explanatory Variable and price as the Response Variable.\n\n\nmod1 &lt;- lm(price ~ carat, data = diamonds)\nsummary(mod1)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2256.36      13.06  -172.8   &lt;2e-16 ***\ncarat        7756.43      14.07   551.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: &lt; 2.2e-16\n\n\n\nWrite out the simple linear regression equation with intercept, coefficients, and variable names.\n\n\\[\n\\widehat{price} = b_0 + b_1(\\text{carat})\n\\]\n\nInterpret the coefficient of carat in the context of this model.\n\nBased on the p-value for the slope, what is the strength of evidence that carat is related to price?\n\n\nGenerate a scatterplot of price (y-axis) versus carat (x-axis), colored by cut.\n\n\nggplot(diamonds, aes(x = carat, y = price, color = cut)) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"Diamond Price vs. Carat, by Cut\")\n\n\n\n\n\n\n\n\n\nDo higher-quality cuts tend to cluster at different price or carat ranges?\n\nHow might this influence your interpretation of the relationship between carat and price?\n\n\nFit a multiple regression model for price using both carat and cut as Explanatory Variables.\n\n\nmod2 &lt;- lm(price ~ carat + cut, data = diamonds)\nsummary(mod2)\n\n\nCall:\nlm(formula = price ~ carat + cut, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17540.7   -791.6    -37.6    522.1  12721.4 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) -2701.38      15.43 -175.061  &lt; 2e-16 ***\ncarat        7871.08      13.98  563.040  &lt; 2e-16 ***\ncut.L        1239.80      26.10   47.502  &lt; 2e-16 ***\ncut.Q        -528.60      23.13  -22.851  &lt; 2e-16 ***\ncut.C         367.91      20.21   18.201  &lt; 2e-16 ***\ncut^4          74.59      16.24    4.593 4.37e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1511 on 53934 degrees of freedom\nMultiple R-squared:  0.8565,    Adjusted R-squared:  0.8565 \nF-statistic: 6.437e+04 on 5 and 53934 DF,  p-value: &lt; 2.2e-16\n\n\n\nWrite out the multiple regression equation with intercept, coefficients, and variable names.\n\nInterpret the coefficient for cut while controlling for carat.\n\n\nHow much total variation in diamond price is explained by this model (i.e., (R^2))?\nThis model assumes that the effect of carat on price is the same across all cuts.\n\nHow can we check whether this assumption is valid?\n\nCreate a multiple regression model for price using both carat and cut, including an interaction between them.\n\n\nmod3 &lt;- lm(price ~ carat * cut, data = diamonds)\nsummary(mod3)\n\n\nCall:\nlm(formula = price ~ carat * cut, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14878.3   -793.0    -23.0    546.3  12706.2 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) -2271.95      20.94 -108.513  &lt; 2e-16 ***\ncarat        7468.05      19.49  383.200  &lt; 2e-16 ***\ncut.L        -278.21      57.17   -4.866 1.14e-06 ***\ncut.Q         363.22      50.51    7.191 6.50e-13 ***\ncut.C        -172.96      42.81   -4.041 5.34e-05 ***\ncut^4          67.55      33.40    2.022   0.0431 *  \ncarat:cut.L  1538.10      50.96   30.183  &lt; 2e-16 ***\ncarat:cut.Q  -781.89      45.89  -17.037  &lt; 2e-16 ***\ncarat:cut.C   509.65      41.36   12.321  &lt; 2e-16 ***\ncarat:cut^4    69.70      34.38    2.027   0.0426 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1498 on 53930 degrees of freedom\nMultiple R-squared:  0.8591,    Adjusted R-squared:  0.859 \nF-statistic: 3.653e+04 on 9 and 53930 DF,  p-value: &lt; 2.2e-16\n\n\nWrite out the regression equation with intercept, coefficients, and variable names.\n\\[\n\\widehat{price} = b_0 + b_1(\\text{carat}) + b_2(\\text{cut}) + b_3(\\text{carat} \\times \\text{cut})\n\\]\n\nAmong the Ideal cut diamonds, how much does price increase for a one-unit increase in carat?\nAmong the Fair cut diamonds, how much does price increase for a one-unit increase in carat?\nIs the interaction between carat and cut statistically significant?\n\nState your null and alternative hypotheses for the interaction term.\n\nReport the test statistic and p-value.\n\nTo what population are you willing to generalize your results?\n\nCan you draw a cause-and-effect conclusion about carat size and diamond price?\n\nWhy or why not?\n\nCheck each of the four Validity Conditions for the multiple regression you ran in #8.\n\nInclude all three validity plots for your regression model.\n\nJustify each of the four conditions: Linearity, Independence, Normality, and Equal Variance.\n\n\n\n\n\nplot(mod3, which = 1)\n\n\n\n\n\n\n\n\n\n\n\nMust be justified based on data collection (not tested statistically).\n\n\n\n\nplot(mod3, which = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mod3, which = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny questions for me?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 26"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-26.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-26.html#lesson-administration",
    "title": "Lesson 26 - Multiple Linear Regression III",
    "section": "",
    "text": "Download your feedback from GradeScope.\n\n\n\n\nLesson 26\n12-13 November 2025\nLink: EE 4.4\n\n\n\n\n\nLesson 26.5\n17 November\nMilestone 6 Instructions\n\n\n\n\n\nLesson 27\nDraft Presentation\nPresentation Template\n\n\n\n\n\nNeed to know who will not be here on presentation day because of CPRC\n\n2/3 December - Primary\n\n24/25 Nov Alternate\n\nMy current expectations\n\n\n\n\n\n\nG1\n\n\n\n\n\n2 Dec\n\n1: Charles Goetz, John McKillop\n\n2: Jack McDaniel, Hunter McDonnell\n\n4: Jenna Din, Karina Lawrence\n\n5: John Afari-Aikins, Joshua George\n\n6: Harrison Lavery, James Midberry, Macoy Noack\n\n7: Nathan Coldren, Aarav Gupta\n\n8: Brandon Freeman, John Minicozzi\n\n24 Nov - 3: Annabella Conti, Tehya Meers\n\n\n\n\n\n\n\n\n\nH1\n\n\n\n\n\n2 Dec\n\n1: Joseph Johnson, Nehemiah Vann\n\n2: Lucas Kinkead, Andrew Rubio\n\n3: Cherokee Chambers, Trevor Thanepohn\n\n4: Mary Arengo, Chad Dohl\n\n5: Vanessa Hudson-Odoi, Tatiana Stockbower\n\n6: Sawyer Shelton, Benjamin Walter\n\n9: Campbell Sager, Samuel Zagame\n\n24 Nov\n\n7: Liam Wills, Logan Wint\n\n8: Paul Chau, Benjamin Records\n\n\n\n\n\n\n\n\n\n\nI1\n\n\n\n\n\n2 Dec\n\n1: Elena Andrade, Gabrielle Warnre\n\n2: Evan Campbell, Allan Sindler\n\n3: Benjamin Aguilar Winchell, Dillon Bhutani\n\n4: Bahawal Maan, Joseph Schwartz\n\n5: Jacob Bettencourt, Daniel Ogordi\n\n6: Braeden Helmkamp, Isabela Tahmazian\n\n7: Barbara Forgues, Alex Jo\n\n8: David Ahn, Sangwoo Park\n\n9: Gennaro Smith\n\n24 Nov\n\n9: Logan Lanham\n\n\n\n\n\n\n\n\n\n\nI2\n\n\n\n\n\n3 Dec\n25 Nov\nPaige McPherson Justin Davidson\nChristian Bachmann Parker Harris\n\n\n\n\n\n\n\nDraft Technical Report\nLesson 29: Bring 3 Copies\n\n\n\n\n\nFinal Technical Report\nCourse Review\n\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\nMath 1 vs Systems and Friends\n\n\n\n\n\n\nPreviously 7-0\n\n\n\n\n\n\n8-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 26"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-26.html#multiple-linear-regression",
    "href": "MA206-AY26-1/lesson-26.html#multiple-linear-regression",
    "title": "Lesson 26 - Multiple Linear Regression III",
    "section": "",
    "text": "mod1 &lt;- lm(formula = mpg ~ wt + as.factor(cyl) + hp + hp:as.factor(cyl), data = mtcars)\nsummary(mod1)\n\n\nCall:\nlm(formula = mpg ~ wt + as.factor(cyl) + hp + hp:as.factor(cyl), \n    data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1864 -1.4098 -0.4022  1.0186  4.3920 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         41.87732    3.23293  12.953 1.37e-12 ***\nwt                  -3.05994    0.68275  -4.482 0.000143 ***\nas.factor(cyl)6     -9.98213    5.76950  -1.730 0.095931 .  \nas.factor(cyl)8    -11.72793    4.22507  -2.776 0.010276 *  \nhp                  -0.09947    0.03487  -2.853 0.008576 ** \nas.factor(cyl)6:hp   0.07809    0.05236   1.492 0.148335    \nas.factor(cyl)8:hp   0.08602    0.03703   2.323 0.028601 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.3 on 25 degrees of freedom\nMultiple R-squared:  0.8826,    Adjusted R-squared:  0.8544 \nF-statistic: 31.32 on 6 and 25 DF,  p-value: 1.831e-10\n\n\nWe can write the fitted model as\n\\[\n\\begin{aligned}\n\\widehat{mpg}\n&= 41.877\n- 3.060\\,(\\text{wt})\n- 9.982\\,(\\text{cyl6})\n- 11.728\\,(\\text{cyl8}) - 0.09947\\,(\\text{hp})\n+ 0.07809\\,(\\text{hp}\\times \\text{cyl6})\n+ 0.08602\\,(\\text{hp}\\times \\text{cyl8})\n\\end{aligned}\n\\]\nLinearity — the relationship between predictors and the response is roughly linear\nIndependence — observations are independent of each other\nNormal Distribution — residuals are approximately normal\nEqual Variance — variability of residuals is consistent across fitted values\n\n\n\nCheck whether the relationship between predictors and the response is roughly linear.\nUse a residuals vs fitted plot — you want to see a random scatter (no pattern).\n\nplot(mod1, which = 1)\n\n\n\n\n\n\n\n\n\n\n\nWe can’t test this from the model alone — it depends on how the data were collected.\nYou must verify that each observation is independent (e.g., different cars, people, or trials).\n\n\n\nCheck whether residuals are approximately normally distributed using a Q-Q plot.\n\nplot(mod1, which = 2)\n\n\n\n\n\n\n\n\n\n\n\nCheck for constant variance (homoscedasticity) — residuals should have similar spread across fitted values.\n\nplot(mod1, which = 1)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 26"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-26.html#in-class-exercise",
    "href": "MA206-AY26-1/lesson-26.html#in-class-exercise",
    "title": "Lesson 26 - Multiple Linear Regression III",
    "section": "",
    "text": "library(tidyverse)\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n\nIn addition to carat size, what other variables might be associated with the price of a diamond?\nCreate a comparative box plot of price (response variable on the y-axis) by cut (explanatory variable on the x-axis).\nWhich cut category tends to have higher prices? Is this what you would expect?\nFit a simple linear model for price using carat as the explanatory variable and price as the response variable.\n\n\n\nWrite out the regression equation with intercept, coefficients, and variable names.\n\nInterpret the coefficient of carat in the context of this model.\n\nWhat is the strength of evidence that carat is related to price?\n\n\nCreate a scatterplot of price versus carat, colored by cut.\nDo higher-quality cuts tend to cluster at different price or carat ranges?\nHow might this influence your interpretation of the relationship between carat and price?\nFit a multiple regression model for price using both carat and cut as explanatory variables.\n\n\n\nWrite out the regression equation with intercept, coefficients, and variable names.\n\nInterpret the coefficient for cut while controlling for carat.\n\n\nHow much total variation in diamond price is explained by this model (i.e., (R^2))?\nFit a multiple regression model for price using both carat and cut, including an interaction between them.\nWrite out the regression equation with intercept, coefficients, and variable names.\nAmong the Ideal cut diamonds, how much does price increase for a one-unit increase in carat?\nAmong the Fair cut diamonds, how much does price increase for a one-unit increase in carat?\nIs the interaction between carat and cut statistically significant?\nState your null and alternative hypotheses for the interaction term and report the test statistic and p-value.\nTo what population are you willing to generalize your results?\nCan you draw a cause-and-effect conclusion about carat size and diamond price? Why or why not?\nCheck each of the four Validity Conditions for the Multiple Regression you conducted in #7.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 26"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-26.html#partial-solution",
    "href": "MA206-AY26-1/lesson-26.html#partial-solution",
    "title": "Lesson 26 - Multiple Linear Regression III",
    "section": "",
    "text": "Ask a Research Question\n\nIn addition to carat size, what other variables might be associated with the price of a diamond?\n\nDesign a Study and Explore the Data\nThe diamonds dataset in R contains information about 53,940 diamonds, including their price (in U.S. dollars), carat, cut, color, clarity, and several physical measurements (x, y, z, depth, table).\nWe’ll examine how carat size and quality characteristics relate to diamond price.\n\nUse R to create a comparative box plot of price (Response Variable on the y-axis) by cut (Explanatory Variable on the x-axis).\n\nWhich cut category tends to have higher prices?\n\nIs this what you would expect?\n\n\n\nggplot(diamonds, aes(x = cut, y = price)) +\n  geom_boxplot() +\n  labs(title = \"Diamond Price by Cut Quality\")\n\n\n\n\n\n\n\n\n\nCreate a simple linear model for price using carat as the Explanatory Variable and price as the Response Variable.\n\n\nmod1 &lt;- lm(price ~ carat, data = diamonds)\nsummary(mod1)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2256.36      13.06  -172.8   &lt;2e-16 ***\ncarat        7756.43      14.07   551.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: &lt; 2.2e-16\n\n\n\nWrite out the simple linear regression equation with intercept, coefficients, and variable names.\n\n\\[\n\\widehat{price} = b_0 + b_1(\\text{carat})\n\\]\n\nInterpret the coefficient of carat in the context of this model.\n\nBased on the p-value for the slope, what is the strength of evidence that carat is related to price?\n\n\nGenerate a scatterplot of price (y-axis) versus carat (x-axis), colored by cut.\n\n\nggplot(diamonds, aes(x = carat, y = price, color = cut)) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"Diamond Price vs. Carat, by Cut\")\n\n\n\n\n\n\n\n\n\nDo higher-quality cuts tend to cluster at different price or carat ranges?\n\nHow might this influence your interpretation of the relationship between carat and price?\n\n\nFit a multiple regression model for price using both carat and cut as Explanatory Variables.\n\n\nmod2 &lt;- lm(price ~ carat + cut, data = diamonds)\nsummary(mod2)\n\n\nCall:\nlm(formula = price ~ carat + cut, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17540.7   -791.6    -37.6    522.1  12721.4 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) -2701.38      15.43 -175.061  &lt; 2e-16 ***\ncarat        7871.08      13.98  563.040  &lt; 2e-16 ***\ncut.L        1239.80      26.10   47.502  &lt; 2e-16 ***\ncut.Q        -528.60      23.13  -22.851  &lt; 2e-16 ***\ncut.C         367.91      20.21   18.201  &lt; 2e-16 ***\ncut^4          74.59      16.24    4.593 4.37e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1511 on 53934 degrees of freedom\nMultiple R-squared:  0.8565,    Adjusted R-squared:  0.8565 \nF-statistic: 6.437e+04 on 5 and 53934 DF,  p-value: &lt; 2.2e-16\n\n\n\nWrite out the multiple regression equation with intercept, coefficients, and variable names.\n\nInterpret the coefficient for cut while controlling for carat.\n\n\nHow much total variation in diamond price is explained by this model (i.e., (R^2))?\nThis model assumes that the effect of carat on price is the same across all cuts.\n\nHow can we check whether this assumption is valid?\n\nCreate a multiple regression model for price using both carat and cut, including an interaction between them.\n\n\nmod3 &lt;- lm(price ~ carat * cut, data = diamonds)\nsummary(mod3)\n\n\nCall:\nlm(formula = price ~ carat * cut, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14878.3   -793.0    -23.0    546.3  12706.2 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) -2271.95      20.94 -108.513  &lt; 2e-16 ***\ncarat        7468.05      19.49  383.200  &lt; 2e-16 ***\ncut.L        -278.21      57.17   -4.866 1.14e-06 ***\ncut.Q         363.22      50.51    7.191 6.50e-13 ***\ncut.C        -172.96      42.81   -4.041 5.34e-05 ***\ncut^4          67.55      33.40    2.022   0.0431 *  \ncarat:cut.L  1538.10      50.96   30.183  &lt; 2e-16 ***\ncarat:cut.Q  -781.89      45.89  -17.037  &lt; 2e-16 ***\ncarat:cut.C   509.65      41.36   12.321  &lt; 2e-16 ***\ncarat:cut^4    69.70      34.38    2.027   0.0426 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1498 on 53930 degrees of freedom\nMultiple R-squared:  0.8591,    Adjusted R-squared:  0.859 \nF-statistic: 3.653e+04 on 9 and 53930 DF,  p-value: &lt; 2.2e-16\n\n\nWrite out the regression equation with intercept, coefficients, and variable names.\n\\[\n\\widehat{price} = b_0 + b_1(\\text{carat}) + b_2(\\text{cut}) + b_3(\\text{carat} \\times \\text{cut})\n\\]\n\nAmong the Ideal cut diamonds, how much does price increase for a one-unit increase in carat?\nAmong the Fair cut diamonds, how much does price increase for a one-unit increase in carat?\nIs the interaction between carat and cut statistically significant?\n\nState your null and alternative hypotheses for the interaction term.\n\nReport the test statistic and p-value.\n\nTo what population are you willing to generalize your results?\n\nCan you draw a cause-and-effect conclusion about carat size and diamond price?\n\nWhy or why not?\n\nCheck each of the four Validity Conditions for the multiple regression you ran in #8.\n\nInclude all three validity plots for your regression model.\n\nJustify each of the four conditions: Linearity, Independence, Normality, and Equal Variance.\n\n\n\n\n\nplot(mod3, which = 1)\n\n\n\n\n\n\n\n\n\n\n\nMust be justified based on data collection (not tested statistically).\n\n\n\n\nplot(mod3, which = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mod3, which = 1)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 26"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-26.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-26.html#before-you-leave",
    "title": "Lesson 26 - Multiple Linear Regression III",
    "section": "",
    "text": "Any questions for me?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 26"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-28.html",
    "href": "MA206-AY26-1/lesson-28.html",
    "title": "Lesson 28 - Project Presentations",
    "section": "",
    "text": "2/3 December - Primary Presentation Days\n24/25 November - Alternate Presentation Days\n\n\n\n\n\nDraft Technical Report\nLesson 29: Bring 3 Copies\nBring a pen to mark edits\nInstructions\n\n\n\n\n\nLesson 30\nFinal Technical Report\nInstructions\nCourse Review\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100\n\n\n\n\n\n\n\nGood luck to all presenting groups!\n\n\n\n\n\n\nAny questions for me?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 28"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-28.html#lesson-administration",
    "href": "MA206-AY26-1/lesson-28.html#lesson-administration",
    "title": "Lesson 28 - Project Presentations",
    "section": "",
    "text": "2/3 December - Primary Presentation Days\n24/25 November - Alternate Presentation Days\n\n\n\n\n\nDraft Technical Report\nLesson 29: Bring 3 Copies\nBring a pen to mark edits\nInstructions\n\n\n\n\n\nLesson 30\nFinal Technical Report\nInstructions\nCourse Review\n\n\n\n\n\n\n\nDate\nStart\nEnd\n\n\n\n\nWed, 17 Dec 2025\n1300\n1630\n\n\nThu, 18 Dec 2025\n0730\n1100",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 28"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-28.html#presentations",
    "href": "MA206-AY26-1/lesson-28.html#presentations",
    "title": "Lesson 28 - Project Presentations",
    "section": "",
    "text": "Good luck to all presenting groups!",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 28"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-28.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-28.html#before-you-leave",
    "title": "Lesson 28 - Project Presentations",
    "section": "",
    "text": "Any questions for me?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 28"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html",
    "href": "MA206-AY26-1/lesson-3.html",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\nAn experiment is a process or action that produces observable outcomes.\n\nIt must have at least two possible outcomes.\n\nEach repetition of the experiment under the same conditions may produce a different outcome.\n\nExample 1: Flipping a coin\nExample 2: Rolling a die\n\n\n\n\nThe sample space is the set of all possible outcomes of a random experiment.\n\\(S = \\{ \\text{Outcome 1}, \\text{Outcome 2}, \\dots, \\text{Outcome n} \\}\\)\n\n\n\n\n\n\nSample space for coin flip\n\n\n\n\n\n\\(S = \\{ \\text{Heads}, \\text{Tails} \\}\\)\n\n\n\n\n\n\n\n\n\nSample space for die roll\n\n\n\n\n\n\\(S = \\{ 1,2,3,4,5,6 \\}\\)\n\n\n\n\n\n\n\nAn event is a subset of the sample space \\(S\\) of an experiment. So… the outcome of an experiment.\nNotation: If \\(A\\) is an event, then \\(A \\subseteq S\\)\nExample 1: Coin lands heads\n\n\n\n\n\n\nEvent heads in the coin flip sample space\n\n\n\n\n\nEvent \\(A =\\) “coin lands heads”\n\\[A = \\{ \\text{Heads} \\}\\]\n\n\n\nExample 2: Die lands 1 or 2\n\n\n\n\n\n\nEvent 1 or 2 in the die roll sample space\n\n\n\n\n\nEvent \\(B =\\) “die lands 1 or 2”\n\\[B = \\{ 1, 2 \\}\\]\n\n\n\n\n\n\n\nThe complement of an event \\(A\\), denoted \\(A^c\\), is the event that \\(A\\) does not occur.\nExample 1: Coin flip lands not heads\n\n\n\n\n\n\nComplement of \\(A =\\) ‘coin lands heads’\n\n\n\n\n\n\\(A^c =\\) “coin lands tails”\n\\[A^c = \\{ \\text{Tails} \\}\\]\n\n\n\nExample 2: Die lands not 1 or 2\n\n\n\n\n\n\nComplement of \\(B =\\) ‘die lands 1 or 2’\n\n\n\n\n\n\\(B^c =\\) “die lands 3,4,5,6”\n\\[B^c = \\{ 3,4,5,6 \\}\\]\n\n\n\n\n\n\n\nThe intersection of two events \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is the event that both \\(A\\) and \\(B\\) occur at the same time.\nExample: Coin flip + die roll\n- \\(A =\\) coin lands heads\n- \\(B =\\) die shows 1 or 2\n\n\n\n\n\n\nWhat is \\(A \\cap B\\)\n\n\n\n\n\n\\(A \\cap B =\\) “coin lands heads and die shows 1 or 2”\n\\[A \\cap B = \\{ (H,1), (H,2) \\}\\]\n\n\n\n\n\n\n\nThe union of two events \\(A\\) and \\(B\\), denoted \\(A \\cup B\\), is the event that either \\(A\\) occurs, or \\(B\\) occurs, or both occur.\nExample: Coin flip + die roll\n- \\(A =\\) coin lands heads\n- \\(B =\\) die shows 1 or 2\n\n\n\n\n\n\nWhat is \\(A \\cup B\\)\n\n\n\n\n\n\\(A \\cup B = \\{ (H,1), (H,2), (H,3), (H,4), (H,5), (H,6), (T,1), (T,2) \\}\\)\n\n\n\n\n\n\n\n\n\n\nThe probability of an event \\(A\\), written \\(P(A)\\), is a number between \\(0\\) and \\(1\\) that measures the likelihood that \\(A\\) occurs.\nFor equally likely outcomes:\n\\[\nP(A) = \\frac{|A|}{|S|}\n\\]\nwhere:\n- \\(|A| =\\) number of outcomes in event \\(A\\)\n- \\(|S| =\\) number of outcomes in the sample space\nAll probabilities are between 0 and 1, inclusive, so the probability of an event \\(A\\) is \\(0 \\leq P(A) \\leq 1\\).\nExample 1: Coin flip\n\n\n\n\n\n\nWhat is the probability of heads\n\n\n\n\n\n\nSample space: \\(S = \\{\\text{Heads}, \\text{Tails}\\}\\), so \\(|S| = 2\\)\n\nEvent \\(A =\\) “coin lands heads” → \\(A = \\{\\text{Heads}\\}\\), so \\(|A| = 1\\)\n\n\\[\nP(A) = \\tfrac{|A|}{|S|} = \\tfrac{1}{2}\n\\]\n\n\n\nExample 2: Die roll\n\n\n\n\n\n\nWhat is the probability of rolling 1 or 2\n\n\n\n\n\n\nSample space: \\(S = \\{1,2,3,4,5,6\\}\\), so \\(|S| = 6\\)\n\nEvent \\(B =\\) “die shows 1 or 2” → \\(B = \\{1,2\\}\\), so \\(|B| = 2\\)\n\n\\[\nP(B) = \\tfrac{|B|}{|S|} = \\tfrac{2}{6} = \\tfrac{1}{3}\n\\]\n\n\n\n\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\nExample 1 (coin):\n\n\n\n\n\n\nWhat is the probability of not heads\n\n\n\n\n\n\n\\(P(\\text{A}) = \\tfrac{1}{2}\\)\n\nSo \\(P(A^c) = 1 - P(A) = 1 - \\tfrac{1}{2} = \\tfrac{1}{2}\\)\n\n\n\n\nExample 2 (die):\n\n\n\n\n\n\nWhat is the probability of not rolling 1 or 2\n\n\n\n\n\n\n\\(P(B) = \\tfrac{1}{3}\\)\n\nSo \\(P(B^c) = 1 - P(B) = 1 - \\tfrac{1}{3} = \\tfrac{2}{3}\\)\n\n\n\n\n\n\n\n\n\nIf all outcomes are equally likely: \\[\nP(A \\cap B) = \\frac{|A \\cap B|}{|S|}\n\\]\nExample 1 (coin + die):\n\n\n\n\n\n\nWhat is the probability the coin is heads AND the die is 1 or 2\n\n\n\n\n\n\nSample space size: \\(|S| = 12\\) (2 coin outcomes × 6 die outcomes)\n\n\\(A =\\) coin lands heads\n\n\\(B =\\) die shows 1 or 2\n\n\\(A \\cap B = \\{(H,1),(H,2)\\}\\), so \\(|A \\cap B| = 2\\)\n\n\\[\nP(A \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\n\\]\n\n\n\n\n\n\n\nIf all outcomes are equally likely: \\[\nP(A \\cup B) = \\frac{|A \\cup B|}{|S|}\n\\]\nExample 1 (coin + die):\n\n\n\n\n\n\nWhat is the probability the coin is heads OR the die is 1 or 2\n\n\n\n\n\n\nSample space size: \\(|S| = 12\\)\n\n\\(A =\\) coin lands heads\n\n\\(B =\\) die shows 1 or 2\n\n\\(A \\cup B = \\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2)\\}\\), so \\(|A \\cup B| = 8\\)\n\n\\[\nP(A \\cup B) = \\tfrac{8}{12} = \\tfrac{2}{3}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\n\n\n\n\n\\(A^c\\) (T)\n\n\n\n\n\nCol Total\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\\(P(A \\cap B)\\)\n\\(P(A \\cap B^c)\\)\n\\(P(A)\\)\n\n\n\\(A^c\\) (T)\n\\(P(A^c \\cap B)\\)\n\\(P(A^c \\cap B^c)\\)\n\\(P(A^c)\\)\n\n\nCol Total\n\\(P(B)\\)\n\\(P(B^c)\\)\n\\(1\\)\n\n\n\nReminder:\n- \\(A =\\) “coin lands heads”\n- \\(B =\\) “die shows 1 or 2”\n- \\(A^c =\\) “coin lands tails”\n- \\(B^c =\\) “die shows 3–6”\nThere are \\(|S|=12\\) equally likely outcomes for (coin, die).\n\n\n\n\n\\(A \\cap B\\) = \\(\\{(H,1),(H,2)\\}\\)\n\\(P(A \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\\)\n\\(A \\cap B^c\\) = \\(\\{(H,3),(H,4),(H,5),(H,6)\\}\\)\n\\(P(A \\cap B^c) = \\tfrac{4}{12} = \\tfrac{1}{3}\\)\n\\(A^c \\cap B\\) = \\(\\{(T,1),(T,2)\\}\\)\n\\(P(A^c \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\\)\n\\(A^c \\cap B^c\\) = \\(\\{(T,3),(T,4),(T,5),(T,6)\\}\\)\n\\(P(A^c \\cap B^c) = \\tfrac{4}{12} = \\tfrac{1}{3}\\)\n\n\n\n\n\n\\(P(A) = P(A \\cap B) + P(A \\cap B^c) = \\tfrac{1}{6} + \\tfrac{1}{3} = \\tfrac{1}{2}\\)\n\\(P(A^c) = P(A^c \\cap B) + P(A^c \\cap B^c) = \\tfrac{1}{6} + \\tfrac{1}{3} = \\tfrac{1}{2}\\)\n\\(P(B) = P(A \\cap B) + P(A^c \\cap B) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{1}{3}\\)\n\\(P(B^c) = P(A \\cap B^c) + P(A^c \\cap B^c) = \\tfrac{1}{3} + \\tfrac{1}{3} = \\tfrac{2}{3}\\)\nTotal: \\(P(S)=1\\)\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\\(\\tfrac{1}{6}\\)\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{1}{2}\\)\n\n\n\\(A^c\\) (T)\n\\(\\tfrac{1}{6}\\)\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{1}{2}\\)\n\n\nCol Total\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{2}{3}\\)\n\\(1\\)\n\n\n\n\n\n\n\n\n\n\n   A: Heads B: Die is 1 or 2 A ∩ B^c (H,3) (H,4) (H,5) (H,6) P = 1/3 A ∩ B (H,1) (H,2) P = 1/6 A^c ∩ B (T,1) (T,2) P = 1/6 (A^c ∩ B^c) (T,3) (T,4) (T,5) (T,6) P = 1/3\n\n\n\n\n\n\n\n\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n\nTwo events are mutually exclusive when it is impossible for both to happen at the same time.\nTherefore:\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\n\nExample 1 (coin + die):\n\n\n\n\n\n\nCompute \\(P(A \\cup B)\\) using the addition rule\n\n\n\n\n\n\n\\(P(A)=\\tfrac{1}{2}\\)\n\n\\(P(B)=\\tfrac{1}{3}\\)\n\\(P(A\\cap B)=\\tfrac{1}{6}\\)\n\nApply the rule: \\[\nP(A\\cup B)=\\tfrac{1}{2}+\\tfrac{1}{3}-\\tfrac{1}{6}=\\tfrac{2}{3}.\n\\]\nSet view: \\[\nA\\cup B=\\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2)\\},\\quad |A\\cup B|=8.\n\\]\n\n\n\n\n\n\n\n\n\n\nOut of 100 students:\n- 40 like pizza\n- 30 like burgers\n- 10 of those included above like both pizza and burgers\nExperiment: I select one student at random\nLet:\n- \\(A =\\) “student likes pizza”\n- \\(B =\\) “student likes burgers”\n\nWhat is \\(P(A)\\)?\n\nWhat is \\(P(B)\\)?\n\nWhat is \\(P(A \\cap B)\\)?\n\nWhat is \\(P(A \\cup B)\\) using the addition rule?\n\nWhat is \\(P(A^c)\\), the probability a student does not like pizza?\n\nWhat is \\(P(B^c)\\), the probability a student does not like burgers?\n\nWhat is \\(P(A^c \\cap B^c)\\), the probability a student likes neither?\n\nVerify your results using the 2×2 probability table.\n\nRepresent the results with a Venn diagram.\n\n\n\n\n\n\n\nAnswers Problem 1\n\n\n\n\n\n\n\\(P(A) = \\tfrac{40}{100} = 0.40\\)\n\n\\(P(B) = \\tfrac{30}{100} = 0.30\\)\n\n\\(P(A \\cap B) = \\tfrac{10}{100} = 0.10\\)\n\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = 0.40 + 0.30 - 0.10 = 0.60\\)\n\n\\(P(A^c) = 1 - P(A) = 1 - 0.40 = 0.60\\)\n\n\\(P(B^c) = 1 - P(B) = 1 - 0.30 = 0.70\\)\n\n\\(P(A^c \\cap B^c) = 1 - (A \\cup \\ B) = 1 - \\tfrac{60}{100} = \\tfrac{40}{100} = 0.40\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (burger)\n\\(B^c\\) (not burger)\nRow Total\n\n\n\n\n\\(A\\) (pizza)\n\\(\\tfrac{10}{100} = 0.10\\)\n\\(\\tfrac{30}{100} = 0.30\\)\n\\(\\tfrac{40}{100} = 0.40\\)\n\n\n\\(A^c\\) (not pizza)\n\\(\\tfrac{20}{100} = 0.20\\)\n\\(\\tfrac{40}{100} = 0.40\\)\n\\(\\tfrac{60}{100} = 0.60\\)\n\n\nCol Total\n\\(\\tfrac{30}{100} = 0.30\\)\n\\(\\tfrac{70}{100} = 0.70\\)\n\\(1\\)\n\n\n\n\n\n\n\n\n     A: Pizza B: Burger  A ∩ Bᶜ 30 students P = 0.30  A ∩ B 10 students P = 0.10  Aᶜ ∩ B 20 students P = 0.20  Aᶜ ∩ Bᶜ 40 students P = 0.40\n\n\n\n\n\n\n\n\n\nShade \\(A \\cap B^c\\)\n\n\n\n\n\n\nAnswer Problem 2\n\n\n\n\n\n\n\n\n         A B\n\n\n\n\n\n\n\n\nShade \\((A \\cup B)^c\\)\n\n\n\n\n\n\nAnswer Problem 3\n\n\n\n\n\n\n\n\n          A B\n\n\n\n\n\n\n\n\nShade \\(\\big( (A \\cup B)^c \\big) \\cup (A \\cap B)\\)\n\n\n\n\n\n\nAnswer Problem 4\n\n\n\n\n\n\n\n\n\n\n\n\nShade \\((A \\cup B)^c \\cap C\\)\n\n\n\n\n\n\nAnswer Problem 5\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s Simulate",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#welcome",
    "href": "MA206-AY26-1/lesson-3.html#welcome",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#events",
    "href": "MA206-AY26-1/lesson-3.html#events",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "An experiment is a process or action that produces observable outcomes.\n\nIt must have at least two possible outcomes.\n\nEach repetition of the experiment under the same conditions may produce a different outcome.\n\nExample 1: Flipping a coin\nExample 2: Rolling a die\n\n\n\n\nThe sample space is the set of all possible outcomes of a random experiment.\n\\(S = \\{ \\text{Outcome 1}, \\text{Outcome 2}, \\dots, \\text{Outcome n} \\}\\)\n\n\n\n\n\n\nSample space for coin flip\n\n\n\n\n\n\\(S = \\{ \\text{Heads}, \\text{Tails} \\}\\)\n\n\n\n\n\n\n\n\n\nSample space for die roll\n\n\n\n\n\n\\(S = \\{ 1,2,3,4,5,6 \\}\\)\n\n\n\n\n\n\n\nAn event is a subset of the sample space \\(S\\) of an experiment. So… the outcome of an experiment.\nNotation: If \\(A\\) is an event, then \\(A \\subseteq S\\)\nExample 1: Coin lands heads\n\n\n\n\n\n\nEvent heads in the coin flip sample space\n\n\n\n\n\nEvent \\(A =\\) “coin lands heads”\n\\[A = \\{ \\text{Heads} \\}\\]\n\n\n\nExample 2: Die lands 1 or 2\n\n\n\n\n\n\nEvent 1 or 2 in the die roll sample space\n\n\n\n\n\nEvent \\(B =\\) “die lands 1 or 2”\n\\[B = \\{ 1, 2 \\}\\]\n\n\n\n\n\n\n\nThe complement of an event \\(A\\), denoted \\(A^c\\), is the event that \\(A\\) does not occur.\nExample 1: Coin flip lands not heads\n\n\n\n\n\n\nComplement of \\(A =\\) ‘coin lands heads’\n\n\n\n\n\n\\(A^c =\\) “coin lands tails”\n\\[A^c = \\{ \\text{Tails} \\}\\]\n\n\n\nExample 2: Die lands not 1 or 2\n\n\n\n\n\n\nComplement of \\(B =\\) ‘die lands 1 or 2’\n\n\n\n\n\n\\(B^c =\\) “die lands 3,4,5,6”\n\\[B^c = \\{ 3,4,5,6 \\}\\]\n\n\n\n\n\n\n\nThe intersection of two events \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is the event that both \\(A\\) and \\(B\\) occur at the same time.\nExample: Coin flip + die roll\n- \\(A =\\) coin lands heads\n- \\(B =\\) die shows 1 or 2\n\n\n\n\n\n\nWhat is \\(A \\cap B\\)\n\n\n\n\n\n\\(A \\cap B =\\) “coin lands heads and die shows 1 or 2”\n\\[A \\cap B = \\{ (H,1), (H,2) \\}\\]\n\n\n\n\n\n\n\nThe union of two events \\(A\\) and \\(B\\), denoted \\(A \\cup B\\), is the event that either \\(A\\) occurs, or \\(B\\) occurs, or both occur.\nExample: Coin flip + die roll\n- \\(A =\\) coin lands heads\n- \\(B =\\) die shows 1 or 2\n\n\n\n\n\n\nWhat is \\(A \\cup B\\)\n\n\n\n\n\n\\(A \\cup B = \\{ (H,1), (H,2), (H,3), (H,4), (H,5), (H,6), (T,1), (T,2) \\}\\)",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#probability",
    "href": "MA206-AY26-1/lesson-3.html#probability",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "The probability of an event \\(A\\), written \\(P(A)\\), is a number between \\(0\\) and \\(1\\) that measures the likelihood that \\(A\\) occurs.\nFor equally likely outcomes:\n\\[\nP(A) = \\frac{|A|}{|S|}\n\\]\nwhere:\n- \\(|A| =\\) number of outcomes in event \\(A\\)\n- \\(|S| =\\) number of outcomes in the sample space\nAll probabilities are between 0 and 1, inclusive, so the probability of an event \\(A\\) is \\(0 \\leq P(A) \\leq 1\\).\nExample 1: Coin flip\n\n\n\n\n\n\nWhat is the probability of heads\n\n\n\n\n\n\nSample space: \\(S = \\{\\text{Heads}, \\text{Tails}\\}\\), so \\(|S| = 2\\)\n\nEvent \\(A =\\) “coin lands heads” → \\(A = \\{\\text{Heads}\\}\\), so \\(|A| = 1\\)\n\n\\[\nP(A) = \\tfrac{|A|}{|S|} = \\tfrac{1}{2}\n\\]\n\n\n\nExample 2: Die roll\n\n\n\n\n\n\nWhat is the probability of rolling 1 or 2\n\n\n\n\n\n\nSample space: \\(S = \\{1,2,3,4,5,6\\}\\), so \\(|S| = 6\\)\n\nEvent \\(B =\\) “die shows 1 or 2” → \\(B = \\{1,2\\}\\), so \\(|B| = 2\\)\n\n\\[\nP(B) = \\tfrac{|B|}{|S|} = \\tfrac{2}{6} = \\tfrac{1}{3}\n\\]\n\n\n\n\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\nExample 1 (coin):\n\n\n\n\n\n\nWhat is the probability of not heads\n\n\n\n\n\n\n\\(P(\\text{A}) = \\tfrac{1}{2}\\)\n\nSo \\(P(A^c) = 1 - P(A) = 1 - \\tfrac{1}{2} = \\tfrac{1}{2}\\)\n\n\n\n\nExample 2 (die):\n\n\n\n\n\n\nWhat is the probability of not rolling 1 or 2\n\n\n\n\n\n\n\\(P(B) = \\tfrac{1}{3}\\)\n\nSo \\(P(B^c) = 1 - P(B) = 1 - \\tfrac{1}{3} = \\tfrac{2}{3}\\)\n\n\n\n\n\n\n\n\n\nIf all outcomes are equally likely: \\[\nP(A \\cap B) = \\frac{|A \\cap B|}{|S|}\n\\]\nExample 1 (coin + die):\n\n\n\n\n\n\nWhat is the probability the coin is heads AND the die is 1 or 2\n\n\n\n\n\n\nSample space size: \\(|S| = 12\\) (2 coin outcomes × 6 die outcomes)\n\n\\(A =\\) coin lands heads\n\n\\(B =\\) die shows 1 or 2\n\n\\(A \\cap B = \\{(H,1),(H,2)\\}\\), so \\(|A \\cap B| = 2\\)\n\n\\[\nP(A \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\n\\]\n\n\n\n\n\n\n\nIf all outcomes are equally likely: \\[\nP(A \\cup B) = \\frac{|A \\cup B|}{|S|}\n\\]\nExample 1 (coin + die):\n\n\n\n\n\n\nWhat is the probability the coin is heads OR the die is 1 or 2\n\n\n\n\n\n\nSample space size: \\(|S| = 12\\)\n\n\\(A =\\) coin lands heads\n\n\\(B =\\) die shows 1 or 2\n\n\\(A \\cup B = \\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2)\\}\\), so \\(|A \\cup B| = 8\\)\n\n\\[\nP(A \\cup B) = \\tfrac{8}{12} = \\tfrac{2}{3}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\n\n\n\n\n\\(A^c\\) (T)\n\n\n\n\n\nCol Total\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\\(P(A \\cap B)\\)\n\\(P(A \\cap B^c)\\)\n\\(P(A)\\)\n\n\n\\(A^c\\) (T)\n\\(P(A^c \\cap B)\\)\n\\(P(A^c \\cap B^c)\\)\n\\(P(A^c)\\)\n\n\nCol Total\n\\(P(B)\\)\n\\(P(B^c)\\)\n\\(1\\)\n\n\n\nReminder:\n- \\(A =\\) “coin lands heads”\n- \\(B =\\) “die shows 1 or 2”\n- \\(A^c =\\) “coin lands tails”\n- \\(B^c =\\) “die shows 3–6”\nThere are \\(|S|=12\\) equally likely outcomes for (coin, die).\n\n\n\n\n\\(A \\cap B\\) = \\(\\{(H,1),(H,2)\\}\\)\n\\(P(A \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\\)\n\\(A \\cap B^c\\) = \\(\\{(H,3),(H,4),(H,5),(H,6)\\}\\)\n\\(P(A \\cap B^c) = \\tfrac{4}{12} = \\tfrac{1}{3}\\)\n\\(A^c \\cap B\\) = \\(\\{(T,1),(T,2)\\}\\)\n\\(P(A^c \\cap B) = \\tfrac{2}{12} = \\tfrac{1}{6}\\)\n\\(A^c \\cap B^c\\) = \\(\\{(T,3),(T,4),(T,5),(T,6)\\}\\)\n\\(P(A^c \\cap B^c) = \\tfrac{4}{12} = \\tfrac{1}{3}\\)\n\n\n\n\n\n\\(P(A) = P(A \\cap B) + P(A \\cap B^c) = \\tfrac{1}{6} + \\tfrac{1}{3} = \\tfrac{1}{2}\\)\n\\(P(A^c) = P(A^c \\cap B) + P(A^c \\cap B^c) = \\tfrac{1}{6} + \\tfrac{1}{3} = \\tfrac{1}{2}\\)\n\\(P(B) = P(A \\cap B) + P(A^c \\cap B) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{1}{3}\\)\n\\(P(B^c) = P(A \\cap B^c) + P(A^c \\cap B^c) = \\tfrac{1}{3} + \\tfrac{1}{3} = \\tfrac{2}{3}\\)\nTotal: \\(P(S)=1\\)\n\n\n\n\n\n\n\n\n\\(B\\) (1 or 2)\n\\(B^c\\) (3–6)\nRow Total\n\n\n\n\n\\(A\\) (H)\n\\(\\tfrac{1}{6}\\)\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{1}{2}\\)\n\n\n\\(A^c\\) (T)\n\\(\\tfrac{1}{6}\\)\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{1}{2}\\)\n\n\nCol Total\n\\(\\tfrac{1}{3}\\)\n\\(\\tfrac{2}{3}\\)\n\\(1\\)\n\n\n\n\n\n\n\n\n\n\n   A: Heads B: Die is 1 or 2 A ∩ B^c (H,3) (H,4) (H,5) (H,6) P = 1/3 A ∩ B (H,1) (H,2) P = 1/6 A^c ∩ B (T,1) (T,2) P = 1/6 (A^c ∩ B^c) (T,3) (T,4) (T,5) (T,6) P = 1/3",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#probability-rules",
    "href": "MA206-AY26-1/lesson-3.html#probability-rules",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n\nTwo events are mutually exclusive when it is impossible for both to happen at the same time.\nTherefore:\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\n\nExample 1 (coin + die):\n\n\n\n\n\n\nCompute \\(P(A \\cup B)\\) using the addition rule\n\n\n\n\n\n\n\\(P(A)=\\tfrac{1}{2}\\)\n\n\\(P(B)=\\tfrac{1}{3}\\)\n\\(P(A\\cap B)=\\tfrac{1}{6}\\)\n\nApply the rule: \\[\nP(A\\cup B)=\\tfrac{1}{2}+\\tfrac{1}{3}-\\tfrac{1}{6}=\\tfrac{2}{3}.\n\\]\nSet view: \\[\nA\\cup B=\\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2)\\},\\quad |A\\cup B|=8.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#board-problems",
    "href": "MA206-AY26-1/lesson-3.html#board-problems",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "Out of 100 students:\n- 40 like pizza\n- 30 like burgers\n- 10 of those included above like both pizza and burgers\nExperiment: I select one student at random\nLet:\n- \\(A =\\) “student likes pizza”\n- \\(B =\\) “student likes burgers”\n\nWhat is \\(P(A)\\)?\n\nWhat is \\(P(B)\\)?\n\nWhat is \\(P(A \\cap B)\\)?\n\nWhat is \\(P(A \\cup B)\\) using the addition rule?\n\nWhat is \\(P(A^c)\\), the probability a student does not like pizza?\n\nWhat is \\(P(B^c)\\), the probability a student does not like burgers?\n\nWhat is \\(P(A^c \\cap B^c)\\), the probability a student likes neither?\n\nVerify your results using the 2×2 probability table.\n\nRepresent the results with a Venn diagram.\n\n\n\n\n\n\n\nAnswers Problem 1\n\n\n\n\n\n\n\\(P(A) = \\tfrac{40}{100} = 0.40\\)\n\n\\(P(B) = \\tfrac{30}{100} = 0.30\\)\n\n\\(P(A \\cap B) = \\tfrac{10}{100} = 0.10\\)\n\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = 0.40 + 0.30 - 0.10 = 0.60\\)\n\n\\(P(A^c) = 1 - P(A) = 1 - 0.40 = 0.60\\)\n\n\\(P(B^c) = 1 - P(B) = 1 - 0.30 = 0.70\\)\n\n\\(P(A^c \\cap B^c) = 1 - (A \\cup \\ B) = 1 - \\tfrac{60}{100} = \\tfrac{40}{100} = 0.40\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\) (burger)\n\\(B^c\\) (not burger)\nRow Total\n\n\n\n\n\\(A\\) (pizza)\n\\(\\tfrac{10}{100} = 0.10\\)\n\\(\\tfrac{30}{100} = 0.30\\)\n\\(\\tfrac{40}{100} = 0.40\\)\n\n\n\\(A^c\\) (not pizza)\n\\(\\tfrac{20}{100} = 0.20\\)\n\\(\\tfrac{40}{100} = 0.40\\)\n\\(\\tfrac{60}{100} = 0.60\\)\n\n\nCol Total\n\\(\\tfrac{30}{100} = 0.30\\)\n\\(\\tfrac{70}{100} = 0.70\\)\n\\(1\\)\n\n\n\n\n\n\n\n\n     A: Pizza B: Burger  A ∩ Bᶜ 30 students P = 0.30  A ∩ B 10 students P = 0.10  Aᶜ ∩ B 20 students P = 0.20  Aᶜ ∩ Bᶜ 40 students P = 0.40\n\n\n\n\n\n\n\n\n\nShade \\(A \\cap B^c\\)\n\n\n\n\n\n\nAnswer Problem 2\n\n\n\n\n\n\n\n\n         A B\n\n\n\n\n\n\n\n\nShade \\((A \\cup B)^c\\)\n\n\n\n\n\n\nAnswer Problem 3\n\n\n\n\n\n\n\n\n          A B\n\n\n\n\n\n\n\n\nShade \\(\\big( (A \\cup B)^c \\big) \\cup (A \\cap B)\\)\n\n\n\n\n\n\nAnswer Problem 4\n\n\n\n\n\n\n\n\n\n\n\n\nShade \\((A \\cup B)^c \\cap C\\)\n\n\n\n\n\n\nAnswer Problem 5",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-3.html#simulation",
    "href": "MA206-AY26-1/lesson-3.html#simulation",
    "title": "Lesson 3: Project Dataset Exploration",
    "section": "",
    "text": "Let’s Simulate",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 3"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html",
    "href": "MA206-AY26-1/lesson-4.html",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Project Milestone 2\n\nTidyverse Tutorial applied to your team’s data\nAnnex B (for addressing feedback)\n\nExploration 11.3B\n\n\n\n\n\n\nNot Cal, but….\n\n  Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\nPreviously 0-0\n\n\n\n\n\n\n1-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy I become an Engineer\nWhat I think you should do\nWhat do I do as an Engineer\nWhat is my branch now?\n\n\n\n\n\n\nRemember this?\nOut of 100 students:\n- 40 like pizza\n- 30 like burgers\n- 10 of those included above like both pizza and burgers\nExperiment: I select one student at random\nLet:\n- \\(A =\\) “student likes pizza”\n- \\(B =\\) “student likes burgers”\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\nWhat is the probablity that a randomly selected student doesn’t like pizza (\\(P(A^c)\\))?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe know \\(P(A) = 40/100 = 0.40\\).\nBy the complement rule:\n\\[\nP(A^c) = 1 - P(A) = 1 - 0.40 = 0.60\n\\]\n\n\n\n\n\n\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n   A B\n\n\nWhat is the probability a randomly selected cadet likes pizza or burgers? \\(P(A \\cup B)\\)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy the addition rule:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\nSubstitute values:\n\\[\nP(A \\cup B) = 0.40 + 0.30 - 0.10 = 0.60\n\\]\n\n\n\n\n\n\nTwo events are mutually exclusive when it is impossible for both to happen at the same time.\nTherefore:\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n   A B\n\n\nAre events A and B mutually exclusive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNo because:\n\\[\nP(A \\cup B) \\neq P(A) + P(B)\n\\]\n\n\n\n\n\n\nTwo events \\(A\\) and \\(B\\) are independent if knowing that one occurs does not change the probability of the other.\n\\[\nP(A \\cap B) = P(A)P(B)\n\\]\nEquivalently (if \\(P(B) &gt; 0\\)):\n\\[\nP(A \\mid B) = P(A)\n\\]\nIntuition: The outcome of one event gives no information about the other.\nDoes Mutually Exclusive imply independence or no or undetermined?\nAre \\(A =\\) “likes pizza” and \\(B =\\) “likes burgers” independent?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe check using the multiplication rule for independence:\n\\[\nP(A)P(B) = (0.40)(0.30) = 0.12\n\\]\nBut from the data:\n\\[\nP(A \\cap B) = 0.10\n\\]\nSince \\(0.10 \\neq 0.12\\), the events are not independent.\nAlso, they are not mutually exclusive either, because\n\\[\nP(A \\cap B) = 0.10 &gt; 0.\n\\]\nSo in this example, \\(A\\) and \\(B\\) are neither independent nor mutually exclusive.\nGeneral Note:\nIf \\(A\\) and \\(B\\) are mutually exclusive with nonzero probabilities, they cannot be independent.\n\n\n\n\n\n\n\nWe’ll use a single Factory Machines scenario for all concepts in this section.\nFactory Setup (used for all examples)\nA factory produces 1,000 items per day using two machines:\n\nMachine \\(M_A\\) produces 400 items, of which 40 are defective.\n\nMachine \\(M_B\\) produces 600 items, of which 24 are defective.\n\nDefine events:\n- \\(M_A =\\) “item came from Machine A”\n- \\(M_B =\\) “item came from Machine B”\n- \\(D =\\) “item is defective”\n- \\(D^c =\\) “item is not defective”\nFrom the counts:\n- \\(P(M_A) = 400/1000 = 0.40\\)\n- \\(P(M_B) = 600/1000 = 0.60\\)\n- \\(P(D \\cap M_A) = 40/1000 = 0.04\\)\n- \\(P(D \\cap M_B) = 24/1000 = 0.024\\)\n- \\(P(D) = (40+24)/1000 = 0.064\\)\n\n\nThe probability that event \\(A\\) occurs given that event \\(B\\) occurs:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\n\\]\nIntuition: Restrict the sample space to \\(B\\); ask what fraction of those outcomes also fall in \\(A\\).\n\n\n      A B\n\n\nExample (Factory Machines)\nWhat is \\(P(D \\mid M_A)\\), the probability an item is defective given it was made by Machine \\(A\\)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy definition:\n\\[\nP(D \\mid M_A) = \\frac{P(D \\cap M_A)}{P(M_A)}.\n\\]\nFrom the setup:\n- \\(P(D \\cap M_A) = 40/1000 = 0.04\\)\n- \\(P(M_A) = 400/1000 = 0.40\\)\nSo:\n\\[\nP(D \\mid M_A) = \\frac{0.04}{0.40} = 0.10.\n\\]\n\n\n\n\n\n\nRelates intersections to conditional probabilities.\n\\[\\begin{align}\nP(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\n&& \\text{definition of conditional probability} \\\\[24pt]\n\nP(A \\mid B)\\,P(B) &= \\frac{P(A \\cap B)}{P(B)} \\cdot P(B)\n&& \\text{multiply both sides by $P(B)$} \\\\[24pt]\n\nP(A \\mid B)\\,P(B) &= P(A \\cap B)\n&& \\text{simplify} \\\\[24pt]\n\nP(B \\mid A)\\,P(A) &= P(A \\cap B)\n&& \\text{swap $A$ and $B$} \\\\[24pt]\n\nP(A \\cap B) &= P(A \\mid B)P(B) = P(B \\mid A)P(A)\n&& \\text{final multiplication rule}\n\\end{align}\\]\nIntuition: To find the chance that both happen, compute the chance that one happens, then multiply by the chance the other happens given that.\nExample (Factory Machines)\nUsing the rule, compute \\(P(D \\cap M_B)\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy the multiplication rule:\n\\[\nP(D \\cap M_B) = P(D \\mid M_B)\\,P(M_B).\n\\]\nFrom counts:\n- \\(P(D \\mid M_B) = 24/600 = 0.04\\)\n- \\(P(M_B) = 600/1000 = 0.60\\)\nSo:\n\\[\nP(D \\cap M_B) = (0.04)(0.60) = 0.024.\n\\]\nThis matches the direct count calculation \\(24/1000=0.024\\).\n\n\n\n\n\nThe multiplication rule shows us how to express the probability of two events happening together in terms of a conditional probability:\n\\[\nP(A \\cap B) = P(A \\mid B)\\, P(B).\n\\]\nNow imagine that the whole sample space can be split into two non-overlapping cases (aka mutually exclusive) \\(B_1\\) and \\(B_2\\) (for example: “athlete” vs. “not an athlete”).\nSince \\(B_1\\) and \\(B_2\\) cover all possibilities, any time \\(A\\) happens, it must happen either with \\(B_1\\) or with \\(B_2\\).\n\n\n\n\n\nTo find \\(P(A)\\), we know that it is the union of where A intersects B in all ways that B can happen. so:\n\\[\nP(A) = P((A \\cap B_1) \\cup (A \\cap B_2))\n\\]\nBecause \\((A \\cap B_1)\\) and \\((A \\cap B_2)\\) are mutually exclusive, we can use the mutually exclusive version of the addition rule \\(P(E_1 \\cup E_2) = P(E_1) + P(E_2)\\) to get:\n\\[\nP((A \\cap B_1) \\cup (A \\cap B_2)) = P(A \\cap B_1) + P(A \\cap B_2).\n\\]\nAnd if we apply the multiplication rule (\\(P(E_1 \\cap E_2) = P(E_1 \\mid E_2)\\, P(E_2)\\)) to each intersection, we get exactly the Law of Total Probability.\n\\[\nP(A) = P(A | B_1)P(B_1) + P(A | B_2)P(B_2).\n\\]\n\n\n\n\nWhat we just derived for two outcomes extends to any partition of the sample space.\nIf \\(B_1, B_2, \\ldots, B_k\\) are mutually exclusive and exhaustive events, then for any event \\(A\\):\n\\[\nP(A) = \\sum_{i=1}^{k} P(A \\mid B_i)\\, P(B_i).\n\\]\nFor two outcomes:\n\\[\nP(A) = P(A \\mid B_1)\\,P(B_1) \\;+\\; P(A \\mid B_2)\\,P(B_2).\n\\]\nIntuition:\nInstead of calculating \\(P(A)\\) directly, we “partition” the sample space into simpler pieces \\(B_1, B_2, \\ldots, B_k\\). We compute \\(P(A)\\) by adding up the contributions from each path.\nExample (Factory Machines)\nWhat is the overall probability that a randomly chosen item is defective (\\(P(D)\\))?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPartition by which machine made the item:\n\\[\nP(D) = P(D \\mid M_A)P(M_A) + P(D \\mid M_B)P(M_B).\n\\]\nFrom counts:\n- \\(P(D \\mid M_A)=40/400=0.10\\), \\(P(M_A)=0.40\\)\n- \\(P(D \\mid M_B)=24/600=0.04\\), \\(P(M_B)=0.60\\)\nSo:\n\\[\nP(D) = (0.10)(0.40) + (0.04)(0.60) = 0.04 + 0.024 = 0.064.\n\\]\nSo 6.4% of all items are defective.\n\n\n\n\n\n\nBayes’ Theorem lets us “flip” conditional probabilities. It tells us how to update beliefs about a cause (\\(B\\)) when we observe some evidence (\\(A\\)).\nFor events \\(A\\) and \\(B\\) with \\(P(B) &gt; 0\\):\n\\[\nP(B \\mid A) = \\frac{P(A \\mid B)\\, P(B)}{P(A)}.\n\\]\nUsing the Law of Total Probability for \\(P(A)\\):\n\\[\nP(B_j \\mid A) = \\frac{P(A \\mid B_j)\\, P(B_j)}{\\sum_{i=1}^{k} P(A \\mid B_i)\\, P(B_i)}.\n\\]\n\\[\nP(B_2 \\mid A) = \\frac{P(A \\mid B_2)\\, P(B_2)}{P(A \\mid B_1)P(B_1) + P(A \\mid B_2)P(B_2)}.\n\\]\nIntuition:\nThink of Bayes’ theorem as a way to reverse the condition. If we know how likely \\(A\\) is when \\(B\\) happens, Bayes tells us how likely \\(B\\) is given that we saw \\(A\\).\nExample (Factory Machines)\nIf an item is defective, what is the probability it came from Machine A?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe want \\(P(M_A \\mid D)\\).\nBy Bayes’ theorem:\n\\[\nP(M_A \\mid D) = \\frac{P(D \\mid M_A)\\, P(M_A)}{P(D)}.\n\\]\nThe denominator \\(P(D)\\) comes from the Law of Total Probability:\n\\[\nP(D) = P(D \\mid M_A)P(M_A) + P(D \\mid M_B)P(M_B).\n\\]\nFrom counts:\n- \\(P(D \\mid M_A)=40/400=0.10\\), \\(P(M_A)=400/1000=0.40\\)\n- \\(P(D \\mid M_B)=24/600=0.04\\), \\(P(M_B)=600/1000=0.60\\)\nSo:\n\\[\nP(D) = (0.10)(0.40) + (0.04)(0.60) = 0.064\n\\]\nNow substitute into Bayes’ theorem:\n\\[\nP(M_A \\mid D) = \\frac{0.10 \\cdot 0.40}{0.064} = \\frac{0.04}{0.064} = 0.625.\n\\]\nThus, if an item is defective, there is a 62.5% chance it was made by Machine A.\n\n\n\n\n\n\nAnother way to picture the Law of Total Probability is to start with the partition (\\(M_A\\) vs \\(M_B\\)), then branch into whether \\(D\\) happens or not under each case.\nStart\n├── M_A (0.40)\n│   ├── D (40/400 = 0.10)   ⇒ 0.40 · 0.10 = 0.04\n│   └── D^c (360/400 = 0.90) ⇒ 0.40 · 0.90 = 0.36\n└── M_B (0.60)\n    ├── D (24/600 = 0.04)   ⇒ 0.60 · 0.04 = 0.024\n    └── D^c (576/600 = 0.96) ⇒ 0.60 · 0.96 = 0.576\nAdding the two disjoint paths where \\(D\\) occurs:\n\\[\nP(D) = 0.04 + 0.024 = 0.064.\n\\]\nThis confirms the Law of Total Probability result and sets up Bayes’ theorem\n\n\n\n\nA certain disease affects 1% of a population.\nA test is used to detect the disease:\n\nIf a person has the disease, the test is positive 92% of the time.\n\nIf a person does not have the disease, the test is positive 7% of the time.\n\nLet’s define events:\n\n\\(D =\\) person has the disease.\n\n\\(D^c =\\) person does not have the disease.\n\n\\(+\\) = test is positive.\n\n\\(-\\) = test is negative.\n\n\n\nWhat is \\(P(+ \\mid D)\\), the probability that a person tests positive given they have the disease?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFrom the problem statement:\n\\[\nP(+ \\mid D) = 0.92\n\\]\n\n\n\n\n\n\nWhat is \\(P(D \\cap +)\\), the probability that a randomly chosen person both has the disease and tests positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy the multiplication rule:\n\\[\nP(D \\cap +) = P(+ \\mid D)\\, P(D).\n\\]\nSubstitute the known values:\n\\[\nP(D \\cap +) = (0.92)(0.01).\n\\]\nSimplify:\n\\[\nP(D \\cap +) = 0.0092\n\\]\n\n\n\n\n\n\nDraw a tree diagram for this situation.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBranch probabilities\n\n\\(P(D)=0.01,\\quad P(D^c)=0.99\\)\n\\(P(+\\mid D)=0.92,\\quad P(-\\mid D)=1-0.92=0.08\\)\n\\(P(+\\mid D^c)=0.07,\\quad P(-\\mid D^c)=1-0.07=0.93\\)\n\nTree with path (leaf) probabilities\nStart\n├── D (0.01)\n│   ├── + (0.92)  ⇒ P(D ∩ +)  = 0.01 · 0.92  = 0.0092\n│   └── − (0.08)  ⇒ P(D ∩ −)  = 0.01 · 0.08  = 0.0008\n└── D^c (0.99)\n    ├── + (0.07)  ⇒ P(D^c ∩ +) = 0.99 · 0.07  = 0.0693\n    └── − (0.93)  ⇒ P(D^c ∩ −) = 0.99 · 0.93  = 0.9207\n\n\n\n\n\n\nWhat is \\(P(+)\\), the probability that a randomly chosen person tests positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPartition into whether the person has the disease (\\(D\\)) or not (\\(D^c\\)):\n\\[\nP(+) = P(+ \\mid D)\\,P(D) + P(+ \\mid D^c)\\,P(D^c).\n\\]\nSubstitute values:\n\\[\nP(+) = (0.92)(0.01) + (0.07)(0.99).\n\\]\nSimplify:\n\\[\nP(+) = 0.0092 + 0.0693 = 0.0785\n\\]\n\n\n\n\n\n\nWhat is \\(P(D \\mid +)\\), the probability that a person actually has the disease given that their test is positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy Bayes’ theorem:\n\\[\nP(D \\mid +) = \\frac{P(+ \\mid D)\\, P(D)}{P(+)}.\n\\]\nSubstitute values:\n\\[\nP(D \\mid +) = \\frac{0.92 \\cdot 0.01}{0.0785}.\n\\]\nSimplify:\n\\[\nP(D \\mid +) \\approx 0.117\n\\]\n\n\n\n\n\n\n\nYour campus email system uses an automated spam filter. From long-run monitoring. About 16% of incoming messages are spam. When a message is spam, the filter flags it about 90% of the time. When a message is not spam the filter still flags it (a false positive) about 8% of the time.\nAnswer the questions below.\n\n\n\nDefine, in words, the four basic events used in this problem:\n• the message is spam,\n• the message is not spam,\n• the message is flagged by the filter,\n• the message is not flagged.\n(Choose symbols for each and state your choices.)\nFrom the story, write down the numerical values for all probabilities needed to solve the problem set, including:\n• the probability a message is spam,\n• the probability a message is not spam,\n• the probability a message is flagged given it is spam,\n• the probability a message is not flagged given it is spam,\n• the probability a message is flagged given it is not spam,\n• the probability a message is not flagged given it is not spam.\n(Express each using the symbols you chose in part 1.)\nDraw a tree diagram that first branches on whether the message is spam vs. not spam, and then on flagged vs. not flagged under each branch. Label every branch with the appropriate probability.\nWhat is the probability that a message is flagged?\nWhat is the probability that a message is both spam and flagged?\nGiven that a message is flagged, what is the probability that it is spam?\nFind the probability that a message is not flagged, and the conditional probabilities of not flagged given spam and given not spam.\nCompute the probabilities of each joint outcome:\n• spam and not flagged,\n• not spam and flagged,\n• not spam and not flagged.\nAre the events “spam” and “flagged” independent? Justify with a numerical check using your probabilities.\nWhat is the probability that a message is either spam or flagged (or both)?\nBuild the 2×2 probability table with rows “spam / not spam” and columns “flagged / not flagged.” Fill in each cell with the corresponding probability, and verify that row and column totals match your earlier results and that the four cells sum to 1.\n\n\n\n\n\n\n\n\nWorked solutions\n\n\n\n\n\n1) Definitions (words)\n\\(S\\): “message is spam.”\n\\(S^c\\): “message is not spam.”\n\\(+\\): “message is flagged by the filter.”\n\\(-\\): “message is not flagged.”\n2) Defined probabilities (from the story and complements)\n\\[\nP(S)=0.16 \\qquad P(S^c)=1-P(S)=0.84\n\\] \\[\nP(+\\mid S)=0.90 \\qquad P(-\\mid S)=1-P(+\\mid S)=0.10\n\\] \\[\nP(+\\mid S^c)=0.08 \\qquad P(-\\mid S^c)=1-P(+\\mid S^c)=0.92\n\\]\n3) Tree diagram (branches & path probabilities)\nStart\n├── S (0.16)\n│   ├── + (0.90)  ⇒ P(S ∩ +)  = 0.16 · 0.90 = 0.144\n│   └── − (0.10)  ⇒ P(S ∩ −)  = 0.16 · 0.10 = 0.016\n└── S^c (0.84)\n    ├── + (0.08)  ⇒ P(S^c ∩ +) = 0.84 · 0.08 = 0.0672\n    └── − (0.92)  ⇒ P(S^c ∩ −) = 0.84 · 0.92 = 0.7728\n4) \\(P(+)\\)\nUses Law of Total Probability\n\\[\nP(+)=P(+\\mid S)P(S)+P(+\\mid S^c)P(S^c)\n=0.90(0.16)+0.08(0.84)\n=0.144+0.0672\n=0.2112\n\\]\n5) \\(P(S\\cap +)\\)\n\\[\nP(S\\cap +)=P(+\\mid S)P(S)=0.90\\cdot 0.16=0.144\n\\]\n6) \\(P(S\\mid +)\\)\nUsing earlier parts:\nFrom part 5, \\(P(S\\cap +)=0.144\\).\nFrom part 4, \\(P(+)=0.2112\\).\n\\[\nP(S\\mid +)=\\frac{P(S\\cap +)}{P(+)}=\\frac{0.144}{0.2112}\n=\\frac{15}{22}\\approx 0.6818\n\\]\nBayes’ Theorem (full form)\n\\[\nP(S\\mid +)=\\frac{P(+\\mid S)\\,P(S)}{P(+\\mid S)\\,P(S)\\;+\\;P(+\\mid S^c)\\,P(S^c)}\n\\]\nSubstitute the values (from the story / part 2): \\(P(+\\mid S)=0.90\\), \\(P(S)=0.16\\), \\(P(+\\mid S^c)=0.08\\), \\(P(S^c)=0.84\\).\n\\[\nP(S\\mid +)=\\frac{0.90\\cdot 0.16}{0.90\\cdot 0.16+0.08\\cdot 0.84}\n=\\frac{0.144}{0.144+0.0672}\n=\\frac{0.144}{0.2112}\n=\\frac{15}{22}\\approx 0.6818\n\\]\n7) \\(P(-)\\), \\(P(-\\mid S)\\), \\(P(-\\mid S^c)\\)\n\\[\nP(-)=1-P(+)=1-0.2112=0.7888.\n\\]\nFrom part 2, \\(P(+\\mid S)=0.90\\) and \\(P(+\\mid S^c)=0.08\\). Therefore \\[\nP(-\\mid S)=1-P(+\\mid S)=1-0.90=0.10,\\qquad\nP(-\\mid S^c)=1-P(+\\mid S^c)=1-0.08=0.92.\n\\]\nOptional cross-check via paths (from parts 3 & 5): \\[\nP(-\\mid S)=\\frac{P(S\\cap -)}{P(S)}=\\frac{0.016}{0.16}=0.10,\\qquad\nP(-\\mid S^c)=\\frac{P(S^c\\cap -)}{P(S^c)}=\\frac{0.7728}{0.84}=0.92.\n\\]\n8) \\(P(S\\cap -)\\), \\(P(S^c\\cap +)\\), \\(P(S^c\\cap -)\\)\nUsing the multiplication rule and earlier parts:\n\nFrom part 7 and part 2: \\[\nP(S\\cap -)=P(-\\mid S)\\,P(S)=(0.10)(0.16)=0.016.\n\\] (Cross-check from part 5 and part 2: (P(S-)=P(S)-P(S+)=0.16-0.144=0.016).)\nFrom part 2: \\[\nP(S^c\\cap +)=P(+\\mid S^c)\\,P(S^c)=(0.08)(0.84)=0.0672.\n\\]\nFrom part 7 and part 2: \\[\nP(S^c\\cap -)=P(-\\mid S^c)\\,P(S^c)=(0.92)(0.84)=0.7728.\n\\]\n\nCheck (with (P(S+)) from part 5): \\[\n0.144+0.016+0.0672+0.7728=1.\n\\]\n9) Independence check (\\(S\\) vs \\(+\\))\nIndependent would require \\(P(S\\cap +)=P(S)\\,P(+)\\).\n\\[\nP(S)\\,P(+)=0.16\\cdot 0.2112=0.033792\\neq 0.144\n\\] So \\(S\\) and \\(+\\) are not independent.\n10) \\(P(S\\cup +)\\)\n\\[\nP(S\\cup +)=P(S)+P(+)-P(S\\cap +)\n=0.16+0.2112-0.144\n=0.2272\n\\]\n11) \\(2\\times 2\\) probability table\n\n\n\n\n\\(+\\)\n\\(-\\)\nRow total\n\n\n\n\n\\(S\\)\n\\(0.144\\)\n\\(0.016\\)\n\\(0.160\\)\n\n\n\\(S^c\\)\n\\(0.0672\\)\n\\(0.7728\\)\n\\(0.840\\)\n\n\nCol total\n0.2112\n0.7888\n1.000\n\n\n\nRow/column totals match; the four cells sum to \\(1\\).\n\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\nProject Milestone 2\nAnnex B\n\n\n\n\nLesson 5\n\n\n\n\n\nTidyverse Tutorial: In Class Lesson 5\nProject Milestone 2: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#welcome",
    "href": "MA206-AY26-1/lesson-4.html#welcome",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Project Milestone 2\n\nTidyverse Tutorial applied to your team’s data\nAnnex B (for addressing feedback)\n\nExploration 11.3B\n\n\n\n\n\n\nNot Cal, but….\n\n  Your browser does not support the video tag. \n\n\n  Your browser does not support the video tag. \n\n\n\n\n\n\n\n\n\n\nPreviously 0-0\n\n\n\n\n\n\n1-0",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#branch-week",
    "href": "MA206-AY26-1/lesson-4.html#branch-week",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Why I become an Engineer\nWhat I think you should do\nWhat do I do as an Engineer\nWhat is my branch now?",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#probability-review",
    "href": "MA206-AY26-1/lesson-4.html#probability-review",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Remember this?\nOut of 100 students:\n- 40 like pizza\n- 30 like burgers\n- 10 of those included above like both pizza and burgers\nExperiment: I select one student at random\nLet:\n- \\(A =\\) “student likes pizza”\n- \\(B =\\) “student likes burgers”\n\n\n\nFor any event \\(A\\):\n\\[\nP(A^c) = 1 - P(A)\n\\]\nWhat is the probablity that a randomly selected student doesn’t like pizza (\\(P(A^c)\\))?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe know \\(P(A) = 40/100 = 0.40\\).\nBy the complement rule:\n\\[\nP(A^c) = 1 - P(A) = 1 - 0.40 = 0.60\n\\]\n\n\n\n\n\n\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n   A B\n\n\nWhat is the probability a randomly selected cadet likes pizza or burgers? \\(P(A \\cup B)\\)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy the addition rule:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\nSubstitute values:\n\\[\nP(A \\cup B) = 0.40 + 0.30 - 0.10 = 0.60\n\\]\n\n\n\n\n\n\nTwo events are mutually exclusive when it is impossible for both to happen at the same time.\nTherefore:\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n   A B\n\n\nAre events A and B mutually exclusive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNo because:\n\\[\nP(A \\cup B) \\neq P(A) + P(B)\n\\]\n\n\n\n\n\n\nTwo events \\(A\\) and \\(B\\) are independent if knowing that one occurs does not change the probability of the other.\n\\[\nP(A \\cap B) = P(A)P(B)\n\\]\nEquivalently (if \\(P(B) &gt; 0\\)):\n\\[\nP(A \\mid B) = P(A)\n\\]\nIntuition: The outcome of one event gives no information about the other.\nDoes Mutually Exclusive imply independence or no or undetermined?\nAre \\(A =\\) “likes pizza” and \\(B =\\) “likes burgers” independent?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe check using the multiplication rule for independence:\n\\[\nP(A)P(B) = (0.40)(0.30) = 0.12\n\\]\nBut from the data:\n\\[\nP(A \\cap B) = 0.10\n\\]\nSince \\(0.10 \\neq 0.12\\), the events are not independent.\nAlso, they are not mutually exclusive either, because\n\\[\nP(A \\cap B) = 0.10 &gt; 0.\n\\]\nSo in this example, \\(A\\) and \\(B\\) are neither independent nor mutually exclusive.\nGeneral Note:\nIf \\(A\\) and \\(B\\) are mutually exclusive with nonzero probabilities, they cannot be independent.",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#new-probability-material",
    "href": "MA206-AY26-1/lesson-4.html#new-probability-material",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "We’ll use a single Factory Machines scenario for all concepts in this section.\nFactory Setup (used for all examples)\nA factory produces 1,000 items per day using two machines:\n\nMachine \\(M_A\\) produces 400 items, of which 40 are defective.\n\nMachine \\(M_B\\) produces 600 items, of which 24 are defective.\n\nDefine events:\n- \\(M_A =\\) “item came from Machine A”\n- \\(M_B =\\) “item came from Machine B”\n- \\(D =\\) “item is defective”\n- \\(D^c =\\) “item is not defective”\nFrom the counts:\n- \\(P(M_A) = 400/1000 = 0.40\\)\n- \\(P(M_B) = 600/1000 = 0.60\\)\n- \\(P(D \\cap M_A) = 40/1000 = 0.04\\)\n- \\(P(D \\cap M_B) = 24/1000 = 0.024\\)\n- \\(P(D) = (40+24)/1000 = 0.064\\)\n\n\nThe probability that event \\(A\\) occurs given that event \\(B\\) occurs:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\n\\]\nIntuition: Restrict the sample space to \\(B\\); ask what fraction of those outcomes also fall in \\(A\\).\n\n\n      A B\n\n\nExample (Factory Machines)\nWhat is \\(P(D \\mid M_A)\\), the probability an item is defective given it was made by Machine \\(A\\)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy definition:\n\\[\nP(D \\mid M_A) = \\frac{P(D \\cap M_A)}{P(M_A)}.\n\\]\nFrom the setup:\n- \\(P(D \\cap M_A) = 40/1000 = 0.04\\)\n- \\(P(M_A) = 400/1000 = 0.40\\)\nSo:\n\\[\nP(D \\mid M_A) = \\frac{0.04}{0.40} = 0.10.\n\\]\n\n\n\n\n\n\nRelates intersections to conditional probabilities.\n\\[\\begin{align}\nP(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\n&& \\text{definition of conditional probability} \\\\[24pt]\n\nP(A \\mid B)\\,P(B) &= \\frac{P(A \\cap B)}{P(B)} \\cdot P(B)\n&& \\text{multiply both sides by $P(B)$} \\\\[24pt]\n\nP(A \\mid B)\\,P(B) &= P(A \\cap B)\n&& \\text{simplify} \\\\[24pt]\n\nP(B \\mid A)\\,P(A) &= P(A \\cap B)\n&& \\text{swap $A$ and $B$} \\\\[24pt]\n\nP(A \\cap B) &= P(A \\mid B)P(B) = P(B \\mid A)P(A)\n&& \\text{final multiplication rule}\n\\end{align}\\]\nIntuition: To find the chance that both happen, compute the chance that one happens, then multiply by the chance the other happens given that.\nExample (Factory Machines)\nUsing the rule, compute \\(P(D \\cap M_B)\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy the multiplication rule:\n\\[\nP(D \\cap M_B) = P(D \\mid M_B)\\,P(M_B).\n\\]\nFrom counts:\n- \\(P(D \\mid M_B) = 24/600 = 0.04\\)\n- \\(P(M_B) = 600/1000 = 0.60\\)\nSo:\n\\[\nP(D \\cap M_B) = (0.04)(0.60) = 0.024.\n\\]\nThis matches the direct count calculation \\(24/1000=0.024\\).\n\n\n\n\n\nThe multiplication rule shows us how to express the probability of two events happening together in terms of a conditional probability:\n\\[\nP(A \\cap B) = P(A \\mid B)\\, P(B).\n\\]\nNow imagine that the whole sample space can be split into two non-overlapping cases (aka mutually exclusive) \\(B_1\\) and \\(B_2\\) (for example: “athlete” vs. “not an athlete”).\nSince \\(B_1\\) and \\(B_2\\) cover all possibilities, any time \\(A\\) happens, it must happen either with \\(B_1\\) or with \\(B_2\\).\n\n\n\n\n\nTo find \\(P(A)\\), we know that it is the union of where A intersects B in all ways that B can happen. so:\n\\[\nP(A) = P((A \\cap B_1) \\cup (A \\cap B_2))\n\\]\nBecause \\((A \\cap B_1)\\) and \\((A \\cap B_2)\\) are mutually exclusive, we can use the mutually exclusive version of the addition rule \\(P(E_1 \\cup E_2) = P(E_1) + P(E_2)\\) to get:\n\\[\nP((A \\cap B_1) \\cup (A \\cap B_2)) = P(A \\cap B_1) + P(A \\cap B_2).\n\\]\nAnd if we apply the multiplication rule (\\(P(E_1 \\cap E_2) = P(E_1 \\mid E_2)\\, P(E_2)\\)) to each intersection, we get exactly the Law of Total Probability.\n\\[\nP(A) = P(A | B_1)P(B_1) + P(A | B_2)P(B_2).\n\\]\n\n\n\n\nWhat we just derived for two outcomes extends to any partition of the sample space.\nIf \\(B_1, B_2, \\ldots, B_k\\) are mutually exclusive and exhaustive events, then for any event \\(A\\):\n\\[\nP(A) = \\sum_{i=1}^{k} P(A \\mid B_i)\\, P(B_i).\n\\]\nFor two outcomes:\n\\[\nP(A) = P(A \\mid B_1)\\,P(B_1) \\;+\\; P(A \\mid B_2)\\,P(B_2).\n\\]\nIntuition:\nInstead of calculating \\(P(A)\\) directly, we “partition” the sample space into simpler pieces \\(B_1, B_2, \\ldots, B_k\\). We compute \\(P(A)\\) by adding up the contributions from each path.\nExample (Factory Machines)\nWhat is the overall probability that a randomly chosen item is defective (\\(P(D)\\))?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPartition by which machine made the item:\n\\[\nP(D) = P(D \\mid M_A)P(M_A) + P(D \\mid M_B)P(M_B).\n\\]\nFrom counts:\n- \\(P(D \\mid M_A)=40/400=0.10\\), \\(P(M_A)=0.40\\)\n- \\(P(D \\mid M_B)=24/600=0.04\\), \\(P(M_B)=0.60\\)\nSo:\n\\[\nP(D) = (0.10)(0.40) + (0.04)(0.60) = 0.04 + 0.024 = 0.064.\n\\]\nSo 6.4% of all items are defective.\n\n\n\n\n\n\nBayes’ Theorem lets us “flip” conditional probabilities. It tells us how to update beliefs about a cause (\\(B\\)) when we observe some evidence (\\(A\\)).\nFor events \\(A\\) and \\(B\\) with \\(P(B) &gt; 0\\):\n\\[\nP(B \\mid A) = \\frac{P(A \\mid B)\\, P(B)}{P(A)}.\n\\]\nUsing the Law of Total Probability for \\(P(A)\\):\n\\[\nP(B_j \\mid A) = \\frac{P(A \\mid B_j)\\, P(B_j)}{\\sum_{i=1}^{k} P(A \\mid B_i)\\, P(B_i)}.\n\\]\n\\[\nP(B_2 \\mid A) = \\frac{P(A \\mid B_2)\\, P(B_2)}{P(A \\mid B_1)P(B_1) + P(A \\mid B_2)P(B_2)}.\n\\]\nIntuition:\nThink of Bayes’ theorem as a way to reverse the condition. If we know how likely \\(A\\) is when \\(B\\) happens, Bayes tells us how likely \\(B\\) is given that we saw \\(A\\).\nExample (Factory Machines)\nIf an item is defective, what is the probability it came from Machine A?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe want \\(P(M_A \\mid D)\\).\nBy Bayes’ theorem:\n\\[\nP(M_A \\mid D) = \\frac{P(D \\mid M_A)\\, P(M_A)}{P(D)}.\n\\]\nThe denominator \\(P(D)\\) comes from the Law of Total Probability:\n\\[\nP(D) = P(D \\mid M_A)P(M_A) + P(D \\mid M_B)P(M_B).\n\\]\nFrom counts:\n- \\(P(D \\mid M_A)=40/400=0.10\\), \\(P(M_A)=400/1000=0.40\\)\n- \\(P(D \\mid M_B)=24/600=0.04\\), \\(P(M_B)=600/1000=0.60\\)\nSo:\n\\[\nP(D) = (0.10)(0.40) + (0.04)(0.60) = 0.064\n\\]\nNow substitute into Bayes’ theorem:\n\\[\nP(M_A \\mid D) = \\frac{0.10 \\cdot 0.40}{0.064} = \\frac{0.04}{0.064} = 0.625.\n\\]\nThus, if an item is defective, there is a 62.5% chance it was made by Machine A.\n\n\n\n\n\n\nAnother way to picture the Law of Total Probability is to start with the partition (\\(M_A\\) vs \\(M_B\\)), then branch into whether \\(D\\) happens or not under each case.\nStart\n├── M_A (0.40)\n│   ├── D (40/400 = 0.10)   ⇒ 0.40 · 0.10 = 0.04\n│   └── D^c (360/400 = 0.90) ⇒ 0.40 · 0.90 = 0.36\n└── M_B (0.60)\n    ├── D (24/600 = 0.04)   ⇒ 0.60 · 0.04 = 0.024\n    └── D^c (576/600 = 0.96) ⇒ 0.60 · 0.96 = 0.576\nAdding the two disjoint paths where \\(D\\) occurs:\n\\[\nP(D) = 0.04 + 0.024 = 0.064.\n\\]\nThis confirms the Law of Total Probability result and sets up Bayes’ theorem",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#example-medical-test-and-probability-rules",
    "href": "MA206-AY26-1/lesson-4.html#example-medical-test-and-probability-rules",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "A certain disease affects 1% of a population.\nA test is used to detect the disease:\n\nIf a person has the disease, the test is positive 92% of the time.\n\nIf a person does not have the disease, the test is positive 7% of the time.\n\nLet’s define events:\n\n\\(D =\\) person has the disease.\n\n\\(D^c =\\) person does not have the disease.\n\n\\(+\\) = test is positive.\n\n\\(-\\) = test is negative.\n\n\n\nWhat is \\(P(+ \\mid D)\\), the probability that a person tests positive given they have the disease?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFrom the problem statement:\n\\[\nP(+ \\mid D) = 0.92\n\\]\n\n\n\n\n\n\nWhat is \\(P(D \\cap +)\\), the probability that a randomly chosen person both has the disease and tests positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy the multiplication rule:\n\\[\nP(D \\cap +) = P(+ \\mid D)\\, P(D).\n\\]\nSubstitute the known values:\n\\[\nP(D \\cap +) = (0.92)(0.01).\n\\]\nSimplify:\n\\[\nP(D \\cap +) = 0.0092\n\\]\n\n\n\n\n\n\nDraw a tree diagram for this situation.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBranch probabilities\n\n\\(P(D)=0.01,\\quad P(D^c)=0.99\\)\n\\(P(+\\mid D)=0.92,\\quad P(-\\mid D)=1-0.92=0.08\\)\n\\(P(+\\mid D^c)=0.07,\\quad P(-\\mid D^c)=1-0.07=0.93\\)\n\nTree with path (leaf) probabilities\nStart\n├── D (0.01)\n│   ├── + (0.92)  ⇒ P(D ∩ +)  = 0.01 · 0.92  = 0.0092\n│   └── − (0.08)  ⇒ P(D ∩ −)  = 0.01 · 0.08  = 0.0008\n└── D^c (0.99)\n    ├── + (0.07)  ⇒ P(D^c ∩ +) = 0.99 · 0.07  = 0.0693\n    └── − (0.93)  ⇒ P(D^c ∩ −) = 0.99 · 0.93  = 0.9207\n\n\n\n\n\n\nWhat is \\(P(+)\\), the probability that a randomly chosen person tests positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPartition into whether the person has the disease (\\(D\\)) or not (\\(D^c\\)):\n\\[\nP(+) = P(+ \\mid D)\\,P(D) + P(+ \\mid D^c)\\,P(D^c).\n\\]\nSubstitute values:\n\\[\nP(+) = (0.92)(0.01) + (0.07)(0.99).\n\\]\nSimplify:\n\\[\nP(+) = 0.0092 + 0.0693 = 0.0785\n\\]\n\n\n\n\n\n\nWhat is \\(P(D \\mid +)\\), the probability that a person actually has the disease given that their test is positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy Bayes’ theorem:\n\\[\nP(D \\mid +) = \\frac{P(+ \\mid D)\\, P(D)}{P(+)}.\n\\]\nSubstitute values:\n\\[\nP(D \\mid +) = \\frac{0.92 \\cdot 0.01}{0.0785}.\n\\]\nSimplify:\n\\[\nP(D \\mid +) \\approx 0.117\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#take-home-problem-spam-filter",
    "href": "MA206-AY26-1/lesson-4.html#take-home-problem-spam-filter",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Your campus email system uses an automated spam filter. From long-run monitoring. About 16% of incoming messages are spam. When a message is spam, the filter flags it about 90% of the time. When a message is not spam the filter still flags it (a false positive) about 8% of the time.\nAnswer the questions below.\n\n\n\nDefine, in words, the four basic events used in this problem:\n• the message is spam,\n• the message is not spam,\n• the message is flagged by the filter,\n• the message is not flagged.\n(Choose symbols for each and state your choices.)\nFrom the story, write down the numerical values for all probabilities needed to solve the problem set, including:\n• the probability a message is spam,\n• the probability a message is not spam,\n• the probability a message is flagged given it is spam,\n• the probability a message is not flagged given it is spam,\n• the probability a message is flagged given it is not spam,\n• the probability a message is not flagged given it is not spam.\n(Express each using the symbols you chose in part 1.)\nDraw a tree diagram that first branches on whether the message is spam vs. not spam, and then on flagged vs. not flagged under each branch. Label every branch with the appropriate probability.\nWhat is the probability that a message is flagged?\nWhat is the probability that a message is both spam and flagged?\nGiven that a message is flagged, what is the probability that it is spam?\nFind the probability that a message is not flagged, and the conditional probabilities of not flagged given spam and given not spam.\nCompute the probabilities of each joint outcome:\n• spam and not flagged,\n• not spam and flagged,\n• not spam and not flagged.\nAre the events “spam” and “flagged” independent? Justify with a numerical check using your probabilities.\nWhat is the probability that a message is either spam or flagged (or both)?\nBuild the 2×2 probability table with rows “spam / not spam” and columns “flagged / not flagged.” Fill in each cell with the corresponding probability, and verify that row and column totals match your earlier results and that the four cells sum to 1.\n\n\n\n\n\n\n\n\nWorked solutions\n\n\n\n\n\n1) Definitions (words)\n\\(S\\): “message is spam.”\n\\(S^c\\): “message is not spam.”\n\\(+\\): “message is flagged by the filter.”\n\\(-\\): “message is not flagged.”\n2) Defined probabilities (from the story and complements)\n\\[\nP(S)=0.16 \\qquad P(S^c)=1-P(S)=0.84\n\\] \\[\nP(+\\mid S)=0.90 \\qquad P(-\\mid S)=1-P(+\\mid S)=0.10\n\\] \\[\nP(+\\mid S^c)=0.08 \\qquad P(-\\mid S^c)=1-P(+\\mid S^c)=0.92\n\\]\n3) Tree diagram (branches & path probabilities)\nStart\n├── S (0.16)\n│   ├── + (0.90)  ⇒ P(S ∩ +)  = 0.16 · 0.90 = 0.144\n│   └── − (0.10)  ⇒ P(S ∩ −)  = 0.16 · 0.10 = 0.016\n└── S^c (0.84)\n    ├── + (0.08)  ⇒ P(S^c ∩ +) = 0.84 · 0.08 = 0.0672\n    └── − (0.92)  ⇒ P(S^c ∩ −) = 0.84 · 0.92 = 0.7728\n4) \\(P(+)\\)\nUses Law of Total Probability\n\\[\nP(+)=P(+\\mid S)P(S)+P(+\\mid S^c)P(S^c)\n=0.90(0.16)+0.08(0.84)\n=0.144+0.0672\n=0.2112\n\\]\n5) \\(P(S\\cap +)\\)\n\\[\nP(S\\cap +)=P(+\\mid S)P(S)=0.90\\cdot 0.16=0.144\n\\]\n6) \\(P(S\\mid +)\\)\nUsing earlier parts:\nFrom part 5, \\(P(S\\cap +)=0.144\\).\nFrom part 4, \\(P(+)=0.2112\\).\n\\[\nP(S\\mid +)=\\frac{P(S\\cap +)}{P(+)}=\\frac{0.144}{0.2112}\n=\\frac{15}{22}\\approx 0.6818\n\\]\nBayes’ Theorem (full form)\n\\[\nP(S\\mid +)=\\frac{P(+\\mid S)\\,P(S)}{P(+\\mid S)\\,P(S)\\;+\\;P(+\\mid S^c)\\,P(S^c)}\n\\]\nSubstitute the values (from the story / part 2): \\(P(+\\mid S)=0.90\\), \\(P(S)=0.16\\), \\(P(+\\mid S^c)=0.08\\), \\(P(S^c)=0.84\\).\n\\[\nP(S\\mid +)=\\frac{0.90\\cdot 0.16}{0.90\\cdot 0.16+0.08\\cdot 0.84}\n=\\frac{0.144}{0.144+0.0672}\n=\\frac{0.144}{0.2112}\n=\\frac{15}{22}\\approx 0.6818\n\\]\n7) \\(P(-)\\), \\(P(-\\mid S)\\), \\(P(-\\mid S^c)\\)\n\\[\nP(-)=1-P(+)=1-0.2112=0.7888.\n\\]\nFrom part 2, \\(P(+\\mid S)=0.90\\) and \\(P(+\\mid S^c)=0.08\\). Therefore \\[\nP(-\\mid S)=1-P(+\\mid S)=1-0.90=0.10,\\qquad\nP(-\\mid S^c)=1-P(+\\mid S^c)=1-0.08=0.92.\n\\]\nOptional cross-check via paths (from parts 3 & 5): \\[\nP(-\\mid S)=\\frac{P(S\\cap -)}{P(S)}=\\frac{0.016}{0.16}=0.10,\\qquad\nP(-\\mid S^c)=\\frac{P(S^c\\cap -)}{P(S^c)}=\\frac{0.7728}{0.84}=0.92.\n\\]\n8) \\(P(S\\cap -)\\), \\(P(S^c\\cap +)\\), \\(P(S^c\\cap -)\\)\nUsing the multiplication rule and earlier parts:\n\nFrom part 7 and part 2: \\[\nP(S\\cap -)=P(-\\mid S)\\,P(S)=(0.10)(0.16)=0.016.\n\\] (Cross-check from part 5 and part 2: (P(S-)=P(S)-P(S+)=0.16-0.144=0.016).)\nFrom part 2: \\[\nP(S^c\\cap +)=P(+\\mid S^c)\\,P(S^c)=(0.08)(0.84)=0.0672.\n\\]\nFrom part 7 and part 2: \\[\nP(S^c\\cap -)=P(-\\mid S^c)\\,P(S^c)=(0.92)(0.84)=0.7728.\n\\]\n\nCheck (with (P(S+)) from part 5): \\[\n0.144+0.016+0.0672+0.7728=1.\n\\]\n9) Independence check (\\(S\\) vs \\(+\\))\nIndependent would require \\(P(S\\cap +)=P(S)\\,P(+)\\).\n\\[\nP(S)\\,P(+)=0.16\\cdot 0.2112=0.033792\\neq 0.144\n\\] So \\(S\\) and \\(+\\) are not independent.\n10) \\(P(S\\cup +)\\)\n\\[\nP(S\\cup +)=P(S)+P(+)-P(S\\cap +)\n=0.16+0.2112-0.144\n=0.2272\n\\]\n11) \\(2\\times 2\\) probability table\n\n\n\n\n\\(+\\)\n\\(-\\)\nRow total\n\n\n\n\n\\(S\\)\n\\(0.144\\)\n\\(0.016\\)\n\\(0.160\\)\n\n\n\\(S^c\\)\n\\(0.0672\\)\n\\(0.7728\\)\n\\(0.840\\)\n\n\nCol total\n0.2112\n0.7888\n1.000\n\n\n\nRow/column totals match; the four cells sum to \\(1\\).",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-4.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-4.html#before-you-leave",
    "title": "Lesson 4: Conditional Probability",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\nProject Milestone 2\nAnnex B\n\n\n\n\nLesson 5\n\n\n\n\n\nTidyverse Tutorial: In Class Lesson 5\nProject Milestone 2: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-6.html",
    "href": "MA206-AY26-1/lesson-6.html",
    "title": "Lesson 6: Discrete Distributions of Random Variables",
    "section": "",
    "text": "Math 1 vs Math 2\n\n\n\n\n\n\nPreviously 1-0\n\n\n\n\n\n\n2-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLets render a document\nDon’t forget Annex B\n\n\n\n\n\n\n\n\n\nNot that type of discrete random variable…\nA random variable is a rule (or function) that assigns a number to each possible outcome of a random process.\n\nDiscrete random variable: takes on a countable set of values (like dice rolls, coin flips, number of emails received in a day).\n\nContinuous random variable: takes on values from an interval or continuum (like height, time, or temperature).\n\n\n\nLet \\(X\\in\\{1,2,3,4,5,6\\}\\) be the points shown on one fair die roll.\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P(X=x)\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\n\n\n\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean or expected value of a discrete random variable refers to the long-run average value of the random process if the process is repeated indefinitely under identical conditions. The mean of the random variable is a weighted average of the possible values the random variable can take, weighting each outcome according to its probability:\n\\[\n\\mu_X = E[X] = \\sum_x x \\cdot P(X=x).\n\\]\n\\[\nE[X] = 1\\cdot\\tfrac16 + 2\\cdot\\tfrac16 + 3\\cdot\\tfrac16 + 4\\cdot\\tfrac16 + 5\\cdot\\tfrac16 + 6\\cdot\\tfrac16\n= \\tfrac{21}{6} = 3.5.\n\\]\n\n\n\nOver many rolls, the long-run average face value is \\(3.5\\) (even though no single roll shows \\(3.5\\)).\n\n\n\nThe variance of a discrete random variable measures the average squared deviation from its mean: \\[\n\\mathrm{Var}(X)=\\sum_x (x-\\mu_X)^2\\,P(X=x)\n\\] and the standard deviation is \\[\n\\mathrm{SD}(X)=\\sqrt{\\mathrm{Var}(X)}.\n\\]\n\\[\n\\mathrm{Var}(X)=\\left(1-3.5\\right)^2\\cdot\\tfrac16\n+\\left(2-3.5\\right)^2\\cdot\\tfrac16\n+\\left(3-3.5\\right)^2\\cdot\\tfrac16\n+\\left(4-3.5\\right)^2\\cdot\\tfrac16\n+\\left(5-3.5\\right)^2\\cdot\\tfrac16\n+\\left(6-3.5\\right)^2\\cdot\\tfrac16\n=\\tfrac{35}{12}\n\\]\n\\[\n\\mathrm{SD}(X)=\\sqrt{\\tfrac{35}{12}}\\approx1.7078.\n\\]\n\n\n\nExample A — Fair die\nLarger variance/SD indicates outcomes are more spread out around the mean; smaller variance/SD indicates the outcomes are more tightly clustered.\nBecause most values of a distribution tend to fall within about two standard deviations of the mean, we would expect nearly all rolls to land between about \\(3.5 - 2(1.71)\\approx0\\) and \\(3.5 + 2(1.71)\\approx7.0\\). Since the die only takes values \\(1\\) through \\(6\\), this tells us practically all the possible outcomes fall within that range around the mean.\n\n\n\n\nProbability that \\(X \\leq 2\\)\n\\[\nP(X \\leq 2) = P(X=1) + P(X=2) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]\nProbability that \\(X\\) is less than one standard deviation above the mean\n\n\n\nMean: \\(\\mu = 3.5\\)\n\nStandard deviation: \\(\\sigma \\approx 1.71\\)\n\nOne standard deviation above the mean: \\(\\mu + \\sigma \\approx 3.5 + 1.71 = 5.21\\)\n\nSo we want \\(P(X &lt; 5.21)\\), i.e. \\(P(X \\in \\{1,2,3,4,5\\})\\).\n\\[\nP(X &lt; 5.21) = 5 \\cdot \\tfrac{1}{6} = \\tfrac{5}{6}.\n\\]\n\nProbability that \\(X &gt; 4\\)\n\\[\nP(X &gt; 4) = P(X=5) + P(X=6) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]\n\n\n\n\n\n4 Basketball Shots, \\(p=\\tfrac12\\)\nA player takes 4 shots. Each shot has probability \\(p=\\tfrac12\\) of going in.\nLet \\(Y =\\) the number of made shots in 4 independent attempts.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nLet M = make, X = miss.\n\n\\(Y=0\\) (no makes): \\(XXXX\\)\n\n\\(Y=1\\) (exactly one make): \\(MXXX, XMXX, XXMX, XXXM\\)\n\n\\(Y=2\\) (exactly two makes): \\(MMXX, MXMX, MXXM, XMMX, XMXM, XXMM\\)\n\n\\(Y=3\\) (exactly three makes): \\(MMMX, MMXM, MXMM, XMMM\\)\n\n\\(Y=4\\) (all makes): \\(MMMM\\)\n\nSo:\n\\(P(Y=0)=\\tfrac{1}{16}, \\quad\nP(Y=1)=\\tfrac{4}{16}, \\quad\nP(Y=2)=\\tfrac{6}{16}, \\quad\nP(Y=3)=\\tfrac{4}{16}, \\quad\nP(Y=4)=\\tfrac{1}{16}.\\)\n\n\n\n\\(k\\) (makes)\n0\n1\n2\n3\n4\n\n\n\n\n\\(P(Y=k)\\)\n\\(1/16\\)\n\\(4/16\\)\n\\(6/16\\)\n\\(4/16\\)\n\\(1/16\\)\n\n\n\nAnd in function form:\n\\[\nP(Y=k) =\n\\begin{cases}\n\\dfrac{1}{16}, & k=0, \\\\[6pt]\n\\dfrac{4}{16}, & k=1, \\\\[6pt]\n\\dfrac{6}{16}, & k=2, \\\\[6pt]\n\\dfrac{4}{16}, & k=3, \\\\[6pt]\n\\dfrac{1}{16}, & k=4, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\\(E[Y] = \\sum_{k=0}^4 k \\, P(Y=k)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(E[Y] = 0 \\, P(Y=0) + 1 \\, P(Y=1) + 2 \\, P(Y=2) + 3 \\, P(Y=3) + 4 \\, P(Y=4)\\)\n\\(= 0 \\, \\tfrac{1}{16} + 1 \\, \\tfrac{4}{16} + 2 \\, \\tfrac{6}{16} + 3 \\, \\tfrac{4}{16} + 4 \\, \\tfrac{1}{16}\\)\n\\(= \\tfrac{32}{16} = 2\\)\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOn average, the player makes 2 shots out of 4.\nThis is the long-run average across many 4-shot sequences, even though most sequences are not exactly 2 makes.\n\n\n\n\n\n\n\\(\\mathrm{Var}(Y) = \\sum_{k=0}^4 (k - E[Y])^2 \\cdot P(Y=k).\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\mathrm{Var}(Y)\n= (0-2)^2\\cdot\\tfrac{1}{16}\n+ (1-2)^2\\cdot\\tfrac{4}{16}\n+ (2-2)^2\\cdot\\tfrac{6}{16}\n+ (3-2)^2\\cdot\\tfrac{4}{16}\n+ (4-2)^2\\cdot\\tfrac{1}{16}.\\)\n\\(= 4\\cdot\\tfrac{1}{16} + 1\\cdot\\tfrac{4}{16} + 0\\cdot\\tfrac{6}{16} + 1\\cdot\\tfrac{4}{16} + 4\\cdot\\tfrac{1}{16}\n= \\tfrac{16}{16} = 1.\\)\n\\(\\mathrm{SD}(Y) = \\sqrt{1} = 1.\\)\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe standard deviation of 1 means that the number of made shots is typically within about 1 of the mean.\nSo in most 4-shot sequences, the player makes about 1 to 3 shots.\n\n\n\n\n\n\n\nProbability that \\(Y \\leq 1\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(Y \\leq 1) = P(Y=0) + P(Y=1) = \\tfrac{1}{16} + \\tfrac{4}{16} = \\tfrac{5}{16}.\n\\]\n\n\n\n\nProbability that \\(Y\\) is less than one standard deviation above the mean\n\n\nMean: \\(E[Y] = 2\\)\n\nStandard deviation: \\(\\mathrm{SD}(Y) = 1\\)\n\nOne standard deviation above the mean: \\(2 + 1 = 3\\)\n\nSo we want \\(P(Y &lt; 3)\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(Y &lt; 3) = P(Y=0) + P(Y=1) + P(Y=2)  \n= \\tfrac{1}{16} + \\tfrac{4}{16} + \\tfrac{6}{16}  \n= \\tfrac{11}{16}.\n\\]\n\n\n\n\nProbability that \\(Y &gt; 2\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(Y &gt; 2) = P(Y=3) + P(Y=4) = \\tfrac{4}{16} + \\tfrac{1}{16} = \\tfrac{5}{16}.\n\\]\n\n\n\n\n\n\n\nA bag contains 3 balls: one red, one blue, and one green.\nYou draw one ball at random. Let\n\\(X =\\) the number of points you score, where\n- red = 3 points\n- blue = 6 points\n- green = 12 points\n\nWhat is the probability distribution of \\(X\\)?\n\nPlot the probability distribution.\nFind the expected value \\(E[X]\\).\n\nInterpret the expected value.\n\nFind the variance and standard deviation of \\(X\\).\n\nInterpret the variance and standard deviation.\n\nWhat is the probability that \\(X \\leq 6\\)?\n\nWhat is the probability that \\(X\\) is less than one standard deviation above the mean?\n\nWhat is the probability that \\(X &gt; 6\\)?\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nEach outcome is equally likely:\n\n\\(P(X=3) = \\tfrac{1}{3}, \\quad P(X=6) = \\tfrac{1}{3}, \\quad P(X=12) = \\tfrac{1}{3}\\)\n\n\n\n\\(x\\)\n3\n6\n12\n\n\n\n\n\\(P(X=x)\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1/3\\)\n\n\n\n\nPlot the probability distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(E[X] = 3 \\, P(X=3) + 6 \\, P(X=6) + 12 \\, P(X=12)\\)\n\\(= 3 \\cdot \\tfrac{1}{3} + 6 \\cdot \\tfrac{1}{3} + 12 \\cdot \\tfrac{1}{3}\\)\n\\(= \\tfrac{21}{3} = 7\\)\n\n\n\nOn average, you score 7 points per draw.\nThis doesn’t mean you ever score exactly 7, but across many draws, the outcomes balance to that average.\n\n\n\n\\(\\mathrm{Var}(X) = (3-7)^2 \\, \\tfrac{1}{3} + (6-7)^2 \\, \\tfrac{1}{3} + (12-7)^2 \\, \\tfrac{1}{3}\\)\n\\(= (16)\\tfrac{1}{3} + (1)\\tfrac{1}{3} + (25)\\tfrac{1}{3}\\)\n\\(= \\tfrac{42}{3} = 14\\)\n\\(\\mathrm{SD}(X) = \\sqrt{14} \\approx 3.742\\)\n\n\n\nMost outcomes fall within about 3.7 points of the mean of 7.\nThat means the scores vary moderately, depending on whether you draw the low (3), medium (6), or high (12) ball.\n\n\n\n\\[\nP(X \\leq 6) = P(X=3) + P(X=6) = \\tfrac{1}{3} + \\tfrac{1}{3} = \\tfrac{2}{3}.\n\\]\n\n\n\nMean \\(= 7\\), \\(\\ \\mathrm{SD}(X) \\approx 3.742\\), so one standard deviation above the mean is about \\(10.742\\).\n\\[\nP(X &lt; 10.742) = P(X=3) + P(X=6) = \\tfrac{2}{3}.\n\\]\n\n\n\n\\[\nP(X &gt; 6) = P(X=12) = \\tfrac{1}{3}.\n\\]\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\nLesson 7\n\n\n\n\n\nMilestone 2 / Tidyverse Tutorial: Due 0700 Lesson 7\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 6"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-6.html#discrete-distributions-of-random-variables",
    "href": "MA206-AY26-1/lesson-6.html#discrete-distributions-of-random-variables",
    "title": "Lesson 6: Discrete Distributions of Random Variables",
    "section": "",
    "text": "Not that type of discrete random variable…\nA random variable is a rule (or function) that assigns a number to each possible outcome of a random process.\n\nDiscrete random variable: takes on a countable set of values (like dice rolls, coin flips, number of emails received in a day).\n\nContinuous random variable: takes on values from an interval or continuum (like height, time, or temperature).\n\n\n\nLet \\(X\\in\\{1,2,3,4,5,6\\}\\) be the points shown on one fair die roll.\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P(X=x)\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\\(1/6\\)\n\n\n\n\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean or expected value of a discrete random variable refers to the long-run average value of the random process if the process is repeated indefinitely under identical conditions. The mean of the random variable is a weighted average of the possible values the random variable can take, weighting each outcome according to its probability:\n\\[\n\\mu_X = E[X] = \\sum_x x \\cdot P(X=x).\n\\]\n\\[\nE[X] = 1\\cdot\\tfrac16 + 2\\cdot\\tfrac16 + 3\\cdot\\tfrac16 + 4\\cdot\\tfrac16 + 5\\cdot\\tfrac16 + 6\\cdot\\tfrac16\n= \\tfrac{21}{6} = 3.5.\n\\]\n\n\n\nOver many rolls, the long-run average face value is \\(3.5\\) (even though no single roll shows \\(3.5\\)).\n\n\n\nThe variance of a discrete random variable measures the average squared deviation from its mean: \\[\n\\mathrm{Var}(X)=\\sum_x (x-\\mu_X)^2\\,P(X=x)\n\\] and the standard deviation is \\[\n\\mathrm{SD}(X)=\\sqrt{\\mathrm{Var}(X)}.\n\\]\n\\[\n\\mathrm{Var}(X)=\\left(1-3.5\\right)^2\\cdot\\tfrac16\n+\\left(2-3.5\\right)^2\\cdot\\tfrac16\n+\\left(3-3.5\\right)^2\\cdot\\tfrac16\n+\\left(4-3.5\\right)^2\\cdot\\tfrac16\n+\\left(5-3.5\\right)^2\\cdot\\tfrac16\n+\\left(6-3.5\\right)^2\\cdot\\tfrac16\n=\\tfrac{35}{12}\n\\]\n\\[\n\\mathrm{SD}(X)=\\sqrt{\\tfrac{35}{12}}\\approx1.7078.\n\\]\n\n\n\nExample A — Fair die\nLarger variance/SD indicates outcomes are more spread out around the mean; smaller variance/SD indicates the outcomes are more tightly clustered.\nBecause most values of a distribution tend to fall within about two standard deviations of the mean, we would expect nearly all rolls to land between about \\(3.5 - 2(1.71)\\approx0\\) and \\(3.5 + 2(1.71)\\approx7.0\\). Since the die only takes values \\(1\\) through \\(6\\), this tells us practically all the possible outcomes fall within that range around the mean.\n\n\n\n\nProbability that \\(X \\leq 2\\)\n\\[\nP(X \\leq 2) = P(X=1) + P(X=2) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]\nProbability that \\(X\\) is less than one standard deviation above the mean\n\n\n\nMean: \\(\\mu = 3.5\\)\n\nStandard deviation: \\(\\sigma \\approx 1.71\\)\n\nOne standard deviation above the mean: \\(\\mu + \\sigma \\approx 3.5 + 1.71 = 5.21\\)\n\nSo we want \\(P(X &lt; 5.21)\\), i.e. \\(P(X \\in \\{1,2,3,4,5\\})\\).\n\\[\nP(X &lt; 5.21) = 5 \\cdot \\tfrac{1}{6} = \\tfrac{5}{6}.\n\\]\n\nProbability that \\(X &gt; 4\\)\n\\[\nP(X &gt; 4) = P(X=5) + P(X=6) = \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{2}{6} = \\tfrac{1}{3}.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 6"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-6.html#practice-problem",
    "href": "MA206-AY26-1/lesson-6.html#practice-problem",
    "title": "Lesson 6: Discrete Distributions of Random Variables",
    "section": "",
    "text": "4 Basketball Shots, \\(p=\\tfrac12\\)\nA player takes 4 shots. Each shot has probability \\(p=\\tfrac12\\) of going in.\nLet \\(Y =\\) the number of made shots in 4 independent attempts.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nLet M = make, X = miss.\n\n\\(Y=0\\) (no makes): \\(XXXX\\)\n\n\\(Y=1\\) (exactly one make): \\(MXXX, XMXX, XXMX, XXXM\\)\n\n\\(Y=2\\) (exactly two makes): \\(MMXX, MXMX, MXXM, XMMX, XMXM, XXMM\\)\n\n\\(Y=3\\) (exactly three makes): \\(MMMX, MMXM, MXMM, XMMM\\)\n\n\\(Y=4\\) (all makes): \\(MMMM\\)\n\nSo:\n\\(P(Y=0)=\\tfrac{1}{16}, \\quad\nP(Y=1)=\\tfrac{4}{16}, \\quad\nP(Y=2)=\\tfrac{6}{16}, \\quad\nP(Y=3)=\\tfrac{4}{16}, \\quad\nP(Y=4)=\\tfrac{1}{16}.\\)\n\n\n\n\\(k\\) (makes)\n0\n1\n2\n3\n4\n\n\n\n\n\\(P(Y=k)\\)\n\\(1/16\\)\n\\(4/16\\)\n\\(6/16\\)\n\\(4/16\\)\n\\(1/16\\)\n\n\n\nAnd in function form:\n\\[\nP(Y=k) =\n\\begin{cases}\n\\dfrac{1}{16}, & k=0, \\\\[6pt]\n\\dfrac{4}{16}, & k=1, \\\\[6pt]\n\\dfrac{6}{16}, & k=2, \\\\[6pt]\n\\dfrac{4}{16}, & k=3, \\\\[6pt]\n\\dfrac{1}{16}, & k=4, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\\(E[Y] = \\sum_{k=0}^4 k \\, P(Y=k)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(E[Y] = 0 \\, P(Y=0) + 1 \\, P(Y=1) + 2 \\, P(Y=2) + 3 \\, P(Y=3) + 4 \\, P(Y=4)\\)\n\\(= 0 \\, \\tfrac{1}{16} + 1 \\, \\tfrac{4}{16} + 2 \\, \\tfrac{6}{16} + 3 \\, \\tfrac{4}{16} + 4 \\, \\tfrac{1}{16}\\)\n\\(= \\tfrac{32}{16} = 2\\)\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOn average, the player makes 2 shots out of 4.\nThis is the long-run average across many 4-shot sequences, even though most sequences are not exactly 2 makes.\n\n\n\n\n\n\n\\(\\mathrm{Var}(Y) = \\sum_{k=0}^4 (k - E[Y])^2 \\cdot P(Y=k).\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\mathrm{Var}(Y)\n= (0-2)^2\\cdot\\tfrac{1}{16}\n+ (1-2)^2\\cdot\\tfrac{4}{16}\n+ (2-2)^2\\cdot\\tfrac{6}{16}\n+ (3-2)^2\\cdot\\tfrac{4}{16}\n+ (4-2)^2\\cdot\\tfrac{1}{16}.\\)\n\\(= 4\\cdot\\tfrac{1}{16} + 1\\cdot\\tfrac{4}{16} + 0\\cdot\\tfrac{6}{16} + 1\\cdot\\tfrac{4}{16} + 4\\cdot\\tfrac{1}{16}\n= \\tfrac{16}{16} = 1.\\)\n\\(\\mathrm{SD}(Y) = \\sqrt{1} = 1.\\)\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe standard deviation of 1 means that the number of made shots is typically within about 1 of the mean.\nSo in most 4-shot sequences, the player makes about 1 to 3 shots.\n\n\n\n\n\n\n\nProbability that \\(Y \\leq 1\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(Y \\leq 1) = P(Y=0) + P(Y=1) = \\tfrac{1}{16} + \\tfrac{4}{16} = \\tfrac{5}{16}.\n\\]\n\n\n\n\nProbability that \\(Y\\) is less than one standard deviation above the mean\n\n\nMean: \\(E[Y] = 2\\)\n\nStandard deviation: \\(\\mathrm{SD}(Y) = 1\\)\n\nOne standard deviation above the mean: \\(2 + 1 = 3\\)\n\nSo we want \\(P(Y &lt; 3)\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(Y &lt; 3) = P(Y=0) + P(Y=1) + P(Y=2)  \n= \\tfrac{1}{16} + \\tfrac{4}{16} + \\tfrac{6}{16}  \n= \\tfrac{11}{16}.\n\\]\n\n\n\n\nProbability that \\(Y &gt; 2\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(Y &gt; 2) = P(Y=3) + P(Y=4) = \\tfrac{4}{16} + \\tfrac{1}{16} = \\tfrac{5}{16}.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 6"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-6.html#board-problem",
    "href": "MA206-AY26-1/lesson-6.html#board-problem",
    "title": "Lesson 6: Discrete Distributions of Random Variables",
    "section": "",
    "text": "A bag contains 3 balls: one red, one blue, and one green.\nYou draw one ball at random. Let\n\\(X =\\) the number of points you score, where\n- red = 3 points\n- blue = 6 points\n- green = 12 points\n\nWhat is the probability distribution of \\(X\\)?\n\nPlot the probability distribution.\nFind the expected value \\(E[X]\\).\n\nInterpret the expected value.\n\nFind the variance and standard deviation of \\(X\\).\n\nInterpret the variance and standard deviation.\n\nWhat is the probability that \\(X \\leq 6\\)?\n\nWhat is the probability that \\(X\\) is less than one standard deviation above the mean?\n\nWhat is the probability that \\(X &gt; 6\\)?\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nEach outcome is equally likely:\n\n\\(P(X=3) = \\tfrac{1}{3}, \\quad P(X=6) = \\tfrac{1}{3}, \\quad P(X=12) = \\tfrac{1}{3}\\)\n\n\n\n\\(x\\)\n3\n6\n12\n\n\n\n\n\\(P(X=x)\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1/3\\)\n\n\n\n\nPlot the probability distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(E[X] = 3 \\, P(X=3) + 6 \\, P(X=6) + 12 \\, P(X=12)\\)\n\\(= 3 \\cdot \\tfrac{1}{3} + 6 \\cdot \\tfrac{1}{3} + 12 \\cdot \\tfrac{1}{3}\\)\n\\(= \\tfrac{21}{3} = 7\\)\n\n\n\nOn average, you score 7 points per draw.\nThis doesn’t mean you ever score exactly 7, but across many draws, the outcomes balance to that average.\n\n\n\n\\(\\mathrm{Var}(X) = (3-7)^2 \\, \\tfrac{1}{3} + (6-7)^2 \\, \\tfrac{1}{3} + (12-7)^2 \\, \\tfrac{1}{3}\\)\n\\(= (16)\\tfrac{1}{3} + (1)\\tfrac{1}{3} + (25)\\tfrac{1}{3}\\)\n\\(= \\tfrac{42}{3} = 14\\)\n\\(\\mathrm{SD}(X) = \\sqrt{14} \\approx 3.742\\)\n\n\n\nMost outcomes fall within about 3.7 points of the mean of 7.\nThat means the scores vary moderately, depending on whether you draw the low (3), medium (6), or high (12) ball.\n\n\n\n\\[\nP(X \\leq 6) = P(X=3) + P(X=6) = \\tfrac{1}{3} + \\tfrac{1}{3} = \\tfrac{2}{3}.\n\\]\n\n\n\nMean \\(= 7\\), \\(\\ \\mathrm{SD}(X) \\approx 3.742\\), so one standard deviation above the mean is about \\(10.742\\).\n\\[\nP(X &lt; 10.742) = P(X=3) + P(X=6) = \\tfrac{2}{3}.\n\\]\n\n\n\n\\[\nP(X &gt; 6) = P(X=12) = \\tfrac{1}{3}.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 6"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-6.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-6.html#before-you-leave",
    "title": "Lesson 6: Discrete Distributions of Random Variables",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\nLesson 7\n\n\n\n\n\nMilestone 2 / Tidyverse Tutorial: Due 0700 Lesson 7\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 6"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html",
    "href": "MA206-AY26-1/lesson-8.html",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "Math 1 vs DPE\n\n\n\n\n\n\nPreviously 2-0\n\n\n\n\n\n\n3-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nThis is the probability mass function of a fair die.\nWhat do we know about it?\n\nProbabilities are non-negative: \\(P(X=x) \\geq 0\\).\n\nThe total probability is 1:\n\\[\n\\sum_x P(X=x) = 1\n\\]\nWe can compute probabilities of events by adding up the masses.\n\nExample: \\(P(X \\leq 3) = \\tfrac{1}{6} + \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{1}{2}\\).\n\n\n\n\n\nThis probability distribution function isn’t all that different from the discrete version.\nHere are the rules that all PDFs must follow:\n\n\\(f(x) \\geq 0\\) for all \\(x\\).\n\nThe total area under the curve is 1:\n\\[\n\\int_{-\\infty}^{\\infty} f(x)\\,dx = 1\n\\]\nProbabilities are found by areas, not points:\n\\[\nP(a \\leq X \\leq b) = \\int_a^b f(x)\\,dx\n\\]\nFor any single point,\n\\[\nP(X=c) = 0\n\\]\n\n\n\n\nConsider the function\n\\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\leq x \\leq 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nCheck the properties:\n\nNonnegative?\n\\(f(x) = 2x \\geq 0\\) on \\([0,1]\\). ✅\nTotal area?\n\\[\n\\int_{-\\infty}^\\infty f(x)\\,dx = \\int_0^1 2x\\,dx = \\left[x^2\\right]_0^1 = 1\n\\] So it integrates to 1. ✅\n\n\n\n\n\n\n\n\n\n\n\nProbability of an interval?\n\\[\nP(0.2 \\leq X \\leq 0.5) = \\int_{0.2}^{0.5} 2x \\, dx\n= \\left[x^2\\right]_{0.2}^{0.5}\n= (0.25 - 0.04) = 0.21\n\\]\n\n\n\n\n\n\n\n\n\n\nSo this is a valid PDF! It’s a “triangle-shaped” distribution on \\([0,1]\\) that places more weight near 1 than near 0.\n\nProbability of a single point is zero: \\(P(X=c)=0\\).\nFor a continuous random variable with PDF \\(f(x)\\), probability comes from area, so any single point has zero width: \\[\nP(X=c) \\;=\\; \\int_{c}^{c} f(x)\\,dx \\;=\\; 0.\n\\]\n\nFor example: \\(P(X=.6)=0\\)\n\\[\nP(X=.6) \\;=\\; \\int_{.6}^{.6} f(x)\\,dx \\;=\\; 0.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 1, \\\\[6pt]\n\\dfrac{1}{6}, & 1 \\leq x &lt; 2, \\\\[6pt]\n\\dfrac{2}{6}, & 2 \\leq x &lt; 3, \\\\[6pt]\n\\dfrac{3}{6}, & 3 \\leq x &lt; 4, \\\\[6pt]\n\\dfrac{4}{6}, & 4 \\leq x &lt; 5, \\\\[6pt]\n\\dfrac{5}{6}, & 5 \\leq x &lt; 6, \\\\[6pt]\n1, & x \\geq 6.\n\\end{cases}\n\\]\nThis step function is the CDF of the discrete die. It accumulates probability as we move to the right.\n\n\n\nFor any random variable \\(X\\) (discrete or continuous), the cumulative distribution function is\n\\[\nF(x) \\;=\\; P(X \\le x).\n\\]\nThings that are true of all CDFs:\n- Nondecreasing: if \\(a &lt; b\\) then \\(F(a) \\le F(b)\\).\n- Right-continuous: \\(\\lim_{x \\downarrow c} F(x) = F(c)\\).\n - Event probabilities from \\(F\\): for any \\(a &lt; b\\),\n\\[\nP(a \\le X \\le b) \\;=\\; F(b) - F(a).\n\\]\n\n\n\nRecall our PDF: \\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\le x \\le 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nIntegrate to get \\(F\\):\n\\[\nF(x) \\;=\\; \\int_{0}^{x} 2t\\,dt \\;=\\; x^{2}.\n\\]\nSo for any \\(x\\) in \\([0,1]\\),\n\\[\nF(x) = x^2.\n\\]\nQuick checks:\n- \\(F\\) is nondecreasing and continuous (no jumps).\n- \\(F(0)=0\\), \\(F(1)=1\\).\n- Example reads: \\(F(0.5)=0.5^2=0.25\\), \\(F(0.7)=0.49\\).\n- Interval probability via CDF:\n\\[\nP(0.2 \\le X \\le 0.5) = F(0.5) - F(0.2) = 0.25 - 0.04 = 0.21.\n\\]\n\n\n\n\n\n\n\n\n\nReading probabilities from the CDF:\n\n\\(P(X \\le 0.5) = F(0.5) = 0.25\\)\n\\[\nP(X \\le 0.5) = \\int_{0}^{0.5} 2t\\,dt = 0.25\n\\]\n\\(P(X &gt; 0.7) = 1 - F(0.7) = 1 - 0.49 = 0.51\\)\n\\[\nP(X &gt; 0.7) = \\int_{0.7}^{1} 2t\\,dt = 0.51\n\\]\n\\(P(0.2 \\le X \\le 0.5) = F(0.5) - F(0.2) = 0.21\\)\n\\[\nP(0.2 \\le X \\le 0.5) = \\int_{0.2}^{0.5} 2t\\,dt = 0.21\n\\]\n\n\n\n\n\n\n\nFor a discrete random variable (like a fair die), the expected value is the weighted average of the possible outcomes:\n\\[\nE[X] = \\sum_x x \\cdot P(X=x).\n\\]\nFor the fair die: \\[\nE[X] = 1\\cdot \\tfrac{1}{6} + 2\\cdot \\tfrac{1}{6} + 3\\cdot \\tfrac{1}{6} + 4\\cdot \\tfrac{1}{6} + 5\\cdot \\tfrac{1}{6} + 6\\cdot \\tfrac{1}{6} = 3.5\n\\]\n\n\n\nFor a continuous random variable with PDF \\(f(x)\\), the expected value is defined by an integral:\n\\[\nE[X] = \\int_{-\\infty}^{\\infty} x \\, f(x)\\,dx.\n\\]\nThis is the continuous version of the same weighted average idea — instead of summing over points, we integrate over the real line.\n\n\n\nRecall our PDF:\n\\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\leq x \\leq 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nCompute the expected value:\n\\[\nE[X] = \\int_{0}^{1} x \\cdot (2x)\\,dx = \\int_{0}^{1} 2x^{2}\\,dx.\n\\]\n\\[\nE[X] = \\left[\\tfrac{2}{3}x^{3}\\right]_{0}^{1} = \\tfrac{2}{3}.\n\\]\nSo the mean of this distribution is \\(\\tfrac{2}{3} \\approx 0.667\\).\n\n\n\n\n\n\nFor a discrete random variable, variance measures how spread out the values are around the mean:\n\\[\n\\text{Var}(X) = \\sum_x (x - \\mu)^2 \\, P(X=x),\n\\]\nwhere \\(\\mu = E[X]\\).\nThe standard deviation is just the square root of the variance:\n\\[\n\\sigma = \\sqrt{\\text{Var}(X)}.\n\\]\nFor a fair six-sided die:\n\nWe already know the mean is\n\\[\nE[X] = \\frac{1+2+3+4+5+6}{6} = 3.5.\n\\]\nCompute the variance:\n\\[\n\\text{Var}(X) = \\sum_{x=1}^{6} (x - 3.5)^2 \\cdot \\tfrac{1}{6}.\n\\]\n\n\\[\n= \\frac{(1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-3.5)^2}{6}\n= \\frac{35}{12} \\approx 2.92.\n\\]\n\nThen the standard deviation is\n\\[\n\\sigma = \\sqrt{\\tfrac{35}{12}} \\approx 1.71.\n\\]\n\nSo the die’s outcomes are typically about 1.7 away from the mean value of 3.5.\n\n\n\nFor a continuous random variable with PDF \\(f(x)\\), we replace the sum with an integral:\n\\[\n\\text{Var}(X) = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x)\\,dx,\n\\]\nwhere \\(\\mu = E[X]\\).\nBut this integral is often hard to compute directly, so there is a shortcut formula we can use:\n\\[\n\\text{Var}(X) = E[X^2] - \\big(E[X]\\big)^2,\n\\]\nwhere\n\\[\nE[X^2] = \\int_{-\\infty}^{\\infty} x^2 f(x)\\,dx.\n\\]\n\n\n\n\\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\leq x \\leq 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nFrom before, the mean is \\(\\mu = E[X] = \\tfrac{2}{3}\\).\nCompute \\(E[X^2]\\):\n\\[\nE[X^2] = \\int_{0}^{1} x^2 \\cdot (2x)\\,dx\n= \\int_{0}^{1} 2x^3\\,dx\n= \\left[\\tfrac{1}{2}x^4\\right]_{0}^{1} = \\tfrac{1}{2}.\n\\]\nNow variance:\n\\[\n\\text{Var}(X) = \\tfrac{1}{2} - \\left(\\tfrac{2}{3}\\right)^2\n= \\tfrac{1}{2} - \\tfrac{4}{9}\n= \\tfrac{1}{18}.\n\\]\nStandard deviation:\n\\[\n\\sigma = \\sqrt{\\tfrac{1}{18}} \\approx 0.236.\n\\]\n\n\n\n\n\nConsider the random variable \\(X\\) with PDF\n\\[\nf(x) =\n\\begin{cases}\n2(1-x), & 0 \\le x \\le 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n1. Plot the PDF\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Find \\(P(X = 0.4)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor continuous random variables:\n\\[\nP(X=0.4) = \\int_{0.4}^{0.4} f(x)\\,dx = 0.\n\\]\n\n\n\n3. Find \\(P(0.2 \\le X \\le 0.6)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(0.2 \\le X \\le 0.6) = \\int_{0.2}^{0.6} 2(1-x)\\,dx\n= \\Big[2x - x^2\\Big]_{0.2}^{0.6}.\n\\]\nAt \\(0.6\\): \\(2(0.6)-0.6^2=0.84\\).\nAt \\(0.2\\): \\(0.36\\).\nDifference: \\(0.48\\).\n\n\n\n4. Find the CDF \\(F(x)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor \\(0 \\le x \\le 1\\):\n\\[\nF(x) = \\int_{0}^{x} 2(1-t)\\,dt = 2x - x^2.\n\\]\nSo\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 0, \\\\\n2x - x^2, & 0 \\le x \\le 1, \\\\\n1, & x \\ge 1.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n5. Find \\(P(0.2 \\le X \\le 0.6)\\) using the CDF\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(0.2 \\le X \\le 0.6) = F(0.6) - F(0.2).\n\\]\n\\(F(0.6)=0.84\\), \\(F(0.2)=0.36\\), difference = \\(0.48\\).\n✅ Same as before.\n\n\n\n6. Find the expected value \\(E[X]\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nE[X] = \\int_{0}^{1} x \\cdot 2(1-x)\\,dx\n= \\int_{0}^{1} (2x - 2x^2)\\,dx.\n\\]\n\\[\n= \\Big[x^2 - \\tfrac{2}{3}x^3\\Big]_0^1\n= \\tfrac{1}{3}.\n\\]\n\n\n\n7. Find the variance \\(\\text{Var}(X)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCompute \\(E[X^2]\\):\n\\[\nE[X^2] = \\int_{0}^{1} x^2 \\cdot 2(1-x)\\,dx\n= \\tfrac{1}{6}.\n\\]\nSo\n\\[\n\\text{Var}(X) = E[X^2] - (E[X])^2\n= \\tfrac{1}{6} - \\left(\\tfrac{1}{3}\\right)^2\n= \\tfrac{1}{18}.\n\\]\n\n\n\n8. Find the standard deviation\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\n\\sigma = \\sqrt{\\tfrac{1}{18}} \\approx 0.236.\n\\]\n\n\n\n9. Find \\(P(\\mu - 2\\sigma \\le X \\le \\mu + 2\\sigma)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\mu=\\tfrac{1}{3}\\), \\(\\sigma \\approx 0.236\\), so \\(2\\sigma \\approx 0.472\\).\nRange: \\([\\tfrac{1}{3} - 0.472,\\;\\tfrac{1}{3} + 0.472] = [-0.139,\\;0.805]\\).\nSince the support is \\([0,1]\\), we use \\([0,0.805]\\).\n\\[\nP(0 \\le X \\le 0.805) = F(0.805) - F(0) = F(0.805).\n\\]\n\\(F(0.805) = 2(0.805) - (0.805)^2 \\approx 0.962\\).\n\n\n\n10. Find \\(P(X &lt; \\mu - \\sigma \\;\\text{ or }\\; X &gt; \\mu + \\sigma)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\mu=\\tfrac{1}{3}\\), \\(\\sigma \\approx 0.236\\).\nInterval within 1 SD: \\([0.097,\\,0.569]\\).\nSo the probability outside is\n\\[\nP(X &lt; 0.097) + P(X &gt; 0.569).\n\\]\nUsing the CDF:\n\\[\nP(X &lt; 0.097) = F(0.097) \\approx 0.185,\n\\] \\[\nP(X &gt; 0.569) = 1 - F(0.569) \\approx 1 - 0.814 = 0.186.\n\\]\nTotal = \\(0.185 + 0.186 = 0.371\\).\n\n\n\n\n\n\nConsider the random variable \\(X\\) with PDF\n\\[\nf(x) =\n\\begin{cases}\n\\dfrac{1}{4}, & 2 \\le x \\le 6,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nPlot the PDF\n\nFind \\(P(X = 3)\\)\n\nFind \\(P(2.5 \\le X \\le 4.5)\\)\n\nFind the CDF \\(F(x)\\)\n\nFind \\(P(2.5 \\le X \\le 4.5)\\) using the CDF. Is it the same as before?\n\nFind the expected value \\(E[X]\\)\n\nFind the variance \\(\\mathrm{Var}(X)\\)\n\nFind the standard deviation\n\nFind \\(P(\\mu - 2\\sigma \\le X \\le \\mu + 2\\sigma)\\)\n\nFind \\(P(X &lt; \\mu - \\sigma \\;\\text{ or }\\; X &gt; \\mu + \\sigma)\\)\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n1. Plot the PDF\n\n\n\n\n\n\n\n\n\n2. \\(P(X=3)\\)\nFor a continuous RV:\n\\[\nP(X=3) = \\int_{3}^{3} f(x)\\,dx = 0.\n\\]\n3. \\(P(2.5 \\le X \\le 4.5)\\)\n\\[\nP(2.5 \\le X \\le 4.5) = \\int_{2.5}^{4.5} \\tfrac{1}{4}\\,dx\n= \\tfrac{1}{4}\\,(4.5-2.5) = \\tfrac{1}{2}.\n\\]\n4. CDF \\(F(x)\\)\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 2,\\\\[4pt]\n\\tfrac{x-2}{4}, & 2 \\le x \\le 6,\\\\[8pt]\n1, & x \\ge 6.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n5. \\(P(2.5 \\le X \\le 4.5)\\) using \\(F\\)\n\\[\nP(2.5 \\le X \\le 4.5) = F(4.5) - F(2.5).\n\\]\n\\(F(4.5) = (4.5-2)/4 = 0.625\\),\n\\(F(2.5) = (2.5-2)/4 = 0.125\\).\nDifference = \\(0.5\\). ✅ Same as before.\n6. Expected value \\(E[X]\\)\n\\[\nE[X] = \\int_{2}^{6} x \\cdot \\tfrac{1}{4}\\,dx\n= \\tfrac{1}{4}\\,\\left[\\tfrac{x^2}{2}\\right]_{2}^{6}\n= \\tfrac{1}{4}\\,\\Big(\\tfrac{36}{2}-\\tfrac{4}{2}\\Big)\n= \\tfrac{1}{4}\\,(18-2) = 4.\n\\]\n7. Variance \\(\\mathrm{Var}(X)\\)\nFirst compute\n\\[\nE[X^2] = \\int_{2}^{6} x^2 \\cdot \\tfrac{1}{4}\\,dx\n= \\tfrac{1}{4}\\,\\left[\\tfrac{x^3}{3}\\right]_{2}^{6}\n= \\tfrac{1}{4}\\,\\Big(\\tfrac{216}{3}-\\tfrac{8}{3}\\Big)\n= \\tfrac{1}{4}\\cdot \\tfrac{208}{3} = \\tfrac{52}{3}.\n\\]\nSo\n\\[\n\\mathrm{Var}(X) = E[X^2] - (E[X])^2\n= \\tfrac{52}{3} - 4^2 = \\tfrac{52}{3} - 16 = \\tfrac{4}{3}.\n\\]\n8. Standard deviation\n\\[\n\\sigma = \\sqrt{\\tfrac{4}{3}} = \\tfrac{2}{\\sqrt{3}} \\approx 1.155.\n\\]\n9. Probability within \\(2\\sigma\\) of the mean\nHere \\(\\mu=4\\), \\(2\\sigma \\approx 2.309\\).\nInterval \\([4-2.309,\\,4+2.309] = [1.691,\\,6.309]\\).\nIntersect with support \\([2,6]\\) gives \\([2,6]\\).\nSo\n\\[\nP(\\mu-2\\sigma \\le X \\le \\mu+2\\sigma) = 1.\n\\]\n10. Probability more than \\(1\\sigma\\) from the mean\nInterval within \\(1\\sigma\\):\n\\[\n[\\mu-\\sigma,\\;\\mu+\\sigma] = [4-1.155,\\,4+1.155] = [2.845,\\,5.155].\n\\]\nLength of this interval = \\(2.31\\).\nSince the PDF is flat on \\([2,6]\\) (length 4):\n\\[\nP(|X-\\mu|\\le\\sigma) = \\tfrac{2.31}{4} \\approx 0.577.\n\\]\nTherefore\n\\[\nP(|X-\\mu| &gt; \\sigma) = 1 - 0.577 = 0.423.\n\\]\n\n\n\n\n\n\n\n\n\nAny questions for me?\n\n\n\n\n\n\n\n\n\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#understand-the-purpose-of-a-probability-density-function-for-a-continuous-random-variable",
    "href": "MA206-AY26-1/lesson-8.html#understand-the-purpose-of-a-probability-density-function-for-a-continuous-random-variable",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "\\[\nP(X=x) =\n\\begin{cases}\n\\dfrac{1}{6}, & x=1, \\\\[6pt]\n\\dfrac{1}{6}, & x=2, \\\\[6pt]\n\\dfrac{1}{6}, & x=3, \\\\[6pt]\n\\dfrac{1}{6}, & x=4, \\\\[6pt]\n\\dfrac{1}{6}, & x=5, \\\\[6pt]\n\\dfrac{1}{6}, & x=6, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nThis is the probability mass function of a fair die.\nWhat do we know about it?\n\nProbabilities are non-negative: \\(P(X=x) \\geq 0\\).\n\nThe total probability is 1:\n\\[\n\\sum_x P(X=x) = 1\n\\]\nWe can compute probabilities of events by adding up the masses.\n\nExample: \\(P(X \\leq 3) = \\tfrac{1}{6} + \\tfrac{1}{6} + \\tfrac{1}{6} = \\tfrac{1}{2}\\).\n\n\n\n\n\nThis probability distribution function isn’t all that different from the discrete version.\nHere are the rules that all PDFs must follow:\n\n\\(f(x) \\geq 0\\) for all \\(x\\).\n\nThe total area under the curve is 1:\n\\[\n\\int_{-\\infty}^{\\infty} f(x)\\,dx = 1\n\\]\nProbabilities are found by areas, not points:\n\\[\nP(a \\leq X \\leq b) = \\int_a^b f(x)\\,dx\n\\]\nFor any single point,\n\\[\nP(X=c) = 0\n\\]\n\n\n\n\nConsider the function\n\\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\leq x \\leq 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nCheck the properties:\n\nNonnegative?\n\\(f(x) = 2x \\geq 0\\) on \\([0,1]\\). ✅\nTotal area?\n\\[\n\\int_{-\\infty}^\\infty f(x)\\,dx = \\int_0^1 2x\\,dx = \\left[x^2\\right]_0^1 = 1\n\\] So it integrates to 1. ✅\n\n\n\n\n\n\n\n\n\n\n\nProbability of an interval?\n\\[\nP(0.2 \\leq X \\leq 0.5) = \\int_{0.2}^{0.5} 2x \\, dx\n= \\left[x^2\\right]_{0.2}^{0.5}\n= (0.25 - 0.04) = 0.21\n\\]\n\n\n\n\n\n\n\n\n\n\nSo this is a valid PDF! It’s a “triangle-shaped” distribution on \\([0,1]\\) that places more weight near 1 than near 0.\n\nProbability of a single point is zero: \\(P(X=c)=0\\).\nFor a continuous random variable with PDF \\(f(x)\\), probability comes from area, so any single point has zero width: \\[\nP(X=c) \\;=\\; \\int_{c}^{c} f(x)\\,dx \\;=\\; 0.\n\\]\n\nFor example: \\(P(X=.6)=0\\)\n\\[\nP(X=.6) \\;=\\; \\int_{.6}^{.6} f(x)\\,dx \\;=\\; 0.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#understand-the-purpose-of-a-cumulative-density-function-for-a-continuous-random-variable",
    "href": "MA206-AY26-1/lesson-8.html#understand-the-purpose-of-a-cumulative-density-function-for-a-continuous-random-variable",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 1, \\\\[6pt]\n\\dfrac{1}{6}, & 1 \\leq x &lt; 2, \\\\[6pt]\n\\dfrac{2}{6}, & 2 \\leq x &lt; 3, \\\\[6pt]\n\\dfrac{3}{6}, & 3 \\leq x &lt; 4, \\\\[6pt]\n\\dfrac{4}{6}, & 4 \\leq x &lt; 5, \\\\[6pt]\n\\dfrac{5}{6}, & 5 \\leq x &lt; 6, \\\\[6pt]\n1, & x \\geq 6.\n\\end{cases}\n\\]\nThis step function is the CDF of the discrete die. It accumulates probability as we move to the right.\n\n\n\nFor any random variable \\(X\\) (discrete or continuous), the cumulative distribution function is\n\\[\nF(x) \\;=\\; P(X \\le x).\n\\]\nThings that are true of all CDFs:\n- Nondecreasing: if \\(a &lt; b\\) then \\(F(a) \\le F(b)\\).\n- Right-continuous: \\(\\lim_{x \\downarrow c} F(x) = F(c)\\).\n - Event probabilities from \\(F\\): for any \\(a &lt; b\\),\n\\[\nP(a \\le X \\le b) \\;=\\; F(b) - F(a).\n\\]\n\n\n\nRecall our PDF: \\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\le x \\le 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nIntegrate to get \\(F\\):\n\\[\nF(x) \\;=\\; \\int_{0}^{x} 2t\\,dt \\;=\\; x^{2}.\n\\]\nSo for any \\(x\\) in \\([0,1]\\),\n\\[\nF(x) = x^2.\n\\]\nQuick checks:\n- \\(F\\) is nondecreasing and continuous (no jumps).\n- \\(F(0)=0\\), \\(F(1)=1\\).\n- Example reads: \\(F(0.5)=0.5^2=0.25\\), \\(F(0.7)=0.49\\).\n- Interval probability via CDF:\n\\[\nP(0.2 \\le X \\le 0.5) = F(0.5) - F(0.2) = 0.25 - 0.04 = 0.21.\n\\]\n\n\n\n\n\n\n\n\n\nReading probabilities from the CDF:\n\n\\(P(X \\le 0.5) = F(0.5) = 0.25\\)\n\\[\nP(X \\le 0.5) = \\int_{0}^{0.5} 2t\\,dt = 0.25\n\\]\n\\(P(X &gt; 0.7) = 1 - F(0.7) = 1 - 0.49 = 0.51\\)\n\\[\nP(X &gt; 0.7) = \\int_{0.7}^{1} 2t\\,dt = 0.51\n\\]\n\\(P(0.2 \\le X \\le 0.5) = F(0.5) - F(0.2) = 0.21\\)\n\\[\nP(0.2 \\le X \\le 0.5) = \\int_{0.2}^{0.5} 2t\\,dt = 0.21\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#calculate-and-interpret-the-expected-value-of-a-continuous-random-variable",
    "href": "MA206-AY26-1/lesson-8.html#calculate-and-interpret-the-expected-value-of-a-continuous-random-variable",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "For a discrete random variable (like a fair die), the expected value is the weighted average of the possible outcomes:\n\\[\nE[X] = \\sum_x x \\cdot P(X=x).\n\\]\nFor the fair die: \\[\nE[X] = 1\\cdot \\tfrac{1}{6} + 2\\cdot \\tfrac{1}{6} + 3\\cdot \\tfrac{1}{6} + 4\\cdot \\tfrac{1}{6} + 5\\cdot \\tfrac{1}{6} + 6\\cdot \\tfrac{1}{6} = 3.5\n\\]\n\n\n\nFor a continuous random variable with PDF \\(f(x)\\), the expected value is defined by an integral:\n\\[\nE[X] = \\int_{-\\infty}^{\\infty} x \\, f(x)\\,dx.\n\\]\nThis is the continuous version of the same weighted average idea — instead of summing over points, we integrate over the real line.\n\n\n\nRecall our PDF:\n\\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\leq x \\leq 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nCompute the expected value:\n\\[\nE[X] = \\int_{0}^{1} x \\cdot (2x)\\,dx = \\int_{0}^{1} 2x^{2}\\,dx.\n\\]\n\\[\nE[X] = \\left[\\tfrac{2}{3}x^{3}\\right]_{0}^{1} = \\tfrac{2}{3}.\n\\]\nSo the mean of this distribution is \\(\\tfrac{2}{3} \\approx 0.667\\).",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#calculate-and-interpret-the-variance-and-standard-deviation-of-a-continuous-random-variable",
    "href": "MA206-AY26-1/lesson-8.html#calculate-and-interpret-the-variance-and-standard-deviation-of-a-continuous-random-variable",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "For a discrete random variable, variance measures how spread out the values are around the mean:\n\\[\n\\text{Var}(X) = \\sum_x (x - \\mu)^2 \\, P(X=x),\n\\]\nwhere \\(\\mu = E[X]\\).\nThe standard deviation is just the square root of the variance:\n\\[\n\\sigma = \\sqrt{\\text{Var}(X)}.\n\\]\nFor a fair six-sided die:\n\nWe already know the mean is\n\\[\nE[X] = \\frac{1+2+3+4+5+6}{6} = 3.5.\n\\]\nCompute the variance:\n\\[\n\\text{Var}(X) = \\sum_{x=1}^{6} (x - 3.5)^2 \\cdot \\tfrac{1}{6}.\n\\]\n\n\\[\n= \\frac{(1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-3.5)^2}{6}\n= \\frac{35}{12} \\approx 2.92.\n\\]\n\nThen the standard deviation is\n\\[\n\\sigma = \\sqrt{\\tfrac{35}{12}} \\approx 1.71.\n\\]\n\nSo the die’s outcomes are typically about 1.7 away from the mean value of 3.5.\n\n\n\nFor a continuous random variable with PDF \\(f(x)\\), we replace the sum with an integral:\n\\[\n\\text{Var}(X) = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x)\\,dx,\n\\]\nwhere \\(\\mu = E[X]\\).\nBut this integral is often hard to compute directly, so there is a shortcut formula we can use:\n\\[\n\\text{Var}(X) = E[X^2] - \\big(E[X]\\big)^2,\n\\]\nwhere\n\\[\nE[X^2] = \\int_{-\\infty}^{\\infty} x^2 f(x)\\,dx.\n\\]\n\n\n\n\\[\nf(x) =\n\\begin{cases}\n2x, & 0 \\leq x \\leq 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nFrom before, the mean is \\(\\mu = E[X] = \\tfrac{2}{3}\\).\nCompute \\(E[X^2]\\):\n\\[\nE[X^2] = \\int_{0}^{1} x^2 \\cdot (2x)\\,dx\n= \\int_{0}^{1} 2x^3\\,dx\n= \\left[\\tfrac{1}{2}x^4\\right]_{0}^{1} = \\tfrac{1}{2}.\n\\]\nNow variance:\n\\[\n\\text{Var}(X) = \\tfrac{1}{2} - \\left(\\tfrac{2}{3}\\right)^2\n= \\tfrac{1}{2} - \\tfrac{4}{9}\n= \\tfrac{1}{18}.\n\\]\nStandard deviation:\n\\[\n\\sigma = \\sqrt{\\tfrac{1}{18}} \\approx 0.236.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#practice-problem",
    "href": "MA206-AY26-1/lesson-8.html#practice-problem",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "Consider the random variable \\(X\\) with PDF\n\\[\nf(x) =\n\\begin{cases}\n2(1-x), & 0 \\le x \\le 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n1. Plot the PDF\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Find \\(P(X = 0.4)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor continuous random variables:\n\\[\nP(X=0.4) = \\int_{0.4}^{0.4} f(x)\\,dx = 0.\n\\]\n\n\n\n3. Find \\(P(0.2 \\le X \\le 0.6)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(0.2 \\le X \\le 0.6) = \\int_{0.2}^{0.6} 2(1-x)\\,dx\n= \\Big[2x - x^2\\Big]_{0.2}^{0.6}.\n\\]\nAt \\(0.6\\): \\(2(0.6)-0.6^2=0.84\\).\nAt \\(0.2\\): \\(0.36\\).\nDifference: \\(0.48\\).\n\n\n\n4. Find the CDF \\(F(x)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor \\(0 \\le x \\le 1\\):\n\\[\nF(x) = \\int_{0}^{x} 2(1-t)\\,dt = 2x - x^2.\n\\]\nSo\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 0, \\\\\n2x - x^2, & 0 \\le x \\le 1, \\\\\n1, & x \\ge 1.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n5. Find \\(P(0.2 \\le X \\le 0.6)\\) using the CDF\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nP(0.2 \\le X \\le 0.6) = F(0.6) - F(0.2).\n\\]\n\\(F(0.6)=0.84\\), \\(F(0.2)=0.36\\), difference = \\(0.48\\).\n✅ Same as before.\n\n\n\n6. Find the expected value \\(E[X]\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\nE[X] = \\int_{0}^{1} x \\cdot 2(1-x)\\,dx\n= \\int_{0}^{1} (2x - 2x^2)\\,dx.\n\\]\n\\[\n= \\Big[x^2 - \\tfrac{2}{3}x^3\\Big]_0^1\n= \\tfrac{1}{3}.\n\\]\n\n\n\n7. Find the variance \\(\\text{Var}(X)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCompute \\(E[X^2]\\):\n\\[\nE[X^2] = \\int_{0}^{1} x^2 \\cdot 2(1-x)\\,dx\n= \\tfrac{1}{6}.\n\\]\nSo\n\\[\n\\text{Var}(X) = E[X^2] - (E[X])^2\n= \\tfrac{1}{6} - \\left(\\tfrac{1}{3}\\right)^2\n= \\tfrac{1}{18}.\n\\]\n\n\n\n8. Find the standard deviation\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\n\\sigma = \\sqrt{\\tfrac{1}{18}} \\approx 0.236.\n\\]\n\n\n\n9. Find \\(P(\\mu - 2\\sigma \\le X \\le \\mu + 2\\sigma)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\mu=\\tfrac{1}{3}\\), \\(\\sigma \\approx 0.236\\), so \\(2\\sigma \\approx 0.472\\).\nRange: \\([\\tfrac{1}{3} - 0.472,\\;\\tfrac{1}{3} + 0.472] = [-0.139,\\;0.805]\\).\nSince the support is \\([0,1]\\), we use \\([0,0.805]\\).\n\\[\nP(0 \\le X \\le 0.805) = F(0.805) - F(0) = F(0.805).\n\\]\n\\(F(0.805) = 2(0.805) - (0.805)^2 \\approx 0.962\\).\n\n\n\n10. Find \\(P(X &lt; \\mu - \\sigma \\;\\text{ or }\\; X &gt; \\mu + \\sigma)\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\mu=\\tfrac{1}{3}\\), \\(\\sigma \\approx 0.236\\).\nInterval within 1 SD: \\([0.097,\\,0.569]\\).\nSo the probability outside is\n\\[\nP(X &lt; 0.097) + P(X &gt; 0.569).\n\\]\nUsing the CDF:\n\\[\nP(X &lt; 0.097) = F(0.097) \\approx 0.185,\n\\] \\[\nP(X &gt; 0.569) = 1 - F(0.569) \\approx 1 - 0.814 = 0.186.\n\\]\nTotal = \\(0.185 + 0.186 = 0.371\\).",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#board-problem",
    "href": "MA206-AY26-1/lesson-8.html#board-problem",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "Consider the random variable \\(X\\) with PDF\n\\[\nf(x) =\n\\begin{cases}\n\\dfrac{1}{4}, & 2 \\le x \\le 6,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nPlot the PDF\n\nFind \\(P(X = 3)\\)\n\nFind \\(P(2.5 \\le X \\le 4.5)\\)\n\nFind the CDF \\(F(x)\\)\n\nFind \\(P(2.5 \\le X \\le 4.5)\\) using the CDF. Is it the same as before?\n\nFind the expected value \\(E[X]\\)\n\nFind the variance \\(\\mathrm{Var}(X)\\)\n\nFind the standard deviation\n\nFind \\(P(\\mu - 2\\sigma \\le X \\le \\mu + 2\\sigma)\\)\n\nFind \\(P(X &lt; \\mu - \\sigma \\;\\text{ or }\\; X &gt; \\mu + \\sigma)\\)\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n1. Plot the PDF\n\n\n\n\n\n\n\n\n\n2. \\(P(X=3)\\)\nFor a continuous RV:\n\\[\nP(X=3) = \\int_{3}^{3} f(x)\\,dx = 0.\n\\]\n3. \\(P(2.5 \\le X \\le 4.5)\\)\n\\[\nP(2.5 \\le X \\le 4.5) = \\int_{2.5}^{4.5} \\tfrac{1}{4}\\,dx\n= \\tfrac{1}{4}\\,(4.5-2.5) = \\tfrac{1}{2}.\n\\]\n4. CDF \\(F(x)\\)\n\\[\nF(x) =\n\\begin{cases}\n0, & x &lt; 2,\\\\[4pt]\n\\tfrac{x-2}{4}, & 2 \\le x \\le 6,\\\\[8pt]\n1, & x \\ge 6.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n5. \\(P(2.5 \\le X \\le 4.5)\\) using \\(F\\)\n\\[\nP(2.5 \\le X \\le 4.5) = F(4.5) - F(2.5).\n\\]\n\\(F(4.5) = (4.5-2)/4 = 0.625\\),\n\\(F(2.5) = (2.5-2)/4 = 0.125\\).\nDifference = \\(0.5\\). ✅ Same as before.\n6. Expected value \\(E[X]\\)\n\\[\nE[X] = \\int_{2}^{6} x \\cdot \\tfrac{1}{4}\\,dx\n= \\tfrac{1}{4}\\,\\left[\\tfrac{x^2}{2}\\right]_{2}^{6}\n= \\tfrac{1}{4}\\,\\Big(\\tfrac{36}{2}-\\tfrac{4}{2}\\Big)\n= \\tfrac{1}{4}\\,(18-2) = 4.\n\\]\n7. Variance \\(\\mathrm{Var}(X)\\)\nFirst compute\n\\[\nE[X^2] = \\int_{2}^{6} x^2 \\cdot \\tfrac{1}{4}\\,dx\n= \\tfrac{1}{4}\\,\\left[\\tfrac{x^3}{3}\\right]_{2}^{6}\n= \\tfrac{1}{4}\\,\\Big(\\tfrac{216}{3}-\\tfrac{8}{3}\\Big)\n= \\tfrac{1}{4}\\cdot \\tfrac{208}{3} = \\tfrac{52}{3}.\n\\]\nSo\n\\[\n\\mathrm{Var}(X) = E[X^2] - (E[X])^2\n= \\tfrac{52}{3} - 4^2 = \\tfrac{52}{3} - 16 = \\tfrac{4}{3}.\n\\]\n8. Standard deviation\n\\[\n\\sigma = \\sqrt{\\tfrac{4}{3}} = \\tfrac{2}{\\sqrt{3}} \\approx 1.155.\n\\]\n9. Probability within \\(2\\sigma\\) of the mean\nHere \\(\\mu=4\\), \\(2\\sigma \\approx 2.309\\).\nInterval \\([4-2.309,\\,4+2.309] = [1.691,\\,6.309]\\).\nIntersect with support \\([2,6]\\) gives \\([2,6]\\).\nSo\n\\[\nP(\\mu-2\\sigma \\le X \\le \\mu+2\\sigma) = 1.\n\\]\n10. Probability more than \\(1\\sigma\\) from the mean\nInterval within \\(1\\sigma\\):\n\\[\n[\\mu-\\sigma,\\;\\mu+\\sigma] = [4-1.155,\\,4+1.155] = [2.845,\\,5.155].\n\\]\nLength of this interval = \\(2.31\\).\nSince the PDF is flat on \\([2,6]\\) (length 4):\n\\[\nP(|X-\\mu|\\le\\sigma) = \\tfrac{2.31}{4} \\approx 0.577.\n\\]\nTherefore\n\\[\nP(|X-\\mu| &gt; \\sigma) = 1 - 0.577 = 0.423.\n\\]",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206-AY26-1/lesson-8.html#before-you-leave",
    "href": "MA206-AY26-1/lesson-8.html#before-you-leave",
    "title": "Lesson 8: Continuous Random Variables",
    "section": "",
    "text": "Any questions for me?\n\n\n\n\n\n\n\n\n\nWPR 1: Lesson 10\nProject Milestone 3: Due Canvas Lesson 7",
    "crumbs": [
      "MA206-AY26-1",
      "Lesson 8"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/index.html",
    "href": "MA206x-AY26-2/index.html",
    "title": "MA206x: Probability and Statistics",
    "section": "",
    "text": "AY26-2 | United States Military Academy\n\n\n\n\n\n\n\n\n Canvas Grades & Assignments \n\n\n\n\n Syllabus Course Syllabus \n\n\n\n\n Textbook Devore: Prob & Stats for Engineering \n\n\n\n\n Calendar Course Calendar \n\n\n\n\n\n\n\n\n\nLTC Dusty Turner, PhD Academy Professor, Department of Mathematical Sciences\n\nOffice: Thayer Hall\nEmail: dusty.turner@westpoint.edu\nWebsite: dustysturner.com\n\nEmail to coordinate Additional Instruction.\n\n\n\n\n\n\nThis site contains instructor notes and lesson resources for MA206x. Use the sidebar to navigate through lessons, or visit the links above for official course materials.",
    "crumbs": [
      "MA206x-AY26-2",
      "Course Information"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/index.html#quick-links",
    "href": "MA206x-AY26-2/index.html#quick-links",
    "title": "MA206x: Probability and Statistics",
    "section": "",
    "text": "Canvas Grades & Assignments \n\n\n\n\n Syllabus Course Syllabus \n\n\n\n\n Textbook Devore: Prob & Stats for Engineering \n\n\n\n\n Calendar Course Calendar",
    "crumbs": [
      "MA206x-AY26-2",
      "Course Information"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/index.html#instructor",
    "href": "MA206x-AY26-2/index.html#instructor",
    "title": "MA206x: Probability and Statistics",
    "section": "",
    "text": "LTC Dusty Turner, PhD Academy Professor, Department of Mathematical Sciences\n\nOffice: Thayer Hall\nEmail: dusty.turner@westpoint.edu\nWebsite: dustysturner.com\n\nEmail to coordinate Additional Instruction.",
    "crumbs": [
      "MA206x-AY26-2",
      "Course Information"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/index.html#about-this-site",
    "href": "MA206x-AY26-2/index.html#about-this-site",
    "title": "MA206x: Probability and Statistics",
    "section": "",
    "text": "This site contains instructor notes and lesson resources for MA206x. Use the sidebar to navigate through lessons, or visit the links above for official course materials.",
    "crumbs": [
      "MA206x-AY26-2",
      "Course Information"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-2.html",
    "href": "MA206x-AY26-2/lesson-2.html",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n\n  Your browser does not support the video tag. \n\n\n\n\n\nMath vs TBD\n\n\n\n\n\n\nPreviously 6-1\n\n\n\n\n\n\n6-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcademic Integrity Brief\nAcademic Security\n\n\n\n\n\nAI is welcome and encouraged on WebAssign. Screenshot the question… maybe.\nBut when WPR time comes - no AI. So use it as a tool for learning, not a crutch.\nWebAssign is 15% of the course. You should get most/all those points.\nDon’t get those points at the peril of your WPR grades - that’s 60% of the course.\nWebAssign is forgiving: 5 tries per sub-question.\nDon’t forget documentation. Just tell me what you did.\n\n\n\n\n\n\n\nAssignment\nPoints\n\n\n\n\nWebAssign Homework\n150\n\n\nWPR I\n175\n\n\nWPR II\n175\n\n\nExploratory Data Analysis\n25\n\n\nTech Report\n125\n\n\nProject Presentation\n50\n\n\nTEE\n300\n\n\nTotal\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nAction Required\n\n\n\nBefore we continue, everyone needs to request an account on Vantage - the Army’s data science platform where we’ll be using R this semester.\n\nGo to https://vantage.army.mil/\nClick Request Account\nFill out the required information\nFor Commander email, use: jonathan.l.day3.mil@army.mil\nWait for approval (may take 1-2 days)\n\nWe’ll be using Vantage for the project throughout the course.\n\n\n\n\n\n\n\n\nWhy do we collect data? Because we want to learn about something bigger than what we can directly observe.\n\nPopulation: The entire group we want to learn about\nSample: The subset we actually observe\nProcess: An ongoing mechanism that generates data over time\n\nThe whole course builds on this: we use samples to make claims about populations. That’s inference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation\nSample\n\n\n\n\nWhat we have\nUsually unknown\nObservable data\n\n\nNotation\nGreek letters (\\(\\mu\\), \\(\\sigma\\), \\(\\pi\\))\nLatin letters (\\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\))\n\n\n\n\nA statistic estimates a parameter. This is the foundation of Blocks 2 and 3.\n\n\n\n\n\nCategorical: Labels or categories (e.g., branch, major, yes/no)\nQuantitative: Numbers with meaningful arithmetic (e.g., height, GPA, time)\n\nWhy does this matter? The type of variable determines:\n\nHow you summarize it (proportions vs means)\nHow you visualize it (bar charts vs histograms)\nWhich inference method you use (z-test for proportions vs t-test for means)\n\n\n\n\n\n\n\n\n\nConstruct and interpret stem-and-leaf displays\nCreate and interpret dotplots\nBuild and analyze histograms and frequency distributions\nDescribe distributions in terms of shape, center, spread, and outliers\n\n\n\n\nDevore, Section 1.2: Pictorial and Tabular Methods in Descriptive Statistics\n\n\n\n\n\nBefore calculating any numbers, we should look at the data. Visual displays help us:\n\nIdentify the shape of the distribution\nFind a typical value (center)\nSee how much variability exists (spread)\nSpot outliers and gaps\nCheck for symmetry or skewness\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisplay\nDescription\nBest Sample Size\nPreserves Exact Values?\n\n\n\n\nStem-and-leaf\nSplit each number into stem (leading digits) and leaf (trailing digit)\nSmall (n &lt; 50)\nYes\n\n\nDotplot\nPlace a dot for each observation on a number line; stack repeated values\nSmall (n &lt; 30)\nYes\n\n\nHistogram (Frequency)\nDivide range into bins; bar height = count in each bin\nAny size\nNo\n\n\nHistogram (Relative Freq)\nDivide range into bins; bar height = proportion in each bin\nAny size\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisplay\nShape\nCenter\nSpread\nOutliers\nExact Values\nCompare Groups\n\n\n\n\nStem-and-leaf\nYes\nYes\nYes\nYes\nYes\nLimited\n\n\nDotplot\nYes\nYes\nYes\nYes\nYes\nGood\n\n\nHistogram (Frequency)\nYes\nApproximate\nYes\nYes\nNo\nDifficult\n\n\nHistogram (Relative Freq)\nYes\nApproximate\nYes\nYes\nNo\nBest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisplay\nUse When…\nExample\n\n\n\n\nStem-and-leaf\nSmall dataset, want to preserve exact values\nQuiz scores for your 18-person section\n\n\nDotplot\nSmall dataset with repeated values\nNumber of absences per cadet\n\n\nHistogram (Frequency)\nLarger dataset, want raw counts per bin\nAPFT scores for an entire battalion\n\n\nHistogram (Relative Freq)\nComparing groups of different sizes\nRun times: Company A (120 soldiers) vs Company B (85 soldiers)\n\n\n\n\n\n\n\n\n\n\nWhen describing any distribution, always address:\n\nShape: Symmetric, skewed left, skewed right, unimodal, bimodal?\nCenter: Where is the “typical” value?\nSpread: How much variability is there?\nOutliers: Any unusual observations?\n\n\n\n\n\n\n\nMemory Aid\n\n\n\nS-C-S-O: Shape, Center, Spread, Outliers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShape\nDescription\nRelationship\n\n\n\n\nSymmetric\nLeft and right sides are mirror images\nMean ≈ Median\n\n\nSkewed Right\nLong tail extends to the right\nMean &gt; Median\n\n\nSkewed Left\nLong tail extends to the left\nMean &lt; Median\n\n\n\nExamples:\n\nSkewed right: Income, home prices\nSkewed left: Age at retirement, exam scores with a ceiling\n\n\n\n\n\n\nUnimodal: One peak (most common)\nBimodal: Two distinct peaks (may indicate two subgroups)\nMultimodal: Multiple peaks\nUniform: Roughly flat, no clear peak\n\n\n\n\n\n\n\n\n\n\n\nOutlier\n\n\n\nAn observation that falls far from the rest of the data. Could indicate:\n\nData entry error\nMeasurement error\nA genuinely unusual observation\nA different population\n\n\n\nAlways investigate outliers - don’t automatically remove them!\n\n\n\n\n\nLet’s use simulated height data for 30 cadets to demonstrate all four display types.\n\n\nHeights (inches): 65, 66, 67, 67, 68, 68, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 73, 73, 73, 73, 74, 74, 75\n\n\n\n\n\n\n\nStem | Leaves\n\n\n-----+--------\n\n\n  6 | 5 6 7 7 8 8 9 9\n  7 | 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 3 3 3 3 4 4 5\n\n\nReading it: The row “6 | 5 6 7 7 8 8 9 9” represents heights of 65, 66, 67, 67, 68, 68, 69, 69 inches.\nInterpreting with S-C-S-O:\n\nShape: Roughly symmetric (similar number of leaves on each stem), unimodal\nCenter: The 7 stem has many more leaves, so center is around 70 inches\nSpread: Values range from 65 to 75 inches (range = 10 inches)\nOutliers: No values stand apart from the rest\n\nUnique advantage: We can recover every exact data value! We know there are exactly two cadets who are 67 inches tall.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting with S-C-S-O:\n\nShape: Roughly symmetric with one clear peak (unimodal); dots cluster in the middle\nCenter: The tallest stack of dots is at 70 inches - this is our center\nSpread: Dots extend from about 65 to 75 inches\nOutliers: No dots are isolated far from the others\n\nUnique advantage: Easy to see exact values AND repeated values (stacked dots). The height of 70 inches appears 9 times - we can count the dots!\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting with S-C-S-O:\n\nShape: Roughly symmetric (bars rise then fall), unimodal (one peak)\nCenter: Tallest bar is at 70-72 inches, so center is around 70-71\nSpread: Data spans from about 64 to 76 inches\nOutliers: No bars are isolated from the main distribution\n\nUnique advantage: Shows counts - we can say “15 cadets are between 70-72 inches tall.” Good for understanding raw numbers.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting with S-C-S-O:\n\nShape: Same as frequency histogram - symmetric, unimodal\nCenter: Peak is at 70-72 inches\nSpread: Same range, 64 to 76 inches\nOutliers: None visible\n\nUnique advantage: Shows proportions - we can say “about 50% of cadets are between 70-72 inches tall.” Essential for comparing groups of different sizes (e.g., comparing this section of 30 to another section of 18).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing S-C-S-O:\n\nShape: Roughly symmetric, unimodal\nCenter: Around 70 inches\nSpread: Ranges from about 64 to 76 inches\nOutliers: None obvious\n\n\n\n\n\n\nThe following data represents the number of hours cadets studied for a WPR:\n3, 5, 4, 8, 6, 5, 12, 4, 5, 7, 6, 5, 4, 6, 5, 3, 5, 6, 4, 5\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCreate a stem-and-leaf display for this data\nDescribe the distribution (shape, center, spread, outliers)\nWhat is the relative frequency of cadets who studied 5 or more hours?\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nStem-and-leaf display:\n\nStem | Leaves\n   0 | 3 3 4 4 4 4 5 5 5 5 5 5 5 6 6 6 6 7 8\n   1 | 2\n\nShape: Roughly symmetric with a possible outlier; unimodal Center: Around 5 hours Spread: 3 to 12 hours (range of 9) Outliers: 12 hours appears to be an outlier\nCount of values ≥ 5: 13 out of 20 Relative frequency = 13/20 = 0.65 or 65%\n\n\n\n\n\n\n\n\n\n\n\nStem-and-leaf displays: preserve data while showing shape\nDotplots: simple visual for small datasets\nHistograms: frequency distributions for larger datasets\nDescribing distributions: Shape, Center, Spread, Outliers (S-C-S-O)\n\nAny questions?\n\n\n\n\nLesson 3: Measures of Location\n\nMean, median, and mode\nPercentiles and quartiles\nComparing measures of center for different distributions\n\n\n\n\n\n\nWebAssign 1.3 - Due before Lesson 3\nExploratory Data Analysis - Due Lesson 9\nWPR I - Lesson 16",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-2.html#admin",
    "href": "MA206x-AY26-2/lesson-2.html#admin",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "",
    "text": "Academic Integrity Brief\nAcademic Security\n\n\n\n\n\nAI is welcome and encouraged on WebAssign. Screenshot the question… maybe.\nBut when WPR time comes - no AI. So use it as a tool for learning, not a crutch.\nWebAssign is 15% of the course. You should get most/all those points.\nDon’t get those points at the peril of your WPR grades - that’s 60% of the course.\nWebAssign is forgiving: 5 tries per sub-question.\nDon’t forget documentation. Just tell me what you did.\n\n\n\n\n\n\n\nAssignment\nPoints\n\n\n\n\nWebAssign Homework\n150\n\n\nWPR I\n175\n\n\nWPR II\n175\n\n\nExploratory Data Analysis\n25\n\n\nTech Report\n125\n\n\nProject Presentation\n50\n\n\nTEE\n300\n\n\nTotal\n1000",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-2.html#request-a-vantage-account",
    "href": "MA206x-AY26-2/lesson-2.html#request-a-vantage-account",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "",
    "text": "Action Required\n\n\n\nBefore we continue, everyone needs to request an account on Vantage - the Army’s data science platform where we’ll be using R this semester.\n\nGo to https://vantage.army.mil/\nClick Request Account\nFill out the required information\nFor Commander email, use: jonathan.l.day3.mil@army.mil\nWait for approval (may take 1-2 days)\n\nWe’ll be using Vantage for the project throughout the course.",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-2.html#lesson-1-review",
    "href": "MA206x-AY26-2/lesson-2.html#lesson-1-review",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "",
    "text": "Why do we collect data? Because we want to learn about something bigger than what we can directly observe.\n\nPopulation: The entire group we want to learn about\nSample: The subset we actually observe\nProcess: An ongoing mechanism that generates data over time\n\nThe whole course builds on this: we use samples to make claims about populations. That’s inference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation\nSample\n\n\n\n\nWhat we have\nUsually unknown\nObservable data\n\n\nNotation\nGreek letters (\\(\\mu\\), \\(\\sigma\\), \\(\\pi\\))\nLatin letters (\\(\\bar{x}\\), \\(s\\), \\(\\hat{p}\\))\n\n\n\n\nA statistic estimates a parameter. This is the foundation of Blocks 2 and 3.\n\n\n\n\n\nCategorical: Labels or categories (e.g., branch, major, yes/no)\nQuantitative: Numbers with meaningful arithmetic (e.g., height, GPA, time)\n\nWhy does this matter? The type of variable determines:\n\nHow you summarize it (proportions vs means)\nHow you visualize it (bar charts vs histograms)\nWhich inference method you use (z-test for proportions vs t-test for means)",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-2.html#lesson-2-content",
    "href": "MA206x-AY26-2/lesson-2.html#lesson-2-content",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "",
    "text": "Construct and interpret stem-and-leaf displays\nCreate and interpret dotplots\nBuild and analyze histograms and frequency distributions\nDescribe distributions in terms of shape, center, spread, and outliers\n\n\n\n\nDevore, Section 1.2: Pictorial and Tabular Methods in Descriptive Statistics",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-2.html#why-visualize-data",
    "href": "MA206x-AY26-2/lesson-2.html#why-visualize-data",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "",
    "text": "Before calculating any numbers, we should look at the data. Visual displays help us:\n\nIdentify the shape of the distribution\nFind a typical value (center)\nSee how much variability exists (spread)\nSpot outliers and gaps\nCheck for symmetry or skewness",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-2.html#four-types-of-displays",
    "href": "MA206x-AY26-2/lesson-2.html#four-types-of-displays",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "",
    "text": "Display\nDescription\nBest Sample Size\nPreserves Exact Values?\n\n\n\n\nStem-and-leaf\nSplit each number into stem (leading digits) and leaf (trailing digit)\nSmall (n &lt; 50)\nYes\n\n\nDotplot\nPlace a dot for each observation on a number line; stack repeated values\nSmall (n &lt; 30)\nYes\n\n\nHistogram (Frequency)\nDivide range into bins; bar height = count in each bin\nAny size\nNo\n\n\nHistogram (Relative Freq)\nDivide range into bins; bar height = proportion in each bin\nAny size\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisplay\nShape\nCenter\nSpread\nOutliers\nExact Values\nCompare Groups\n\n\n\n\nStem-and-leaf\nYes\nYes\nYes\nYes\nYes\nLimited\n\n\nDotplot\nYes\nYes\nYes\nYes\nYes\nGood\n\n\nHistogram (Frequency)\nYes\nApproximate\nYes\nYes\nNo\nDifficult\n\n\nHistogram (Relative Freq)\nYes\nApproximate\nYes\nYes\nNo\nBest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisplay\nUse When…\nExample\n\n\n\n\nStem-and-leaf\nSmall dataset, want to preserve exact values\nQuiz scores for your 18-person section\n\n\nDotplot\nSmall dataset with repeated values\nNumber of absences per cadet\n\n\nHistogram (Frequency)\nLarger dataset, want raw counts per bin\nAPFT scores for an entire battalion\n\n\nHistogram (Relative Freq)\nComparing groups of different sizes\nRun times: Company A (120 soldiers) vs Company B (85 soldiers)",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-2.html#describing-distributions",
    "href": "MA206x-AY26-2/lesson-2.html#describing-distributions",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "",
    "text": "When describing any distribution, always address:\n\nShape: Symmetric, skewed left, skewed right, unimodal, bimodal?\nCenter: Where is the “typical” value?\nSpread: How much variability is there?\nOutliers: Any unusual observations?\n\n\n\n\n\n\n\nMemory Aid\n\n\n\nS-C-S-O: Shape, Center, Spread, Outliers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShape\nDescription\nRelationship\n\n\n\n\nSymmetric\nLeft and right sides are mirror images\nMean ≈ Median\n\n\nSkewed Right\nLong tail extends to the right\nMean &gt; Median\n\n\nSkewed Left\nLong tail extends to the left\nMean &lt; Median\n\n\n\nExamples:\n\nSkewed right: Income, home prices\nSkewed left: Age at retirement, exam scores with a ceiling\n\n\n\n\n\n\nUnimodal: One peak (most common)\nBimodal: Two distinct peaks (may indicate two subgroups)\nMultimodal: Multiple peaks\nUniform: Roughly flat, no clear peak\n\n\n\n\n\n\n\n\n\n\n\nOutlier\n\n\n\nAn observation that falls far from the rest of the data. Could indicate:\n\nData entry error\nMeasurement error\nA genuinely unusual observation\nA different population\n\n\n\nAlways investigate outliers - don’t automatically remove them!",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-2.html#example-cadet-heights",
    "href": "MA206x-AY26-2/lesson-2.html#example-cadet-heights",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "",
    "text": "Let’s use simulated height data for 30 cadets to demonstrate all four display types.\n\n\nHeights (inches): 65, 66, 67, 67, 68, 68, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 73, 73, 73, 73, 74, 74, 75\n\n\n\n\n\n\n\nStem | Leaves\n\n\n-----+--------\n\n\n  6 | 5 6 7 7 8 8 9 9\n  7 | 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 3 3 3 3 4 4 5\n\n\nReading it: The row “6 | 5 6 7 7 8 8 9 9” represents heights of 65, 66, 67, 67, 68, 68, 69, 69 inches.\nInterpreting with S-C-S-O:\n\nShape: Roughly symmetric (similar number of leaves on each stem), unimodal\nCenter: The 7 stem has many more leaves, so center is around 70 inches\nSpread: Values range from 65 to 75 inches (range = 10 inches)\nOutliers: No values stand apart from the rest\n\nUnique advantage: We can recover every exact data value! We know there are exactly two cadets who are 67 inches tall.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting with S-C-S-O:\n\nShape: Roughly symmetric with one clear peak (unimodal); dots cluster in the middle\nCenter: The tallest stack of dots is at 70 inches - this is our center\nSpread: Dots extend from about 65 to 75 inches\nOutliers: No dots are isolated far from the others\n\nUnique advantage: Easy to see exact values AND repeated values (stacked dots). The height of 70 inches appears 9 times - we can count the dots!\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting with S-C-S-O:\n\nShape: Roughly symmetric (bars rise then fall), unimodal (one peak)\nCenter: Tallest bar is at 70-72 inches, so center is around 70-71\nSpread: Data spans from about 64 to 76 inches\nOutliers: No bars are isolated from the main distribution\n\nUnique advantage: Shows counts - we can say “15 cadets are between 70-72 inches tall.” Good for understanding raw numbers.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting with S-C-S-O:\n\nShape: Same as frequency histogram - symmetric, unimodal\nCenter: Peak is at 70-72 inches\nSpread: Same range, 64 to 76 inches\nOutliers: None visible\n\nUnique advantage: Shows proportions - we can say “about 50% of cadets are between 70-72 inches tall.” Essential for comparing groups of different sizes (e.g., comparing this section of 30 to another section of 18).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing S-C-S-O:\n\nShape: Roughly symmetric, unimodal\nCenter: Around 70 inches\nSpread: Ranges from about 64 to 76 inches\nOutliers: None obvious",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-2.html#practice-problem",
    "href": "MA206x-AY26-2/lesson-2.html#practice-problem",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "",
    "text": "The following data represents the number of hours cadets studied for a WPR:\n3, 5, 4, 8, 6, 5, 12, 4, 5, 7, 6, 5, 4, 6, 5, 3, 5, 6, 4, 5\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCreate a stem-and-leaf display for this data\nDescribe the distribution (shape, center, spread, outliers)\nWhat is the relative frequency of cadets who studied 5 or more hours?\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nStem-and-leaf display:\n\nStem | Leaves\n   0 | 3 3 4 4 4 4 5 5 5 5 5 5 5 6 6 6 6 7 8\n   1 | 2\n\nShape: Roughly symmetric with a possible outlier; unimodal Center: Around 5 hours Spread: 3 to 12 hours (range of 9) Outliers: 12 hours appears to be an outlier\nCount of values ≥ 5: 13 out of 20 Relative frequency = 13/20 = 0.65 or 65%",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-2.html#before-you-leave",
    "href": "MA206x-AY26-2/lesson-2.html#before-you-leave",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "",
    "text": "Stem-and-leaf displays: preserve data while showing shape\nDotplots: simple visual for small datasets\nHistograms: frequency distributions for larger datasets\nDescribing distributions: Shape, Center, Spread, Outliers (S-C-S-O)\n\nAny questions?\n\n\n\n\nLesson 3: Measures of Location\n\nMean, median, and mode\nPercentiles and quartiles\nComparing measures of center for different distributions\n\n\n\n\n\n\nWebAssign 1.3 - Due before Lesson 3\nExploratory Data Analysis - Due Lesson 9\nWPR I - Lesson 16",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 2"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html",
    "href": "MA206x-AY26-2/lesson-4.html",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "123\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath vs AWPAD\n\n\n\n\n\n\nPreviously 7-0\n\n\n\n\n\n\n8-0\n\n\n\n\n\n\n\n\n\nMath vs AWPAD\n\n\n\n\n\n\nPreviously 8-0\n\n\n\n\n\n\n8-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVantage\n\n\n\nLet’s go to Vantage and see if things are working.\n\n\nClass Data\n\n\n\n\n\n\nLast lesson we learned three ways to measure the “center” of a distribution:\n\n\n\nMeasure\nFormula\nResistant?\nBest For\n\n\n\n\nMean\n\\(\\bar{x} = \\frac{\\sum x_i}{n}\\)\nNo\nSymmetric data\n\n\nMedian\nMiddle value\nYes\nSkewed data/outliers\n\n\nMode\nMost frequent\nYes\nCategorical data\n\n\n\nWe also learned about percentiles, quartiles, and the five-number summary.\n\n\n\n\n\n\nKey Insight from Lesson 3\n\n\n\nTwo datasets can have the same mean and median but look very different! Today we learn how to quantify that difference.\n\n\n\n\n\n\n\n\n\n\nCalculate and interpret range and inter-quartile range (IQR)\nCalculate and interpret variance and standard deviation\n\n\n\n\nDevore, Section 1.4: Measures of Variability\n\n\n\n\n\n\n\nConsider two archery targets:\n\n\n\n\n\n\n\n\n\nBoth archers might have similar average positions, but Archer A is clearly more consistent. We need measures of spread or variability to capture this difference.\n\n\n\n\n\nThe range is the simplest measure of spread:\n\\[\n\\text{Range} = \\text{Maximum} - \\text{Minimum}\n\\]\n\n\nQuiz scores: 72, 75, 78, 80, 82, 85, 88 \n\n\nMaximum: 88 \n\n\nMinimum: 72 \n\n\nRange: 16 points\n\n\nProblems with the range:\n\nUses only 2 data points (ignores everything in between)\nVery sensitive to outliers\nTends to increase with sample size\n\n\n\n\n\nThe interquartile range (IQR) measures the spread of the middle 50% of the data:\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\]\n\n\nQuiz scores: 72, 75, 78, 80, 82, 85, 88 \n\n\nQ1 (25th percentile): 76.5 \n\n\nQ3 (75th percentile): 83.5 \n\n\nIQR: 7 points\n\n\nAdvantages of IQR:\n\nResistant to outliers (uses quartiles, not extremes)\nFocuses on the “typical” spread of the data\nUseful for identifying outliers (1.5 × IQR rule)\n\n\n\n\n\nAn observation is a potential outlier if it falls:\n\nBelow \\(Q_1 - 1.5 \\times \\text{IQR}\\), or\nAbove \\(Q_3 + 1.5 \\times \\text{IQR}\\)\n\n\n\nSalaries (thousands): 45, 48, 52, 55, 58, 62, 68, 72, 85, 250 \n\n\nQ1: 52.75 \n\n\nQ3: 71 \n\n\nIQR: 18.25 \n\n\nLower fence: 25.375 \n\n\nUpper fence: 98.375 \n\n\nOutliers: 250 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe boxplot automatically shows values beyond 1.5 × IQR as individual points.\n\n\n\n\n\n\nTo measure spread, we want to know how far observations are from the center. The deviation of observation \\(x_i\\) from the mean is:\n\\[\nx_i - \\bar{x}\n\\]\nProblem: If we sum all deviations, they always equal zero!\n\n\nData: 2, 4, 6, 8, 10 \n\n\nMean: 6 \n\n\nDeviations: -4, -2, 0, 2, 4 \n\n\nSum of deviations: 0 \n\n\nSolution: Square the deviations before summing!\n\n\n\n\nThe sample variance (\\(s^2\\)) is the average of the squared deviations (with a small adjustment):\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\n\n\n\n\n\n\nWhy n-1?\n\n\n\nWe divide by \\(n-1\\) instead of \\(n\\) because we lose one “degree of freedom” when we estimate the mean from the same data. This gives us an unbiased estimate of the population variance. We’ll discuss this more when we cover inference.\n\n\n\n\n\n\nStep-by-step method:\n\nCalculate the mean \\(\\bar{x}\\)\nSubtract the mean from each value: \\((x_i - \\bar{x})\\)\nSquare each deviation: \\((x_i - \\bar{x})^2\\)\nSum the squared deviations: \\(\\sum(x_i - \\bar{x})^2\\)\nDivide by \\(n-1\\)\n\n\n\n\n\n\n\nData: 4, 7, 5, 9, 10 \n\n\nStep 1 - Mean: 7 \n\n\nStep 2-3 - Deviations and Squared Deviations:\n\n\nx_i      (x_i - x̄)  (x_i - x̄)²                     \n\n\n--------------------------------------------------\n\n\n4        -3.0         9.00           \n7        0.0          0.00           \n5        -2.0         4.00           \n9        2.0          4.00           \n10       3.0          9.00           \n\n\n--------------------------------------------------\n\n\n\nStep 4 - Sum of squared deviations: 26 \n\n\nStep 5 - Variance (s²): 26 / 4 = 6.5 \n\n\n\n\n\n\nThe sample standard deviation (\\(s\\)) is the square root of the variance:\n\\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n\\]\n\n\nData: 4, 7, 5, 9, 10 \n\n\nVariance (s²): 6.5 \n\n\nStandard Deviation (s): 2.55 \n\n\nWhy take the square root?\n\nThe variance is in squared units (e.g., dollars², inches²)\nThe standard deviation is in the original units (e.g., dollars, inches)\nThis makes interpretation much easier!\n\n\n\n\n\nThe standard deviation tells us roughly how far a typical observation is from the mean.\n\n\n\n\n\n\n\n\n\nMean: 70.92 \n\n\nStandard Deviation: 4.56 \n\n\nMost scores fall between 66.4 and 75.5 \n\n\n\n\n\n\n\n\n\n\n\n\n\nProperty\nDescription\n\n\n\n\nAlways non-negative\n\\(s \\geq 0\\), equals 0 only when all values are identical\n\n\nSame units as data\nUnlike variance, which is in squared units\n\n\nSensitive to outliers\nLike the mean, outliers inflate \\(s\\)\n\n\nUseful benchmarks\nFor bell-shaped data: ~68% within 1 SD, ~95% within 2 SD\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure\nFormula\nResistant?\nUses All Data?\n\n\n\n\nRange\nMax - Min\nNo\nNo (only 2 points)\n\n\nIQR\n\\(Q_3 - Q_1\\)\nYes\nNo (only quartiles)\n\n\nVariance\n\\(s^2 = \\frac{\\sum(x_i-\\bar{x})^2}{n-1}\\)\nNo\nYes\n\n\nStd Dev\n\\(s = \\sqrt{s^2}\\)\nNo\nYes\n\n\n\n\n\n\n\n\n\n\nSituation\nRecommended Measure\n\n\n\n\nQuick summary\nRange (but note its limitations)\n\n\nData with outliers\nIQR\n\n\nIdentifying outliers\n1.5 × IQR rule\n\n\nGeneral purpose\nStandard deviation\n\n\nStatistical inference\nVariance/Standard deviation\n\n\n\n\n\n\n\n\n\nRule of Thumb\n\n\n\nReport IQR when you report the median; report standard deviation when you report the mean.\n\n\n\n\n\n\n\n\n\nThe following data represents the number of hours cadets slept last night:\n5, 6, 6, 7, 7, 7, 8, 8, 9\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCalculate the range\nCalculate the IQR\nCalculate the variance and standard deviation\nAre there any outliers using the 1.5 × IQR rule?\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nRange = 9 - 5 = 4 hours\nIQR calculation:\n\nOrdered: 5, 6, 6, 7, 7, 7, 8, 8, 9\nQ1 = 6, Q3 = 8\nIQR = 8 - 6 = 2 hours\n\nVariance and SD:\n\nMean = (5+6+6+7+7+7+8+8+9)/9 = 63/9 = 7\nDeviations: -2, -1, -1, 0, 0, 0, 1, 1, 2\nSquared: 4, 1, 1, 0, 0, 0, 1, 1, 4\nSum = 12\nVariance = 12/8 = 1.5 hours²\nStandard deviation = √1.5 = 1.22 hours\n\nOutlier check:\n\nLower fence = Q1 - 1.5(IQR) = 6 - 1.5(2) = 3\nUpper fence = Q3 + 1.5(IQR) = 8 + 1.5(2) = 11\nAll values between 3 and 11, so no outliers\n\n\n\n\n\n\n\n\n\nTwo PT instructors recorded 2-mile run times (in minutes) for their sections:\nSection A: 14, 15, 15, 16, 16, 16, 17, 17, 18 Section B: 12, 13, 16, 16, 16, 16, 19, 20, 20\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCalculate the mean for each section\nCalculate the standard deviation for each section\nWhich section is more consistent? How do you know?\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\n\nSection A:\n\n\n  Mean: 16 minutes\n\n\n  SD: 1.22 minutes\n\n\nSection B:\n\n\n  Mean: 16.44 minutes\n\n\n  SD: 2.83 minutes\n\n\n\nBoth sections have a mean of approximately 16 minutes\nSection A: SD ≈ 1.22 minutes Section B: SD ≈ 2.92 minutes\nSection A is more consistent because it has a smaller standard deviation. Even though both sections have the same average run time, Section A’s times are clustered more tightly around the mean.\n\n\n\n\n\n\n\n\nA dataset has the following five-number summary: - Min = 10 - Q1 = 25 - Median = 35 - Q3 = 45 - Max = 100\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCalculate the IQR\nDetermine the outlier fences\nIs the maximum value (100) an outlier?\nSketch what the boxplot would look like\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nIQR = Q3 - Q1 = 45 - 25 = 20\nOutlier fences:\n\nLower fence = Q1 - 1.5(IQR) = 25 - 1.5(20) = 25 - 30 = -5\nUpper fence = Q3 + 1.5(IQR) = 45 + 1.5(20) = 45 + 30 = 75\n\nYes, 100 is an outlier because it exceeds the upper fence of 75\nBoxplot sketch:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRange: Max - Min; simple but sensitive to outliers\nIQR: Q3 - Q1; resistant to outliers, measures middle 50%\n1.5 × IQR Rule: Values beyond \\(Q_1 - 1.5 \\times \\text{IQR}\\) or \\(Q_3 + 1.5 \\times \\text{IQR}\\) are potential outliers\nVariance (\\(s^2\\)): Average squared deviation from the mean\nStandard Deviation (\\(s\\)): Square root of variance; in original units\n\n\n\n\n\n\n\nRemember\n\n\n\n\nReport IQR with the median (for skewed data or outliers)\nReport standard deviation with the mean (for symmetric data)\nStandard deviation ≈ “typical distance from the mean”\n\n\n\n\n\n\n\n\n\n\n\nRange and IQR as simple measures of spread\nThe 1.5 × IQR rule for identifying outliers\nVariance and standard deviation as comprehensive measures\nWhen to use each measure of variability\n\nAny questions?\n\n\n\n\nLesson 5: Exploratory Data Analysis Lab\n\nExecute EDA using appropriate graphs and summaries\nJustify choices of displays for variable types\nCommunicate findings with clear, concise annotations\n\n\n\n\n\n\nWebAssign 1.4 - Due before Lesson 5\nExploratory Data Analysis - Due Lesson 9\nWPR I - Lesson 16",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#go-to-vantage",
    "href": "MA206x-AY26-2/lesson-4.html#go-to-vantage",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "Vantage\n\n\n\nLet’s go to Vantage and see if things are working.\n\n\nClass Data",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#lesson-3-review",
    "href": "MA206x-AY26-2/lesson-4.html#lesson-3-review",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "Last lesson we learned three ways to measure the “center” of a distribution:\n\n\n\nMeasure\nFormula\nResistant?\nBest For\n\n\n\n\nMean\n\\(\\bar{x} = \\frac{\\sum x_i}{n}\\)\nNo\nSymmetric data\n\n\nMedian\nMiddle value\nYes\nSkewed data/outliers\n\n\nMode\nMost frequent\nYes\nCategorical data\n\n\n\nWe also learned about percentiles, quartiles, and the five-number summary.\n\n\n\n\n\n\nKey Insight from Lesson 3\n\n\n\nTwo datasets can have the same mean and median but look very different! Today we learn how to quantify that difference.",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#lesson-4-content",
    "href": "MA206x-AY26-2/lesson-4.html#lesson-4-content",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "Calculate and interpret range and inter-quartile range (IQR)\nCalculate and interpret variance and standard deviation\n\n\n\n\nDevore, Section 1.4: Measures of Variability",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#why-measure-variability",
    "href": "MA206x-AY26-2/lesson-4.html#why-measure-variability",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "Consider two archery targets:\n\n\n\n\n\n\n\n\n\nBoth archers might have similar average positions, but Archer A is clearly more consistent. We need measures of spread or variability to capture this difference.",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#the-range",
    "href": "MA206x-AY26-2/lesson-4.html#the-range",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "The range is the simplest measure of spread:\n\\[\n\\text{Range} = \\text{Maximum} - \\text{Minimum}\n\\]\n\n\nQuiz scores: 72, 75, 78, 80, 82, 85, 88 \n\n\nMaximum: 88 \n\n\nMinimum: 72 \n\n\nRange: 16 points\n\n\nProblems with the range:\n\nUses only 2 data points (ignores everything in between)\nVery sensitive to outliers\nTends to increase with sample size",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#the-interquartile-range-iqr",
    "href": "MA206x-AY26-2/lesson-4.html#the-interquartile-range-iqr",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "The interquartile range (IQR) measures the spread of the middle 50% of the data:\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\]\n\n\nQuiz scores: 72, 75, 78, 80, 82, 85, 88 \n\n\nQ1 (25th percentile): 76.5 \n\n\nQ3 (75th percentile): 83.5 \n\n\nIQR: 7 points\n\n\nAdvantages of IQR:\n\nResistant to outliers (uses quartiles, not extremes)\nFocuses on the “typical” spread of the data\nUseful for identifying outliers (1.5 × IQR rule)",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#the-1.5-iqr-rule-for-outliers",
    "href": "MA206x-AY26-2/lesson-4.html#the-1.5-iqr-rule-for-outliers",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "An observation is a potential outlier if it falls:\n\nBelow \\(Q_1 - 1.5 \\times \\text{IQR}\\), or\nAbove \\(Q_3 + 1.5 \\times \\text{IQR}\\)\n\n\n\nSalaries (thousands): 45, 48, 52, 55, 58, 62, 68, 72, 85, 250 \n\n\nQ1: 52.75 \n\n\nQ3: 71 \n\n\nIQR: 18.25 \n\n\nLower fence: 25.375 \n\n\nUpper fence: 98.375 \n\n\nOutliers: 250 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe boxplot automatically shows values beyond 1.5 × IQR as individual points.",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#variance-and-standard-deviation",
    "href": "MA206x-AY26-2/lesson-4.html#variance-and-standard-deviation",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "To measure spread, we want to know how far observations are from the center. The deviation of observation \\(x_i\\) from the mean is:\n\\[\nx_i - \\bar{x}\n\\]\nProblem: If we sum all deviations, they always equal zero!\n\n\nData: 2, 4, 6, 8, 10 \n\n\nMean: 6 \n\n\nDeviations: -4, -2, 0, 2, 4 \n\n\nSum of deviations: 0 \n\n\nSolution: Square the deviations before summing!",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#the-sample-variance",
    "href": "MA206x-AY26-2/lesson-4.html#the-sample-variance",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "The sample variance (\\(s^2\\)) is the average of the squared deviations (with a small adjustment):\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\n\n\n\n\n\n\nWhy n-1?\n\n\n\nWe divide by \\(n-1\\) instead of \\(n\\) because we lose one “degree of freedom” when we estimate the mean from the same data. This gives us an unbiased estimate of the population variance. We’ll discuss this more when we cover inference.",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#computing-the-sample-variance",
    "href": "MA206x-AY26-2/lesson-4.html#computing-the-sample-variance",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "Step-by-step method:\n\nCalculate the mean \\(\\bar{x}\\)\nSubtract the mean from each value: \\((x_i - \\bar{x})\\)\nSquare each deviation: \\((x_i - \\bar{x})^2\\)\nSum the squared deviations: \\(\\sum(x_i - \\bar{x})^2\\)\nDivide by \\(n-1\\)",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#example-calculating-variance",
    "href": "MA206x-AY26-2/lesson-4.html#example-calculating-variance",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "Data: 4, 7, 5, 9, 10 \n\n\nStep 1 - Mean: 7 \n\n\nStep 2-3 - Deviations and Squared Deviations:\n\n\nx_i      (x_i - x̄)  (x_i - x̄)²                     \n\n\n--------------------------------------------------\n\n\n4        -3.0         9.00           \n7        0.0          0.00           \n5        -2.0         4.00           \n9        2.0          4.00           \n10       3.0          9.00           \n\n\n--------------------------------------------------\n\n\n\nStep 4 - Sum of squared deviations: 26 \n\n\nStep 5 - Variance (s²): 26 / 4 = 6.5",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#the-sample-standard-deviation",
    "href": "MA206x-AY26-2/lesson-4.html#the-sample-standard-deviation",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "The sample standard deviation (\\(s\\)) is the square root of the variance:\n\\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n\\]\n\n\nData: 4, 7, 5, 9, 10 \n\n\nVariance (s²): 6.5 \n\n\nStandard Deviation (s): 2.55 \n\n\nWhy take the square root?\n\nThe variance is in squared units (e.g., dollars², inches²)\nThe standard deviation is in the original units (e.g., dollars, inches)\nThis makes interpretation much easier!",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#interpreting-standard-deviation",
    "href": "MA206x-AY26-2/lesson-4.html#interpreting-standard-deviation",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "The standard deviation tells us roughly how far a typical observation is from the mean.\n\n\n\n\n\n\n\n\n\nMean: 70.92 \n\n\nStandard Deviation: 4.56 \n\n\nMost scores fall between 66.4 and 75.5",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#properties-of-standard-deviation",
    "href": "MA206x-AY26-2/lesson-4.html#properties-of-standard-deviation",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "Property\nDescription\n\n\n\n\nAlways non-negative\n\\(s \\geq 0\\), equals 0 only when all values are identical\n\n\nSame units as data\nUnlike variance, which is in squared units\n\n\nSensitive to outliers\nLike the mean, outliers inflate \\(s\\)\n\n\nUseful benchmarks\nFor bell-shaped data: ~68% within 1 SD, ~95% within 2 SD",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#comparing-measures-of-spread",
    "href": "MA206x-AY26-2/lesson-4.html#comparing-measures-of-spread",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "Measure\nFormula\nResistant?\nUses All Data?\n\n\n\n\nRange\nMax - Min\nNo\nNo (only 2 points)\n\n\nIQR\n\\(Q_3 - Q_1\\)\nYes\nNo (only quartiles)\n\n\nVariance\n\\(s^2 = \\frac{\\sum(x_i-\\bar{x})^2}{n-1}\\)\nNo\nYes\n\n\nStd Dev\n\\(s = \\sqrt{s^2}\\)\nNo\nYes\n\n\n\n\n\n\n\n\n\n\nSituation\nRecommended Measure\n\n\n\n\nQuick summary\nRange (but note its limitations)\n\n\nData with outliers\nIQR\n\n\nIdentifying outliers\n1.5 × IQR rule\n\n\nGeneral purpose\nStandard deviation\n\n\nStatistical inference\nVariance/Standard deviation\n\n\n\n\n\n\n\n\n\nRule of Thumb\n\n\n\nReport IQR when you report the median; report standard deviation when you report the mean.",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#practice-problems",
    "href": "MA206x-AY26-2/lesson-4.html#practice-problems",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "The following data represents the number of hours cadets slept last night:\n5, 6, 6, 7, 7, 7, 8, 8, 9\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCalculate the range\nCalculate the IQR\nCalculate the variance and standard deviation\nAre there any outliers using the 1.5 × IQR rule?\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nRange = 9 - 5 = 4 hours\nIQR calculation:\n\nOrdered: 5, 6, 6, 7, 7, 7, 8, 8, 9\nQ1 = 6, Q3 = 8\nIQR = 8 - 6 = 2 hours\n\nVariance and SD:\n\nMean = (5+6+6+7+7+7+8+8+9)/9 = 63/9 = 7\nDeviations: -2, -1, -1, 0, 0, 0, 1, 1, 2\nSquared: 4, 1, 1, 0, 0, 0, 1, 1, 4\nSum = 12\nVariance = 12/8 = 1.5 hours²\nStandard deviation = √1.5 = 1.22 hours\n\nOutlier check:\n\nLower fence = Q1 - 1.5(IQR) = 6 - 1.5(2) = 3\nUpper fence = Q3 + 1.5(IQR) = 8 + 1.5(2) = 11\nAll values between 3 and 11, so no outliers\n\n\n\n\n\n\n\n\n\nTwo PT instructors recorded 2-mile run times (in minutes) for their sections:\nSection A: 14, 15, 15, 16, 16, 16, 17, 17, 18 Section B: 12, 13, 16, 16, 16, 16, 19, 20, 20\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCalculate the mean for each section\nCalculate the standard deviation for each section\nWhich section is more consistent? How do you know?\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\n\nSection A:\n\n\n  Mean: 16 minutes\n\n\n  SD: 1.22 minutes\n\n\nSection B:\n\n\n  Mean: 16.44 minutes\n\n\n  SD: 2.83 minutes\n\n\n\nBoth sections have a mean of approximately 16 minutes\nSection A: SD ≈ 1.22 minutes Section B: SD ≈ 2.92 minutes\nSection A is more consistent because it has a smaller standard deviation. Even though both sections have the same average run time, Section A’s times are clustered more tightly around the mean.\n\n\n\n\n\n\n\n\nA dataset has the following five-number summary: - Min = 10 - Q1 = 25 - Median = 35 - Q3 = 45 - Max = 100\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nCalculate the IQR\nDetermine the outlier fences\nIs the maximum value (100) an outlier?\nSketch what the boxplot would look like\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nIQR = Q3 - Q1 = 45 - 25 = 20\nOutlier fences:\n\nLower fence = Q1 - 1.5(IQR) = 25 - 1.5(20) = 25 - 30 = -5\nUpper fence = Q3 + 1.5(IQR) = 45 + 1.5(20) = 45 + 30 = 75\n\nYes, 100 is an outlier because it exceeds the upper fence of 75\nBoxplot sketch:",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#summary",
    "href": "MA206x-AY26-2/lesson-4.html#summary",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "Range: Max - Min; simple but sensitive to outliers\nIQR: Q3 - Q1; resistant to outliers, measures middle 50%\n1.5 × IQR Rule: Values beyond \\(Q_1 - 1.5 \\times \\text{IQR}\\) or \\(Q_3 + 1.5 \\times \\text{IQR}\\) are potential outliers\nVariance (\\(s^2\\)): Average squared deviation from the mean\nStandard Deviation (\\(s\\)): Square root of variance; in original units\n\n\n\n\n\n\n\nRemember\n\n\n\n\nReport IQR with the median (for skewed data or outliers)\nReport standard deviation with the mean (for symmetric data)\nStandard deviation ≈ “typical distance from the mean”",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  },
  {
    "objectID": "MA206x-AY26-2/lesson-4.html#before-you-leave",
    "href": "MA206x-AY26-2/lesson-4.html#before-you-leave",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "Range and IQR as simple measures of spread\nThe 1.5 × IQR rule for identifying outliers\nVariance and standard deviation as comprehensive measures\nWhen to use each measure of variability\n\nAny questions?\n\n\n\n\nLesson 5: Exploratory Data Analysis Lab\n\nExecute EDA using appropriate graphs and summaries\nJustify choices of displays for variable types\nCommunicate findings with clear, concise annotations\n\n\n\n\n\n\nWebAssign 1.4 - Due before Lesson 5\nExploratory Data Analysis - Due Lesson 9\nWPR I - Lesson 16",
    "crumbs": [
      "MA206x-AY26-2",
      "Lesson 4"
    ]
  }
]