[
["index.html", "Intermediate R 1 Class Introduction 1.1 Expectations 1.2 Distance Learning Challenges 1.3 Class Structure 1.4 Learning Recommendations 1.5 Class Introductions 1.6 Instructors Introduction 1.7 Course Intent 1.8 Tidy Ecosystem 1.9 Get Course Documents 1.10 Prerequisite Packages", " Intermediate R MAJ Dusty Turner and Robert Ward 15 JUN 2020 1 Class Introduction Topics Data Manipulation Data Visualization Functional Programming Data Modeling Text Analysis Incorporating Intermediate R Techniques 1.1 Expectations By taking this course, we assume you have a working knowledge of the following… Base R and RStudio Functionality Basic ‘tidy’ concepts Introductory Statistics Dad Jokes Give it a shot, if its too much or too little, will not be offended if you leave. Its okay to struggle. 1.2 Distance Learning Challenges Expected to be in person Unknown number of students in class Asking Questions - both verbally and in chat How to know if you are following along Speed of going through the material 1.3 Class Structure 7 HOURS! ~ 50:10 Work to break - 1 HR Lunch We will ask for feedback to get a feel for understanding We will try to answer questions in chat 1.4 Learning Recommendations Use RMD files provided in repo (will get this this later) Take notes with comments Execute code along with us Ask questions in chat 1.5 Class Introductions Might pass on this given the classroom size. Around the (virtual) room: Where you are from: Who you work for: How you are involved in Data Science: (ie - statistics, coding, application building, manager, etc) R Coding Experience Favorite sports team: 1.6 Instructors Introduction 1.6.1 MAJ Dusty Turner Army Combat Engineer Platoon Leader / XO / Company Commander Geospatial / Sapper / Route Clearance Hawaii / White Sands Missile Range / Iraq / Afghanistan Education West Point ’07 Operations Research, BS Missouri University of Science and Technology ’12 Engineering Management, MS THE Ohio State ’16 Integrated Systems Engineering, MS Applied Statistics, Graduate Minor Data Science R User Since ’14 Catch me on Twitter @dtdusty http://dusty-turner.netlify.com/ 1.6.2 Robert Ward Education University of Chicago, ’13 Political Science &amp; English, BA Columbia University School of International and Public Affairs, ’18 Master of International Affairs, Specialization in Advanced Policy and Economic Analysis Data Science R user since 2011; also know some python and forgot some Stata Worked for GAO Applied Research &amp; Methods ORSA at CAA and Army Leader Dashboard/Vantage PM team 1.7 Course Intent Be interactive Ask questions at any point Don’t let me move too fast (or too slow) Run the code with me Use course materials (we’ll get to that in a second) 1.8 Tidy Ecosystem 1.9 Get Course Documents github repo SSH: git clone git@github.com:dusty-turner/MORS-Intermediate-R.git HTTPS: https://github.com/dusty-turner/MORS-Intermediate-R.git 1.9.1 {-} R Markdown While we’re here, lets do a quick overview of R Markdown. 1.10 Prerequisite Packages install.packages( &quot;tidyverse&quot;, &quot;janitor&quot;, &quot;gganimate&quot;, &quot;purrr&quot;, &quot;tidymodels&quot;, &quot;tune&quot;, &quot;doFuture&quot;, &quot;vip&quot;, &quot;stringi&quot;, &quot;topicmodels&quot;, &quot;yardstick&quot;, &quot;recipe&quot;, dependencies = TRUE ) "],
["2-data-manipulation.html", "2 Data Manipulation 2.1 Read in Data 2.2 Analysis with dplyr 2.3 Joins 2.4 Other dplyr Tricks", " 2 Data Manipulation In this section, you will learn: Some nuances of reading in data. The basics of dplyr to manipulate data. select mutate filter group_by summarize arrange A few more advanced dplyr concepts. How to do ‘joins’. In this section, we will use the following libraries: library(tidyverse) library(janitor) 2.1 Read in Data read_csv(&quot;data_sources/Batting.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## playerID = col_character(), ## teamID = col_character(), ## lgID = col_character(), ## SF = col_logical(), ## GIDP = col_logical() ## ) ## See spec(...) for full column specifications. ## Warning: 45441 parsing failures. ## row col expected actual file ## 25015 GIDP 1/0/T/F/TRUE/FALSE 2 &#39;data_sources/Batting.csv&#39; ## 25016 GIDP 1/0/T/F/TRUE/FALSE 10 &#39;data_sources/Batting.csv&#39; ## 25018 GIDP 1/0/T/F/TRUE/FALSE 4 &#39;data_sources/Batting.csv&#39; ## 25028 GIDP 1/0/T/F/TRUE/FALSE 8 &#39;data_sources/Batting.csv&#39; ## 25030 GIDP 1/0/T/F/TRUE/FALSE 3 &#39;data_sources/Batting.csv&#39; ## ..... .... .................. ...... .......................... ## See problems(...) for more details. ## # A tibble: 102,816 x 22 ## playerID yearID stint teamID lgID G AB R H `2B` `3B` HR RBI SB CS BB SO IBB HBP SH SF GIDP ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 abercda01 1871 1 TRO &lt;NA&gt; 1 4 0 0 0 0 0 0 0 0 0 0 NA NA NA NA NA ## 2 addybo01 1871 1 RC1 &lt;NA&gt; 25 118 30 32 6 0 0 13 8 1 4 0 NA NA NA NA NA ## 3 allisar01 1871 1 CL1 &lt;NA&gt; 29 137 28 40 4 5 0 19 3 1 2 5 NA NA NA NA NA ## 4 allisdo01 1871 1 WS3 &lt;NA&gt; 27 133 28 44 10 2 2 27 1 1 0 2 NA NA NA NA NA ## 5 ansonca01 1871 1 RC1 &lt;NA&gt; 25 120 29 39 11 3 0 16 6 2 2 1 NA NA NA NA NA ## 6 armstbo01 1871 1 FW1 &lt;NA&gt; 12 49 9 11 2 1 0 5 0 1 0 1 NA NA NA NA NA ## 7 barkeal01 1871 1 RC1 &lt;NA&gt; 1 4 0 1 0 0 0 2 0 0 1 0 NA NA NA NA NA ## 8 barnero01 1871 1 BS1 &lt;NA&gt; 31 157 66 63 10 9 0 34 11 6 13 1 NA NA NA NA NA ## 9 barrebi01 1871 1 FW1 &lt;NA&gt; 1 5 1 1 1 0 0 1 0 0 0 0 NA NA NA NA NA ## 10 barrofr01 1871 1 BS1 &lt;NA&gt; 18 86 13 13 2 1 0 11 1 0 0 0 NA NA NA NA NA ## # ... with 102,806 more rows Fix Read In Errors read_csv Looks at the first 1000 rows of data to guess column types, so it often makes mistakes if those 1000 rows are empty for a specific column. We can specify the types of certain columns, or tell it to look at more rows before guessing data types. read_csv(&quot;data_sources/Batting.csv&quot;, col_types = cols(SF = col_double(), GIDP = col_double())) ## # A tibble: 102,816 x 22 ## playerID yearID stint teamID lgID G AB R H `2B` `3B` HR RBI SB CS BB SO IBB HBP SH SF GIDP ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 1871 1 TRO &lt;NA&gt; 1 4 0 0 0 0 0 0 0 0 0 0 NA NA NA NA NA ## 2 addybo01 1871 1 RC1 &lt;NA&gt; 25 118 30 32 6 0 0 13 8 1 4 0 NA NA NA NA NA ## 3 allisar01 1871 1 CL1 &lt;NA&gt; 29 137 28 40 4 5 0 19 3 1 2 5 NA NA NA NA NA ## 4 allisdo01 1871 1 WS3 &lt;NA&gt; 27 133 28 44 10 2 2 27 1 1 0 2 NA NA NA NA NA ## 5 ansonca01 1871 1 RC1 &lt;NA&gt; 25 120 29 39 11 3 0 16 6 2 2 1 NA NA NA NA NA ## 6 armstbo01 1871 1 FW1 &lt;NA&gt; 12 49 9 11 2 1 0 5 0 1 0 1 NA NA NA NA NA ## 7 barkeal01 1871 1 RC1 &lt;NA&gt; 1 4 0 1 0 0 0 2 0 0 1 0 NA NA NA NA NA ## 8 barnero01 1871 1 BS1 &lt;NA&gt; 31 157 66 63 10 9 0 34 11 6 13 1 NA NA NA NA NA ## 9 barrebi01 1871 1 FW1 &lt;NA&gt; 1 5 1 1 1 0 0 1 0 0 0 0 NA NA NA NA NA ## 10 barrofr01 1871 1 BS1 &lt;NA&gt; 18 86 13 13 2 1 0 11 1 0 0 0 NA NA NA NA NA ## # ... with 102,806 more rows read_csv(&quot;data_sources/Batting.csv&quot;, guess_max = 10000) ## Parsed with column specification: ## cols( ## .default = col_double(), ## playerID = col_character(), ## teamID = col_character(), ## lgID = col_character() ## ) ## See spec(...) for full column specifications. ## # A tibble: 102,816 x 22 ## playerID yearID stint teamID lgID G AB R H `2B` `3B` HR RBI SB CS BB SO IBB HBP SH SF GIDP ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 1871 1 TRO &lt;NA&gt; 1 4 0 0 0 0 0 0 0 0 0 0 NA NA NA NA NA ## 2 addybo01 1871 1 RC1 &lt;NA&gt; 25 118 30 32 6 0 0 13 8 1 4 0 NA NA NA NA NA ## 3 allisar01 1871 1 CL1 &lt;NA&gt; 29 137 28 40 4 5 0 19 3 1 2 5 NA NA NA NA NA ## 4 allisdo01 1871 1 WS3 &lt;NA&gt; 27 133 28 44 10 2 2 27 1 1 0 2 NA NA NA NA NA ## 5 ansonca01 1871 1 RC1 &lt;NA&gt; 25 120 29 39 11 3 0 16 6 2 2 1 NA NA NA NA NA ## 6 armstbo01 1871 1 FW1 &lt;NA&gt; 12 49 9 11 2 1 0 5 0 1 0 1 NA NA NA NA NA ## 7 barkeal01 1871 1 RC1 &lt;NA&gt; 1 4 0 1 0 0 0 2 0 0 1 0 NA NA NA NA NA ## 8 barnero01 1871 1 BS1 &lt;NA&gt; 31 157 66 63 10 9 0 34 11 6 13 1 NA NA NA NA NA ## 9 barrebi01 1871 1 FW1 &lt;NA&gt; 1 5 1 1 1 0 0 1 0 0 0 0 NA NA NA NA NA ## 10 barrofr01 1871 1 BS1 &lt;NA&gt; 18 86 13 13 2 1 0 11 1 0 0 0 NA NA NA NA NA ## # ... with 102,806 more rows Clean Column Names clean_names provides some helpful processing, such as making column names lowercase and replacing spaces and periods with underscores. read_csv(&quot;data_sources/Batting.csv&quot;, col_types = cols(SF = col_double(), GIDP = col_double())) %&gt;% clean_names() ## # A tibble: 102,816 x 22 ## player_id year_id stint team_id lg_id g ab r h x2b x3b hr rbi sb cs bb so ibb hbp sh sf gidp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 1871 1 TRO &lt;NA&gt; 1 4 0 0 0 0 0 0 0 0 0 0 NA NA NA NA NA ## 2 addybo01 1871 1 RC1 &lt;NA&gt; 25 118 30 32 6 0 0 13 8 1 4 0 NA NA NA NA NA ## 3 allisar01 1871 1 CL1 &lt;NA&gt; 29 137 28 40 4 5 0 19 3 1 2 5 NA NA NA NA NA ## 4 allisdo01 1871 1 WS3 &lt;NA&gt; 27 133 28 44 10 2 2 27 1 1 0 2 NA NA NA NA NA ## 5 ansonca01 1871 1 RC1 &lt;NA&gt; 25 120 29 39 11 3 0 16 6 2 2 1 NA NA NA NA NA ## 6 armstbo01 1871 1 FW1 &lt;NA&gt; 12 49 9 11 2 1 0 5 0 1 0 1 NA NA NA NA NA ## 7 barkeal01 1871 1 RC1 &lt;NA&gt; 1 4 0 1 0 0 0 2 0 0 1 0 NA NA NA NA NA ## 8 barnero01 1871 1 BS1 &lt;NA&gt; 31 157 66 63 10 9 0 34 11 6 13 1 NA NA NA NA NA ## 9 barrebi01 1871 1 FW1 &lt;NA&gt; 1 5 1 1 1 0 0 1 0 0 0 0 NA NA NA NA NA ## 10 barrofr01 1871 1 BS1 &lt;NA&gt; 18 86 13 13 2 1 0 11 1 0 0 0 NA NA NA NA NA ## # ... with 102,806 more rows 2.2 Analysis with dplyr Who has the highest career slugging percentage? Let’s save the data we figured out how to correctly read in as an R object and start analyzing it. Select data &lt;- read_csv(&quot;data_sources/Batting.csv&quot;, col_types = cols(SF = col_double(), GIDP = col_double())) %&gt;% clean_names() select allows you to keep only certain columns. While this isn’t always necessary, it can be helpful to make it easier to glance at the data and see what you’re interested in. data %&gt;% select(player_id, year_id, h ,x2b, x3b, hr, ab, g) ## # A tibble: 102,816 x 8 ## player_id year_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 1871 0 0 0 0 4 1 ## 2 addybo01 1871 32 6 0 0 118 25 ## 3 allisar01 1871 40 4 5 0 137 29 ## 4 allisdo01 1871 44 10 2 2 133 27 ## 5 ansonca01 1871 39 11 3 0 120 25 ## 6 armstbo01 1871 11 2 1 0 49 12 ## 7 barkeal01 1871 1 0 0 0 4 1 ## 8 barnero01 1871 63 10 9 0 157 31 ## 9 barrebi01 1871 1 1 0 0 5 1 ## 10 barrofr01 1871 13 2 1 0 86 18 ## # ... with 102,806 more rows There are a number of helper functions that can be used in select to make it easier to “search” for columns to pick. In addition, a minus sign can be used to deselect columns. data %&gt;% select(contains(&quot;id&quot;)) ## # A tibble: 102,816 x 5 ## player_id year_id team_id lg_id gidp ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 abercda01 1871 TRO &lt;NA&gt; NA ## 2 addybo01 1871 RC1 &lt;NA&gt; NA ## 3 allisar01 1871 CL1 &lt;NA&gt; NA ## 4 allisdo01 1871 WS3 &lt;NA&gt; NA ## 5 ansonca01 1871 RC1 &lt;NA&gt; NA ## 6 armstbo01 1871 FW1 &lt;NA&gt; NA ## 7 barkeal01 1871 RC1 &lt;NA&gt; NA ## 8 barnero01 1871 BS1 &lt;NA&gt; NA ## 9 barrebi01 1871 FW1 &lt;NA&gt; NA ## 10 barrofr01 1871 BS1 &lt;NA&gt; NA ## # ... with 102,806 more rows data %&gt;% select(ends_with(&quot;_id&quot;), -lg_id) ## # A tibble: 102,816 x 3 ## player_id year_id team_id ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 abercda01 1871 TRO ## 2 addybo01 1871 RC1 ## 3 allisar01 1871 CL1 ## 4 allisdo01 1871 WS3 ## 5 ansonca01 1871 RC1 ## 6 armstbo01 1871 FW1 ## 7 barkeal01 1871 RC1 ## 8 barnero01 1871 BS1 ## 9 barrebi01 1871 FW1 ## 10 barrofr01 1871 BS1 ## # ... with 102,806 more rows Group_by One immediate challenge we can see in the data is that we have statistics for each year of each player’s career. To get career stats, we’ll need to add up all rows of data for each player. We can do this with the combination of group_by and summarize. The first step, group_by, doesn’t actually do anything to change the data - it essentially sets a flag on the dataframe that lets future dplyr functions know that they should operate on the data by group, instead of operating on the entire dataframe at once. data %&gt;% select(player_id, year_id, h ,x2b, x3b, hr, ab, g) %&gt;% group_by(player_id) ## # A tibble: 102,816 x 8 ## # Groups: player_id [18,915] ## player_id year_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 1871 0 0 0 0 4 1 ## 2 addybo01 1871 32 6 0 0 118 25 ## 3 allisar01 1871 40 4 5 0 137 29 ## 4 allisdo01 1871 44 10 2 2 133 27 ## 5 ansonca01 1871 39 11 3 0 120 25 ## 6 armstbo01 1871 11 2 1 0 49 12 ## 7 barkeal01 1871 1 0 0 0 4 1 ## 8 barnero01 1871 63 10 9 0 157 31 ## 9 barrebi01 1871 1 1 0 0 5 1 ## 10 barrofr01 1871 13 2 1 0 86 18 ## # ... with 102,806 more rows Summarize Summarize runs the requested functions on the dataframe, by group, and returns a dataframe with one row per group and one column per specified summary. Here, we want to get each player’s career hits, doubles, triples, home runs, at-bats, and games, so that we can calculate slugging percentage, so we group by player_id and sum each of those columns. data %&gt;% select(player_id, year_id, h ,x2b, x3b, hr, ab, g) %&gt;% group_by(player_id) %&gt;% summarize(h = sum(h), x2b = sum(x2b), x3b = sum(x3b), hr = sum(hr), ab = sum(ab), g = sum(g)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 18,915 x 7 ## player_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aardsda01 0 0 0 0 4 331 ## 2 aaronha01 3771 624 98 755 12364 3298 ## 3 aaronto01 216 42 6 13 944 437 ## 4 aasedo01 0 0 0 0 5 448 ## 5 abadan01 2 0 0 0 21 15 ## 6 abadfe01 1 0 0 0 9 315 ## 7 abadijo01 11 0 0 0 49 12 ## 8 abbated01 772 99 43 11 3044 855 ## 9 abbeybe01 38 3 3 0 225 79 ## 10 abbeych01 492 67 46 19 1751 451 ## # ... with 18,905 more rows Filter We might only care about players with longer careers; players with very short but successful careers might bias our statistics. Filter allows us to only keep rows of our dataframe that return TRUE for a given logical statement. Here, we only keep rows (players) with more than 1000 career games. We filter after summarizing, so this refers to each player’s career stats; filtering before summarizing, here, would remove all of our data, unless a player somehow figured out how to play 1000 games in a single season! data %&gt;% select(player_id, year_id, h ,x2b, x3b, hr, ab, g) %&gt;% group_by(player_id) %&gt;% summarize(h = sum(h), x2b = sum(x2b), x3b = sum(x3b), hr = sum(hr), ab = sum(ab), g = sum(g)) %&gt;% filter(g &gt; 1000) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 1,564 x 7 ## player_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 3771 624 98 755 12364 3298 ## 2 abreubo01 2470 574 59 288 8480 2425 ## 3 adairje01 1022 163 19 57 4019 1165 ## 4 adamsbo03 1082 188 49 37 4019 1281 ## 5 adamssp01 1588 249 48 9 5557 1424 ## 6 adcocjo01 1832 295 35 336 6606 1959 ## 7 ageeto01 999 170 27 130 3912 1129 ## 8 ainsmed01 707 108 54 22 3048 1078 ## 9 alfoned01 1532 282 18 146 5385 1506 ## 10 alicelu01 1031 189 53 47 3971 1341 ## # ... with 1,554 more rows Mutate Mutate allows us to create or modify columns of our dataframe. Here, we use it to calculate slugging percentage for each row (player). We can refer to other columns with bare column names and use them in our calculations. data %&gt;% select(player_id, year_id, h ,x2b, x3b, hr, ab, g) %&gt;% group_by(player_id) %&gt;% summarize(h = sum(h), x2b = sum(x2b), x3b = sum(x3b), hr = sum(hr), ab = sum(ab), g = sum(g)) %&gt;% filter(g &gt; 1000) %&gt;% mutate(slg = (h + x2b + 2 * x3b + 3 * hr) / ab) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 1,564 x 8 ## player_id h x2b x3b hr ab g slg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 3771 624 98 755 12364 3298 0.555 ## 2 abreubo01 2470 574 59 288 8480 2425 0.475 ## 3 adairje01 1022 163 19 57 4019 1165 0.347 ## 4 adamsbo03 1082 188 49 37 4019 1281 0.368 ## 5 adamssp01 1588 249 48 9 5557 1424 0.353 ## 6 adcocjo01 1832 295 35 336 6606 1959 0.485 ## 7 ageeto01 999 170 27 130 3912 1129 0.412 ## 8 ainsmed01 707 108 54 22 3048 1078 0.324 ## 9 alfoned01 1532 282 18 146 5385 1506 0.425 ## 10 alicelu01 1031 189 53 47 3971 1341 0.369 ## # ... with 1,554 more rows To modify an existing column, just set the column name for the output of mutate to an existing column name. For instance, to round our new slg column to the conventional three digits: data %&gt;% select(player_id, year_id, h ,x2b, x3b, hr, ab, g) %&gt;% group_by(player_id) %&gt;% summarize(h = sum(h), x2b = sum(x2b), x3b = sum(x3b), hr = sum(hr), ab = sum(ab), g = sum(g)) %&gt;% filter(g &gt; 1000) %&gt;% mutate(slg = (h + x2b + 2 * x3b + 3 * hr) / ab) %&gt;% mutate(slg = round(slg, 3)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 1,564 x 8 ## player_id h x2b x3b hr ab g slg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 3771 624 98 755 12364 3298 0.555 ## 2 abreubo01 2470 574 59 288 8480 2425 0.475 ## 3 adairje01 1022 163 19 57 4019 1165 0.347 ## 4 adamsbo03 1082 188 49 37 4019 1281 0.368 ## 5 adamssp01 1588 249 48 9 5557 1424 0.353 ## 6 adcocjo01 1832 295 35 336 6606 1959 0.485 ## 7 ageeto01 999 170 27 130 3912 1129 0.412 ## 8 ainsmed01 707 108 54 22 3048 1078 0.324 ## 9 alfoned01 1532 282 18 146 5385 1506 0.425 ## 10 alicelu01 1031 189 53 47 3971 1341 0.369 ## # ... with 1,554 more rows Arrange arrange sorts dataframes by one or more columns. It sorts in ascending order by default; to sort descending, wrap a column name in desc(). slgdata &lt;- data %&gt;% select(player_id, year_id, h ,x2b, x3b, hr, ab, g) %&gt;% group_by(player_id) %&gt;% summarize(h = sum(h), x2b = sum(x2b), x3b = sum(x3b), hr = sum(hr), ab = sum(ab), g = sum(g)) %&gt;% filter(g&gt;1000) %&gt;% mutate(slg = (h + x2b + 2*x3b + 3*hr)/ab) %&gt;% mutate(slg = round(slg, 3)) %&gt;% arrange(desc(slg)) ## `summarise()` ungrouping output (override with `.groups` argument) slgdata ## # A tibble: 1,564 x 8 ## player_id h x2b x3b hr ab g slg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ruthba01 2873 506 136 714 8398 2503 0.69 ## 2 willite01 2654 525 71 521 7706 2292 0.634 ## 3 gehrilo01 2721 534 163 493 8001 2164 0.632 ## 4 foxxji01 2646 458 125 534 8134 2317 0.609 ## 5 bondsba01 2935 601 77 762 9847 2986 0.607 ## 6 greenha01 1628 379 71 331 5193 1394 0.605 ## 7 mcgwima01 1626 252 6 583 6187 1874 0.588 ## 8 ramirma02 2574 547 20 555 8244 2302 0.585 ## 9 dimagjo01 2214 389 131 361 6821 1736 0.579 ## 10 hornsro01 2930 541 169 301 8173 2259 0.577 ## # ... with 1,554 more rows 2.3 Joins In many projects, you will not have all of the data you need contained in a single table. You can use dplyr’s SQL-style join functions to combine tables. This isn’t always intuitive at first, especially with large amounts of data joining on multiple columns, but it’s dramatically easier and quicker than tools like VLOOKUP once you’re used to it. Player ID Data Our existing player data only has player ids with partial names. However, we have a dataset with lots of additional player information. player_info &lt;- read_csv(&quot;data_sources/Master.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_character(), ## birthYear = col_double(), ## birthMonth = col_double(), ## birthDay = col_double(), ## deathYear = col_double(), ## deathMonth = col_double(), ## deathDay = col_double(), ## weight = col_double(), ## height = col_double(), ## debut = col_date(format = &quot;&quot;), ## finalGame = col_date(format = &quot;&quot;) ## ) ## See spec(...) for full column specifications. player_info ## # A tibble: 19,105 x 24 ## playerID birthYear birthMonth birthDay birthCountry birthState birthCity deathYear deathMonth deathDay deathCountry deathState deathCity nameFirst nameLast nameGiven weight ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 aardsda~ 1981 12 27 USA CO Denver NA NA NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; David Aardsma David Al~ 215 ## 2 aaronha~ 1934 2 5 USA AL Mobile NA NA NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Hank Aaron Henry Lo~ 180 ## 3 aaronto~ 1939 8 5 USA AL Mobile 1984 8 16 USA GA Atlanta Tommie Aaron Tommie L~ 190 ## 4 aasedo01 1954 9 8 USA CA Orange NA NA NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Don Aase Donald W~ 190 ## 5 abadan01 1972 8 25 USA FL Palm Bea~ NA NA NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Andy Abad Fausto A~ 184 ## 6 abadfe01 1985 12 17 D.R. La Romana La Romana NA NA NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Fernando Abad Fernando~ 220 ## 7 abadijo~ 1850 11 4 USA PA Philadel~ 1905 5 17 USA NJ Pemberton John Abadie John W. 192 ## 8 abbated~ 1877 4 15 USA PA Latrobe 1957 1 6 USA FL Fort Lau~ Ed Abbatic~ Edward J~ 170 ## 9 abbeybe~ 1869 11 11 USA VT Essex 1962 6 11 USA VT Colchest~ Bert Abbey Bert Wood 175 ## 10 abbeych~ 1866 10 14 USA NE Falls Ci~ 1926 4 27 USA CA San Fran~ Charlie Abbey Charles ~ 169 ## # ... with 19,095 more rows, and 7 more variables: height &lt;dbl&gt;, bats &lt;chr&gt;, throws &lt;chr&gt;, debut &lt;date&gt;, finalGame &lt;date&gt;, retroID &lt;chr&gt;, bbrefID &lt;chr&gt; Let’s create a single player column from the first and last name columns, and drop everything else besides our player id and name columns. player_names &lt;- player_info %&gt;% select(playerID, nameFirst, nameLast) %&gt;% mutate(player = str_c(nameFirst, &quot; &quot;, nameLast)) %&gt;% select(-starts_with(&quot;name&quot;)) player_names ## # A tibble: 19,105 x 2 ## playerID player ## &lt;chr&gt; &lt;chr&gt; ## 1 aardsda01 David Aardsma ## 2 aaronha01 Hank Aaron ## 3 aaronto01 Tommie Aaron ## 4 aasedo01 Don Aase ## 5 abadan01 Andy Abad ## 6 abadfe01 Fernando Abad ## 7 abadijo01 John Abadie ## 8 abbated01 Ed Abbaticchio ## 9 abbeybe01 Bert Abbey ## 10 abbeych01 Charlie Abbey ## # ... with 19,095 more rows Join the Data A “left join” on dataframes A and B keeps all of table A and adds columns from table B, matching up rows based on one or more specified joining columns. Here, we want to add the player names from B, matching by player id. However, the player id columns in the two dataframes are not spelled exactly the same way, so we have to tell left_join precisely which two columns to match up between dataframes. slgdata %&gt;% left_join(player_names, by = c(&quot;player_id&quot; = &quot;playerID&quot;)) ## # A tibble: 1,564 x 9 ## player_id h x2b x3b hr ab g slg player ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ruthba01 2873 506 136 714 8398 2503 0.69 Babe Ruth ## 2 willite01 2654 525 71 521 7706 2292 0.634 Ted Williams ## 3 gehrilo01 2721 534 163 493 8001 2164 0.632 Lou Gehrig ## 4 foxxji01 2646 458 125 534 8134 2317 0.609 Jimmie Foxx ## 5 bondsba01 2935 601 77 762 9847 2986 0.607 Barry Bonds ## 6 greenha01 1628 379 71 331 5193 1394 0.605 Hank Greenberg ## 7 mcgwima01 1626 252 6 583 6187 1874 0.588 Mark McGwire ## 8 ramirma02 2574 547 20 555 8244 2302 0.585 Manny Ramirez ## 9 dimagjo01 2214 389 131 361 6821 1736 0.579 Joe DiMaggio ## 10 hornsro01 2930 541 169 301 8173 2259 0.577 Rogers Hornsby ## # ... with 1,554 more rows slgname &lt;- slgdata %&gt;% left_join(player_names, by = c(&quot;player_id&quot; = &quot;playerID&quot;)) Rename Columns rename makes it easy to rename columns without changing anything else in a dataframe. It uses the format newname = oldname. slgname %&gt;% rename(doubles = x2b, triples = x3b) ## # A tibble: 1,564 x 9 ## player_id h doubles triples hr ab g slg player ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ruthba01 2873 506 136 714 8398 2503 0.69 Babe Ruth ## 2 willite01 2654 525 71 521 7706 2292 0.634 Ted Williams ## 3 gehrilo01 2721 534 163 493 8001 2164 0.632 Lou Gehrig ## 4 foxxji01 2646 458 125 534 8134 2317 0.609 Jimmie Foxx ## 5 bondsba01 2935 601 77 762 9847 2986 0.607 Barry Bonds ## 6 greenha01 1628 379 71 331 5193 1394 0.605 Hank Greenberg ## 7 mcgwima01 1626 252 6 583 6187 1874 0.588 Mark McGwire ## 8 ramirma02 2574 547 20 555 8244 2302 0.585 Manny Ramirez ## 9 dimagjo01 2214 389 131 361 6821 1736 0.579 Joe DiMaggio ## 10 hornsro01 2930 541 169 301 8173 2259 0.577 Rogers Hornsby ## # ... with 1,554 more rows slgname &lt;- slgname %&gt;% rename(doubles = x2b, triples = x3b) Reorder Columns slgname %&gt;% select(player_id, player, everything()) ## # A tibble: 1,564 x 9 ## player_id player h doubles triples hr ab g slg ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ruthba01 Babe Ruth 2873 506 136 714 8398 2503 0.69 ## 2 willite01 Ted Williams 2654 525 71 521 7706 2292 0.634 ## 3 gehrilo01 Lou Gehrig 2721 534 163 493 8001 2164 0.632 ## 4 foxxji01 Jimmie Foxx 2646 458 125 534 8134 2317 0.609 ## 5 bondsba01 Barry Bonds 2935 601 77 762 9847 2986 0.607 ## 6 greenha01 Hank Greenberg 1628 379 71 331 5193 1394 0.605 ## 7 mcgwima01 Mark McGwire 1626 252 6 583 6187 1874 0.588 ## 8 ramirma02 Manny Ramirez 2574 547 20 555 8244 2302 0.585 ## 9 dimagjo01 Joe DiMaggio 2214 389 131 361 6821 1736 0.579 ## 10 hornsro01 Rogers Hornsby 2930 541 169 301 8173 2259 0.577 ## # ... with 1,554 more rows As of dplyr 1.0.0, there is a new relocate function that moves the specified columns to the front by default. slgname %&gt;% dplyr::relocate(contains(&quot;player&quot;)) ## # A tibble: 1,564 x 9 ## player_id player h doubles triples hr ab g slg ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ruthba01 Babe Ruth 2873 506 136 714 8398 2503 0.69 ## 2 willite01 Ted Williams 2654 525 71 521 7706 2292 0.634 ## 3 gehrilo01 Lou Gehrig 2721 534 163 493 8001 2164 0.632 ## 4 foxxji01 Jimmie Foxx 2646 458 125 534 8134 2317 0.609 ## 5 bondsba01 Barry Bonds 2935 601 77 762 9847 2986 0.607 ## 6 greenha01 Hank Greenberg 1628 379 71 331 5193 1394 0.605 ## 7 mcgwima01 Mark McGwire 1626 252 6 583 6187 1874 0.588 ## 8 ramirma02 Manny Ramirez 2574 547 20 555 8244 2302 0.585 ## 9 dimagjo01 Joe DiMaggio 2214 389 131 361 6821 1736 0.579 ## 10 hornsro01 Rogers Hornsby 2930 541 169 301 8173 2259 0.577 ## # ... with 1,554 more rows We could also then move the slg column so that it’s right after the player columns. slgname %&gt;% dplyr::relocate(contains(&quot;player&quot;)) %&gt;% dplyr::relocate(slg, .after = player) ## # A tibble: 1,564 x 9 ## player_id player slg h doubles triples hr ab g ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ruthba01 Babe Ruth 0.69 2873 506 136 714 8398 2503 ## 2 willite01 Ted Williams 0.634 2654 525 71 521 7706 2292 ## 3 gehrilo01 Lou Gehrig 0.632 2721 534 163 493 8001 2164 ## 4 foxxji01 Jimmie Foxx 0.609 2646 458 125 534 8134 2317 ## 5 bondsba01 Barry Bonds 0.607 2935 601 77 762 9847 2986 ## 6 greenha01 Hank Greenberg 0.605 1628 379 71 331 5193 1394 ## 7 mcgwima01 Mark McGwire 0.588 1626 252 6 583 6187 1874 ## 8 ramirma02 Manny Ramirez 0.585 2574 547 20 555 8244 2302 ## 9 dimagjo01 Joe DiMaggio 0.579 2214 389 131 361 6821 1736 ## 10 hornsro01 Rogers Hornsby 0.577 2930 541 169 301 8173 2259 ## # ... with 1,554 more rows 2.4 Other dplyr Tricks Count count creates tidy-format frequency tables for a given combination of columns. It’s a shortcut for group_by(cols) %&gt;% summarize(n = n()). data %&gt;% count(player_id) ## # A tibble: 18,915 x 2 ## player_id n ## &lt;chr&gt; &lt;int&gt; ## 1 aardsda01 9 ## 2 aaronha01 23 ## 3 aaronto01 7 ## 4 aasedo01 13 ## 5 abadan01 3 ## 6 abadfe01 8 ## 7 abadijo01 2 ## 8 abbated01 10 ## 9 abbeybe01 6 ## 10 abbeych01 5 ## # ... with 18,905 more rows data %&gt;% count(player_id, sort = TRUE) ## # A tibble: 18,915 x 2 ## player_id n ## &lt;chr&gt; &lt;int&gt; ## 1 mcguide01 31 ## 2 henderi01 29 ## 3 newsobo01 29 ## 4 johnto01 28 ## 5 kaatji01 28 ## 6 ansonca01 27 ## 7 baineha01 27 ## 8 carltst01 27 ## 9 moyerja01 27 ## 10 ryanno01 27 ## # ... with 18,905 more rows 2.4.1 Grouped Mutate group_by doesn’t just allow you to summarize - it also allows you to do mutate calculations within each group. For example, to calculate the proportion of their career home runs that each player hit during each year of their career, we could do the following. The key here is that the sum inside the mutate only sums up home runs within each group. prop_hr &lt;- data %&gt;% select(player_id, year_id, hr) %&gt;% group_by(player_id) %&gt;% mutate(prop_career_hr = hr/sum(hr)) prop_hr %&gt;% filter(player_id == &quot;ruthba01&quot;) ## # A tibble: 22 x 4 ## # Groups: player_id [1] ## player_id year_id hr prop_career_hr ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ruthba01 1914 0 0 ## 2 ruthba01 1915 4 0.00560 ## 3 ruthba01 1916 3 0.00420 ## 4 ruthba01 1917 2 0.00280 ## 5 ruthba01 1918 11 0.0154 ## 6 ruthba01 1919 29 0.0406 ## 7 ruthba01 1920 54 0.0756 ## 8 ruthba01 1921 59 0.0826 ## 9 ruthba01 1922 35 0.0490 ## 10 ruthba01 1923 41 0.0574 ## # ... with 12 more rows Conditional verb variants dplyr provides conditional variants of many of the main verbs, ending in _at, _if, and _all. These are most often used with summarize, mutate, or rename to operate on multiple columns at once. The rather wordy summarize that we used earlier can be replaced with a summarize_at that specifies the columns to be summarized and one or more functions to apply to each columns. Anything that would work in select() can be wrapped in vars() to select columns, and function names can be bare, base-R-style anonymous functions, or purrr-style formula notation, which will be covered in section 4. slg_summary &lt;- data %&gt;% group_by(player_id) %&gt;% summarize(h = sum(h), x2b = sum(x2b), x3b = sum(x3b), hr = sum(hr), ab = sum(ab), g = sum(g)) ## `summarise()` ungrouping output (override with `.groups` argument) head(slg_summary) ## # A tibble: 6 x 7 ## player_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aardsda01 0 0 0 0 4 331 ## 2 aaronha01 3771 624 98 755 12364 3298 ## 3 aaronto01 216 42 6 13 944 437 ## 4 aasedo01 0 0 0 0 5 448 ## 5 abadan01 2 0 0 0 21 15 ## 6 abadfe01 1 0 0 0 9 315 # single function with no additional arguments: bare function name concise_slg_summary &lt;- data %&gt;% group_by(player_id) %&gt;% summarize_at(.vars = vars(h, x2b, x3b, hr, ab, g), .funs = sum) concise_slg_summary ## # A tibble: 18,915 x 7 ## player_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aardsda01 0 0 0 0 4 331 ## 2 aaronha01 3771 624 98 755 12364 3298 ## 3 aaronto01 216 42 6 13 944 437 ## 4 aasedo01 0 0 0 0 5 448 ## 5 abadan01 2 0 0 0 21 15 ## 6 abadfe01 1 0 0 0 9 315 ## 7 abadijo01 11 0 0 0 49 12 ## 8 abbated01 772 99 43 11 3044 855 ## 9 abbeybe01 38 3 3 0 225 79 ## 10 abbeych01 492 67 46 19 1751 451 ## # ... with 18,905 more rows Passing a named list of functions as the .funs argument allows you to run multiple summary functions on each selected column and name the resulting summary columns. multi_summary &lt;- data %&gt;% group_by(player_id) %&gt;% summarize_at(.vars = vars(h, x2b, x3b, hr, ab, g), .funs = list(&quot;sum&quot; = sum, &quot;sd&quot; = sd)) multi_summary ## # A tibble: 18,915 x 13 ## player_id h_sum x2b_sum x3b_sum hr_sum ab_sum g_sum h_sd x2b_sd x3b_sd hr_sd ab_sd g_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aardsda01 0 0 0 0 4 331 0 0 0 0 0.726 22.0 ## 2 aaronha01 3771 624 98 755 12364 3298 40.6 10.1 3.84 11.2 98.4 18.5 ## 3 aaronto01 216 42 6 13 944 437 29.7 7.02 1.21 2.79 125. 45.5 ## 4 aasedo01 0 0 0 0 5 448 0 0 0 0 1.39 16.2 ## 5 abadan01 2 0 0 0 21 15 1.15 0 0 0 8.72 4 ## 6 abadfe01 1 0 0 0 9 315 0.354 0 0 0 2.42 18.0 ## 7 abadijo01 11 0 0 0 49 12 6.36 0 0 0 29.0 7.07 ## 8 abbated01 772 99 43 11 3044 855 65.7 9.29 4.55 1.20 250. 66.7 ## 9 abbeybe01 38 3 3 0 225 79 3.78 0.548 0.837 0 27.3 10.5 ## 10 abbeych01 492 67 46 19 1751 451 53.8 8.88 5.40 3.56 170. 41.8 ## # ... with 18,905 more rows The _if variant allows the user to run a dplyr operation on all columns for which some function returns TRUE. For instance, to add the string \"_numeric\" to the names of all numeric columns, we could use rename_if. We’ll run this on a subset of columns to simplify the display. numeric_rename &lt;- data %&gt;% select(player_id, year_id, lg_id, g, ab) %&gt;% rename_if(is.numeric, ~str_c(.x, &quot;_numeric&quot;)) numeric_rename ## # A tibble: 102,816 x 5 ## player_id year_id_numeric lg_id g_numeric ab_numeric ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 1871 &lt;NA&gt; 1 4 ## 2 addybo01 1871 &lt;NA&gt; 25 118 ## 3 allisar01 1871 &lt;NA&gt; 29 137 ## 4 allisdo01 1871 &lt;NA&gt; 27 133 ## 5 ansonca01 1871 &lt;NA&gt; 25 120 ## 6 armstbo01 1871 &lt;NA&gt; 12 49 ## 7 barkeal01 1871 &lt;NA&gt; 1 4 ## 8 barnero01 1871 &lt;NA&gt; 31 157 ## 9 barrebi01 1871 &lt;NA&gt; 1 5 ## 10 barrofr01 1871 &lt;NA&gt; 18 86 ## # ... with 102,806 more rows The _all variant simply runs the same operation on all columns. We’ll select down to a set of numeric columns, and then multiply all of them by two. data %&gt;% select(hr, ab, g) %&gt;% mutate_all(~ .x * 2) ## # A tibble: 102,816 x 3 ## hr ab g ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 8 2 ## 2 0 236 50 ## 3 0 274 58 ## 4 4 266 54 ## 5 0 240 50 ## 6 0 98 24 ## 7 0 8 2 ## 8 0 314 62 ## 9 0 10 2 ## 10 0 172 36 ## # ... with 102,806 more rows 2.4.2 across() The newest version of dplyr (1.0.0) introduced a new, arguably simpler way to achieve the same goals as these conditional function variants: the across function. across can be used with many of the dplyr verbs to operate on a specific subset of columns, specified with the same language as select. For example, across could be used in similar ways as the conditional variants above: multi_summary_across &lt;- data %&gt;% group_by(player_id) %&gt;% summarize(across(c(h, x2b, x3b, hr, ab, g), .funs = list(&quot;sum&quot; = sum, &quot;sd&quot; = sd))) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) multi_summary_across ## # A tibble: 102,816 x 7 ## # Groups: player_id [18,915] ## player_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aardsda01 0 0 0 0 0 11 ## 2 aardsda01 0 0 0 0 2 45 ## 3 aardsda01 0 0 0 0 0 25 ## 4 aardsda01 0 0 0 0 1 47 ## 5 aardsda01 0 0 0 0 0 73 ## 6 aardsda01 0 0 0 0 0 53 ## 7 aardsda01 0 0 0 0 0 1 ## 8 aardsda01 0 0 0 0 0 43 ## 9 aardsda01 0 0 0 0 1 33 ## 10 aaronha01 131 27 6 13 468 122 ## # ... with 102,806 more rows # multiply all selected numeric columns by 2 mutate_numeric_across &lt;- data %&gt;% select(player_id, hr, ab, g) %&gt;% mutate(across(where(is.numeric), ~ .x * 2)) mutate_numeric_across ## # A tibble: 102,816 x 4 ## player_id hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 0 8 2 ## 2 addybo01 0 236 50 ## 3 allisar01 0 274 58 ## 4 allisdo01 4 266 54 ## 5 ansonca01 0 240 50 ## 6 armstbo01 0 98 24 ## 7 barkeal01 0 8 2 ## 8 barnero01 0 314 62 ## 9 barrebi01 0 10 2 ## 10 barrofr01 0 172 36 ## # ... with 102,806 more rows However, across is very new and occasionally has serious performance issues, so it is not used above. More information can be found (here)[https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-colwise/]; 2.4.3 Write Data We will write this file to a folder so we can access it in later lessons / chapters. slgname %&gt;% write_csv(&quot;data_sources/saved_data/slgname.csv&quot;) "],
["3-data-visualization.html", "3 Data Visualization 3.1 Bar Chart 3.2 Scatter Plot 3.3 Animation", " 3 Data Visualization In this section, you will learn: The structure of ggplot. A few techniques to make your plots better. A taste of animation. In this section, we will use the following libraries and data: library(tidyverse) library(gganimate) slgname &lt;- read_csv(&quot;data_sources/saved_data/slgname.csv&quot;) ## Parsed with column specification: ## cols( ## player_id = col_character(), ## h = col_double(), ## doubles = col_double(), ## triples = col_double(), ## hr = col_double(), ## ab = col_double(), ## g = col_double(), ## slg = col_double(), ## player = col_character() ## ) To drive towards an end goal as we learn about ggplot, lets create a visualization to depict the annual hit totals for players with the top 20 total career hits. 3.1 Bar Chart slgname ## # A tibble: 1,564 x 9 ## player_id h doubles triples hr ab g slg player ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ruthba01 2873 506 136 714 8398 2503 0.69 Babe Ruth ## 2 willite01 2654 525 71 521 7706 2292 0.634 Ted Williams ## 3 gehrilo01 2721 534 163 493 8001 2164 0.632 Lou Gehrig ## 4 foxxji01 2646 458 125 534 8134 2317 0.609 Jimmie Foxx ## 5 bondsba01 2935 601 77 762 9847 2986 0.607 Barry Bonds ## 6 greenha01 1628 379 71 331 5193 1394 0.605 Hank Greenberg ## 7 mcgwima01 1626 252 6 583 6187 1874 0.588 Mark McGwire ## 8 ramirma02 2574 547 20 555 8244 2302 0.585 Manny Ramirez ## 9 dimagjo01 2214 389 131 361 6821 1736 0.579 Joe DiMaggio ## 10 hornsro01 2930 541 169 301 8173 2259 0.577 Rogers Hornsby ## # ... with 1,554 more rows Canvas The base layer of any ggplot object is simply the “canvas” in which you will build your plot. slgname %&gt;% ggplot() Mapping We will “map” each element of the data to the canvas. Elements which can be mapped include x, y, color, fill, size, alpha, and a few others. slgname %&gt;% ggplot(aes(x=player, y = hr)) Filter Data To keep the plot clean, we’ll filter for the top 20 players by career hits. slgname %&gt;% slice_max(order_by = h, n = 20) %&gt;% # top_n(20, h) %&gt;% ggplot(aes(x=player, y = hr)) Plot ‘mechanism’ Next we add the type of plot. There are a ton and can be explored here. We will only explore a couple. Also, we should point out the simplicity of ggplot. Each ‘layer’ is added sequentially. slgname %&gt;% slice_max(order_by = h, n = 20) %&gt;% ggplot(aes(x=player, y = hr)) + geom_col() Coordinate Flip Bar plots often suffer from difficult to read x axes. We can fix that with a coordinate flip. slgname %&gt;% slice_max(order_by = h, n = 20) %&gt;% ggplot(aes(x=player, y = hr)) + geom_col() + coord_flip() Reorder Factors This is nice, but it doesn’t easily present the information we are trying to convey. Lets reorder the factors of the barplot. slgname %&gt;% slice_max(order_by = h, n = 20) %&gt;% ggplot(aes(x=fct_reorder(player,hr), y = hr)) + geom_col() + coord_flip() Color Bars Lets add some more information to the plot by coloring the bars by the players slugging percentage. slgname %&gt;% slice_max(order_by = h, n = 20) %&gt;% ggplot(aes(x=fct_reorder(player,hr), y = hr, fill = slg)) + geom_col() + coord_flip() Update Labels Lets update our plot labels to help the plot tell the story of the data. Notice in the labs() function, you simply provide the information you would like to present to the mapped value. slgname %&gt;% slice_max(order_by = h, n = 20) %&gt;% ggplot(aes(x=fct_reorder(player,hr), y = hr, fill = slg)) + geom_col() + coord_flip() + labs(x = &quot;Player&quot;, y = &quot;Home Runs&quot;, title = &quot;Top 20 Home Run Hitters&quot;, fill = &quot;Slugging Percentage&quot;, subtitle = &quot;1871-2016&quot;, caption = &quot;*Among Players Who&#39;ve Played at least 1000 games&quot;) 3.2 Scatter Plot Next, lets show the functionality of the scatter plot. Lets explore the trajectory of the home runs per season since 1990 Lets start with our original data: data &lt;- read_csv(&quot;data_sources/Batting.csv&quot;, col_types = cols(SF = col_double(), GIDP = col_double())) %&gt;% clean_names() data ## # A tibble: 102,816 x 22 ## player_id year_id stint team_id lg_id g ab r h x2b x3b hr rbi sb cs bb so ibb hbp sh sf gidp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 1871 1 TRO &lt;NA&gt; 1 4 0 0 0 0 0 0 0 0 0 0 NA NA NA NA NA ## 2 addybo01 1871 1 RC1 &lt;NA&gt; 25 118 30 32 6 0 0 13 8 1 4 0 NA NA NA NA NA ## 3 allisar01 1871 1 CL1 &lt;NA&gt; 29 137 28 40 4 5 0 19 3 1 2 5 NA NA NA NA NA ## 4 allisdo01 1871 1 WS3 &lt;NA&gt; 27 133 28 44 10 2 2 27 1 1 0 2 NA NA NA NA NA ## 5 ansonca01 1871 1 RC1 &lt;NA&gt; 25 120 29 39 11 3 0 16 6 2 2 1 NA NA NA NA NA ## 6 armstbo01 1871 1 FW1 &lt;NA&gt; 12 49 9 11 2 1 0 5 0 1 0 1 NA NA NA NA NA ## 7 barkeal01 1871 1 RC1 &lt;NA&gt; 1 4 0 1 0 0 0 2 0 0 1 0 NA NA NA NA NA ## 8 barnero01 1871 1 BS1 &lt;NA&gt; 31 157 66 63 10 9 0 34 11 6 13 1 NA NA NA NA NA ## 9 barrebi01 1871 1 FW1 &lt;NA&gt; 1 5 1 1 1 0 0 1 0 0 0 0 NA NA NA NA NA ## 10 barrofr01 1871 1 BS1 &lt;NA&gt; 18 86 13 13 2 1 0 11 1 0 0 0 NA NA NA NA NA ## # ... with 102,806 more rows scatterdat &lt;- data %&gt;% filter(lg_id == &quot;AL&quot;) %&gt;% filter(year_id &gt;= 1990) %&gt;% mutate(team_id = fct_lump(team_id, n = 12)) %&gt;% filter(team_id != &quot;Other&quot;) %&gt;% filter(complete.cases(.)) %&gt;% group_by(team_id,year_id) %&gt;% summarise(across(.cols = c(g:gidp),.fns = sum)) ## dplyr 1.0.0 ## `summarise()` regrouping output by &#39;team_id&#39; (override with `.groups` argument) scatterdat ## # A tibble: 324 x 19 ## # Groups: team_id [12] ## team_id year_id g ab r h x2b x3b hr rbi sb cs bb so ibb hbp sh sf gidp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BAL 1990 2253 5410 669 1328 234 22 132 623 94 52 660 962 50 40 72 41 131 ## 2 BAL 1991 2365 5604 686 1421 256 29 170 660 50 33 528 974 33 33 47 45 147 ## 3 BAL 1992 2183 5485 705 1423 243 36 148 680 89 48 647 827 55 51 50 59 139 ## 4 BAL 1993 2127 5508 786 1470 287 24 157 744 73 54 655 930 52 41 49 56 131 ## 5 BAL 1994 1473 3856 589 1047 185 20 139 557 69 13 438 655 23 39 16 35 89 ## 6 BAL 1995 2046 4837 704 1267 229 27 173 668 92 45 574 803 36 39 40 41 119 ## 7 BAL 1996 2245 5689 949 1557 299 29 257 914 76 40 645 915 49 61 31 67 134 ## 8 BAL 1997 2282 5584 812 1498 264 22 196 780 63 26 586 952 43 65 46 59 121 ## 9 BAL 1998 2359 5565 817 1520 303 11 214 783 86 48 593 903 30 58 44 44 136 ## 10 BAL 1999 2300 5637 851 1572 299 21 203 804 107 46 615 890 34 61 41 55 146 ## # ... with 314 more rows Canvas scatterdat %&gt;% ggplot() Mapping scatterdat %&gt;% ggplot(aes(x=year_id,y=hr,color = team_id)) Plot ‘mechanism’: geom_point() geom_point() creates a scatterplot. scatterdat %&gt;% ggplot(aes(x=year_id,y=hr,color = team_id)) + geom_point() 3.2.1 Arrange Teams by Least to Most Home Runs Using fct_reorder, we can order the teams by most to least home runs. scatterdat %&gt;% ggplot(aes(x=year_id,y=hr,color = fct_reorder(team_id,-hr))) + geom_point() Plot ‘mechanism’: geom_smooth() We can stack layers to help tell the story of our data. scatterdat %&gt;% ggplot(aes(x=year_id,y=hr)) + geom_point(aes(color = fct_reorder(team_id,-hr))) + geom_smooth() How does ggplot determine that line? LOWESS - LOcally WEighted Scatterplot Smoothing LOESS - LOcally Estimated Scatterplot Smoothing Controlled by the term span. Smaller equals more “wigglyness” span = 1 scatterdat %&gt;% ggplot(aes(x=year_id,y=hr)) + geom_point(aes(color = fct_reorder(team_id,-hr))) + geom_smooth(span = 1) span = .1 scatterdat %&gt;% ggplot(aes(x=year_id,y=hr)) + geom_point(aes(color = fct_reorder(team_id,-hr))) + geom_smooth(span = .1) Both scatterdat %&gt;% ggplot(aes(x=year_id,y=hr)) + geom_point(aes(color = fct_reorder(team_id,-hr))) + geom_smooth(span = .1) + geom_smooth(span = 1, color = &quot;red&quot;) Facet Plots ggplot offers the ability to ‘facet’ plots by a variable. This can help show contrast between different factors. scatterdat %&gt;% ggplot(aes(x=year_id,y=hr)) + geom_point(aes(color = fct_reorder(team_id,-hr))) + geom_smooth() + facet_wrap(~team_id) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Update Labels As before, we can update the labels. scatterdat %&gt;% ggplot(aes(x=year_id,y=hr)) + geom_point(aes(color = fct_reorder(team_id,-hr))) + geom_smooth() + facet_wrap(~team_id) + labs(title = &quot;Home Run Changes Over Time&quot;, subtitle = str_c(&quot;From &quot;, min(scatterdat$year_id), &quot; to &quot;, max(scatterdat$year_id)), color = &quot;Team&quot;, x = &quot;Season&quot;, y = &quot;Season Home Run Total&quot;) Change Smoothing Line Earlier, we talked about the LOESS smoother. We can change the line type we impose on our scatter plots. In this instance, we can apply a best fit linear regression line. scatterdat %&gt;% ggplot(aes(x=year_id,y=hr)) + geom_point(aes(color = fct_reorder(team_id,-hr))) + geom_smooth(method = &quot;lm&quot;) + facet_wrap(~team_id) + labs(title = &quot;Home Run Changes Over Time&quot;, subtitle = paste(&quot;From&quot;, min(scatterdat$year_id), &quot;to&quot;, max(scatterdat$year_id)), color = &quot;Team&quot;, x = &quot;Season&quot;, y = &quot;Season Home Run Total&quot;) 3.3 Animation While not rendered here, we will briefly display the ability to animate a ggplot. We’ve created some new data to support animation showing the change in the number of home runs and strike outs over the years. animdata &lt;- data %&gt;% filter(!is.na(lg_id)) %&gt;% group_by(year_id, team_id) %&gt;% summarise(across(.cols = c(hr, so, bb, ab), .fns = ~ sum(., na.rm = TRUE))) %&gt;% ungroup() %&gt;% inner_join(data %&gt;% select(year_id, team_id, lg_id) %&gt;% distinct() ) animdata ## # A tibble: 2,785 x 7 ## year_id team_id hr so bb ab lg_id ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1876 BSN 9 98 58 2722 NL ## 2 1876 CHN 8 45 70 2748 NL ## 3 1876 CN1 4 136 41 2372 NL ## 4 1876 HAR 2 78 39 2664 NL ## 5 1876 LS1 6 98 24 2570 NL ## 6 1876 NY3 2 35 18 2180 NL ## 7 1876 PHN 7 36 27 2387 NL ## 8 1876 SL3 2 63 59 2478 NL ## 9 1877 BSN 4 121 65 2368 NL ## 10 1877 CHN 0 111 57 2273 NL ## # ... with 2,775 more rows We’ll talk through the code below. While it does not render in the book, we’ll run it in console and talk through the process. There are many gganimate options and you can explore further here p &lt;- animdata %&gt;% ggplot(aes(x = so,y = hr, color = bb,size = ab,group = year_id)) + geom_point() + facet_wrap(~ lg_id) + transition_states(year_id,transition_length = 1,state_length = 30) + labs(title = &quot;The Change in Home Runs and Strike Outs Over The Years&quot;, subtitle = &#39;Year: {closest_state}&#39;, x = &quot;Strike Outs&quot;, y = &quot;Home Runs&quot;) + # labs(title = &#39;Year: {closest_state, cache = TRUE}&#39;) + enter_fade() + exit_fade() animate(p,nframes = length(unique(data$year_id)) * 2) "],
["4-functional-programming.html", "4 Functional Programming 4.1 An Interesting Question 4.2 A More General Question 4.3 Even More Generally: Writing a Function 4.4 Iteration with functions: purrr", " 4 Functional Programming In this section, you will learn: How to write a basic function. How to run this function for many inputs. In this section, we will use the following libraries and data: library(tidyverse) library(purrr) hitters &lt;- read_csv(&quot;data_sources/Batting.csv&quot;, guess_max = 10000) ## Parsed with column specification: ## cols( ## .default = col_double(), ## playerID = col_character(), ## teamID = col_character(), ## lgID = col_character() ## ) ## See spec(...) for full column specifications. data &lt;- read_csv(&quot;data_sources/Batting.csv&quot;, guess_max = 10000) %&gt;% janitor::clean_names() ## Parsed with column specification: ## cols( ## .default = col_double(), ## playerID = col_character(), ## teamID = col_character(), ## lgID = col_character() ## ) ## See spec(...) for full column specifications. 4.1 An Interesting Question Who played the most games and hit the most home runs in the 90s in the state of Texas? This question is fairly easy to answer with the tools we learned in the Data Manipulation chapter. data %&gt;% filter(year_id %in% 1990:1999) %&gt;% filter(team_id %in% c(&quot;HOU&quot;,&quot;TEX&quot;)) %&gt;% group_by(player_id, team_id) %&gt;% summarize(g = sum(g), hr = sum(hr)) %&gt;% arrange(desc(g, hr)) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) ## # A tibble: 411 x 4 ## # Groups: player_id [398] ## player_id team_id g hr ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biggicr01 HOU 1515 136 ## 2 bagweje01 HOU 1317 263 ## 3 gonzaju03 TEX 1224 339 ## 4 rodriiv01 TEX 1169 144 ## 5 greerru01 TEX 809 103 ## 6 palmera01 TEX 790 146 ## 7 caminke01 HOU 772 74 ## 8 palmede01 TEX 758 154 ## 9 gonzalu01 HOU 745 62 ## 10 bellde01 HOU 683 74 ## # ... with 401 more rows 4.2 A More General Question What if wanted to be able to easily answer this question for any range of years, teams, and statistics? We could pull out each of these variables, set them ahead of time, and then run a slightly modified version of the above code that uses our newly created variables. Note that this version of the code uses across, mentioned briefly in the Data Manipulation section; it could also be written with summarize_at, but using across makes our next step easier. years &lt;- 1990:1999 teams_chosen &lt;- c(&quot;HOU&quot;, &quot;TEX&quot;) category &lt;- c(&quot;g&quot;, &quot;hr&quot;) data %&gt;% filter(year_id %in% years) %&gt;% filter(team_id %in% teams_chosen) %&gt;% group_by(player_id, team_id) %&gt;% summarize(across(category, sum)) %&gt;% # requires dplyr 1.0.0 arrange(desc(across(all_of(category)))) # requires dplyr 1.0.0 ## # A tibble: 411 x 4 ## # Groups: player_id [398] ## player_id team_id g hr ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biggicr01 HOU 1515 136 ## 2 bagweje01 HOU 1317 263 ## 3 gonzaju03 TEX 1224 339 ## 4 rodriiv01 TEX 1169 144 ## 5 greerru01 TEX 809 103 ## 6 palmera01 TEX 790 146 ## 7 caminke01 HOU 772 74 ## 8 palmede01 TEX 758 154 ## 9 gonzalu01 HOU 745 62 ## 10 bellde01 HOU 683 74 ## # ... with 401 more rows 4.3 Even More Generally: Writing a Function The format above works fine for occasional ad-hoc queries, but if we wanted to answer the question for multiple sets of parameters, we’d have to copy and paste all of this code - and then make sure, every time we edited it in the future, that those edits got made to every single instance in exactly the same way. One way to solve this problem would be to use a for loop, or nested for loops, with different sets of parameters, but even this quickly gets clunky. The most flexible solution is to turn our code into its own function. In the code below, we define a function. We: Give it a name (subset_batting_stats); Define the arguments - the inputs - that will be required; Write the code that will be run, using the input names we’ve defined; Define the value that the function returns; Run it with various parameters to test it out. subset_batting_stats &lt;- function(batting_data, years, teams_chosen, category){ batting_data_subset_summary &lt;- batting_data %&gt;% filter(year_id %in% years) %&gt;% filter(team_id %in% teams_chosen) %&gt;% group_by(player_id, team_id) %&gt;% summarize(across(category, sum)) %&gt;% # requires dplyr 1.0.0 arrange(desc(across(all_of(category)))) # requires dplyr 1.0.0 # summarize_at(vars(all_of(category)), sum) %&gt;% # older/fancier/more general method # arrange(desc(!!!rlang::syms(category))) # older/fancier/more general method return(batting_data_subset_summary) } # Texas in the 90s? subset_batting_stats(batting_data = data, years = 1990:1999, teams_chosen = c(&quot;HOU&quot;, &quot;TEX&quot;), category = c(&quot;g&quot;, &quot;hr&quot;)) ## # A tibble: 411 x 4 ## # Groups: player_id [398] ## player_id team_id g hr ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biggicr01 HOU 1515 136 ## 2 bagweje01 HOU 1317 263 ## 3 gonzaju03 TEX 1224 339 ## 4 rodriiv01 TEX 1169 144 ## 5 greerru01 TEX 809 103 ## 6 palmera01 TEX 790 146 ## 7 caminke01 HOU 772 74 ## 8 palmede01 TEX 758 154 ## 9 gonzalu01 HOU 745 62 ## 10 bellde01 HOU 683 74 ## # ... with 401 more rows This is nice and elegant, but it hasn’t accomplished anything different than what we did before. Below you can see the power of this function as we can easily change the parameters to answer a different question. #Los Angeles in the 2010s? subset_batting_stats(batting_data = data, years = 2010:2019, teams_chosen = c(&quot;LAN&quot;, &quot;LAA&quot;), category = c(&quot;hr&quot;, &quot;g&quot;)) ## # A tibble: 388 x 4 ## # Groups: player_id [371] ## player_id team_id hr g ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 troutmi01 LAA 168 811 ## 2 pujolal01 LAA 146 721 ## 3 kempma01 LAN 121 652 ## 4 gonzaad01 LAN 98 664 ## 5 trumbma01 LAA 95 460 ## 6 ethiean01 LAN 85 853 ## 7 calhoko01 LAA 69 522 ## 8 hunteto01 LAA 62 448 ## 9 puigya01 LAN 57 435 ## 10 kendrho01 LAA 56 724 ## # ... with 378 more rows 4.4 Iteration with functions: purrr Now that we have our custom function, we can use functions from the package purrr to easily run it multiple times with different inputs. (You may have used various flavors of apply in the past - these also work well, and accomplish most of the same things, but purrr functions have a more convenient and consistent interface.) The most basic function that helps us do this is map. map allows us to run a function many times, varying one input between each of these runs. It takes two inputs: the list or vector of different values for your one varying input, and the function you want to run repeatedly, with values set for any other arguments. Functions from purrr allow us to use a special syntax for writing out the function we want to run, based on the R’s formula syntax. Simply put a tilde (~) in front of the function name, and then replace the value to be iterated over with .x. Here, we use this syntax with map to find the Orioles’ top home run hitters in each multiple decades. Our input is a list of numeric vectors, one for each decade, and our output is a list of dataframes. decades &lt;- 1950:2019 %&gt;% split(sort(rep(1:7, 10))) bal_top_hr_decades &lt;- map(decades, ~subset_batting_stats(batting_data = data, years = .x, teams_chosen = &quot;BAL&quot;, category = &quot;hr&quot;)) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) class(bal_top_hr_decades) ## [1] &quot;list&quot; length(bal_top_hr_decades) ## [1] 7 This is still probably too much data to make use of: maybe it would be more helpful to just have the top home run hitter from each decade. We can take our list of output dataframes and map the function head(1) over each of them, to get the first row, since they’re already sorted by hr. At this point, we’ll have a list of ten one-row dataframes, and we might as well row-bind them. Unlike base R’s rbind, dplyr::bind_rows will accept a list of dataframes and bind them together. (Another option would have been to use map_dfr, which runs bind_rows on its list-formatted output.) bal_top1_hr &lt;- bal_top_hr_decades %&gt;% map(~head(.x, 1)) %&gt;% bind_rows() bal_top1_hr ## # A tibble: 7 x 3 ## # Groups: player_id [7] ## player_id team_id hr ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 triangu01 BAL 107 ## 2 powelbo01 BAL 202 ## 3 mayle01 BAL 116 ## 4 murraed02 BAL 254 ## 5 ripkeca01 BAL 198 ## 6 morame01 BAL 158 ## 7 davisch02 BAL 199 This is much more useful, but we do have a small problem: we’ve lost the information about which decade each one came from! One way to rectify this problem is to map a mutate after we run our custom function to add a column that shows which years each row came from. However, we’ll need map2 for this, since we’re going to be iterating over both a list of dataframes and a list of decade vectors. map2 works just like map, except it takes two lists/vectors as inputs, along with a function, and you can specify these inputs in your function as .x and .y. bal_top1_hr_yearcol &lt;- bal_top_hr_decades %&gt;% map2(decades, ~mutate(.x, years = str_c(first(.y), &quot;-&quot;, last(.y)))) %&gt;% map(~head(.x, 1)) %&gt;% bind_rows() bal_top1_hr_yearcol ## # A tibble: 7 x 4 ## # Groups: player_id [7] ## player_id team_id hr years ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 triangu01 BAL 107 1950-1959 ## 2 powelbo01 BAL 202 1960-1969 ## 3 mayle01 BAL 116 1970-1979 ## 4 murraed02 BAL 254 1980-1989 ## 5 ripkeca01 BAL 198 1990-1999 ## 6 morame01 BAL 158 2000-2009 ## 7 davisch02 BAL 199 2010-2019 What if we wanted to iterate over more than two arguments? We can use pmap for this; better yet, pmap will accept a list or dataframe of input combinations, which makes it easy to set up the right combinations of arguments to iterate over. input_combos &lt;- tibble(batting_data = list(data), years = decades[1:4], teams_chosen = c(&quot;BAL&quot;, &quot;HOU&quot;, &quot;TEX&quot;, &quot;NYA&quot;), category = c(&quot;hr&quot;, &quot;rbi&quot;, &quot;g&quot;, &quot;ab&quot;)) input_combos ## # A tibble: 4 x 4 ## batting_data years teams_chosen category ## &lt;list&gt; &lt;named list&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;tibble [102,816 x 22]&gt; &lt;int [10]&gt; BAL hr ## 2 &lt;tibble [102,816 x 22]&gt; &lt;int [10]&gt; HOU rbi ## 3 &lt;tibble [102,816 x 22]&gt; &lt;int [10]&gt; TEX g ## 4 &lt;tibble [102,816 x 22]&gt; &lt;int [10]&gt; NYA ab pmap_out_top3 &lt;- pmap(input_combos, subset_batting_stats) %&gt;% map2(input_combos$years, ~mutate(.x, years = str_c(first(.y), &quot;-&quot;, last(.y)))) %&gt;% map_dfr(~head(.x, 3)) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) ## `summarise()` regrouping output by &#39;player_id&#39; (override with `.groups` argument) # with decades added, as above, and top 3 taken from each dataframe pmap_out_top3 ## # A tibble: 12 x 7 ## # Groups: player_id [12] ## player_id team_id hr years rbi g ab ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 triangu01 BAL 107 1950-1959 NA NA NA ## 2 niemabo01 BAL 62 1950-1959 NA NA NA ## 3 woodlge01 BAL 32 1950-1959 NA NA NA ## 4 wynnji01 HOU NA 1960-1969 441 NA NA ## 5 asprobo01 HOU NA 1960-1969 385 NA NA ## 6 staubru01 HOU NA 1960-1969 370 NA NA ## 7 harrato01 TEX NA 1970-1979 NA 999 NA ## 8 sundbji01 TEX NA 1970-1979 NA 875 NA ## 9 hargrmi01 TEX NA 1970-1979 NA 726 NA ## 10 winfida01 NYA NA 1980-1989 NA NA 4424 ## 11 randowi01 NYA NA 1980-1989 NA NA 4249 ## 12 mattido01 NYA NA 1980-1989 NA NA 4022 ## tidy up... pmap_out_top3 %&gt;% group_by(team_id) %&gt;% mutate(rank = row_number()) %&gt;% pivot_longer(c(rbi, g, ab, hr), names_to = &quot;stat_type&quot;) %&gt;% filter(!is.na(value)) ## # A tibble: 12 x 6 ## # Groups: team_id [4] ## player_id team_id years rank stat_type value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 triangu01 BAL 1950-1959 1 hr 107 ## 2 niemabo01 BAL 1950-1959 2 hr 62 ## 3 woodlge01 BAL 1950-1959 3 hr 32 ## 4 wynnji01 HOU 1960-1969 1 rbi 441 ## 5 asprobo01 HOU 1960-1969 2 rbi 385 ## 6 staubru01 HOU 1960-1969 3 rbi 370 ## 7 harrato01 TEX 1970-1979 1 g 999 ## 8 sundbji01 TEX 1970-1979 2 g 875 ## 9 hargrmi01 TEX 1970-1979 3 g 726 ## 10 winfida01 NYA 1980-1989 1 ab 4424 ## 11 randowi01 NYA 1980-1989 2 ab 4249 ## 12 mattido01 NYA 1980-1989 3 ab 4022 "],
["5-data-modeling.html", "5 Data Modeling 5.1 Prepare Data For Analysis 5.2 Data Exploration 5.3 Split Data test/train 5.4 Preprocess Data 5.5 Apply Preprocessing 5.6 Prepare Cross Validation 5.7 Specify Models 5.8 Create Grid of Parameters to Validate Over 5.9 Execute Cross Validation 5.10 Select Best Parameters 5.11 Execute Models With Best Parameters 5.12 Compare Models 5.13 Run Best Model on All Data 5.14 Run Model on New Data 5.15 Variable Importance", " 5 Data Modeling In the last year or so, Max Kuhn, working for RStudio, has developed tidymodels. Tidymodels is a modeling framework that follows tidy coding principles. It provides a streamlined technique for preprocessing, execution, and validation In this section, you will learn: A small amount of data exploration. The basics of tidymodels splitting data running multiple models cross validation selecting the best model How to parallelize your code In this section, we will use the following libraries and data: library(tidyverse) library(tidymodels) library(recipes) library(tune) library(janitor) library(doFuture) library(yardstick) library(vip) data &lt;- read_csv(&quot;data_sources/Batting.csv&quot;, col_types = cols(SF = col_double(), GIDP = col_double())) %&gt;% clean_names() hofdata &lt;- read_csv(&quot;data_sources/HallOfFame.csv&quot;) ## Parsed with column specification: ## cols( ## playerID = col_character(), ## yearid = col_double(), ## votedBy = col_character(), ## ballots = col_double(), ## needed = col_double(), ## votes = col_double(), ## inducted = col_character(), ## category = col_character(), ## needed_note = col_character() ## ) head(data) ## # A tibble: 6 x 22 ## player_id year_id stint team_id lg_id g ab r h x2b x3b hr rbi sb cs bb so ibb hbp sh sf gidp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 1871 1 TRO &lt;NA&gt; 1 4 0 0 0 0 0 0 0 0 0 0 NA NA NA NA NA ## 2 addybo01 1871 1 RC1 &lt;NA&gt; 25 118 30 32 6 0 0 13 8 1 4 0 NA NA NA NA NA ## 3 allisar01 1871 1 CL1 &lt;NA&gt; 29 137 28 40 4 5 0 19 3 1 2 5 NA NA NA NA NA ## 4 allisdo01 1871 1 WS3 &lt;NA&gt; 27 133 28 44 10 2 2 27 1 1 0 2 NA NA NA NA NA ## 5 ansonca01 1871 1 RC1 &lt;NA&gt; 25 120 29 39 11 3 0 16 6 2 2 1 NA NA NA NA NA ## 6 armstbo01 1871 1 FW1 &lt;NA&gt; 12 49 9 11 2 1 0 5 0 1 0 1 NA NA NA NA NA head(hofdata) ## # A tibble: 6 x 9 ## playerID yearid votedBy ballots needed votes inducted category needed_note ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 cobbty01 1936 BBWAA 226 170 222 Y Player &lt;NA&gt; ## 2 ruthba01 1936 BBWAA 226 170 215 Y Player &lt;NA&gt; ## 3 wagneho01 1936 BBWAA 226 170 215 Y Player &lt;NA&gt; ## 4 mathech01 1936 BBWAA 226 170 205 Y Player &lt;NA&gt; ## 5 johnswa01 1936 BBWAA 226 170 189 Y Player &lt;NA&gt; ## 6 lajoina01 1936 BBWAA 226 170 146 N Player &lt;NA&gt; Lets try to develop an informed answer to the question: What baseball statistics help indicate whether or not a baseball player will make the Hall of Fame? 5.1 Prepare Data For Analysis For this analysis, lets organize our data to identify players who are in the hall of fame as well as those eligible. hofdata &lt;- hofdata %&gt;% clean_names() %&gt;% select(player_id, inducted) %&gt;% mutate(hof = ifelse(inducted==&quot;Y&quot;,1,0)) %&gt;% filter(hof==1) hofdata %&gt;% count(inducted) ## # A tibble: 1 x 2 ## inducted n ## &lt;chr&gt; &lt;int&gt; ## 1 Y 317 years_played &lt;- data %&gt;% group_by(player_id) %&gt;% summarise(across(year_id, .fns = c(&quot;min&quot; = min, &quot;max&quot; = max))) %&gt;% # dplyr 1.0.0 mutate(total_years = year_id_max - year_id_min) ## `summarise()` ungrouping output (override with `.groups` argument) years_played ## # A tibble: 18,915 x 4 ## player_id year_id_min year_id_max total_years ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aardsda01 2004 2015 11 ## 2 aaronha01 1954 1976 22 ## 3 aaronto01 1962 1971 9 ## 4 aasedo01 1977 1990 13 ## 5 abadan01 2001 2006 5 ## 6 abadfe01 2010 2016 6 ## 7 abadijo01 1875 1875 0 ## 8 abbated01 1897 1910 13 ## 9 abbeybe01 1892 1896 4 ## 10 abbeych01 1893 1897 4 ## # ... with 18,905 more rows hof &lt;- data %&gt;% group_by(player_id) %&gt;% summarise_at(vars(g:gidp), list(~sum(.,na.rm = TRUE))) %&gt;% ungroup() %&gt;% left_join(years_played, by = &quot;player_id&quot;) %&gt;% left_join(hofdata, by = &quot;player_id&quot;) %&gt;% mutate(inducted = if_else(inducted==&quot;Y&quot;,1,0)) %&gt;% mutate(inducted = replace_na(inducted, 0)) %&gt;% filter(total_years &gt;= 10) %&gt;% # filter(g&gt;=1000) %&gt;% mutate(inducted = as.factor(inducted)) %&gt;% select(-hof) hof ## # A tibble: 3,475 x 22 ## player_id g ab r h x2b x3b hr rbi sb cs bb so ibb hbp sh sf gidp year_id_min year_id_max total_years inducted ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 aardsda01 331 4 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0 2004 2015 11 0 ## 2 aaronha01 3298 12364 2174 3771 624 98 755 2297 240 73 1402 1383 293 32 21 121 328 1954 1976 22 1 ## 3 aasedo01 448 5 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 1977 1990 13 0 ## 4 abbated01 855 3044 355 772 99 43 11 324 142 0 289 16 0 33 93 0 0 1897 1910 13 0 ## 5 abbotgl01 248 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1973 1984 11 0 ## 6 abbotji01 263 21 0 2 0 0 0 3 0 0 0 10 0 0 3 0 1 1989 1999 10 0 ## 7 abbotpa01 162 20 2 5 1 0 0 2 0 0 0 6 0 0 5 0 0 1990 2004 14 0 ## 8 abernte02 681 181 12 25 3 0 0 9 0 0 6 74 0 2 15 0 4 1955 1972 17 0 ## 9 abreubo01 2425 8480 1453 2470 574 59 288 1363 400 128 1476 1840 115 33 7 85 165 1996 2014 18 0 ## 10 adairje01 1165 4019 378 1022 163 19 57 366 29 29 208 499 31 17 41 30 149 1958 1970 12 0 ## # ... with 3,465 more rows Filter for HOF Eligible Players hof %&gt;% filter(year_id_max &lt;= 2012) %&gt;% count(inducted) ## # A tibble: 2 x 2 ## inducted n ## &lt;fct&gt; &lt;int&gt; ## 1 0 2921 ## 2 1 232 hofmod &lt;- hof %&gt;% filter(year_id_max &lt;= 2012) %&gt;% select(-contains(&quot;year&quot;)) hofmod ## # A tibble: 3,153 x 19 ## player_id g ab r h x2b x3b hr rbi sb cs bb so ibb hbp sh sf gidp inducted ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 aaronha01 3298 12364 2174 3771 624 98 755 2297 240 73 1402 1383 293 32 21 121 328 1 ## 2 aasedo01 448 5 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 ## 3 abbated01 855 3044 355 772 99 43 11 324 142 0 289 16 0 33 93 0 0 0 ## 4 abbotgl01 248 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 5 abbotji01 263 21 0 2 0 0 0 3 0 0 0 10 0 0 3 0 1 0 ## 6 abbotpa01 162 20 2 5 1 0 0 2 0 0 0 6 0 0 5 0 0 0 ## 7 abernte02 681 181 12 25 3 0 0 9 0 0 6 74 0 2 15 0 4 0 ## 8 adairje01 1165 4019 378 1022 163 19 57 366 29 29 208 499 31 17 41 30 149 0 ## 9 adamsba01 482 1019 79 216 31 15 3 75 1 1 53 177 0 1 35 0 0 0 ## 10 adamsbo03 1281 4019 591 1082 188 49 37 303 67 30 414 447 1 17 78 5 62 0 ## # ... with 3,143 more rows hoftest &lt;- hof %&gt;% filter(year_id_max &gt; 2012) %&gt;% select(-contains(&quot;year&quot;),-inducted) hoftest ## # A tibble: 322 x 18 ## player_id g ab r h x2b x3b hr rbi sb cs bb so ibb hbp sh sf gidp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aardsda01 331 4 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0 ## 2 abreubo01 2425 8480 1453 2470 574 59 288 1363 400 128 1476 1840 115 33 7 85 165 ## 3 adamsmi03 410 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 4 affelje01 774 17 0 3 0 0 0 2 0 0 2 6 0 0 0 0 0 ## 5 alberma01 452 35 1 3 1 0 0 0 0 0 0 21 0 0 3 0 0 ## 6 andinro01 481 1344 159 313 58 1 18 97 24 12 113 313 2 6 21 7 42 ## 7 ankieri01 653 1921 260 462 101 10 76 251 21 12 162 555 13 12 8 12 31 ## 8 arroybr01 421 603 35 77 16 0 6 29 1 0 14 266 0 1 82 3 8 ## 9 atchisc01 300 2 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ## 10 ayalalu01 534 14 0 4 1 0 0 0 0 0 0 3 0 0 3 0 0 ## # ... with 312 more rows 5.2 Data Exploration Percentage of players in the dataset who are in the HOF hofmod %&gt;% count(inducted) %&gt;% mutate(prop = n/sum(n)) ## # A tibble: 2 x 3 ## inducted n prop ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 2921 0.926 ## 2 1 232 0.0736 Visual difference in statistics between HOF and non HOF hof %&gt;% select(g:gidp, inducted) %&gt;% pivot_longer(cols = g:gidp) %&gt;% group_by(inducted,name) %&gt;% summarise( lwr_quantile = quantile(value, c(.025)), median = quantile(value, c(.5)), upper_quantile = quantile(value, c(.975)), ) %&gt;% ggplot(aes(x=inducted,y=median)) + geom_point() + geom_errorbar(aes(ymin = lwr_quantile, ymax = upper_quantile)) + theme(legend.position = &quot;none&quot;) + labs(x= &quot;&quot;,y = &quot;&quot;, title = &quot;Differences Between HOF and non-HOF&quot;) + facet_wrap(~name, scales = &quot;free&quot;) ## `summarise()` regrouping output by &#39;inducted&#39; (override with `.groups` argument) Well, won’t players who are in the HOF had played longer? Therefore they should have more hits, home runs, etc? Lets compare the number of games of players in the HOF vs out of the HOF. hof %&gt;% select(g, inducted) %&gt;% ggplot(aes(x=g, fill = inducted)) + geom_density() + labs(x=&quot;Games&quot;, y = &quot;&quot;, fill = &quot;HOF&quot;, title = &quot;Density of Game / HOF Stats&quot;) Lets look at this for every statistic hof %&gt;% select(g:gidp, inducted) %&gt;% pivot_longer(cols = g:gidp) %&gt;% ggplot(aes(x=value, fill = inducted)) + geom_density() + facet_wrap(~name, scales = &quot;free&quot;) + labs(x=&quot;&quot;,y = &quot;&quot;, fill = &quot;HOF&quot;, title = &quot;Density of Stats for Players In/Out of HOF&quot;) 5.3 Split Data test/train To begin modeling, we’ll need to split our data into a testing, training, and validation set. set.seed(str_length(&quot;beatnavy&quot;)) hof_initial_split &lt;- initial_split(hofmod, prop = 0.80) hof_initial_split ## &lt;2523/630/3153&gt; 5.4 Preprocess Data A recipe is a description of what steps should be applied to a data set in order to get it ready for data analysis. tidymodels / recipes currently offers about 30 ‘steps’. More documentation for the recipes package is here We first specify the “recipe” or specify what we are trying to model and with what data. recipe(inducted ~ ., data = training(hof_initial_split)) ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 18 From here, we need to specify other preprocessing steps. We will add these to the recipe. Once we complete these steps, we ‘prep’ the data. preprocessing_recipe &lt;- recipe(inducted ~ ., data = training(hof_initial_split)) %&gt;% step_knnimpute(all_numeric()) %&gt;% step_center(all_numeric()) %&gt;% step_scale(all_numeric()) %&gt;% step_rm(player_id) %&gt;% prep() preprocessing_recipe ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 18 ## ## Training data contained 2523 data points and no missing data. ## ## Operations: ## ## K-nearest neighbor imputation for player_id, ab, r, h, x2b, x3b, hr, rbi, sb, cs, bb, so, ibb, hbp, sh, sf, gidp, g [trained] ## Centering for g, ab, r, h, x2b, x3b, hr, rbi, sb, cs, bb, so, ibb, hbp, sh, sf, gidp [trained] ## Scaling for g, ab, r, h, x2b, x3b, hr, rbi, sb, cs, bb, so, ibb, hbp, sh, sf, gidp [trained] ## Variables removed player_id [trained] 5.5 Apply Preprocessing For a recipe with at least one preprocessing operations that has been trained by recipe(), apply the computations to the data. hof_training_preprocessed_tbl &lt;- preprocessing_recipe %&gt;% bake(training(hof_initial_split)) hof_training_preprocessed_tbl ## # A tibble: 2,523 x 18 ## g ab r h x2b x3b hr rbi sb cs bb so ibb hbp sh sf gidp inducted ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 3.72 3.65 4.29 3.95 3.62 1.99 7.24 4.97 1.69 1.70 3.55 2.83 7.85 0.439 -0.415 4.56 4.65 1 ## 2 -0.770 -1.06 -0.919 -0.980 -0.926 -0.747 -0.606 -0.891 -0.550 -0.619 -0.847 -0.911 -0.444 -0.673 -0.820 -0.590 -0.714 0 ## 3 -1.09 -1.06 -0.919 -0.980 -0.926 -0.747 -0.606 -0.891 -0.550 -0.619 -0.847 -0.920 -0.444 -0.673 -0.820 -0.590 -0.714 0 ## 4 -1.06 -1.05 -0.919 -0.977 -0.926 -0.747 -0.606 -0.884 -0.550 -0.619 -0.847 -0.892 -0.444 -0.673 -0.762 -0.590 -0.697 0 ## 5 -1.22 -1.05 -0.914 -0.973 -0.919 -0.747 -0.606 -0.886 -0.550 -0.619 -0.847 -0.903 -0.444 -0.673 -0.724 -0.590 -0.714 0 ## 6 -0.403 -0.993 -0.890 -0.947 -0.904 -0.747 -0.606 -0.868 -0.550 -0.619 -0.828 -0.719 -0.444 -0.604 -0.531 -0.590 -0.648 0 ## 7 0.360 0.470 -0.0125 0.355 0.262 -0.215 -0.0132 0.0431 -0.279 0.301 -0.195 0.432 0.433 -0.0824 -0.0299 0.688 1.72 0 ## 8 -0.717 -0.674 -0.730 -0.698 -0.700 -0.327 -0.574 -0.700 -0.540 -0.587 -0.681 -0.440 -0.444 -0.638 -0.146 -0.590 -0.714 0 ## 9 0.543 0.470 0.498 0.433 0.444 0.624 -0.221 -0.118 0.0751 0.332 0.452 0.292 -0.416 -0.0824 0.683 -0.377 0.301 0 ## 10 0.768 1.06 1.11 1.09 0.889 0.596 -0.512 0.115 0.886 0.967 0.574 -0.315 -0.444 0.300 1.80 -0.590 -0.485 0 ## # ... with 2,513 more rows 5.6 Prepare Cross Validation This partitions our data into v folds. In our case, 5. This yields a data frame with a nested list of training / testing data. set.seed(str_length(&quot;beatnavy&quot;)) hof_cv_folds &lt;- training(hof_initial_split) %&gt;% bake(preprocessing_recipe, new_data = .) %&gt;% vfold_cv(v = 5) hof_cv_folds ## # 5-fold cross-validation ## # A tibble: 5 x 2 ## splits id ## &lt;named list&gt; &lt;chr&gt; ## 1 &lt;split [2K/505]&gt; Fold1 ## 2 &lt;split [2K/505]&gt; Fold2 ## 3 &lt;split [2K/505]&gt; Fold3 ## 4 &lt;split [2K/504]&gt; Fold4 ## 5 &lt;split [2K/504]&gt; Fold5 5.7 Specify Models Now that we’ve prepared our data, we must specify the models which want to compare. We’ll look at 2. You must first specify the model type (logistic_reg() and rand_forest in the examples below). There are many to choose from in the parsnip package. Documentation can be found here. The motivation behind this package from the documentation, \"Modeling functions across different R packages can have very different interfaces. If you would like to try different approaches, there is a lot of syntactical minutiae to remember. The problem worsens when you move in-between platforms. “parsnip tries to solve this by providing similar interfaces to models. For example, if you are fitting a random forest model and would like to adjust the number of trees in the forest there are different argument names to remember depending on the random forest package you chose…” (such as rf or randomforest). After you specify the model type, you can provide GLM Model glmnet_model &lt;- logistic_reg(mode = &quot;classification&quot;, penalty = tune(), mixture = tune()) %&gt;% set_engine(&quot;glmnet&quot;) glmnet_model ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = tune() ## mixture = tune() ## ## Computational engine: glmnet Random Forest Model Notice how in the previous model and in this model, I specified several tuning parameters. Other than penalty, and mixture, there are many to chose from. These include mode, mtry, trees, and min_n among others.. I can specify these values, but I have chose to ‘tune’ them through cross validation in later steps. forest_model &lt;- rand_forest( mode = &quot;classification&quot;, mtry = tune(), trees = tune(), min_n = tune() ) %&gt;% set_engine(&quot;randomForest&quot;, objective = &quot;reg:squarederror&quot;) forest_model ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = tune() ## min_n = tune() ## ## Engine-Specific Arguments: ## objective = reg:squarederror ## ## Computational engine: randomForest 5.8 Create Grid of Parameters to Validate Over Here we specify the tuning parameters for cross validation and take a look visually at the parameter space we are covering in efforts to reach the best model. GLM Model glmnet_params &lt;- parameters(penalty(), mixture()) glmnet_params ## Collection of 2 parameters for tuning ## ## id parameter type object class ## penalty penalty nparam[+] ## mixture mixture nparam[+] set.seed(str_length(&quot;beatnavy&quot;)) glmnet_grid = grid_max_entropy(glmnet_params, size = 20) glmnet_grid ## # A tibble: 20 x 2 ## penalty mixture ## &lt;dbl&gt; &lt;dbl&gt; ## 1 6.31e- 9 0.558 ## 2 3.73e- 4 0.368 ## 3 2.05e- 9 0.805 ## 4 1.71e-10 0.0903 ## 5 6.00e- 1 0.620 ## 6 2.15e- 8 0.195 ## 7 1.72e- 2 0.850 ## 8 1.78e-10 0.317 ## 9 8.94e- 4 0.947 ## 10 5.45e- 7 0.809 ## 11 3.69e- 5 0.0202 ## 12 3.71e- 2 0.0778 ## 13 3.01e- 7 0.993 ## 14 2.49e- 4 0.703 ## 15 2.49e- 8 0.0277 ## 16 9.08e- 7 0.532 ## 17 3.13e- 1 0.344 ## 18 3.80e-10 0.671 ## 19 6.00e- 7 0.294 ## 20 4.37e- 4 0.176 glmnet_grid %&gt;% ggplot(aes(penalty, mixture)) + geom_point(size = 3) + scale_x_log10() + labs(title = &quot;Max Entropy Grid&quot;, x = &quot;Penalty (log scale)&quot;, y = &quot;Mixture&quot;) Random Forest Model forest_params &lt;- parameters(mtry(c(2,6)), trees(), min_n()) forest_params ## Collection of 3 parameters for tuning ## ## id parameter type object class ## mtry mtry nparam[+] ## trees trees nparam[+] ## min_n min_n nparam[+] set.seed(str_length(&quot;beatnavy&quot;)) forest_grid &lt;- grid_max_entropy(forest_params, size = 30) forest_grid ## # A tibble: 30 x 3 ## mtry trees min_n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 3 1714 5 ## 2 4 368 28 ## 3 3 1607 39 ## 4 3 1207 14 ## 5 5 42 37 ## 6 4 892 27 ## 7 3 1154 25 ## 8 5 1678 2 ## 9 4 607 10 ## 10 3 803 37 ## # ... with 20 more rows 5.9 Execute Cross Validation 5.9.1 Parallel Processing The great thing about cross validation is that tunes parameters to find the model which performs the best on the out-of-sample test data. The problem is that this can be computationally intensive. Thankfully tidymodels is capable of running over multiple cores. You can see from the code below my computer has 4 cores. Your mileage may vary. all_cores &lt;- parallel::detectCores(logical = FALSE) all_cores ## [1] 4 registerDoFuture() cl &lt;- makeCluster(all_cores) plan(cluster, workers = cl) GLM Model The code below kicks off the cross validation of your model. You can see the time it takes to run the code over multiple cores tictoc::tic() glmnet_stage_1_cv_results_tbl &lt;- tune_grid( object = glmnet_model, inducted ~ ., resamples = hof_cv_folds, grid = glmnet_grid, # grid = forest_grid, metrics = metric_set(accuracy, kap, roc_auc), control = control_grid(verbose = TRUE) ) tictoc::toc() ## 37.58 sec elapsed The output of the cross validation is a tibble with nested columns. Of note, in the columns are the data and the performance. glmnet_stage_1_cv_results_tbl ## # 5-fold cross-validation ## # A tibble: 5 x 4 ## splits id .metrics .notes ## * &lt;named list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2K/505]&gt; Fold1 &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt; ## 2 &lt;split [2K/505]&gt; Fold2 &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt; ## 3 &lt;split [2K/505]&gt; Fold3 &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt; ## 4 &lt;split [2K/504]&gt; Fold4 &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt; ## 5 &lt;split [2K/504]&gt; Fold5 &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt; We can manipulate the dataframe to extract the data we need, however, the tune package provides a function to help us out. glmnet_stage_1_cv_results_tbl %&gt;% show_best(&quot;accuracy&quot;, n = 5) %&gt;% bind_rows( glmnet_stage_1_cv_results_tbl %&gt;% show_best(&quot;kap&quot;, n = 5)) %&gt;% bind_rows( glmnet_stage_1_cv_results_tbl %&gt;% show_best(&quot;roc_auc&quot;, n = 5)) ## # A tibble: 15 x 7 ## penalty mixture .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2.05e- 9 0.805 accuracy binary 0.935 5 0.00361 ## 2 6.31e- 9 0.558 accuracy binary 0.935 5 0.00361 ## 3 5.45e- 7 0.809 accuracy binary 0.935 5 0.00361 ## 4 9.08e- 7 0.532 accuracy binary 0.935 5 0.00361 ## 5 3.80e-10 0.671 accuracy binary 0.935 5 0.00348 ## 6 2.05e- 9 0.805 kap binary 0.378 5 0.0273 ## 7 6.31e- 9 0.558 kap binary 0.378 5 0.0273 ## 8 5.45e- 7 0.809 kap binary 0.378 5 0.0273 ## 9 9.08e- 7 0.532 kap binary 0.378 5 0.0273 ## 10 3.80e-10 0.671 kap binary 0.376 5 0.0264 ## 11 6.31e- 9 0.558 roc_auc binary 0.837 5 0.0168 ## 12 9.08e- 7 0.532 roc_auc binary 0.837 5 0.0168 ## 13 3.80e-10 0.671 roc_auc binary 0.837 5 0.0169 ## 14 6.00e- 7 0.294 roc_auc binary 0.836 5 0.0168 ## 15 1.78e-10 0.317 roc_auc binary 0.836 5 0.0168 Random Forest Model This following code took quite a bit of time - but would have taken about 5 times as long if not parallelized. Since we explained the code above, we will not break up this code below. tictoc::tic() forest_stage_1_cv_results_tbl &lt;- tune_grid( formula = inducted ~ ., model = forest_model, resamples = hof_cv_folds, grid = forest_grid, metrics = metric_set(accuracy, kap, roc_auc), control = control_grid(verbose = TRUE) ) tictoc::toc() ## 153.64 sec elapsed forest_stage_1_cv_results_tbl ## # 5-fold cross-validation ## # A tibble: 5 x 4 ## splits id .metrics .notes ## * &lt;named list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2K/505]&gt; Fold1 &lt;tibble [90 x 6]&gt; &lt;tibble [0 x 1]&gt; ## 2 &lt;split [2K/505]&gt; Fold2 &lt;tibble [90 x 6]&gt; &lt;tibble [0 x 1]&gt; ## 3 &lt;split [2K/505]&gt; Fold3 &lt;tibble [90 x 6]&gt; &lt;tibble [0 x 1]&gt; ## 4 &lt;split [2K/504]&gt; Fold4 &lt;tibble [90 x 6]&gt; &lt;tibble [0 x 1]&gt; ## 5 &lt;split [2K/504]&gt; Fold5 &lt;tibble [90 x 6]&gt; &lt;tibble [0 x 1]&gt; forest_stage_1_cv_results_tbl %&gt;% show_best(&quot;accuracy&quot;, n = 5) %&gt;% bind_rows( forest_stage_1_cv_results_tbl %&gt;% show_best(&quot;kap&quot;, n = 5)) %&gt;% bind_rows( forest_stage_1_cv_results_tbl %&gt;% show_best(&quot;roc_auc&quot;, n = 5)) ## # A tibble: 15 x 8 ## mtry trees min_n .metric .estimator mean n std_err ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 6 1096 13 accuracy binary 0.944 5 0.00417 ## 2 3 985 11 accuracy binary 0.944 5 0.00467 ## 3 5 1678 2 accuracy binary 0.944 5 0.00310 ## 4 3 1714 5 accuracy binary 0.943 5 0.00445 ## 5 5 1753 27 accuracy binary 0.943 5 0.00445 ## 6 6 1096 13 kap binary 0.510 5 0.0262 ## 7 5 1678 2 kap binary 0.508 5 0.0214 ## 8 3 985 11 kap binary 0.507 5 0.0294 ## 9 3 1714 5 kap binary 0.505 5 0.0267 ## 10 5 1974 13 kap binary 0.504 5 0.0266 ## 11 3 877 3 roc_auc binary 0.927 5 0.00282 ## 12 3 1714 5 roc_auc binary 0.924 5 0.00193 ## 13 4 906 16 roc_auc binary 0.922 5 0.00275 ## 14 4 607 10 roc_auc binary 0.922 5 0.00148 ## 15 4 1942 21 roc_auc binary 0.922 5 0.00159 5.10 Select Best Parameters Now that both models are cross validated, we can select the tuning parameters which minimized our error metric. params_glmnet_best &lt;- glmnet_stage_1_cv_results_tbl %&gt;% select_best(&quot;roc_auc&quot;) params_glmnet_best ## # A tibble: 1 x 2 ## penalty mixture ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00000000631 0.558 params_forest_best &lt;- forest_stage_1_cv_results_tbl %&gt;% select_best(&quot;roc_auc&quot;) params_forest_best ## # A tibble: 1 x 3 ## mtry trees min_n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 3 877 3 5.11 Execute Models With Best Parameters glmnet_stage_2_model &lt;- glmnet_model %&gt;% finalize_model(parameters = params_glmnet_best) glmnet_stage_2_model ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 6.31006806439131e-09 ## mixture = 0.55818817159161 ## ## Computational engine: glmnet forest_stage_2_model &lt;- forest_model %&gt;% finalize_model(params_forest_best) forest_stage_2_model ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = 3 ## trees = 877 ## min_n = 3 ## ## Engine-Specific Arguments: ## objective = reg:squarederror ## ## Computational engine: randomForest 5.12 Compare Models Now we need to compare the ‘winning’ specifications from our two models to see which one performs best on our error metric. First we apply our preprocessing recipe to our training and test sets. train_processed &lt;- training(hof_initial_split) %&gt;% bake(preprocessing_recipe, new_data = .) test_processed &lt;- testing(hof_initial_split) %&gt;% bake(preprocessing_recipe, new_data = .) Next, we are identifying the variable in which we are predicting (inducted in this instance), then using the metrics function from the yardstick library to estimate the performance of each model. target_expr &lt;- preprocessing_recipe %&gt;% pluck(&quot;last_term_info&quot;) %&gt;% filter(role == &quot;outcome&quot;) %&gt;% pull(variable) %&gt;% sym() glmnet_stage_2_metrics &lt;- glmnet_stage_2_model %&gt;% fit(formula = inducted ~ ., data = train_processed) %&gt;% predict(new_data = test_processed) %&gt;% bind_cols(testing(hof_initial_split)) %&gt;% metrics(!! target_expr, .pred_class) forest_stage_2_metrics &lt;- forest_stage_2_model %&gt;% fit(formula = inducted ~ ., data = train_processed) %&gt;% predict(new_data = test_processed) %&gt;% bind_cols(testing(hof_initial_split)) %&gt;% metrics(!! target_expr, .pred_class) Here, we pretty up the code for easy consumption. glmnet_stage_2_metrics %&gt;% mutate(mod = &quot;glmnet&quot;) %&gt;% bind_rows( forest_stage_2_metrics %&gt;% mutate(mod = &quot;forest&quot;) ) %&gt;% arrange(.metric,-.estimate) ## # A tibble: 4 x 4 ## .metric .estimator .estimate mod ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.946 forest ## 2 accuracy binary 0.941 glmnet ## 3 kap binary 0.425 forest ## 4 kap binary 0.345 glmnet Looks like the Random Forest is the better model. 5.13 Run Best Model on All Data model_final &lt;-forest_stage_2_model %&gt;% fit(inducted ~ . , data = bake(preprocessing_recipe, new_data = hofmod)) 5.14 Run Model on New Data 5.15 Variable Importance vip(model_final) + labs(title = &quot;Random Forest Model Importance - HOF Prediction&quot;) Many Thanks "],
["6-text-analysis.html", "6 Text Analysis 6.1 The Adventures of Tom Sawyer 6.2 Find Chapter Splits 6.3 Tokenize the Book 6.4 Remove ‘stop words’ 6.5 Join Sentiments 6.6 Descriptive Text Statistics 6.7 Visualizations 6.8 N-Gram Analysis 6.9 Term Frequency 6.10 Topic Modeling", " 6 Text Analysis In this section, you will learn. More dplyr More ggplot The basics of tidytext The very basics of topicmodels 6.1 The Adventures of Tom Sawyer library(tidyverse) library(tidytext) library(stringi) library(topicmodels) book_raw &lt;- read_file(&quot;data_sources/The-Adventures-of-Tom-Sawyer.txt&quot;) %&gt;% enframe(name = &quot;Book&quot;) book_raw ## # A tibble: 1 x 2 ## Book value ## &lt;int&gt; &lt;chr&gt; ## 1 1 &quot;The Project Gutenberg EBook of The Adventures of Tom Sawyer, Complete\\r\\n\\r\\nby Mark Twain (Samuel Clemens)\\r\\n\\r\\n\\r\\n\\r\\nThis eBook is for the use of anyone anywhere a~ book_raw %&gt;% nchar() ## Book value ## 1 423754 6.2 Find Chapter Splits To do the analysis, we need to parse the text. The purpose of this section is not a lesson text parsing so we’ll skip the detail. But I will discuss it a little in class. book &lt;- book_raw %&gt;% separate_rows(value, sep = &quot;\\nCHAPTER&quot;) %&gt;% slice(-1) %&gt;% mutate(value = str_remove_all(string = value, pattern = &quot;\\n&quot;)) %&gt;% mutate(value = str_replace(value, &quot;jpg&quot;, &quot;HERE&quot;)) %&gt;% separate(col = &quot;value&quot;, into = c(&quot;Chapter&quot;, &quot;Text&quot;), sep = &quot;HERE&quot;) %&gt;% filter(!is.na(Text)) %&gt;% mutate(Chapter = unlist(str_extract_all(Chapter, &quot;[A-Z]+&quot;))) %&gt;% mutate(Text = str_replace_all(Text, &quot;[.]&quot;,&quot; &quot;)) %&gt;% mutate(Text = str_replace_all(Text, &quot;\\r&quot;,&quot; &quot;)) %&gt;% mutate(Chapter = as.numeric(as.roman(Chapter))) book ## # A tibble: 35 x 3 ## Book Chapter Text ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 &quot; (182K) “TOM!” No answer “TOM!” No answer “What’s gone with that boy, I wonder? You TOM!” No answer The old lady pulled her spectacles down and lo~ ## 2 1 2 &quot; (202K) SATURDAY morning was come, and all the summer world was bright and fresh, and brimming with life There was a song in every heart; and if the heart~ ## 3 1 3 &quot; (197K) TOM presented himself before Aunt Polly, who was sitting by an open window in a pleasant rearward apartment, which was bedroom, breakfast-room, din~ ## 4 1 4 &quot; (218K) THE sun rose upon a tranquil world, and beamed down upon the peaceful village like a benediction Breakfast over, Aunt Polly had family worship: it~ ## 5 1 5 &quot; (205K) ABOUT half-past ten the cracked bell of the small church began to ring, and presently the people began to gather for the morning sermon The Sunday~ ## 6 1 6 &quot; (202K) MONDAY morning found Tom Sawyer miserable Monday morning always found him so—because it began another week’s slow suffering in school He generall~ ## 7 1 7 &quot; (175K) THE harder Tom tried to fasten his mind on his book, the more his ideas wandered So at last, with a sigh and a yawn, he gave it up It seemed to h~ ## 8 1 8 &quot; (195K) TOM dodged hither and thither through lanes until he was well out of the track of returning scholars, and then fell into a moody jog He crossed a ~ ## 9 1 9 &quot; (174K) AT half-past nine, that night, Tom and Sid were sent to bed, as usual They said their prayers, and Sid was soon asleep Tom lay awake and waited, ~ ## 10 1 10 &quot; (171K) THE two boys flew on and on, toward the village, speechless with horror They glanced backward over their shoulders from time to time, apprehensive~ ## # ... with 25 more rows 6.3 Tokenize the Book booktokens &lt;- book %&gt;% unnest_tokens(word, Text) booktokens ## # A tibble: 70,882 x 3 ## Book Chapter word ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 182k ## 2 1 1 tom ## 3 1 1 no ## 4 1 1 answer ## 5 1 1 tom ## 6 1 1 no ## 7 1 1 answer ## 8 1 1 what’s ## 9 1 1 gone ## 10 1 1 with ## # ... with 70,872 more rows 6.4 Remove ‘stop words’ bookstop &lt;- booktokens %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; bookstop ## # A tibble: 26,251 x 3 ## Book Chapter word ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 182k ## 2 1 1 tom ## 3 1 1 answer ## 4 1 1 tom ## 5 1 1 answer ## 6 1 1 what’s ## 7 1 1 boy ## 8 1 1 tom ## 9 1 1 answer ## 10 1 1 lady ## # ... with 26,241 more rows 6.5 Join Sentiments tidytext offers several different sentiment packages. Let’s explore. get_sentiments(lexicon = &quot;afinn&quot;) ## # A tibble: 2,477 x 2 ## word value ## &lt;chr&gt; &lt;dbl&gt; ## 1 abandon -2 ## 2 abandoned -2 ## 3 abandons -2 ## 4 abducted -2 ## 5 abduction -2 ## 6 abductions -2 ## 7 abhor -3 ## 8 abhorred -3 ## 9 abhorrent -3 ## 10 abhors -3 ## # ... with 2,467 more rows get_sentiments(lexicon = &quot;bing&quot;) ## # A tibble: 6,786 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 2-faces negative ## 2 abnormal negative ## 3 abolish negative ## 4 abominable negative ## 5 abominably negative ## 6 abominate negative ## 7 abomination negative ## 8 abort negative ## 9 aborted negative ## 10 aborts negative ## # ... with 6,776 more rows get_sentiments(lexicon = &quot;loughran&quot;) ## # A tibble: 4,150 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abandon negative ## 2 abandoned negative ## 3 abandoning negative ## 4 abandonment negative ## 5 abandonments negative ## 6 abandons negative ## 7 abdicated negative ## 8 abdicates negative ## 9 abdicating negative ## 10 abdication negative ## # ... with 4,140 more rows get_sentiments(lexicon = &quot;nrc&quot;) ## # A tibble: 13,901 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abacus trust ## 2 abandon fear ## 3 abandon negative ## 4 abandon sadness ## 5 abandoned anger ## 6 abandoned fear ## 7 abandoned negative ## 8 abandoned sadness ## 9 abandonment anger ## 10 abandonment fear ## # ... with 13,891 more rows As you can see, each lexicon offers a slightly different way to explore your text. booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) ## Joining, by = &quot;word&quot; ## # A tibble: 70,882 x 4 ## Book Chapter word sentiment ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 182k &lt;NA&gt; ## 2 1 1 tom &lt;NA&gt; ## 3 1 1 no &lt;NA&gt; ## 4 1 1 answer &lt;NA&gt; ## 5 1 1 tom &lt;NA&gt; ## 6 1 1 no &lt;NA&gt; ## 7 1 1 answer &lt;NA&gt; ## 8 1 1 what’s &lt;NA&gt; ## 9 1 1 gone &lt;NA&gt; ## 10 1 1 with &lt;NA&gt; ## # ... with 70,872 more rows booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) ## Joining, by = &quot;word&quot; ## # A tibble: 4,778 x 4 ## Book Chapter word sentiment ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 wonder positive ## 2 1 1 pride positive ## 3 1 1 well positive ## 4 1 1 perplexed negative ## 5 1 1 loud negative ## 6 1 1 enough positive ## 7 1 1 well positive ## 8 1 1 noise negative ## 9 1 1 slack negative ## 10 1 1 well positive ## # ... with 4,768 more rows 6.6 Descriptive Text Statistics booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% count(Chapter,sentiment) ## Joining, by = &quot;word&quot; ## # A tibble: 64 x 3 ## Chapter sentiment n ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 negative 95 ## 2 1 positive 81 ## 3 2 negative 40 ## 4 2 positive 66 ## 5 3 negative 96 ## 6 3 positive 84 ## 7 4 negative 96 ## 8 4 positive 147 ## 9 5 negative 63 ## 10 5 positive 60 ## # ... with 54 more rows 6.7 Visualizations booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% count(Chapter,sentiment) %&gt;% mutate(n = if_else(sentiment == &quot;negative&quot;,n*-1,as.double(n))) %&gt;% group_by(Chapter) %&gt;% mutate(order = cur_group_id()) %&gt;% ## dplyr 1.1.0 summarise(n = sum(n)) %&gt;% mutate(pos = if_else(n&gt;0,&quot;pos&quot;,&quot;neg&quot;)) %&gt;% ungroup() %&gt;% ggplot(aes(x=Chapter,y=n,fill = pos, color = pos)) + geom_col() + scale_fill_manual(values = c(&quot;red&quot;,&quot;green&quot;)) + scale_color_manual(values = c(&quot;black&quot;,&quot;black&quot;)) + theme(legend.position=&quot;none&quot;, axis.text.x = element_text(angle = 90)) + labs(y = &quot;Net Positive Words&quot;, title = &quot;Sentiment Analysis of &#39;The Adventures of Tom Sawyer&#39;&quot;, subtitle = &quot;Net Positive Words by Chapter&quot;) ## Joining, by = &quot;word&quot; ## `summarise()` ungrouping output (override with `.groups` argument) 6.8 N-Gram Analysis 6.8.1 Uni-Grams booktokens %&gt;% count(word, sort = TRUE) ## # A tibble: 7,774 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 the 3708 ## 2 and 3059 ## 3 a 1807 ## 4 to 1696 ## 5 of 1474 ## 6 he 1158 ## 7 was 1126 ## 8 it 1090 ## 9 in 943 ## 10 that 875 ## # ... with 7,764 more rows 6.8.2 Remove Stop Words booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% count(word,sentiment, sort = TRUE) ## Joining, by = &quot;word&quot; ## # A tibble: 1,358 x 3 ## word sentiment n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 well positive 152 ## 2 like positive 113 ## 3 good positive 101 ## 4 work positive 88 ## 5 right positive 83 ## 6 great positive 68 ## 7 dead negative 59 ## 8 enough positive 57 ## 9 poor negative 52 ## 10 cave negative 41 ## # ... with 1,348 more rows 6.8.3 Visualize booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;% filter(!is.na(sentiment)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% group_by(sentiment) %&gt;% top_n(10, n) %&gt;% ungroup() %&gt;% ggplot(aes(x=fct_reorder(word,n), y = n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + coord_flip() + labs(x=&quot;Word&quot;) 6.8.4 Bigrams bookbitokens &lt;- book %&gt;% unnest_tokens(bigram, Text, token = &quot;ngrams&quot;, n = 2, n_min = 2) bookbitokens ## # A tibble: 70,848 x 3 ## Book Chapter bigram ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 182k tom ## 2 1 1 tom no ## 3 1 1 no answer ## 4 1 1 answer tom ## 5 1 1 tom no ## 6 1 1 no answer ## 7 1 1 answer what’s ## 8 1 1 what’s gone ## 9 1 1 gone with ## 10 1 1 with that ## # ... with 70,838 more rows bookbitokens %&gt;% count(bigram, sort = TRUE) ## # A tibble: 41,080 x 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 of the 364 ## 2 in the 298 ## 3 and the 184 ## 4 it was 175 ## 5 to the 175 ## 6 he was 147 ## 7 and then 126 ## 8 was a 116 ## 9 he had 110 ## 10 there was 110 ## # ... with 41,070 more rows 6.8.5 Remove Stop Words in Bigrams bookbitokens %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) ## # A tibble: 70,848 x 4 ## Book Chapter word1 word2 ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 182k tom ## 2 1 1 tom no ## 3 1 1 no answer ## 4 1 1 answer tom ## 5 1 1 tom no ## 6 1 1 no answer ## 7 1 1 answer what’s ## 8 1 1 what’s gone ## 9 1 1 gone with ## 10 1 1 with that ## # ... with 70,838 more rows bigrams &lt;- bookbitokens %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) bigrams %&gt;% count(word1, word2, sort = TRUE) ## # A tibble: 6,910 x 3 ## word1 word2 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 project gutenberg 84 ## 2 gutenberg tm 56 ## 3 injun joe 45 ## 4 aunt polly 42 ## 5 tom sawyer 23 ## 6 injun joe’s 18 ## 7 tm electronic 18 ## 8 muff potter 15 ## 9 archive foundation 13 ## 10 gutenberg literary 13 ## # ... with 6,900 more rows 6.8.6 Visualize Bigrams bigrams %&gt;% unite(col = &quot;bigram&quot;, word1,word2, sep = &quot; &quot;) %&gt;% count(bigram, sort = TRUE) %&gt;% top_n(20) %&gt;% ggplot(aes(x=fct_reorder(bigram,n),y = n)) + geom_col() + coord_flip() + labs(x=&quot;Bigram&quot;,y = &quot;Count&quot;, title = &quot;Top Bigrams&quot;) ## Selecting by n 6.9 Term Frequency Term Frequency: The number of times that a term occurs in the book. Inverse Document Frequency: \\(\\ln(\\frac{Total Number of Documents, cache = TRUE}{Total Number of Documents Containing Specified Word, cache = TRUE})\\): Measure of how much information the word provides. Term Frequency - Inverse Document Frequency: Term Frequency * Inverse Document Frequency 6.9.1 Build TF-IDF Data Words By Chapter booktokens %&gt;% count(Chapter, word, sort = TRUE, name = &quot;count&quot;) %&gt;% add_count(word) %&gt;% spread(Chapter, count) %&gt;% arrange(desc(n)) ## # A tibble: 7,774 x 36 ## word n `1` `2` `3` `4` `5` `6` `7` `8` `9` `10` `11` `12` `13` `14` `15` `16` `17` `18` `19` `20` `21` `22` `23` `24` `25` `27` `28` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 a 32 66 75 62 132 58 94 34 53 54 52 29 42 81 78 36 129 20 56 26 NA 56 42 41 7 NA 32 25 ## 2 all 32 4 8 9 20 7 12 12 6 11 2 3 9 9 6 4 28 5 11 5 NA 9 3 12 1 NA 8 7 ## 3 and 32 102 77 110 166 101 139 96 88 99 87 66 77 113 107 86 269 67 138 28 NA 100 49 62 13 NA 27 41 ## 4 as 32 16 4 12 18 22 14 10 7 12 10 11 9 9 20 5 26 11 27 2 NA 7 6 14 5 NA 6 8 ## 5 be 32 4 5 4 15 7 3 6 11 9 4 5 8 16 7 8 21 3 16 5 NA 10 3 10 3 NA 7 4 ## 6 before 32 3 3 4 2 2 3 4 3 2 3 6 2 2 4 5 8 4 2 1 NA 1 2 4 3 NA 2 2 ## 7 but 32 19 16 18 22 14 33 22 11 11 18 10 13 28 11 13 46 9 25 8 NA 14 9 21 6 NA 4 7 ## 8 for 32 26 11 18 46 27 15 9 6 14 10 12 16 15 12 17 45 3 20 3 NA 11 14 17 3 NA 7 6 ## 9 got 32 6 6 6 7 1 10 4 2 5 7 2 1 4 3 2 13 3 7 4 NA 3 4 4 2 NA 1 6 ## 10 had 32 13 13 22 27 15 13 8 17 9 8 17 12 21 10 11 45 9 19 6 NA 15 10 5 12 NA 12 5 ## # ... with 7,764 more rows, and 7 more variables: `29` &lt;int&gt;, `30` &lt;int&gt;, `31` &lt;int&gt;, `32` &lt;int&gt;, `33` &lt;int&gt;, `34` &lt;int&gt;, `35` &lt;int&gt; Word Frequency Per Chapter and Book booktokens %&gt;% count(Chapter, word, sort = TRUE, name = &quot;Chapter_Total&quot;) %&gt;% left_join( booktokens %&gt;% count(word, sort = TRUE, name = &quot;Book_Total&quot;) ) ## Joining, by = &quot;word&quot; ## # A tibble: 24,205 x 4 ## Chapter word Chapter_Total Book_Total ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 16 the 275 3708 ## 2 16 and 269 3059 ## 3 35 the 245 3708 ## 4 33 the 205 3708 ## 5 30 the 187 3708 ## 6 4 the 185 3708 ## 7 5 the 167 3708 ## 8 4 and 166 3059 ## 9 29 the 166 3708 ## 10 21 the 162 3708 ## # ... with 24,195 more rows Create TF-IDF booktokens %&gt;% count(Chapter, word, sort = TRUE, name = &quot;Chapter_Total&quot;) %&gt;% left_join( booktokens %&gt;% count(word, sort = TRUE, name = &quot;Book_Total&quot;) ) %&gt;% bind_tf_idf(word, Chapter, Chapter_Total) %&gt;% filter(Chapter_Total!=Book_Total) %&gt;% filter(tf&lt;1) %&gt;% arrange(-tf_idf) ## Joining, by = &quot;word&quot; ## # A tibble: 19,903 x 7 ## Chapter word Chapter_Total Book_Total tf idf tf_idf ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25 193k 1 2 0.5 2.83 1.42 ## 2 20 178k 1 3 0.5 2.43 1.21 ## 3 20 t 1 4 0.5 2.43 1.21 ## 4 25 t 1 4 0.5 2.43 1.21 ## 5 35 works 32 33 0.00636 2.83 0.0180 ## 6 19 auntie 6 17 0.00754 2.14 0.0161 ## 7 34 jones 5 9 0.00563 2.83 0.0160 ## 8 32 cave 9 41 0.00863 1.58 0.0136 ## 9 35 e 23 24 0.00457 2.83 0.0129 ## 10 11 potter 10 39 0.00667 1.92 0.0128 ## # ... with 19,893 more rows 6.9.2 Visualize TF-IDF booktokens %&gt;% count(Chapter, word, sort = TRUE, name = &quot;Chapter_Total&quot;) %&gt;% left_join( booktokens %&gt;% count(word, sort = TRUE, name = &quot;Book_Total&quot;) ) %&gt;% bind_tf_idf(word, Chapter, Chapter_Total) %&gt;% filter(Chapter_Total!=Book_Total) %&gt;% filter(tf&lt;1) %&gt;% arrange(-tf_idf) %&gt;% group_by(Chapter) %&gt;% top_n(4) %&gt;% ungroup() %&gt;% mutate(word = fct_reorder(word, tf_idf)) %&gt;% filter(Chapter &lt;= 12) %&gt;% ggplot(aes(x = word,y = tf_idf, fill = Chapter)) + geom_col(show.legend = FALSE) + facet_wrap(~Chapter, scales = &quot;free&quot;, ncol = 4) + coord_flip() ## Joining, by = &quot;word&quot; ## Selecting by tf_idf 6.10 Topic Modeling Create Document Term Matrix bookdtm &lt;- booktokens %&gt;% left_join(get_sentiments(&quot;nrc&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% select(Chapter,word) %&gt;% count(Chapter,word) %&gt;% rename(document = Chapter, term = word, count = n) %&gt;% mutate(document = as.integer(document), count = as.double(count)) %&gt;% cast_dtm(document, term, count) ## Joining, by = &quot;word&quot; Create a reproducible example of two topics lda &lt;- LDA(bookdtm, k = 2, control = list(seed = 1234)) lda ## A LDA_VEM topic model with 2 topics. Extract Topics and ‘Beta’ of each topic. Beta represents topic-word density. Beta: In each topic, how dense is this word? Higher is more dense. Lower is less dense topics &lt;- tidy(lda, matrix = &quot;beta&quot;) ## Warning: `tbl_df()` is deprecated as of dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. topics ## # A tibble: 3,526 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 advantage 3.10e- 4 ## 2 2 advantage 9.02e-79 ## 3 1 adventurous 1.03e- 4 ## 4 2 adventurous 7.65e-79 ## 5 1 afraid 1.95e- 3 ## 6 2 afraid 2.91e- 3 ## 7 1 arrest 1.03e- 4 ## 8 2 arrest 3.63e-79 ## 9 1 astronomer 4.14e- 4 ## 10 2 astronomer 4.49e-78 ## # ... with 3,516 more rows topics %&gt;% arrange(topic,-beta) ## # A tibble: 3,526 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 good 0.0333 ## 2 1 boy 0.0176 ## 3 1 money 0.0152 ## 4 1 found 0.0119 ## 5 1 aunt 0.0116 ## 6 1 white 0.0105 ## 7 1 time 0.0104 ## 8 1 awful 0.00809 ## 9 1 tree 0.00797 ## 10 1 mother 0.00794 ## # ... with 3,516 more rows Top Terms top_terms &lt;- topics %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) top_terms ## # A tibble: 20 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 good 0.0333 ## 2 1 boy 0.0176 ## 3 1 money 0.0152 ## 4 1 found 0.0119 ## 5 1 aunt 0.0116 ## 6 1 white 0.0105 ## 7 1 time 0.0104 ## 8 1 awful 0.00809 ## 9 1 tree 0.00797 ## 10 1 mother 0.00794 ## 11 2 good 0.0279 ## 12 2 found 0.0209 ## 13 2 hope 0.0181 ## 14 2 awful 0.0162 ## 15 2 boy 0.0106 ## 16 2 time 0.0105 ## 17 2 muff 0.00911 ## 18 2 devil 0.00911 ## 19 2 young 0.00895 ## 20 2 murder 0.00820 top_terms %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + coord_flip() + scale_x_reordered() 6.10.1 Comparison of Use Between Topics beta_spread &lt;- topics %&gt;% mutate(topic = paste0(&quot;topic&quot;, topic)) %&gt;% spread(topic, beta) %&gt;% filter(topic1 &gt; .001 | topic2 &gt; .001) %&gt;% mutate(log_ratio = log2(topic2 / topic1)) beta_spread %&gt;% top_n(10, log_ratio) %&gt;% arrange(-log_ratio) ## # A tibble: 10 x 4 ## term topic1 topic2 log_ratio ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 outburst 4.53e-85 0.00106 270. ## 2 including 9.30e-85 0.00137 270. ## 3 freely 1.25e-84 0.00182 270. ## 4 providing 2.22e-84 0.00243 269. ## 5 fee 2.40e-84 0.00243 269. ## 6 worry 1.29e-84 0.00122 269. ## 7 damages 1.54e-84 0.00122 269. ## 8 agreement 8.21e-84 0.00547 268. ## 9 provide 3.84e-84 0.00213 268. ## 10 information 2.79e-84 0.00122 268. beta_spread %&gt;% top_n(-10, log_ratio) %&gt;% arrange(log_ratio) ## # A tibble: 10 x 4 ## term topic1 topic2 log_ratio ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 labor 0.00259 9.14e-79 -251. ## 2 worship 0.00103 3.53e-78 -247. ## 3 owing 0.00290 1.88e-77 -246. ## 4 cutting 0.00103 7.69e-78 -246. ## 5 highest 0.00124 1.96e-77 -245. ## 6 comrade 0.00166 3.32e-77 -245. ## 7 music 0.00155 6.73e-77 -244. ## 8 grim 0.00124 6.40e-77 -243. ## 9 difficulty 0.00166 1.14e-76 -243. ## 10 indifference 0.00103 1.69e-76 -242. “Gamma”: From the documentation: Each of these values is an estimated proportion of words from that document that are generated from that topic. For example, the model estimates that about 41.7% of the words in document 6 were generated from topic 1. 58.3% of the words in document 6 were generated by topic 2. documents &lt;- tidy(lda, matrix = &quot;gamma&quot;) documents %&gt;% arrange(as.numeric(document)) ## # A tibble: 64 x 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1.00 ## 2 1 2 0.0000677 ## 3 2 1 1.00 ## 4 2 2 0.000114 ## 5 3 1 1.00 ## 6 3 2 0.0000601 ## 7 4 1 1.00 ## 8 4 2 0.0000469 ## 9 5 1 1.00 ## 10 5 2 0.0000702 ## # ... with 54 more rows documents %&gt;% filter(document==6) ## # A tibble: 2 x 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 6 1 0.417 ## 2 6 2 0.583 "]
]
